# Part 2: è§‚å¯Ÿæ€§ç ”ç©¶æ–¹æ³• Cheatsheet

> é¢è¯•é€ŸæŸ¥æ‰‹å†Œï¼šæ ¸å¿ƒå…¬å¼ + 2åˆ†é’Ÿä»£ç å®ç° + é«˜é¢‘é¢è¯•é¢˜

---

## ğŸ“ 2åˆ†é’Ÿä»£ç å®ç°é¢˜

### 1. å€¾å‘å¾—åˆ†ä¼°è®¡ (Propensity Score)

```python
from sklearn.linear_model import LogisticRegression

def estimate_propensity_score(X: np.ndarray, T: np.ndarray) -> np.ndarray:
    """
    ä¼°è®¡å€¾å‘å¾—åˆ† e(X) = P(T=1|X)
    """
    model = LogisticRegression(max_iter=1000)
    model.fit(X, T)
    propensity = model.predict_proba(X)[:, 1]
    return propensity
```

### 2. å€¾å‘å¾—åˆ†åŒ¹é… (PSM)

```python
from sklearn.neighbors import NearestNeighbors

def psm_matching(propensity: np.ndarray, treatment: np.ndarray,
                 n_neighbors: int = 1, caliper: float = None):
    """
    æ‰§è¡Œå€¾å‘å¾—åˆ†åŒ¹é…
    """
    treated_idx = np.where(treatment == 1)[0]
    control_idx = np.where(treatment == 0)[0]

    # ä½¿ç”¨ KNN æ‰¾æœ€è¿‘é‚»
    knn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')
    knn.fit(propensity[control_idx].reshape(-1, 1))
    distances, indices = knn.kneighbors(propensity[treated_idx].reshape(-1, 1))

    # åº”ç”¨å¡å°º
    matched_treated = []
    matched_control = []

    for i, (dist, idx) in enumerate(zip(distances, indices)):
        if caliper is None or dist[0] <= caliper:
            matched_treated.append(treated_idx[i])
            matched_control.append(control_idx[idx[0]])

    return np.array(matched_treated), np.array(matched_control)
```

### 3. æ ‡å‡†åŒ–å‡å€¼å·® (SMD) è®¡ç®—

```python
def compute_smd(X_treated: np.ndarray, X_control: np.ndarray) -> np.ndarray:
    """
    è®¡ç®—æ ‡å‡†åŒ–å‡å€¼å·®
    SMD = (mean_treated - mean_control) / pooled_std
    åˆ¤æ–­æ ‡å‡†: |SMD| < 0.1 è¡¨ç¤ºå¹³è¡¡è‰¯å¥½
    """
    mean_treated = X_treated.mean(axis=0)
    mean_control = X_control.mean(axis=0)
    mean_diff = mean_treated - mean_control

    var_treated = X_treated.var(axis=0)
    var_control = X_control.var(axis=0)
    pooled_std = np.sqrt((var_treated + var_control) / 2)

    smd = mean_diff / pooled_std
    return smd
```

### 4. IPW æƒé‡è®¡ç®—

```python
def compute_ipw_weights(propensity: np.ndarray, treatment: np.ndarray) -> np.ndarray:
    """
    è®¡ç®— IPW æƒé‡
    å¤„ç†ç»„: w = 1/e(X)
    æ§åˆ¶ç»„: w = 1/(1-e(X))
    """
    # è£å‰ªå€¾å‘å¾—åˆ†é¿å…æç«¯æƒé‡
    propensity_clipped = np.clip(propensity, 0.01, 0.99)

    # è®¡ç®—æƒé‡
    weights = (treatment / propensity_clipped +
               (1 - treatment) / (1 - propensity_clipped))

    return weights
```

### 5. IPW ä¼°è®¡ ATE (Hajek ä¼°è®¡å™¨)

```python
def estimate_ate_ipw(Y: np.ndarray, treatment: np.ndarray,
                     weights: np.ndarray) -> Tuple[float, float]:
    """
    ä½¿ç”¨ IPW ä¼°è®¡ ATE
    """
    treated_mask = treatment == 1
    control_mask = treatment == 0

    # Hajek ä¼°è®¡å™¨ï¼ˆå½’ä¸€åŒ–æƒé‡ï¼‰
    y1_weighted = (Y[treated_mask] * weights[treated_mask]).sum() / weights[treated_mask].sum()
    y0_weighted = (Y[control_mask] * weights[control_mask]).sum() / weights[control_mask].sum()

    ate = y1_weighted - y0_weighted

    # æ ‡å‡†è¯¯ï¼ˆå½±å“å‡½æ•°æ–¹æ³•ï¼‰
    n = len(Y)
    influence_1 = np.zeros(n)
    influence_1[treated_mask] = (Y[treated_mask] - y1_weighted) * weights[treated_mask]

    influence_0 = np.zeros(n)
    influence_0[control_mask] = (Y[control_mask] - y0_weighted) * weights[control_mask]

    influence = influence_1 - influence_0
    se = np.sqrt(np.var(influence) / n)

    return ate, se
```

### 6. ç¨³å®šæƒé‡è®¡ç®—

```python
def compute_stabilized_weights(propensity: np.ndarray,
                               treatment: np.ndarray) -> np.ndarray:
    """
    è®¡ç®—ç¨³å®šæƒé‡
    w_stab = P(T) / e(X) for treated
    w_stab = (1-P(T)) / (1-e(X)) for control
    """
    marginal_prob = treatment.mean()
    propensity_clipped = np.clip(propensity, 0.01, 0.99)

    weights_stab = np.zeros(len(treatment))
    weights_stab[treatment == 1] = marginal_prob / propensity_clipped[treatment == 1]
    weights_stab[treatment == 0] = (1 - marginal_prob) / (1 - propensity_clipped[treatment == 0])

    return weights_stab
```

### 7. æœ‰æ•ˆæ ·æœ¬é‡ (ESS) è®¡ç®—

```python
def compute_effective_sample_size(weights: np.ndarray) -> float:
    """
    è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡
    ESS = (sum(w))^2 / sum(w^2)
    """
    ess = (weights.sum()) ** 2 / (weights ** 2).sum()
    return ess
```

### 8. AIPW ä¼°è®¡å™¨å®ç°

```python
def estimate_ate_aipw(X: np.ndarray, T: np.ndarray, Y: np.ndarray,
                      propensity: np.ndarray, mu_1: np.ndarray,
                      mu_0: np.ndarray) -> Tuple[float, float]:
    """
    ä½¿ç”¨ AIPW ä¼°è®¡ ATE

    AIPW = E[(mu_1 - mu_0) + T*(Y - mu_1)/e - (1-T)*(Y - mu_0)/(1-e)]
    """
    propensity_clipped = np.clip(propensity, 0.01, 0.99)

    # ç¬¬ä¸€é¡¹ï¼šç»“æœæ¨¡å‹é¢„æµ‹çš„å·®å¼‚
    term1 = mu_1 - mu_0

    # ç¬¬äºŒé¡¹ï¼šå¤„ç†ç»„çš„ IPW ä¿®æ­£
    term2 = T * (Y - mu_1) / propensity_clipped

    # ç¬¬ä¸‰é¡¹ï¼šæ§åˆ¶ç»„çš„ IPW ä¿®æ­£
    term3 = (1 - T) * (Y - mu_0) / (1 - propensity_clipped)

    # AIPW å¾—åˆ†
    aipw_scores = term1 + term2 - term3

    # ATE ä¼°è®¡å’Œæ ‡å‡†è¯¯
    ate = aipw_scores.mean()
    se = aipw_scores.std() / np.sqrt(len(Y))

    return ate, se
```

### 9. ç»“æœæ¨¡å‹ä¼°è®¡

```python
from sklearn.linear_model import Ridge

def estimate_outcome_models(X: np.ndarray, T: np.ndarray,
                           Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä¼°è®¡ç»“æœæ¨¡å‹
    mu_1(X) = E[Y|X, T=1]
    mu_0(X) = E[Y|X, T=0]
    """
    treated_mask = T == 1
    control_mask = T == 0

    # è®­ç»ƒå¤„ç†ç»„çš„ç»“æœæ¨¡å‹
    model_1 = Ridge(alpha=1.0)
    model_1.fit(X[treated_mask], Y[treated_mask])
    mu_1 = model_1.predict(X)

    # è®­ç»ƒæ§åˆ¶ç»„çš„ç»“æœæ¨¡å‹
    model_0 = Ridge(alpha=1.0)
    model_0.fit(X[control_mask], Y[control_mask])
    mu_0 = model_0.predict(X)

    return mu_1, mu_0
```

### 10. Double ML (Cross-fitting)

```python
from sklearn.model_selection import KFold

def double_ml_plr(X: np.ndarray, T: np.ndarray, Y: np.ndarray,
                  n_folds: int = 5) -> Dict:
    """
    Double Machine Learning for Partially Linear Model
    """
    n = len(Y)
    Y_residuals = np.zeros(n)
    T_residuals = np.zeros(n)

    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        T_train, T_test = T[train_idx], T[test_idx]
        Y_train, Y_test = Y[train_idx], Y[test_idx]

        # è®­ç»ƒç»“æœæ¨¡å‹ g(X)
        g_model = Ridge(alpha=1.0)
        g_model.fit(X_train, Y_train)
        g_pred = g_model.predict(X_test)

        # è®­ç»ƒå€¾å‘å¾—åˆ†æ¨¡å‹ m(X)
        m_model = LogisticRegression(max_iter=1000, C=0.1)
        m_model.fit(X_train, T_train)
        m_pred = m_model.predict_proba(X_test)[:, 1]

        # è®¡ç®—æ®‹å·®
        Y_residuals[test_idx] = Y_test - g_pred
        T_residuals[test_idx] = T_test - m_pred

    # ç”¨æ®‹å·®ä¼°è®¡ tau
    tau_hat = (Y_residuals * T_residuals).sum() / (T_residuals ** 2).sum()

    # æ ‡å‡†è¯¯
    psi = (Y_residuals - tau_hat * T_residuals) * T_residuals
    J = (T_residuals ** 2).mean()
    var_tau = (psi ** 2).mean() / (n * J ** 2)
    se = np.sqrt(var_tau)

    return {'tau': tau_hat, 'se': se}
```

### 11. E-value è®¡ç®—

```python
def compute_e_value(observed_rr: float, ci_lower: float = None) -> Dict:
    """
    è®¡ç®— E-value (æ•æ„Ÿæ€§åˆ†æ)
    E = RR + sqrt(RR * (RR - 1))
    """
    # ç¡®ä¿ RR >= 1
    if observed_rr < 1:
        observed_rr = 1 / observed_rr

    # è®¡ç®— E-value
    e_value = observed_rr + np.sqrt(observed_rr * (observed_rr - 1))

    result = {'e_value': e_value}

    # ç½®ä¿¡åŒºé—´ä¸‹ç•Œçš„ E-value
    if ci_lower is not None:
        if ci_lower < 1:
            ci_lower = 1 / ci_lower
        e_value_ci = ci_lower + np.sqrt(ci_lower * (ci_lower - 1))
        result['e_value_ci'] = e_value_ci

    return result
```

---

## ğŸ¯ æ ¸å¿ƒå…¬å¼é€ŸæŸ¥

### å€¾å‘å¾—åˆ†æ–¹æ³•

**å€¾å‘å¾—åˆ†å®šä¹‰**
$$e(X) = P(T=1 | X)$$

**Rosenbaum & Rubin å®šç†**
$$(Y(0), Y(1)) \perp T | X \Rightarrow (Y(0), Y(1)) \perp T | e(X)$$

**PSM ä¼°è®¡ ATT**
$$\widehat{ATT} = \frac{1}{N_T} \sum_{i: T_i=1} \left( Y_i - \frac{1}{M_i} \sum_{j \in \mathcal{M}(i)} Y_j \right)$$

**æ ‡å‡†åŒ–å‡å€¼å·® (SMD)**
$$\text{SMD} = \frac{\bar{X}_{\text{treated}} - \bar{X}_{\text{control}}}{\sqrt{(s^2_{\text{treated}} + s^2_{\text{control}})/2}}$$

åˆ¤æ–­æ ‡å‡†: $|\text{SMD}| < 0.1$ è¡¨ç¤ºå¹³è¡¡è‰¯å¥½

### IPW æ–¹æ³•

**IPW æƒé‡**
$$w_i = \frac{T_i}{e(X_i)} + \frac{1-T_i}{1-e(X_i)}$$

**Horvitz-Thompson ä¼°è®¡å™¨**
$$\hat{E}[Y(1)] = \frac{1}{n}\sum_{i=1}^{n} \frac{T_i Y_i}{e(X_i)}$$
$$\hat{E}[Y(0)] = \frac{1}{n}\sum_{i=1}^{n} \frac{(1-T_i) Y_i}{1-e(X_i)}$$
$$\widehat{ATE} = \hat{E}[Y(1)] - \hat{E}[Y(0)]$$

**ç¨³å®šæƒé‡**
$$w_i^{\text{stab}} = \frac{P(T=T_i)}{P(T=T_i|X_i)}$$

**æœ‰æ•ˆæ ·æœ¬é‡ (ESS)**
$$ESS = \frac{(\sum w_i)^2}{\sum w_i^2}$$

### åŒé‡ç¨³å¥æ–¹æ³•

**AIPW ä¼°è®¡å™¨**
$$\hat{\tau}_{AIPW} = \frac{1}{n}\sum_{i=1}^{n}\left[(\hat{\mu}_1(X_i) - \hat{\mu}_0(X_i)) + \frac{T_i(Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} - \frac{(1-T_i)(Y_i - \hat{\mu}_0(X_i))}{1-\hat{e}(X_i)}\right]$$

**åŒé‡ç¨³å¥æ€§è´¨**
- å¦‚æœ $\hat{e}(X)$ æ­£ç¡® OR $\hat{\mu}(X)$ æ­£ç¡® â†’ ä¼°è®¡ä¸€è‡´
- ä¸¤ä¸ªéƒ½æ­£ç¡® â†’ æ•ˆç‡æœ€ä¼˜
- ä¸¤ä¸ªéƒ½é”™ â†’ ä¸€èˆ¬æœ‰å

### Double ML

**éƒ¨åˆ†çº¿æ€§æ¨¡å‹**
$$Y = \tau \cdot T + g(X) + \epsilon$$
$$T = m(X) + \eta$$

**DML ä¼°è®¡å™¨**
$$\hat{\tau}_{DML} = \frac{\sum_{i=1}^{n}(Y_i - \hat{g}_{-k(i)}(X_i))(T_i - \hat{m}_{-k(i)}(X_i))}{\sum_{i=1}^{n}(T_i - \hat{m}_{-k(i)}(X_i))^2}$$

å…¶ä¸­ $\hat{g}_{-k(i)}$ è¡¨ç¤ºåœ¨ä¸åŒ…å«ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æŠ˜ä¸Šè®­ç»ƒçš„æ¨¡å‹

**Neyman æ­£äº¤æ€§**
$$\frac{\partial}{\partial \eta} E[\psi(W; \tau_0, \eta)] \Big|_{\eta=\eta_0} = 0$$

### æ•æ„Ÿæ€§åˆ†æ

**Rosenbaum æ•æ„Ÿæ€§å‚æ•° Î“**
$$\frac{1}{\Gamma} \leq \frac{P(T_i=1|X)}{P(T_j=1|X)} \leq \Gamma$$

**E-value å…¬å¼**
$$E = RR + \sqrt{RR \times (RR - 1)}$$

E-value è§£è¯»:
- E < 1.5: ç»“è®ºéå¸¸è„†å¼±
- 1.5 â‰¤ E < 2.5: ç»“è®ºä¸­ç­‰ç¨³å¥
- 2.5 â‰¤ E < 4.0: ç»“è®ºè¾ƒä¸ºç¨³å¥
- E â‰¥ 4.0: ç»“è®ºéå¸¸ç¨³å¥

---

## ğŸ’¼ é«˜é¢‘é¢è¯•é¢˜

### Q1: ä»€ä¹ˆæ˜¯å€¾å‘å¾—åˆ†ï¼Ÿä¸ºä»€ä¹ˆå¯ä»¥ç”¨å®ƒæ§åˆ¶æ··æ·†ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:
- å€¾å‘å¾—åˆ†æ˜¯ç»™å®šåå˜é‡ Xï¼Œä¸ªä½“æ¥å—å¤„ç†çš„æ¦‚ç‡ $e(X) = P(T=1|X)$
- **Rosenbaum & Rubin å®šç†**: å¦‚æœæ»¡è¶³æ— æ··æ·†å‡è®¾ $(Y(0), Y(1)) \perp T | X$ï¼Œé‚£ä¹ˆ $(Y(0), Y(1)) \perp T | e(X)$
- **ç»´åº¦ç¼©å‡**: æŠŠé«˜ç»´çš„ X å‹ç¼©æˆä¸€ç»´çš„ e(X)
- **ç›´è§‚ç†è§£**: å€¾å‘å¾—åˆ†ç›¸åŒçš„ä¸ªä½“ï¼Œåœ¨"æ¥å—å¤„ç†çš„å€¾å‘"ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œå¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„åå˜é‡åˆ†å¸ƒç›¸åŒ

### Q2: PSM çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:
1. **åªèƒ½æ§åˆ¶è§‚æµ‹åˆ°çš„æ··æ·†å˜é‡**: å¦‚æœå­˜åœ¨æœªè§‚æµ‹çš„æ··æ·†ï¼ŒPSM æ— èƒ½ä¸ºåŠ›
2. **ä¾èµ–å€¾å‘å¾—åˆ†æ¨¡å‹çš„æ­£ç¡®æ€§**: å¦‚æœæ¨¡å‹è¯¯è®¾å®šï¼Œå€¾å‘å¾—åˆ†ä¼°è®¡æœ‰å
3. **ä¸¢å¼ƒæœªåŒ¹é…æ ·æœ¬**: åŒ¹é…ç‡å¯èƒ½å¾ˆä½ï¼ŒæŸå¤±æ ·æœ¬é‡ï¼›ä¼°è®¡çš„æ˜¯ ATT ä¸æ˜¯ ATE
4. **å…±åŒæ”¯æ’‘å‡è®¾**: å¦‚æœæŸäº›å¤„ç†ç»„ä¸ªä½“çš„å€¾å‘å¾—åˆ†åœ¨æ§åˆ¶ç»„ä¸­æ‰¾ä¸åˆ°å¯¹åº”å€¼ï¼Œæ— æ³•åŒ¹é…
5. **æ ‡å‡†è¯¯è®¡ç®—å¤æ‚**: éœ€è¦è€ƒè™‘å€¾å‘å¾—åˆ†ä¼°è®¡çš„ä¸ç¡®å®šæ€§

### Q3: å¦‚ä½•è¯Šæ–­ PSM çš„åŒ¹é…è´¨é‡ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**1. æ ‡å‡†åŒ–å‡å€¼å·® (SMD)**
- å…¬å¼: $SMD = \frac{\bar{X}_{treated} - \bar{X}_{control}}{\sqrt{(s^2_{treated} + s^2_{control})/2}}$
- é˜ˆå€¼: |SMD| < 0.1 è¡¨ç¤ºè‰¯å¥½å¹³è¡¡
- éœ€è¦æ¯”è¾ƒåŒ¹é…å‰åçš„ SMD

**2. å…±åŒæ”¯æ’‘æ£€æŸ¥**
- å¯è§†åŒ–å€¾å‘å¾—åˆ†çš„åˆ†å¸ƒå›¾
- æ£€æŸ¥å¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„å€¾å‘å¾—åˆ†é‡å åŒºåŸŸ
- è®¡ç®—åœ¨é‡å åŒºåŸŸå¤–çš„æ ·æœ¬æ¯”ä¾‹

**3. æ–¹å·®æ¯”**
- æ£€æŸ¥åŒ¹é…åå¤„ç†ç»„å’Œæ§åˆ¶ç»„å„åå˜é‡çš„æ–¹å·®æ¯”
- ç†æƒ³å€¼åº”æ¥è¿‘ 1

### Q4: PSM ä¼°è®¡çš„æ˜¯ ATE è¿˜æ˜¯ ATTï¼Ÿèƒ½ä¼°è®¡ ATE å—ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**PSM é»˜è®¤ä¼°è®¡ ATT**

**åŸå› **:
- æˆ‘ä»¬æ˜¯ä¸ºå¤„ç†ç»„çš„æ¯ä¸ªä¸ªä½“æ‰¾æ§åˆ¶ç»„çš„åŒ¹é…
- æœªè¢«åŒ¹é…çš„æ§åˆ¶ç»„æ ·æœ¬è¢«ä¸¢å¼ƒ
- æœ€ç»ˆæ ·æœ¬ä»£è¡¨çš„æ˜¯"æ¥å—å¤„ç†çš„é‚£ç¾¤äºº"

**ä¼°è®¡ ATE çš„æ–¹æ³•**:
1. **åŒå‘åŒ¹é…**: ä¸ºå¤„ç†ç»„æ‰¾æ§åˆ¶ç»„åŒ¹é… â†’ ä¼°è®¡ ATTï¼›ä¸ºæ§åˆ¶ç»„æ‰¾å¤„ç†ç»„åŒ¹é… â†’ ä¼°è®¡ ATCï¼›åŠ æƒå¹³å‡: $ATE = P(T=1) \cdot ATT + P(T=0) \cdot ATC$
2. **ä½¿ç”¨ IPW**: IPW å¤©ç„¶ä¼°è®¡ ATE
3. **ä½¿ç”¨ AIPW**: ç»“åˆä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿

### Q5: IPW çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆé‡åŠ æƒå¯ä»¥å»é™¤æ··æ·†åå·®ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

IPW çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**é‡æ–°åŠ æƒ**ï¼Œåˆ›é€ ä¸€ä¸ª"ä¼ªæ€»ä½“"ï¼Œåœ¨è¿™ä¸ªä¼ªæ€»ä½“ä¸­å¤„ç†æ˜¯éšæœºåˆ†é…çš„ã€‚

**ç›´è§‚è§£é‡Š**:
- åœ¨è§‚æµ‹æ•°æ®ä¸­ï¼ŒæŸäº›ç±»å‹çš„äººæ›´å¯èƒ½æ¥å—å¤„ç†
- è¿™å¯¼è‡´å¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„äººç¾¤ä¸å¯æ¯”
- IPW ç»™æ¯ä¸ªäººèµ‹äºˆæƒé‡ï¼š$w_i = \frac{T_i}{e(X_i)} + \frac{1-T_i}{1-e(X_i)}$
- è¿™ä¸ªæƒé‡è®©"ä¸å¤ªå¯èƒ½æ¥å—å¤„ç†ä½†æ¥å—äº†"çš„äººè´¡çŒ®æ›´å¤§
- åŠ æƒåçš„æ•°æ®å°±åƒéšæœºå®éªŒä¸€æ ·ï¼ŒX ä¸ T ç‹¬ç«‹äº†

**æ•°å­¦æœ¬è´¨**: IPW æ˜¯ Horvitz-Thompson ä¼°è®¡é‡ï¼Œé€šè¿‡é€†æ¦‚ç‡åŠ æƒæ¥çº æ­£é€‰æ‹©åå·®

### Q6: IPW çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**ä¸»è¦å±€é™æ€§**:
1. **æç«¯æƒé‡é—®é¢˜**: å½“ $e(X) \approx 0$ æˆ– $e(X) \approx 1$ æ—¶ï¼Œæƒé‡ä¼šéå¸¸å¤§ï¼Œå¯¼è‡´ä¼°è®¡ä¸ç¨³å®š
2. **ä¾èµ–å€¾å‘å¾—åˆ†æ¨¡å‹**: å¦‚æœå€¾å‘å¾—åˆ†æ¨¡å‹è¯¯è®¾å®šï¼ŒIPW ä¼°è®¡æœ‰å
3. **æ•ˆç‡æŸå¤±**: ç›¸æ¯”ç»“æœæ¨¡å‹æ–¹æ³•ï¼Œæ–¹å·®å¯èƒ½æ›´å¤§

**è§£å†³æ–¹æ³•**:
1. **æƒé‡è£å‰ª**: `weights_clipped = np.clip(weights, None, np.percentile(weights, 99))`
2. **ç¨³å®šæƒé‡**: $w_i^{stab} = \frac{P(T=T_i)}{P(T=T_i|X_i)}$ï¼Œå‡å€¼æ¥è¿‘ 1ï¼Œæ–¹å·®æ›´å°
3. **ä¿®å‰ªå€¾å‘å¾—åˆ†**: ä¸¢å¼ƒå€¾å‘å¾—åˆ†è¿‡äºæç«¯çš„æ ·æœ¬ï¼ˆå¦‚ <0.1 æˆ– >0.9ï¼‰
4. **ä½¿ç”¨åŒé‡ç¨³å¥æ–¹æ³• (AIPW)**: ç»“åˆ IPW å’Œç»“æœæ¨¡å‹ï¼Œæ›´ç¨³å¥

### Q7: ä»€ä¹ˆæ˜¯æœ‰æ•ˆæ ·æœ¬é‡(ESS)ï¼Ÿå®ƒçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**å®šä¹‰**: $ESS = \frac{(\sum w_i)^2}{\sum w_i^2}$

**æ„ä¹‰**:
- ESS è¡¡é‡"æœ‰å¤šå°‘æ ·æœ¬åœ¨çœŸæ­£èµ·ä½œç”¨"
- å½“æ‰€æœ‰æƒé‡ç›¸ç­‰æ—¶ï¼ŒESS = nï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½æœ‰æ•ˆï¼‰
- å½“æƒé‡å·®å¼‚å¾ˆå¤§æ—¶ï¼ŒESS << nï¼ˆå°‘æ•°æ ·æœ¬ä¸»å¯¼ï¼‰

**ä¾‹å­**: å¦‚æœ n=1000ï¼Œä½† ESS=100ï¼Œè¯´æ˜å®é™…ä¸Šåªæœ‰ 100 ä¸ªæ ·æœ¬çš„ä¿¡æ¯é‡

**ç»éªŒæ³•åˆ™**:
- ESS / n > 0.5: è‰¯å¥½
- ESS / n < 0.3: è­¦å‘Šï¼Œå¯èƒ½éœ€è¦ä¿®å‰ªæç«¯æƒé‡

### Q8: ä»€ä¹ˆæ˜¯åŒé‡ç¨³å¥æ€§ï¼Ÿä¸ºä»€ä¹ˆ AIPW å…·æœ‰è¿™ä¸ªæ€§è´¨ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

åŒé‡ç¨³å¥æ€§(Double Robustness)æ˜¯æŒ‡ï¼š**åªè¦å€¾å‘å¾—åˆ†æ¨¡å‹æˆ–ç»“æœæ¨¡å‹ä¹‹ä¸€æ­£ç¡®ï¼Œä¼°è®¡é‡å°±æ˜¯ä¸€è‡´çš„**

**AIPW çš„å…¬å¼**:
$$\hat{\tau} = \frac{1}{n}\sum_i \left[(\hat{\mu}_1(X_i) - \hat{\mu}_0(X_i)) + \frac{T_i(Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} - \frac{(1-T_i)(Y_i - \hat{\mu}_0(X_i))}{1-\hat{e}(X_i)}\right]$$

**ä¸ºä»€ä¹ˆå…·æœ‰åŒé‡ç¨³å¥æ€§**:
1. **å¦‚æœå€¾å‘å¾—åˆ†æ­£ç¡®**: IPW ä¿®æ­£é¡¹ä¼šå®Œç¾æŠµæ¶ˆç»“æœæ¨¡å‹çš„è¯¯å·®
2. **å¦‚æœç»“æœæ¨¡å‹æ­£ç¡®**: æ®‹å·® $Y - \hat{\mu}(X)$ çš„æœŸæœ›ä¸º 0ï¼ŒIPW ä¿®æ­£é¡¹ä¸å¼•å…¥åå·®

**ç›´è§‚ç†è§£**: AIPW å°±åƒä¹°äº†ä¸¤ä»½ä¿é™©ï¼Œä»»ä½•ä¸€ä»½æœ‰æ•ˆå°±èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆ

### Q9: AIPW ä¼°è®¡å™¨çš„ä¸‰é¡¹åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆå«ä¹‰ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

| é¡¹ | å…¬å¼ | ç»Ÿè®¡å«ä¹‰ | å› æœå«ä¹‰ |
|---|------|----------|----------|
| ç¬¬ä¸€é¡¹ | $\hat{\mu}_1(X) - \hat{\mu}_0(X)$ | ä¸¤ä¸ªå›å½’æ¨¡å‹é¢„æµ‹çš„å·®å¼‚ | åŸºäºåå˜é‡é¢„æµ‹çš„ä¸ªä½“æ•ˆåº” |
| ç¬¬äºŒé¡¹ | $\frac{T(Y - \hat{\mu}_1(X))}{e(X)}$ | å¤„ç†ç»„çš„åŠ æƒæ®‹å·® | ä¿®æ­£å¤„ç†ç»„é¢„æµ‹è¯¯å·® |
| ç¬¬ä¸‰é¡¹ | $-\frac{(1-T)(Y - \hat{\mu}_0(X))}{1-e(X)}$ | æ§åˆ¶ç»„çš„åŠ æƒæ®‹å·® | ä¿®æ­£æ§åˆ¶ç»„é¢„æµ‹è¯¯å·® |

**å·¥ä½œåŸç†**:
1. ç¬¬ä¸€é¡¹ç»™å‡º"åˆæ­¥ä¼°è®¡"ï¼ˆåŸºäºç»“æœæ¨¡å‹ï¼‰
2. ç¬¬äºŒã€ä¸‰é¡¹å¯¹"è§‚æµ‹åˆ°çš„æ ·æœ¬"çš„é¢„æµ‹è¯¯å·®è¿›è¡ŒåŠ æƒä¿®æ­£
3. æƒé‡ $\frac{1}{e(X)}$ ç¡®ä¿ä¿®æ­£æ˜¯æ— åçš„

### Q10: DML å’Œæ™®é€š AIPW/DR æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦ Cross-fittingï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**æ ¸å¿ƒåŒºåˆ«**: Cross-fitting

**é—®é¢˜æ¥æº**: å½“æˆ‘ä»¬ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå¦‚æœç”¨åŒä¸€ä»½æ•°æ®è®­ç»ƒæ¨¡å‹å’Œé¢„æµ‹ï¼Œæ®‹å·® $Y_i - \hat{g}(X_i)$ ä¼šè¢«ä½ä¼°ï¼ˆoverfitting biasï¼‰ï¼Œå¯¼è‡´æ ‡å‡†è¯¯å¤±æ•ˆã€ç½®ä¿¡åŒºé—´è¿‡çª„ã€‚

**Cross-fitting çš„è§£å†³æ–¹æ¡ˆ**:
1. æŠŠæ•°æ®åˆ†æˆ K æŠ˜
2. å¯¹æ¯ä¸€æŠ˜ï¼Œç”¨å…¶ä»– K-1 æŠ˜è®­ç»ƒæ¨¡å‹
3. ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹å½“å‰æŠ˜

è¿™æ ·æ¯ä¸ª $Y_i$ çš„é¢„æµ‹éƒ½æ¥è‡ªã€Œæ²¡è§è¿‡å®ƒã€çš„æ¨¡å‹ï¼Œæ¶ˆé™¤è¿‡æ‹Ÿåˆåå·®ã€‚

### Q11: ä»€ä¹ˆæ˜¯ Neyman æ­£äº¤æ€§ï¼Ÿä¸ºä»€ä¹ˆå®ƒé‡è¦ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**å®šä¹‰**: Neyman æ­£äº¤æ€§æ˜¯æŒ‡çŸ©å‡½æ•°å¯¹ nuisance å‚æ•°çš„å¯¼æ•°ä¸º 0:
$$\frac{\partial}{\partial \eta} E[\psi(W; \tau_0, \eta)] \Big|_{\eta=\eta_0} = 0$$

**é‡è¦æ€§**:
- åœ¨ç»å…¸æ–¹æ³•ä¸­ï¼Œnuisance å‚æ•°ä¼°è®¡è¯¯å·®å¯¹ç›®æ ‡å‚æ•°ä¼°è®¡çš„å½±å“æ˜¯**ä¸€é˜¶çš„** $O(||\hat{\eta} - \eta||)$
- åœ¨ Neyman æ­£äº¤çš„æ–¹æ³•ä¸­ï¼Œå½±å“æ˜¯**äºŒé˜¶çš„** $O(||\hat{\eta} - \eta||^2)$

**å®é™…æ„ä¹‰**:
- å…è®¸ä½¿ç”¨æ­£åˆ™åŒ– ML æ¨¡å‹ï¼ˆLasso, Ridge, RFï¼‰
- å³ä½¿æ¨¡å‹æœ‰åå·®ï¼Œå¯¹å› æœæ•ˆåº”ä¼°è®¡å½±å“å¾ˆå°
- å¯ä»¥è¾¾åˆ° $\sqrt{n}$-ä¸€è‡´æ€§å’Œæ¸è¿‘æ­£æ€æ€§

### Q12: E-value æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•è§£è¯»ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹**:

**å®šä¹‰**: E-value æ˜¯ä½¿è§‚æµ‹å…³è”å®Œå…¨è¢«æ··æ·†è§£é‡Šæ‰€éœ€çš„æœ€å°é£é™©æ¯”
$$E = RR + \sqrt{RR \times (RR - 1)}$$

**è§£è¯»**: E-value = 3.0 æ„å‘³ç€éœ€è¦ä¸€ä¸ªæœªè§‚æµ‹å› å­ U:
- U ä½¿ã€Œæ¥å—å¤„ç†çš„æ¦‚ç‡ã€æé«˜ 3 å€
- U åŒæ—¶ä½¿ã€Œç»“æœå‘ç”Ÿæ¦‚ç‡ã€æé«˜ 3 å€
- æ‰èƒ½å®Œå…¨è§£é‡Šæ‰è§‚æµ‹åˆ°çš„æ•ˆåº”

**ç¨³å¥æ€§è¯„ä»·**:
- E < 1.5: ç»“è®ºéå¸¸è„†å¼±
- 1.5 â‰¤ E < 2.5: ç»“è®ºä¸­ç­‰ç¨³å¥
- 2.5 â‰¤ E < 4.0: ç»“è®ºè¾ƒä¸ºç¨³å¥
- E â‰¥ 4.0: ç»“è®ºéå¸¸ç¨³å¥

**é‡è¦**: E-value é«˜ä¸ä»£è¡¨æ— æ··æ·†ï¼Œåªè¡¨ç¤ºéœ€è¦å¤šå¼ºçš„æ··æ·†æ‰èƒ½æ¨ç¿»ç»“è®º

---

## ğŸ“Š æ–¹æ³•é€‰æ‹©æŒ‡å—

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **PSM** | ç›´è§‚ã€å¯æ£€æŸ¥å¹³è¡¡ | ä¸¢å¼ƒæ ·æœ¬ã€ä¼°è®¡ ATT | å°æ•°æ®ã€éœ€è¦å¯è§£é‡Šæ€§ |
| **IPW** | ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ã€ä¼°è®¡ ATE | æç«¯æƒé‡é—®é¢˜ | ä¸­ç­‰æ··æ·†ã€å€¾å‘å¾—åˆ†æ¨¡å‹å¯é  |
| **AIPW** | åŒé‡ç¨³å¥ã€æ•ˆç‡é«˜ | è®¡ç®—å¤æ‚ã€æ—  Cross-fitting æœ‰å | **ä¸­ç­‰ç»´åº¦æ¨è** |
| **DML** | é«˜ç»´å¯ç”¨ã€æœ‰æ•ˆæ¨æ–­ | è®¡ç®—é‡å¤§ã€éœ€è¦å¤§æ ·æœ¬ | **é«˜ç»´æ•°æ®æ¨è** |

---

## ğŸ“ ç†è®ºè¦ç‚¹

### æ— æ··æ·†å‡è®¾ (Unconfoundedness)

$$(Y(0), Y(1)) \perp T | X$$

ç»™å®šåå˜é‡ Xï¼Œæ½œåœ¨ç»“æœä¸å¤„ç†åˆ†é…ç‹¬ç«‹

### æ­£å€¼å‡è®¾ (Positivity)

$$0 < P(T=1|X) < 1$$

æ¯ä¸ªåå˜é‡å€¼ä¸‹éƒ½æœ‰ä¸€å®šæ¦‚ç‡æ¥å—æˆ–ä¸æ¥å—å¤„ç†

### å…±åŒæ”¯æ’‘ (Common Support)

å¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„å€¾å‘å¾—åˆ†åˆ†å¸ƒæœ‰é‡å åŒºåŸŸï¼Œæ‰èƒ½è¿›è¡Œæ¯”è¾ƒ

### SUTVA (Stable Unit Treatment Value Assumption)

1. **ä¸€è‡´æ€§**: $Y = T \cdot Y(1) + (1-T) \cdot Y(0)$
2. **æ— å¹²æ‰°**: ä¸€ä¸ªä¸ªä½“çš„æ½œåœ¨ç»“æœä¸å—å…¶ä»–ä¸ªä½“å¤„ç†çŠ¶æ€çš„å½±å“

---

## ğŸ’¡ å®è·µå»ºè®®

1. **å…ˆå¯è§†åŒ–**: åŒ¹é…/åŠ æƒå‰åçš„åå˜é‡åˆ†å¸ƒå¯¹æ¯”
2. **å¤šç§æ–¹æ³•**: å°è¯• PSMã€IPWã€AIPWï¼Œæ¯”è¾ƒç¨³å¥æ€§
3. **æ•æ„Ÿæ€§åˆ†æ**: ä½¿ç”¨ E-value æˆ– Rosenbaum bounds è¯„ä¼°ç¨³å¥æ€§
4. **ä¿ç•™è¯Šæ–­**: æŠ¥å‘ŠåŒ¹é…å‰åçš„å¹³è¡¡æ€§ç»Ÿè®¡ï¼ˆSMDã€æ–¹å·®æ¯”ï¼‰
5. **é€æ˜æŠ¥å‘Š**: è¯´æ˜æ ·æœ¬æŸå¤±ã€åŒ¹é…å‚æ•°é€‰æ‹©ã€æ¨¡å‹è®¾å®š

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- Rosenbaum & Rubin (1983): "The Central Role of the Propensity Score"
- Stuart (2010): "Matching Methods for Causal Inference: A Review"
- Chernozhukov et al. (2018): "Double/Debiased Machine Learning"
- VanderWeele & Ding (2017): "Sensitivity Analysis in Observational Research"

---

**ã€Œå› æœæ¨æ–­ä¸æ˜¯é­”æ³•ï¼Œè€Œæ˜¯åœ¨å‡è®¾ä¸‹çš„ä¸¥è°¨æ¨ç†ã€‚ã€**
