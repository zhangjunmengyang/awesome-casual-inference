{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ² Chapter 5 ç»ƒä¹  2: å› æœæ£®æ— - ä¸“ä¸ºå› æœæ¨æ–­è®¾è®¡çš„ã€Œæ£®æ—ã€\n",
    "\n",
    "## ä»éšæœºæ£®æ—åˆ°å› æœæ£®æ—\n",
    "\n",
    "ä¸Šä¸€èŠ‚æˆ‘ä»¬ç”¨ T-Learner ä¼°è®¡ CATEï¼Œä½†å®ƒæœ‰ä¸€ä¸ªé—®é¢˜ï¼š**å®ƒä¸æ˜¯ä¸ºå› æœæ¨æ–­è®¾è®¡çš„**ã€‚\n",
    "\n",
    "**å› æœæ£®æ— (Causal Forest)** æ˜¯ Athey & Imbens (2018) æå‡ºçš„æ–¹æ³•ï¼Œä¸“é—¨ä¸ºä¼°è®¡**å¼‚è´¨æ€§å¤„ç†æ•ˆåº”**è®¾è®¡ï¼Œæœ‰æ›´å¥½çš„ç»Ÿè®¡æ€§è´¨ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£å› æœæ£®æ—çš„æ ¸å¿ƒåŸç†\n",
    "2. ç†è§£ã€Œè¯šå®åˆ†è£‚ã€çš„æ¦‚å¿µ\n",
    "3. ä½¿ç”¨ econml å®ç°å› æœæ£®æ—\n",
    "4. åˆ†æç‰¹å¾é‡è¦æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ å› æœæ£®æ—çš„ç›´è§‰\n",
    "\n",
    "### éšæœºæ£®æ— vs å› æœæ£®æ—\n",
    "\n",
    "| éšæœºæ£®æ— | å› æœæ£®æ— |\n",
    "|----------|----------|\n",
    "| ç›®æ ‡: é¢„æµ‹ Y | ç›®æ ‡: ä¼°è®¡ CATE |\n",
    "| æœ€å°åŒ–é¢„æµ‹è¯¯å·® | æœ€å¤§åŒ–æ•ˆåº”å¼‚è´¨æ€§ |\n",
    "| åˆ†è£‚å‡†åˆ™: å‡å°‘æ–¹å·® | åˆ†è£‚å‡†åˆ™: æœ€å¤§åŒ–å­èŠ‚ç‚¹é—´çš„ CATE å·®å¼‚ |\n",
    "| åŒä¸€æ•°æ®ç”¨äºåˆ†è£‚å’Œé¢„æµ‹ | **è¯šå®åˆ†è£‚**: ä¸åŒæ•°æ®ç”¨äºåˆ†è£‚å’Œä¼°è®¡ |\n",
    "\n",
    "### è¯šå®åˆ†è£‚ (Honest Splitting)\n",
    "\n",
    "è¿™æ˜¯å› æœæ£®æ—çš„**æ ¸å¿ƒåˆ›æ–°**ï¼\n",
    "\n",
    "æ™®é€šå†³ç­–æ ‘ï¼š\n",
    "```\n",
    "åŒä¸€æ‰¹æ•°æ® -> ç”¨äºæ„å»ºæ ‘ç»“æ„\n",
    "           -> ä¹Ÿç”¨äºå¶èŠ‚ç‚¹é¢„æµ‹\n",
    "```\n",
    "\n",
    "å› æœæ£®æ—ï¼š\n",
    "```\n",
    "æ•°æ®åˆ†æˆä¸¤åŠ:\n",
    "  - åˆ†è£‚æ ·æœ¬ (50%): ç”¨äºæ„å»ºæ ‘ç»“æ„ï¼ˆå†³å®šå¦‚ä½•åˆ†è£‚ï¼‰\n",
    "  - ä¼°è®¡æ ·æœ¬ (50%): ç”¨äºå¶èŠ‚ç‚¹çš„ CATE ä¼°è®¡\n",
    "```\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦è¯šå®åˆ†è£‚ï¼Ÿ\n",
    "\n",
    "æƒ³è±¡ä½ åœ¨åšæŠ•èµ„å›æµ‹ï¼š\n",
    "\n",
    "- **ä¸è¯šå®**: ç”¨è¿‡å»æ•°æ®æ‰¾è§„å¾‹ï¼Œåˆç”¨åŒæ ·çš„æ•°æ®éªŒè¯ â†’ è¿‡æ‹Ÿåˆï¼\n",
    "- **è¯šå®**: ç”¨ä¸€åŠæ•°æ®æ‰¾è§„å¾‹ï¼Œç”¨å¦ä¸€åŠéªŒè¯ â†’ æ›´å¯é ï¼\n",
    "\n",
    "å› æœæ£®æ—çš„è¯šå®åˆ†è£‚ç¡®ä¿äº†**æ¸è¿‘æ­£æ€æ€§**å’Œ**æœ‰æ•ˆçš„ç½®ä¿¡åŒºé—´**ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "### å› æœæ£®æ—çš„ CATE ä¼°è®¡\n",
    "\n",
    "å¯¹äºä¸€ä¸ªæ–°æ ·æœ¬ $x$ï¼Œå› æœæ£®æ—çš„é¢„æµ‹æ˜¯ï¼š\n",
    "\n",
    "$$\\hat{\\tau}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\tau}_b(x)$$\n",
    "\n",
    "å…¶ä¸­ $B$ æ˜¯æ ‘çš„æ•°é‡ï¼Œ$\\hat{\\tau}_b(x)$ æ˜¯ç¬¬ $b$ æ£µæ ‘çš„é¢„æµ‹ã€‚\n",
    "\n",
    "### å•æ£µå› æœæ ‘çš„å¶èŠ‚ç‚¹ä¼°è®¡\n",
    "\n",
    "åœ¨å¶èŠ‚ç‚¹ $L$ ä¸­ï¼š\n",
    "\n",
    "$$\\hat{\\tau}_L = \\frac{\\sum_{i \\in L, T_i=1} Y_i}{|\\{i \\in L: T_i=1\\}|} - \\frac{\\sum_{i \\in L, T_i=0} Y_i}{|\\{i \\in L: T_i=0\\}|}$$\n",
    "\n",
    "ç®€å•è¯´ï¼š**å¶èŠ‚ç‚¹å†…çš„å¤„ç†ç»„å¹³å‡ Y å‡å»æ§åˆ¶ç»„å¹³å‡ Y**\n",
    "\n",
    "### åˆ†è£‚å‡†åˆ™\n",
    "\n",
    "å› æœæ£®æ—ä½¿ç”¨çš„åˆ†è£‚å‡†åˆ™æœ€å¤§åŒ–å­èŠ‚ç‚¹ä¹‹é—´çš„ CATE å¼‚è´¨æ€§ï¼š\n",
    "\n",
    "$$\\text{Criterion} = (\\hat{\\tau}_{\\text{left}} - \\hat{\\tau}_{\\text{right}})^2 \\cdot n_{\\text{left}} \\cdot n_{\\text{right}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ“ Causal Forest æ ¸å¿ƒæ•°å­¦æ¨å¯¼\n\n### Honest Splitting åŸç†\n\n**ä¼ ç»Ÿå†³ç­–æ ‘é—®é¢˜**:\n- ç”¨åŒä¸€æ‰¹æ•°æ® $S$ æ„å»ºæ ‘ç»“æ„\n- åˆç”¨ $S$ ä¼°è®¡å¶èŠ‚ç‚¹å€¼\n- ç»“æœ: è¿‡æ‹Ÿåˆï¼Œç½®ä¿¡åŒºé—´å¤±æ•ˆ\n\n**Honest Splitting è§£å†³æ–¹æ¡ˆ**:\n\nå°†æ ·æœ¬ $S$ éšæœºåˆ†ä¸ºä¸¤ä¸ªäº’æ–¥å­é›†:\n- $S_{split}$: åˆ†è£‚æ ·æœ¬ (æ„å»ºæ ‘ç»“æ„)\n- $S_{est}$: ä¼°è®¡æ ·æœ¬ (å¶èŠ‚ç‚¹ä¼°è®¡)\n\n$$|S_{split}| = |S_{est}| = \\frac{n}{2}$$\n\n### åˆ†è£‚å‡†åˆ™ (æœ€å¤§åŒ–å¼‚è´¨æ€§)\n\n**ç›®æ ‡**: æ‰¾åˆ°åˆ†è£‚ç‚¹ $(j, s)$ ä½¿å¾—å·¦å³å­èŠ‚ç‚¹çš„ CATE å·®å¼‚æœ€å¤§ã€‚\n\n**åˆ†è£‚å¢ç›Š**:\n$$\\Delta(\\tau) = \\frac{n_L n_R}{(n_L + n_R)^2} \\times (\\hat{\\tau}_L - \\hat{\\tau}_R)^2$$\n\nå…¶ä¸­:\n- $n_L, n_R$: å·¦å³èŠ‚ç‚¹æ ·æœ¬æ•°\n- $\\hat{\\tau}_L, \\hat{\\tau}_R$: å·¦å³èŠ‚ç‚¹çš„ CATE ä¼°è®¡\n\n**å½’ä¸€åŒ–å› å­** $\\frac{n_L n_R}{(n_L + n_R)^2}$:\n- å¹³è¡¡å·¦å³èŠ‚ç‚¹å¤§å°\n- å½“ $n_L = n_R = \\frac{n}{2}$ æ—¶æœ€å¤§\n- é¿å…æç«¯ä¸å¹³è¡¡åˆ†è£‚\n\n### å¶èŠ‚ç‚¹ CATE ä¼°è®¡\n\nå¯¹äºå¶èŠ‚ç‚¹ $L$ (åªä½¿ç”¨ $S_{est}$ ä¸­è½å…¥è¯¥å¶çš„æ ·æœ¬):\n\n$$\\hat{\\tau}_L = \\bar{Y}_{1,L} - \\bar{Y}_{0,L}$$\n\nå…¶ä¸­:\n$$\\bar{Y}_{1,L} = \\frac{1}{n_{1,L}} \\sum_{i \\in L, T_i=1} Y_i$$\n$$\\bar{Y}_{0,L} = \\frac{1}{n_{0,L}} \\sum_{i \\in L, T_i=0} Y_i$$\n\n### ç½®ä¿¡åŒºé—´\n\nå› ä¸ºä½¿ç”¨äº† Honest Splittingï¼Œ$\\hat{\\tau}_L$ æ˜¯æ¸è¿‘æ­£æ€çš„:\n\n$$\\hat{\\tau}_L \\sim N\\left(\\tau_L, \\frac{\\sigma^2_{1,L}}{n_{1,L}} + \\frac{\\sigma^2_{0,L}}{n_{0,L}}\\right)$$\n\n**95% ç½®ä¿¡åŒºé—´**:\n$$\\text{CI}_{95\\%} = \\hat{\\tau}_L \\pm 1.96 \\sqrt{\\frac{\\hat{\\sigma}^2_{1,L}}{n_{1,L}} + \\frac{\\hat{\\sigma}^2_{0,L}}{n_{0,L}}}$$\n\nå…¶ä¸­ $\\hat{\\sigma}^2_{t,L}$ æ˜¯å¶èŠ‚ç‚¹å†…çš„æ ·æœ¬æ–¹å·®ä¼°è®¡ã€‚\n\n### Causal Forest çš„é›†æˆ\n\nå•æ£µæ ‘æ–¹å·®å¤§ï¼Œæ‰€ä»¥é›†æˆ $B$ æ£µæ ‘:\n\n$$\\hat{\\tau}_{CF}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\tau}_b(x)$$\n\næ¯æ£µæ ‘:\n1. éšæœºæŠ½æ · bootstrap æ ·æœ¬\n2. éšæœºé€‰æ‹©ç‰¹å¾å­é›† (MTRY)\n3. Honest Splitting æ„å»ºæ ‘\n4. ä¼°è®¡ CATE\n\n**æ–¹å·®å‡å°‘**:\n$$Var[\\hat{\\tau}_{CF}] \\approx \\frac{Var[\\hat{\\tau}_{tree}]}{B}$$\n\n### ä¸ Random Forest çš„åŒºåˆ«\n\n| ç‰¹æ€§ | Random Forest | Causal Forest |\n|------|---------------|---------------|\n| ç›®æ ‡ | é¢„æµ‹ $Y$ | ä¼°è®¡ $\\tau(x)$ |\n| åˆ†è£‚å‡†åˆ™ | MSE å‡å°‘ | CATE å¼‚è´¨æ€§æœ€å¤§åŒ– |\n| Honest Splitting | å¦ | æ˜¯ |\n| å¶èŠ‚ç‚¹å€¼ | $\\bar{Y}$ | $\\bar{Y}_1 - \\bar{Y}_0$ |\n| ç½®ä¿¡åŒºé—´ | ä¸å¯é  | ç†è®ºä¿è¯ |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒè®¾ç½®\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# å°è¯•å¯¼å…¥ econml\n",
    "try:\n",
    "    from econml.grf import CausalForest\n",
    "    ECONML_AVAILABLE = True\n",
    "    print(\"econml å¯¼å…¥æˆåŠŸ! âœ…\")\n",
    "except ImportError:\n",
    "    ECONML_AVAILABLE = False\n",
    "    print(\"è­¦å‘Š: econml æœªå®‰è£… âš ï¸\")\n",
    "    print(\"å®‰è£…å‘½ä»¤: pip install econml\")\n",
    "\n",
    "# ç»˜å›¾è®¾ç½®\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"\\nç¯å¢ƒé…ç½®å®Œæˆ! ğŸŒ²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def honest_split_data(\n    X: np.ndarray,\n    T: np.ndarray,\n    Y: np.ndarray,\n    split_ratio: float = 0.5,\n    seed: int = 42\n) -> Tuple[Tuple, Tuple]:\n    \"\"\"\n    è¯šå®åˆ†è£‚: å°†æ•°æ®åˆ†ä¸ºä¸¤éƒ¨åˆ†\n    \n    - åˆ†è£‚æ ·æœ¬: ç”¨äºæ„å»ºæ ‘ç»“æ„ï¼ˆå†³å®šåœ¨å“ªé‡Œåˆ†è£‚ï¼‰\n    - ä¼°è®¡æ ·æœ¬: ç”¨äºå¶èŠ‚ç‚¹çš„ CATE ä¼°è®¡ï¼ˆè®¡ç®—å®é™…æ•ˆåº”ï¼‰\n    \n    è¿™æ˜¯å› æœæ£®æ—çš„æ ¸å¿ƒåˆ›æ–°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼\n    \n    Args:\n        X, T, Y: ç‰¹å¾ã€å¤„ç†ã€ç»“æœ\n        split_ratio: åˆ†è£‚æ ·æœ¬çš„æ¯”ä¾‹\n        seed: éšæœºç§å­\n    \n    Returns:\n        ((X_split, T_split, Y_split), (X_est, T_est, Y_est))\n    \"\"\"\n    np.random.seed(seed)\n    n = len(X)\n    \n    # éšæœºæ‰“ä¹±ç´¢å¼•\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    \n    # è®¡ç®—åˆ†è£‚ç‚¹\n    split_point = int(n * split_ratio)\n    \n    # åˆ’åˆ†ç´¢å¼•\n    split_idx = indices[:split_point]\n    est_idx = indices[split_point:]\n    \n    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ•°æ®\n    # åˆ†è£‚æ ·æœ¬\n    X_split = X[split_idx]\n    T_split = T[split_idx]\n    Y_split = Y[split_idx]\n    \n    # ä¼°è®¡æ ·æœ¬\n    X_est = X[est_idx]\n    T_est = T[est_idx]\n    Y_est = Y[est_idx]\n    \n    return ((X_split, T_split, Y_split), (X_est, T_est, Y_est))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def honest_split_data(\n    X: np.ndarray,\n    T: np.ndarray,\n    Y: np.ndarray,\n    split_ratio: float = 0.5,\n    seed: int = 42\n) -> Tuple[Tuple, Tuple]:\n    \"\"\"\n    è¯šå®åˆ†è£‚: å°†æ•°æ®åˆ†ä¸ºä¸¤éƒ¨åˆ†\n\n    - åˆ†è£‚æ ·æœ¬: ç”¨äºæ„å»ºæ ‘ç»“æ„ï¼ˆå†³å®šåœ¨å“ªé‡Œåˆ†è£‚ï¼‰\n    - ä¼°è®¡æ ·æœ¬: ç”¨äºå¶èŠ‚ç‚¹çš„ CATE ä¼°è®¡ï¼ˆè®¡ç®—å®é™…æ•ˆåº”ï¼‰\n\n    è¿™æ˜¯å› æœæ£®æ—çš„æ ¸å¿ƒåˆ›æ–°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼\n\n    Args:\n        X, T, Y: ç‰¹å¾ã€å¤„ç†ã€ç»“æœ\n        split_ratio: åˆ†è£‚æ ·æœ¬çš„æ¯”ä¾‹\n        seed: éšæœºç§å­\n\n    Returns:\n        ((X_split, T_split, Y_split), (X_est, T_est, Y_est))\n    \"\"\"\n    np.random.seed(seed)\n    n = len(X)\n\n    # éšæœºæ‰“ä¹±ç´¢å¼•\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n\n    # è®¡ç®—åˆ†è£‚ç‚¹\n    split_point = int(n * split_ratio)\n\n    # åˆ’åˆ†ç´¢å¼•\n    split_idx = indices[:split_point]\n    est_idx = indices[split_point:]\n\n    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ•°æ®\n    # åˆ†è£‚æ ·æœ¬\n    X_split = X[split_idx]\n    T_split = T[split_idx]\n    Y_split = Y[split_idx]\n\n    # ä¼°è®¡æ ·æœ¬\n    X_est = X[est_idx]\n    T_est = T[est_idx]\n    Y_est = Y[est_idx]\n\n    return ((X_split, T_split, Y_split), (X_est, T_est, Y_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "def generate_test_data(n: int = 2000, seed: int = 42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "    \n",
    "    å¼‚è´¨æ€§æ•ˆåº”: tau = 3 + 2*X1 - 1.5*X2\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç‰¹å¾\n",
    "    X1 = np.random.randn(n)\n",
    "    X2 = np.random.randn(n)\n",
    "    X3 = np.random.randn(n)  # å™ªå£°ç‰¹å¾ï¼Œä¸å½±å“ CATE\n",
    "    \n",
    "    X = np.column_stack([X1, X2, X3])\n",
    "    \n",
    "    # éšæœºå¤„ç†\n",
    "    T = np.random.binomial(1, 0.5, n)\n",
    "    \n",
    "    # å¼‚è´¨æ€§æ•ˆåº”\n",
    "    tau = 3.0 + 2.0 * X1 - 1.5 * X2\n",
    "    \n",
    "    # ç»“æœ\n",
    "    Y0 = 10 + 1.5 * X1 + 1.0 * X2 + 0.5 * X3 + np.random.randn(n)\n",
    "    Y1 = Y0 + tau\n",
    "    Y = np.where(T == 1, Y1, Y0)\n",
    "    \n",
    "    return X, T, Y, tau\n",
    "\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "X, T, Y, tau_true = generate_test_data(n=2000)\n",
    "\n",
    "print(\"æ•°æ®ç”ŸæˆæˆåŠŸ! ğŸ‰\")\n",
    "print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}, T={T.shape}, Y={Y.shape}\")\n",
    "print(f\"å¤„ç†æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "print(f\"çœŸå® ATE: {tau_true.mean():.4f}\")\n",
    "print(f\"çœŸå® CATE èŒƒå›´: [{tau_true.min():.2f}, {tau_true.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è¯šå®åˆ†è£‚\n",
    "try:\n",
    "    split_data = honest_split_data(X, T, Y, split_ratio=0.5)\n",
    "    \n",
    "    if split_data is not None:\n",
    "        (X_split, T_split, Y_split), (X_est, T_est, Y_est) = split_data\n",
    "        \n",
    "        if X_split is not None:\n",
    "            print(\"è¯šå®åˆ†è£‚æµ‹è¯•é€šè¿‡! ğŸ‰\")\n",
    "            print(f\"\\nåˆ†è£‚æ ·æœ¬: {len(X_split)} ä¸ª\")\n",
    "            print(f\"  - å¤„ç†ç»„: {T_split.sum()}, æ§åˆ¶ç»„: {(1-T_split).sum()}\")\n",
    "            print(f\"\\nä¼°è®¡æ ·æœ¬: {len(X_est)} ä¸ª\")\n",
    "            print(f\"  - å¤„ç†ç»„: {T_est.sum()}, æ§åˆ¶ç»„: {(1-T_est).sum()}\")\n",
    "            \n",
    "            print(f\"\\nè¯šå®åˆ†è£‚çš„æ„ä¹‰:\")\n",
    "            print(f\"  - åˆ†è£‚æ ·æœ¬ç”¨äºå†³å®šæ ‘çš„ç»“æ„ï¼ˆå“ªäº›ç‰¹å¾é‡è¦ï¼‰\")\n",
    "            print(f\"  - ä¼°è®¡æ ·æœ¬ç”¨äºè®¡ç®—å¶èŠ‚ç‚¹çš„ CATEï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰\")\n",
    "        else:\n",
    "            print(\"[TODO] è¯·å®Œæˆæ•°æ®åˆ’åˆ†éƒ¨åˆ†\")\n",
    "    else:\n",
    "        print(\"[TODO] è¯·å®Œæˆ honest_split_data å‡½æ•°\")\n",
    "except Exception as e:\n",
    "    print(f\"[é”™è¯¯] {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def train_causal_forest(\n    X: np.ndarray,\n    T: np.ndarray,\n    Y: np.ndarray,\n    n_estimators: int = 100,\n    min_samples_leaf: int = 5,\n    max_depth: int = None\n) -> Optional[object]:\n    \"\"\"\n    ä½¿ç”¨ econml è®­ç»ƒå› æœæ£®æ—\n    \n    Args:\n        X: ç‰¹å¾çŸ©é˜µ\n        T: å¤„ç†å˜é‡ (0/1)\n        Y: ç»“æœå˜é‡\n        n_estimators: æ ‘çš„æ•°é‡\n        min_samples_leaf: å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°\n        max_depth: æœ€å¤§æ·±åº¦\n    \n    Returns:\n        è®­ç»ƒå¥½çš„å› æœæ£®æ—æ¨¡å‹\n    \"\"\"\n    if not ECONML_AVAILABLE:\n        print(\"econml ä¸å¯ç”¨\")\n        return None\n    \n    # åˆ›å»º CausalForest å®ä¾‹\n    cf = CausalForest(\n        n_estimators=n_estimators,\n        min_samples_leaf=min_samples_leaf,\n        max_depth=max_depth,\n        honest=True,  # ä½¿ç”¨è¯šå®åˆ†è£‚\n        random_state=42\n    )\n    \n    # è®­ç»ƒæ¨¡å‹\n    # æ³¨æ„: econml çš„ fit æ–¹æ³•æ˜¯ fit(Y, T, X=X)\n    cf.fit(Y, T, X=X)\n    \n    return cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_causal_forest(\n    X: np.ndarray,\n    T: np.ndarray,\n    Y: np.ndarray,\n    n_estimators: int = 100,\n    min_samples_leaf: int = 5,\n    max_depth: int = None\n) -> Optional[object]:\n    \"\"\"\n    ä½¿ç”¨ econml è®­ç»ƒå› æœæ£®æ—\n\n    Args:\n        X: ç‰¹å¾çŸ©é˜µ\n        T: å¤„ç†å˜é‡ (0/1)\n        Y: ç»“æœå˜é‡\n        n_estimators: æ ‘çš„æ•°é‡\n        min_samples_leaf: å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°\n        max_depth: æœ€å¤§æ·±åº¦\n\n    Returns:\n        è®­ç»ƒå¥½çš„å› æœæ£®æ—æ¨¡å‹\n    \"\"\"\n    if not ECONML_AVAILABLE:\n        print(\"econml ä¸å¯ç”¨\")\n        return None\n\n    # åˆ›å»º CausalForest å®ä¾‹\n    cf = CausalForest(\n        n_estimators=n_estimators,\n        min_samples_leaf=min_samples_leaf,\n        max_depth=max_depth,\n        honest=True,  # ä½¿ç”¨è¯šå®åˆ†è£‚\n        random_state=42\n    )\n\n    # è®­ç»ƒæ¨¡å‹\n    # æ³¨æ„: econml çš„ fit æ–¹æ³•æ˜¯ fit(Y, T, X=X)\n    cf.fit(Y, T, X=X)\n\n    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†å‰²è®­ç»ƒ/æµ‹è¯•é›†\n",
    "train_idx = np.random.choice(len(X), int(0.7 * len(X)), replace=False)\n",
    "test_idx = np.array([i for i in range(len(X)) if i not in train_idx])\n",
    "\n",
    "X_train, T_train, Y_train = X[train_idx], T[train_idx], Y[train_idx]\n",
    "X_test, T_test, Y_test = X[test_idx], T[test_idx], Y[test_idx]\n",
    "tau_test = tau_true[test_idx]\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬\")\n",
    "print(f\"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå› æœæ£®æ—\n",
    "if ECONML_AVAILABLE:\n",
    "    print(\"è®­ç»ƒå› æœæ£®æ—...\")\n",
    "    \n",
    "    try:\n",
    "        cf = train_causal_forest(\n",
    "            X_train, T_train, Y_train,\n",
    "            n_estimators=100,\n",
    "            min_samples_leaf=5\n",
    "        )\n",
    "        \n",
    "        if cf is not None:\n",
    "            # é¢„æµ‹ CATE\n",
    "            tau_pred_cf = cf.predict(X_test).flatten()\n",
    "            \n",
    "            # è®¡ç®— PEHE\n",
    "            pehe_cf = np.sqrt(np.mean((tau_test - tau_pred_cf) ** 2))\n",
    "            \n",
    "            # è®¡ç®—ç›¸å…³æ€§\n",
    "            corr_cf = np.corrcoef(tau_test, tau_pred_cf)[0, 1]\n",
    "            \n",
    "            print(\"\\nå› æœæ£®æ—è®­ç»ƒå®Œæˆ! ğŸŒ²\")\n",
    "            print(f\"\\nè¯„ä¼°ç»“æœ:\")\n",
    "            print(f\"  PEHE: {pehe_cf:.4f}\")\n",
    "            print(f\"  ç›¸å…³æ€§: {corr_cf:.4f}\")\n",
    "            print(f\"  ATE é¢„æµ‹: {tau_pred_cf.mean():.4f} (çœŸå®: {tau_test.mean():.4f})\")\n",
    "        else:\n",
    "            print(\"[TODO] è¯·å®Œæˆ train_causal_forest å‡½æ•°\")\n",
    "    except Exception as e:\n",
    "        print(f\"[é”™è¯¯] {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"è¯·å®‰è£… econml: pip install econml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def compare_models(\n    X_train: np.ndarray,\n    T_train: np.ndarray,\n    Y_train: np.ndarray,\n    X_test: np.ndarray,\n    tau_test: np.ndarray\n) -> pd.DataFrame:\n    \"\"\"\n    å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ CATE ä¼°è®¡æ€§èƒ½\n    \n    æ¨¡å‹:\n    1. T-Learner (Random Forest)\n    2. Causal Forest (å¦‚æœå¯ç”¨)\n    \"\"\"\n    results = []\n    predictions = {}\n    \n    # ===== æ¨¡å‹ 1: T-Learner =====\n    print(\"è®­ç»ƒ T-Learner...\")\n    \n    # è®­ç»ƒ T-Learner\n    model_0 = RandomForestRegressor(n_estimators=100, random_state=42)\n    model_1 = RandomForestRegressor(n_estimators=100, random_state=43)\n    \n    # åˆ†åˆ«è®­ç»ƒå¤„ç†ç»„å’Œå¯¹ç…§ç»„æ¨¡å‹\n    mask_0 = (T_train == 0)\n    mask_1 = (T_train == 1)\n    \n    model_0.fit(X_train[mask_0], Y_train[mask_0])\n    model_1.fit(X_train[mask_1], Y_train[mask_1])\n    \n    # é¢„æµ‹ CATE\n    tau_pred_tlearner = model_1.predict(X_test) - model_0.predict(X_test)\n    \n    if tau_pred_tlearner is not None:\n        pehe_tlearner = np.sqrt(np.mean((tau_test - tau_pred_tlearner)**2))\n        corr_tlearner = np.corrcoef(tau_test, tau_pred_tlearner)[0, 1]\n        \n        results.append({\n            'Model': 'T-Learner',\n            'PEHE': pehe_tlearner,\n            'Correlation': corr_tlearner,\n            'ATE Error': abs(tau_test.mean() - tau_pred_tlearner.mean())\n        })\n        predictions['T-Learner'] = tau_pred_tlearner\n    \n    # ===== æ¨¡å‹ 2: Causal Forest =====\n    if ECONML_AVAILABLE:\n        print(\"è®­ç»ƒ Causal Forest...\")\n        \n        try:\n            cf = train_causal_forest(X_train, T_train, Y_train)\n            \n            if cf is not None:\n                tau_pred_cf = cf.predict(X_test).flatten()\n                pehe_cf = np.sqrt(np.mean((tau_test - tau_pred_cf)**2))\n                corr_cf = np.corrcoef(tau_test, tau_pred_cf)[0, 1]\n                \n                results.append({\n                    'Model': 'Causal Forest',\n                    'PEHE': pehe_cf,\n                    'Correlation': corr_cf,\n                    'ATE Error': abs(tau_test.mean() - tau_pred_cf.mean())\n                })\n                predictions['Causal Forest'] = tau_pred_cf\n        except Exception as e:\n            print(f\"Causal Forest è®­ç»ƒå¤±è´¥: {e}\")\n    \n    return pd.DataFrame(results), predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(\n    X_train: np.ndarray,\n    T_train: np.ndarray,\n    Y_train: np.ndarray,\n    X_test: np.ndarray,\n    tau_test: np.ndarray\n) -> pd.DataFrame:\n    \"\"\"\n    å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ CATE ä¼°è®¡æ€§èƒ½\n\n    æ¨¡å‹:\n    1. T-Learner (Random Forest)\n    2. Causal Forest (å¦‚æœå¯ç”¨)\n    \"\"\"\n    results = []\n    predictions = {}\n\n    # ===== æ¨¡å‹ 1: T-Learner =====\n    print(\"è®­ç»ƒ T-Learner...\")\n\n    # è®­ç»ƒ T-Learner\n    model_0 = RandomForestRegressor(n_estimators=100, random_state=42)\n    model_1 = RandomForestRegressor(n_estimators=100, random_state=43)\n\n    # åˆ†åˆ«è®­ç»ƒå¤„ç†ç»„å’Œå¯¹ç…§ç»„æ¨¡å‹\n    mask_0 = (T_train == 0)\n    mask_1 = (T_train == 1)\n\n    model_0.fit(X_train[mask_0], Y_train[mask_0])\n    model_1.fit(X_train[mask_1], Y_train[mask_1])\n\n    # é¢„æµ‹ CATE\n    tau_pred_tlearner = model_1.predict(X_test) - model_0.predict(X_test)\n\n    if tau_pred_tlearner is not None:\n        pehe_tlearner = np.sqrt(np.mean((tau_test - tau_pred_tlearner)**2))\n        corr_tlearner = np.corrcoef(tau_test, tau_pred_tlearner)[0, 1]\n\n        results.append({\n            'Model': 'T-Learner',\n            'PEHE': pehe_tlearner,\n            'Correlation': corr_tlearner,\n            'ATE Error': abs(tau_test.mean() - tau_pred_tlearner.mean())\n        })\n        predictions['T-Learner'] = tau_pred_tlearner\n\n    # ===== æ¨¡å‹ 2: Causal Forest =====\n    if ECONML_AVAILABLE:\n        print(\"è®­ç»ƒ Causal Forest...\")\n\n        try:\n            cf = train_causal_forest(X_train, T_train, Y_train)\n\n            if cf is not None:\n                tau_pred_cf = cf.predict(X_test).flatten()\n                pehe_cf = np.sqrt(np.mean((tau_test - tau_pred_cf)**2))\n                corr_cf = np.corrcoef(tau_test, tau_pred_cf)[0, 1]\n\n                results.append({\n                    'Model': 'Causal Forest',\n                    'PEHE': pehe_cf,\n                    'Correlation': corr_cf,\n                    'ATE Error': abs(tau_test.mean() - tau_pred_cf.mean())\n                })\n                predictions['Causal Forest'] = tau_pred_cf\n        except Exception as e:\n            print(f\"Causal Forest è®­ç»ƒå¤±è´¥: {e}\")\n\n    return pd.DataFrame(results), predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹å¯¹æ¯”\n",
    "print(\"=\" * 60)\n",
    "print(\"æ¨¡å‹å¯¹æ¯”å®éªŒ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    comparison_df, predictions = compare_models(\n",
    "        X_train, T_train, Y_train,\n",
    "        X_test, tau_test\n",
    "    )\n",
    "    \n",
    "    if comparison_df is not None and len(comparison_df) > 0:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"å¯¹æ¯”ç»“æœ:\")\n",
    "        print(\"=\" * 60)\n",
    "        display(comparison_df.round(4))\n",
    "        \n",
    "        print(\"\\nè¯´æ˜:\")\n",
    "        print(\"- PEHE: è¶Šå°è¶Šå¥½ (Precision in Estimation of HTE)\")\n",
    "        print(\"- Correlation: è¶Šæ¥è¿‘ 1 è¶Šå¥½ (CATE æ’åºèƒ½åŠ›)\")\n",
    "        print(\"- ATE Error: è¶Šå°è¶Šå¥½ (å¹³å‡æ•ˆåº”ä¼°è®¡è¯¯å·®)\")\n",
    "    else:\n",
    "        print(\"[TODO] è¯·å®Œæˆ compare_models å‡½æ•°\")\n",
    "except Exception as e:\n",
    "    print(f\"[é”™è¯¯] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "if 'predictions' in dir() and len(predictions) > 0:\n",
    "    n_models = len(predictions)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 6))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (name, tau_pred) in zip(axes, predictions.items()):\n",
    "        ax.scatter(tau_test, tau_pred, alpha=0.3)\n",
    "        \n",
    "        # å®Œç¾é¢„æµ‹çº¿\n",
    "        min_val = min(tau_test.min(), tau_pred.min())\n",
    "        max_val = max(tau_test.max(), tau_pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='å®Œç¾é¢„æµ‹')\n",
    "        \n",
    "        # è®¡ç®— PEHE\n",
    "        pehe = np.sqrt(np.mean((tau_test - tau_pred)**2))\n",
    "        corr = np.corrcoef(tau_test, tau_pred)[0, 1]\n",
    "        \n",
    "        ax.set_xlabel('çœŸå® CATE')\n",
    "        ax.set_ylabel('é¢„æµ‹ CATE')\n",
    "        ax.set_title(f'{name}\\nPEHE={pehe:.4f}, Corr={corr:.4f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  2.4: ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
    "\n",
    "å› æœæ£®æ—å¯ä»¥å‘Šè¯‰æˆ‘ä»¬ï¼š**å“ªäº›ç‰¹å¾å¯¹ CATE çš„å¼‚è´¨æ€§è´¡çŒ®æœ€å¤§ï¼Ÿ**\n",
    "\n",
    "è¿™ä¸é¢„æµ‹æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ä¸åŒï¼š\n",
    "- é¢„æµ‹æ¨¡å‹: å“ªäº›ç‰¹å¾å¯¹é¢„æµ‹ Y é‡è¦ï¼Ÿ\n",
    "- å› æœæ£®æ—: å“ªäº›ç‰¹å¾å¯¼è‡´å¤„ç†æ•ˆåº”ä¸åŒï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(\n    model,\n    feature_names: list\n) -> pd.DataFrame:\n    \"\"\"\n    è·å–å› æœæ£®æ—çš„ç‰¹å¾é‡è¦æ€§\n\n    Returns:\n        DataFrame with columns: feature, importance\n    \"\"\"\n    # è·å–ç‰¹å¾é‡è¦æ€§\n    try:\n        importances = model.feature_importances()\n    except:\n        importances = getattr(model, 'feature_importances_', None)\n\n    if importances is None:\n        return None\n\n    # åˆ›å»º DataFrame å¹¶æ’åº\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances.flatten()\n    })\n\n    # æŒ‰é‡è¦æ€§æ’åº\n    importance_df = importance_df.sort_values('Importance', ascending=False)\n\n    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
    "if ECONML_AVAILABLE and 'cf' in dir() and cf is not None:\n",
    "    feature_names = ['X1', 'X2', 'X3']\n",
    "    \n",
    "    try:\n",
    "        importance_df = get_feature_importances(cf, feature_names)\n",
    "        \n",
    "        if importance_df is not None:\n",
    "            print(\"ç‰¹å¾é‡è¦æ€§åˆ†æ:\")\n",
    "            print(\"=\"*40)\n",
    "            display(importance_df.round(4))\n",
    "            \n",
    "            print(\"\\nå›é¡¾æ•°æ®ç”Ÿæˆè¿‡ç¨‹:\")\n",
    "            print(\"  tau = 3.0 + 2.0*X1 - 1.5*X2\")\n",
    "            print(\"  X3 æ˜¯å™ªå£°ç‰¹å¾ï¼Œä¸å½±å“ CATE\")\n",
    "            print(\"\\né¢„æœŸ: X1 å’Œ X2 é‡è¦æ€§é«˜ï¼ŒX3 é‡è¦æ€§ä½\")\n",
    "            \n",
    "            # å¯è§†åŒ–\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            \n",
    "            colors = ['#3498db', '#e74c3c', '#95a5a6']  # X3 ç”¨ç°è‰²è¡¨ç¤ºå™ªå£°\n",
    "            bars = ax.barh(\n",
    "                importance_df['Feature'], \n",
    "                importance_df['Importance'],\n",
    "                color=[colors[feature_names.index(f)] for f in importance_df['Feature']]\n",
    "            )\n",
    "            \n",
    "            ax.set_xlabel('é‡è¦æ€§')\n",
    "            ax.set_title('å› æœæ£®æ—ç‰¹å¾é‡è¦æ€§\\n(å¯¹ CATE å¼‚è´¨æ€§çš„è´¡çŒ®)')\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for bar, val in zip(bars, importance_df['Importance']):\n",
    "                ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{val:.3f}', va='center')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"æ— æ³•è·å–ç‰¹å¾é‡è¦æ€§\")\n",
    "    except Exception as e:\n",
    "        print(f\"ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}\")\n",
    "else:\n",
    "    print(\"éœ€è¦å…ˆè®­ç»ƒå› æœæ£®æ—æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  2.5: ç½®ä¿¡åŒºé—´\n",
    "\n",
    "å› æœæ£®æ—çš„å¦ä¸€ä¸ªä¼˜ç‚¹ï¼šå¯ä»¥æä¾›**æœ‰æ•ˆçš„ç½®ä¿¡åŒºé—´**ï¼\n",
    "\n",
    "è¿™æ˜¯è¯šå®åˆ†è£‚å¸¦æ¥çš„ç»Ÿè®¡æ€§è´¨ï¼Œè®©æˆ‘ä»¬å¯ä»¥è¿›è¡Œæ¨æ–­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–ç½®ä¿¡åŒºé—´\n",
    "if ECONML_AVAILABLE and 'cf' in dir() and cf is not None:\n",
    "    try:\n",
    "        # è·å–é¢„æµ‹å’Œç½®ä¿¡åŒºé—´\n",
    "        tau_pred = cf.predict(X_test).flatten()\n",
    "        \n",
    "        # å°è¯•è·å–ç½®ä¿¡åŒºé—´\n",
    "        try:\n",
    "            tau_lower, tau_upper = cf.predict_interval(X_test, alpha=0.05)\n",
    "            tau_lower = tau_lower.flatten()\n",
    "            tau_upper = tau_upper.flatten()\n",
    "            has_ci = True\n",
    "        except:\n",
    "            has_ci = False\n",
    "            print(\"æ¨¡å‹ä¸æ”¯æŒç½®ä¿¡åŒºé—´\")\n",
    "        \n",
    "        if has_ci:\n",
    "            print(\"ç½®ä¿¡åŒºé—´åˆ†æ:\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # è®¡ç®—è¦†ç›–ç‡\n",
    "            coverage = np.mean((tau_test >= tau_lower) & (tau_test <= tau_upper))\n",
    "            print(f\"95% ç½®ä¿¡åŒºé—´è¦†ç›–ç‡: {coverage:.2%}\")\n",
    "            print(f\"(ç†è®ºä¸Šåº”è¯¥æ¥è¿‘ 95%)\")\n",
    "            \n",
    "            # å¯è§†åŒ–\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # é€‰æ‹©ä¸€äº›æ ·æœ¬å¯è§†åŒ–\n",
    "            sample_idx = np.random.choice(len(tau_pred), 50, replace=False)\n",
    "            sample_idx = np.sort(sample_idx)\n",
    "            \n",
    "            # å›¾1: å¸¦ç½®ä¿¡åŒºé—´çš„é¢„æµ‹\n",
    "            x_pos = range(len(sample_idx))\n",
    "            axes[0].errorbar(\n",
    "                x_pos, tau_pred[sample_idx],\n",
    "                yerr=[tau_pred[sample_idx] - tau_lower[sample_idx],\n",
    "                      tau_upper[sample_idx] - tau_pred[sample_idx]],\n",
    "                fmt='o', capsize=3, label='é¢„æµ‹ Â± 95% CI'\n",
    "            )\n",
    "            axes[0].scatter(x_pos, tau_test[sample_idx], color='red', \n",
    "                           marker='x', s=50, label='çœŸå®å€¼')\n",
    "            axes[0].set_xlabel('æ ·æœ¬ç´¢å¼•')\n",
    "            axes[0].set_ylabel('CATE')\n",
    "            axes[0].set_title('CATE é¢„æµ‹ä¸ 95% ç½®ä¿¡åŒºé—´')\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # å›¾2: ç½®ä¿¡åŒºé—´å®½åº¦åˆ†å¸ƒ\n",
    "            ci_width = tau_upper - tau_lower\n",
    "            axes[1].hist(ci_width, bins=30, alpha=0.7, color='#2ecc71', edgecolor='black')\n",
    "            axes[1].axvline(x=ci_width.mean(), color='red', linestyle='--',\n",
    "                           label=f'å¹³å‡å®½åº¦: {ci_width.mean():.3f}')\n",
    "            axes[1].set_xlabel('ç½®ä¿¡åŒºé—´å®½åº¦')\n",
    "            axes[1].set_ylabel('é¢‘æ•°')\n",
    "            axes[1].set_title('ç½®ä¿¡åŒºé—´å®½åº¦åˆ†å¸ƒ')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"ç½®ä¿¡åŒºé—´åˆ†æå¤±è´¥: {e}\")\n",
    "else:\n",
    "    print(\"éœ€è¦å…ˆè®­ç»ƒå› æœæ£®æ—æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” æ€è€ƒé¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ¯ Causal Forest é¢è¯•é«˜é¢‘é¢˜\n\n### Q1: ä»€ä¹ˆæ˜¯ Honest Splittingï¼Ÿä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**:\n\n**å®šä¹‰**: å°†æ ·æœ¬åˆ†ä¸ºä¸¤ä¸ªä¸é‡å çš„å­é›†:\n- åˆ†è£‚æ ·æœ¬: å†³å®šæ ‘çš„ç»“æ„\n- ä¼°è®¡æ ·æœ¬: ä¼°è®¡å¶èŠ‚ç‚¹çš„ CATE\n\n**ä¸ºä»€ä¹ˆé‡è¦**:\n1. **æ— åæ€§**: é¿å…é€‰æ‹©åå·® (selection bias)\n2. **ç½®ä¿¡åŒºé—´**: ä¿è¯æ¸è¿‘æ­£æ€æ€§ï¼Œä½¿å¾—ç½®ä¿¡åŒºé—´æœ‰æ•ˆ\n3. **é˜²æ­¢è¿‡æ‹Ÿåˆ**: æ„å»ºå’Œä¼°è®¡ä½¿ç”¨ä¸åŒæ•°æ®\n\n**ä»£ä»·**: éœ€è¦ä¸¤å€æ ·æœ¬é‡æ‰èƒ½è¾¾åˆ°ç›¸åŒç²¾åº¦\n\n### Q2: Causal Forest çš„åˆ†è£‚å‡†åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**:\n\n**ç›®æ ‡**: æœ€å¤§åŒ–å­èŠ‚ç‚¹é—´çš„ CATE å¼‚è´¨æ€§\n\n**åˆ†è£‚å¢ç›Š**:\n$$\\Delta = \\frac{n_L \\cdot n_R}{(n_L + n_R)^2} \\times (\\hat{\\tau}_L - \\hat{\\tau}_R)^2$$\n\n**ä¸ Random Forest çš„åŒºåˆ«**:\n- RF: æœ€å°åŒ– MSE\n- CF: æœ€å¤§åŒ– CATE å·®å¼‚\n\n**ç›´è§‰**: æˆ‘ä»¬å¸Œæœ›ä¸€ä¸ªå¶èŠ‚ç‚¹é‡Œçš„äººæ•ˆåº”éƒ½ç›¸ä¼¼ï¼Œä½†ä¸åŒå¶èŠ‚ç‚¹çš„æ•ˆåº”å·®å¼‚å¤§ã€‚\n\n### Q3: Causal Forest vs T-Learner æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**:\n\n| ç‰¹æ€§ | T-Learner | Causal Forest |\n|------|-----------|---------------|\n| è®¾è®¡ç›®æ ‡ | é€šç”¨ | ä¸“ä¸º CATE è®¾è®¡ |\n| åˆ†è£‚å‡†åˆ™ | é¢„æµ‹ Y | æœ€å¤§åŒ– CATE å¼‚è´¨æ€§ |\n| Honest Splitting | æ—  | æœ‰ |\n| ç½®ä¿¡åŒºé—´ | ä¸å¯é  | æœ‰ç†è®ºä¿è¯ |\n| ç‰¹å¾é‡è¦æ€§ | å¯¹ Y çš„é‡è¦æ€§ | å¯¹ CATE å¼‚è´¨æ€§çš„è´¡çŒ® |\n\n**ä½•æ—¶ç”¨å“ªä¸ª**:\n- T-Learner: å¿«é€ŸåŸå‹ï¼Œæ•°æ®å°‘\n- Causal Forest: éœ€è¦ç½®ä¿¡åŒºé—´ï¼Œæ•°æ®å¤š\n\n### Q4: ç‰¹å¾é‡è¦æ€§åœ¨ Causal Forest ä¸­ä»£è¡¨ä»€ä¹ˆï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**:\n\n**Causal Forest çš„ç‰¹å¾é‡è¦æ€§** è¡¡é‡çš„æ˜¯:\n- è¯¥ç‰¹å¾å¯¹ **CATE å¼‚è´¨æ€§** çš„è´¡çŒ®\n- ä¸æ˜¯å¯¹é¢„æµ‹ Y çš„è´¡çŒ®ï¼\n\n**è®¡ç®—æ–¹å¼**:\n- åŸºäºè¯¥ç‰¹å¾åˆ†è£‚å¸¦æ¥çš„ CATE å·®å¼‚å¢ç›Šæ€»å’Œ\n- ç±»ä¼¼ RF çš„ Gini importanceï¼Œä½†ç›®æ ‡ä¸åŒ\n\n**å®é™…æ„ä¹‰**:\n- é‡è¦æ€§é«˜ â†’ è¯¥ç‰¹å¾æ˜¯æ•ˆåº”å¼‚è´¨æ€§çš„ä¸»è¦é©±åŠ¨å› ç´ \n- å¯ä»¥æŒ‡å¯¼ä¸šåŠ¡ç†è§£\"ä¸ºä»€ä¹ˆä¸åŒäººæ•ˆåº”ä¸åŒ\"\n\n### Q5: Causal Forest çš„è®¡ç®—å¤æ‚åº¦å¦‚ä½•ï¼Ÿä»€ä¹ˆæ—¶å€™ä¸åº”è¯¥ç”¨å®ƒï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**:\n\n**è®¡ç®—å¤æ‚åº¦**:\n- è®­ç»ƒ: $O(B \\cdot n \\log n \\cdot p)$ å…¶ä¸­ B=æ ‘æ•°é‡, n=æ ·æœ¬æ•°, p=ç‰¹å¾æ•°\n- é¢„æµ‹: $O(B \\cdot \\log n)$\n\n**å†…å­˜éœ€æ±‚**: æ¯” T-Learner é«˜ (éœ€è¦å­˜å‚¨æ‰€æœ‰æ ‘)\n\n**ä¸é€‚åˆçš„åœºæ™¯**:\n1. æ ·æœ¬é‡å¾ˆå° (< 500) - Honest Splitting éœ€è¦æ›´å¤šæ•°æ®\n2. æ•ˆåº”æ˜¯ç®€å•çº¿æ€§å…³ç³» - çº¿æ€§æ¨¡å‹å°±å¤Ÿäº†\n3. è®¡ç®—èµ„æºæœ‰é™ - T-Learner æ›´å¿«\n4. åªéœ€è¦ ATE - ä¸éœ€è¦ Causal Forest çš„ç²¾ç»†å»ºæ¨¡\n\n**é€‚åˆçš„åœºæ™¯**:\n1. éœ€è¦ç½®ä¿¡åŒºé—´å’Œç»Ÿè®¡æ¨æ–­\n2. æ•ˆåº”å¼‚è´¨æ€§å¤æ‚ä¸”éçº¿æ€§\n3. éœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æ\n4. æ•°æ®é‡å……è¶³ (> 2000)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 1: ä»€ä¹ˆæ˜¯è¯šå®åˆ†è£‚ (Honest Splitting)ï¼Ÿå®ƒå¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿ\n",
    "\n",
    "answer_1 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 2: å› æœæ£®æ—ä¸æ ‡å‡†éšæœºæ£®æ—æœ‰ä½•ä¸åŒï¼Ÿä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨éšæœºæ£®æ—ä¼°è®¡ CATEï¼Ÿ\n",
    "\n",
    "answer_2 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 3: åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ï¼Œå› æœæ£®æ—æ¯” T-Learner è¡¨ç°æ›´å¥½ï¼Ÿ\n",
    "\n",
    "answer_3 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 4: ç‰¹å¾é‡è¦æ€§åœ¨å› æœæ¨æ–­ä¸­å¦‚ä½•è§£é‡Šï¼Ÿå®ƒä¸é¢„æµ‹æ¨¡å‹ä¸­çš„ç‰¹å¾é‡è¦æ€§æœ‰ä½•ä¸åŒï¼Ÿ\n",
    "\n",
    "answer_4 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 5: å› æœæ£®æ—çš„è®¡ç®—å¤æ‚åº¦å¦‚ä½•ï¼Ÿåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥è€ƒè™‘ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•ï¼Ÿ\n",
    "\n",
    "answer_5 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ€»ç»“\n",
    "\n",
    "### å› æœæ£®æ— vs T-Learner\n",
    "\n",
    "| æ–¹é¢ | T-Learner | å› æœæ£®æ— |\n",
    "|------|-----------|----------|\n",
    "| è®¾è®¡ç›®çš„ | é€šç”¨é¢„æµ‹æ–¹æ³•çš„ç»„åˆ | ä¸“ä¸º CATE è®¾è®¡ |\n",
    "| åˆ†è£‚å‡†åˆ™ | é¢„æµ‹ Y | æœ€å¤§åŒ– CATE å¼‚è´¨æ€§ |\n",
    "| è¯šå®åˆ†è£‚ | æ—  | æœ‰ |\n",
    "| ç½®ä¿¡åŒºé—´ | ä¸ç›´æ¥æ”¯æŒ | ç†è®ºä¿è¯ |\n",
    "| ç‰¹å¾é‡è¦æ€§ | å¯¹ Y çš„é‡è¦æ€§ | å¯¹ CATE å¼‚è´¨æ€§çš„é‡è¦æ€§ |\n",
    "\n",
    "### å› æœæ£®æ—æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "| æ¦‚å¿µ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| è¯šå®åˆ†è£‚ | ç”¨ä¸åŒæ•°æ®æ„å»ºæ ‘å’Œä¼°è®¡æ•ˆåº” |\n",
    "| åˆ†è£‚å‡†åˆ™ | æœ€å¤§åŒ–å­èŠ‚ç‚¹ CATE å·®å¼‚ |\n",
    "| å¶èŠ‚ç‚¹ä¼°è®¡ | å¤„ç†ç»„å‡å€¼ - æ§åˆ¶ç»„å‡å€¼ |\n",
    "| é›†æˆ | å¤šæ£µæ ‘çš„å¹³å‡ |\n",
    "\n",
    "### ä»€ä¹ˆæ—¶å€™ç”¨å› æœæ£®æ—ï¼Ÿ\n",
    "\n",
    "âœ… **é€‚åˆåœºæ™¯**:\n",
    "- éœ€è¦å¯é çš„ç½®ä¿¡åŒºé—´\n",
    "- æƒ³äº†è§£å“ªäº›ç‰¹å¾é©±åŠ¨æ•ˆåº”å¼‚è´¨æ€§\n",
    "- æ•°æ®é‡è¶³å¤Ÿå¤§ï¼ˆè¯šå®åˆ†è£‚éœ€è¦æ›´å¤šæ•°æ®ï¼‰\n",
    "\n",
    "âŒ **ä¸é€‚åˆåœºæ™¯**:\n",
    "- æ•°æ®é‡å¾ˆå°\n",
    "- æ•ˆåº”æ˜¯ç®€å•çš„çº¿æ€§å…³ç³»\n",
    "- è®¡ç®—èµ„æºæœ‰é™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸŒ² æ­å–œå®Œæˆå› æœæ£®æ—ç»ƒä¹ !\")\n",
    "print(\"\\nä½ å·²ç»å­¦ä¼šäº†:\")\n",
    "print(\"  âœ“ è¯šå®åˆ†è£‚çš„æ¦‚å¿µå’Œå®ç°\")\n",
    "print(\"  âœ“ ä½¿ç”¨ econml è®­ç»ƒå› æœæ£®æ—\")\n",
    "print(\"  âœ“ å› æœæ£®æ— vs T-Learner å¯¹æ¯”\")\n",
    "print(\"  âœ“ ç‰¹å¾é‡è¦æ€§åˆ†æ\")\n",
    "print(\"  âœ“ ç½®ä¿¡åŒºé—´ä¼°è®¡\")\n",
    "print(\"\\nä¸‹ä¸€æ­¥: æ•æ„Ÿæ€§åˆ†æ - æ£€éªŒå› æœç»“è®ºçš„ç¨³å¥æ€§!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}