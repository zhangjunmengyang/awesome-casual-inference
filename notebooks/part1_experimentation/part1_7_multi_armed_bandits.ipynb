{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¤šè‡‚è€è™æœº (Multi-Armed Bandits) - è¾¹å­¦è¾¹ä¼˜åŒ–\n",
    "\n",
    "## å¼•è¨€ï¼šæ‹‰æ–¯ç»´åŠ æ–¯çš„æ™ºæ…§\n",
    "\n",
    "æƒ³è±¡ä½ èµ°è¿›æ‹‰æ–¯ç»´åŠ æ–¯çš„èµŒåœºï¼Œé¢å‰æœ‰ 5 å°è€è™æœºã€‚æ¯å°æœºå™¨çš„èµ”ç‡ä¸åŒï¼Œä½†ä½ ä¸çŸ¥é“å“ªå°æœ€å¥½ã€‚ä½ æœ‰ 100 æ¬¡æ¸¸æˆæœºä¼šï¼Œç›®æ ‡æ˜¯èµšæœ€å¤šçš„é’±ã€‚\n",
    "\n",
    "**ç­–ç•¥ Aï¼ˆä¼ ç»Ÿ A/B Testingï¼‰**ï¼š\n",
    "- å‰ 50 æ¬¡å‡åŒ€æµ‹è¯•æ¯å°æœºå™¨ï¼ˆå„ 10 æ¬¡ï¼‰\n",
    "- æ‰¾å‡ºæœ€å¥½çš„æœºå™¨\n",
    "- å 50 æ¬¡å…¨æŠ¼åœ¨æœ€å¥½çš„æœºå™¨ä¸Š\n",
    "\n",
    "**ç­–ç•¥ Bï¼ˆMulti-Armed Banditï¼‰**ï¼š\n",
    "- è¾¹ç©è¾¹å­¦ï¼ŒåŠ¨æ€è°ƒæ•´\n",
    "- å¥½çš„æœºå™¨å¤šç©ï¼Œå·®çš„æœºå™¨å°‘ç©\n",
    "- ä½†ä¹Ÿä¸å®Œå…¨æ”¾å¼ƒå·®çš„æœºå™¨ï¼ˆä¸‡ä¸€è¿æ°”ä¸å¥½å‘¢ï¼Ÿï¼‰\n",
    "\n",
    "è¿™å°±æ˜¯ **Explore-Exploit Trade-off** çš„æ ¸å¿ƒæ€æƒ³ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬èŠ‚ç›®æ ‡\n",
    "\n",
    "1. ç†è§£ MAB ä¸ A/B Testing çš„åŒºåˆ«\n",
    "2. æŒæ¡ Epsilon-Greedyã€UCBã€Thompson Sampling\n",
    "3. å­¦ä¹  Contextual Bandit å¤„ç†ä¸ªæ€§åŒ–åœºæ™¯\n",
    "4. ç†è§£ Regret é—æ†¾åˆ†æ\n",
    "5. åº”ç”¨äºæ¨èç³»ç»Ÿã€å¹¿å‘ŠæŠ•æ”¾ç­‰åœºæ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotly é…ç½®\n",
    "COLORS = {\n",
    "    'blue': '#2D9CDB',\n",
    "    'green': '#27AE60',\n",
    "    'red': '#EB5757',\n",
    "    'yellow': '#F2C94C',\n",
    "    'purple': '#9B51E0',\n",
    "    'gray': '#828282'\n",
    "}\n",
    "\n",
    "print(\"ğŸ“š ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: MAB vs A/B Testing\n",
    "\n",
    "## 1.1 A/B Testing çš„ã€Œé—æ†¾ã€\n",
    "\n",
    "### åœºæ™¯ï¼šç”µå•†ç½‘ç«™æŒ‰é’®é¢œè‰²æµ‹è¯•\n",
    "\n",
    "å‡è®¾ä½ åœ¨æµ‹è¯• 3 ç§æŒ‰é’®é¢œè‰²ï¼š\n",
    "- çº¢è‰²ï¼šè½¬åŒ–ç‡ 5%\n",
    "- è“è‰²ï¼šè½¬åŒ–ç‡ 8%ï¼ˆæœ€ä¼˜ï¼‰\n",
    "- ç»¿è‰²ï¼šè½¬åŒ–ç‡ 6%\n",
    "\n",
    "**ä¼ ç»Ÿ A/B Testing**ï¼š\n",
    "- å‰ 3000 ä¸ªç”¨æˆ·å‡åŒ€åˆ†é…ï¼ˆå„ 1000 ä¸ªï¼‰\n",
    "- ç»Ÿè®¡åˆ†æåé€‰æ‹©è“è‰²\n",
    "- å 7000 ä¸ªç”¨æˆ·å…¨éƒ¨ä½¿ç”¨è“è‰²\n",
    "\n",
    "**é—®é¢˜åœ¨å“ªï¼Ÿ**\n",
    "- å‰ 3000 ä¸ªç”¨æˆ·ä¸­ï¼Œæœ‰ 2000 ä¸ªçœ‹åˆ°äº†æ¬¡ä¼˜æ–¹æ¡ˆ\n",
    "- æŸå¤±äº† $(8\\% - 5.67\\%) \\times 2000 \\approx 46$ ä¸ªè½¬åŒ–\n",
    "\n",
    "è¿™å°±æ˜¯ **Regretï¼ˆé—æ†¾ï¼‰**ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Regret} = \\sum_{t=1}^{T} (\\mu^* - \\mu_{a_t})\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $T$ï¼šæ€»è½®æ•°\n",
    "- $\\mu^*$ï¼šæœ€ä¼˜è‡‚çš„æœŸæœ›æ”¶ç›Š\n",
    "- $\\mu_{a_t}$ï¼šç¬¬ $t$ è½®é€‰æ‹©çš„è‡‚çš„æœŸæœ›æ”¶ç›Š\n",
    "- $a_t$ï¼šç¬¬ $t$ è½®é€‰æ‹©çš„åŠ¨ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿ A/B Testing\n",
    "class TraditionalABTest:\n",
    "    def __init__(self, true_rates: List[float], exploration_ratio: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            true_rates: æ¯ä¸ªè‡‚çš„çœŸå®è½¬åŒ–ç‡\n",
    "            exploration_ratio: æ¢ç´¢é˜¶æ®µå æ¯”\n",
    "        \"\"\"\n",
    "        self.true_rates = np.array(true_rates)\n",
    "        self.n_arms = len(true_rates)\n",
    "        self.exploration_ratio = exploration_ratio\n",
    "        self.best_arm = np.argmax(true_rates)\n",
    "        \n",
    "    def run(self, n_rounds: int) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"è¿è¡Œ A/B æµ‹è¯•\"\"\"\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        exploration_rounds = int(n_rounds * self.exploration_ratio)\n",
    "        \n",
    "        # æ¢ç´¢é˜¶æ®µï¼šå‡åŒ€åˆ†é…\n",
    "        for t in range(exploration_rounds):\n",
    "            arm = t % self.n_arms\n",
    "            reward = np.random.binomial(1, self.true_rates[arm])\n",
    "            actions.append(arm)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        # åˆ©ç”¨é˜¶æ®µï¼šå…¨éƒ¨ä½¿ç”¨æœ€ä¼˜è‡‚\n",
    "        for t in range(exploration_rounds, n_rounds):\n",
    "            arm = self.best_arm  # å‡è®¾æˆ‘ä»¬èƒ½æ‰¾åˆ°æœ€ä¼˜è‡‚\n",
    "            reward = np.random.binomial(1, self.true_rates[arm])\n",
    "            actions.append(arm)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return actions, rewards\n",
    "\n",
    "# è¿è¡Œå®éªŒ\n",
    "true_rates = [0.05, 0.08, 0.06]  # çº¢ã€è“ã€ç»¿\n",
    "n_rounds = 10000\n",
    "\n",
    "ab_test = TraditionalABTest(true_rates)\n",
    "actions, rewards = ab_test.run(n_rounds)\n",
    "\n",
    "# è®¡ç®—ç´¯ç§¯é—æ†¾\n",
    "best_rate = max(true_rates)\n",
    "cumulative_regret = []\n",
    "regret = 0\n",
    "for action in actions:\n",
    "    regret += best_rate - true_rates[action]\n",
    "    cumulative_regret.append(regret)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, n_rounds + 1)),\n",
    "    y=cumulative_regret,\n",
    "    mode='lines',\n",
    "    name='Cumulative Regret',\n",
    "    line=dict(color=COLORS['red'], width=2),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(235, 87, 87, 0.1)'\n",
    "))\n",
    "\n",
    "# æ ‡æ³¨æ¢ç´¢é˜¶æ®µ\n",
    "exploration_end = int(n_rounds * 0.3)\n",
    "fig.add_vline(\n",
    "    x=exploration_end, \n",
    "    line_dash=\"dash\", \n",
    "    line_color=COLORS['gray'],\n",
    "    annotation_text=\"æ¢ç´¢é˜¶æ®µç»“æŸ\",\n",
    "    annotation_position=\"top\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Traditional A/B Testing: ç´¯ç§¯é—æ†¾éšæ—¶é—´å¢é•¿',\n",
    "    xaxis_title='è½®æ•°',\n",
    "    yaxis_title='ç´¯ç§¯é—æ†¾ï¼ˆæŸå¤±çš„æœŸæœ›æ”¶ç›Šï¼‰',\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š A/B Testing ç»“æœï¼š\")\n",
    "print(f\"æ€»é—æ†¾ï¼š{cumulative_regret[-1]:.2f}\")\n",
    "print(f\"å¹³å‡æ¯è½®é—æ†¾ï¼š{cumulative_regret[-1] / n_rounds:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MAB çš„ Explore-Exploit æƒè¡¡\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**Explorationï¼ˆæ¢ç´¢ï¼‰**ï¼šå°è¯•ä¸ç¡®å®šçš„é€‰é¡¹ï¼Œè·å–ä¿¡æ¯  \n",
    "**Exploitationï¼ˆåˆ©ç”¨ï¼‰**ï¼šé€‰æ‹©å½“å‰æœ€ä¼˜çš„é€‰é¡¹ï¼Œæœ€å¤§åŒ–æ”¶ç›Š\n",
    "\n",
    "### ç”Ÿæ´»ä¸­çš„ä¾‹å­\n",
    "\n",
    "- **é¤å…é€‰æ‹©**ï¼šå»ç†Ÿæ‚‰çš„é¤å…ï¼ˆåˆ©ç”¨ï¼‰è¿˜æ˜¯å°è¯•æ–°é¤å…ï¼ˆæ¢ç´¢ï¼‰ï¼Ÿ\n",
    "- **èŒä¸šå‘å±•**ï¼šæ·±è€•å½“å‰é¢†åŸŸï¼ˆåˆ©ç”¨ï¼‰è¿˜æ˜¯å­¦ä¹ æ–°æŠ€èƒ½ï¼ˆæ¢ç´¢ï¼‰ï¼Ÿ\n",
    "- **æŠ•èµ„ç†è´¢**ï¼šæŒæœ‰è“ç­¹è‚¡ï¼ˆåˆ©ç”¨ï¼‰è¿˜æ˜¯æŠ•èµ„åˆ›ä¸šå…¬å¸ï¼ˆæ¢ç´¢ï¼‰ï¼Ÿ\n",
    "\n",
    "### MAB çš„ä¼˜åŠ¿\n",
    "\n",
    "1. **åŠ¨æ€è°ƒæ•´**ï¼šæ ¹æ®å®æ—¶åé¦ˆè°ƒæ•´ç­–ç•¥\n",
    "2. **å‡å°‘é—æ†¾**ï¼šæ›´å¿«åœ°å°†æµé‡åˆ†é…ç»™å¥½çš„æ–¹æ¡ˆ\n",
    "3. **è‡ªé€‚åº”**ï¼šç¯å¢ƒå˜åŒ–æ—¶èƒ½å¿«é€Ÿå“åº”\n",
    "\n",
    "### ä»€ä¹ˆæ—¶å€™ç”¨ MABï¼Ÿ\n",
    "\n",
    "âœ… **é€‚åˆ MAB**ï¼š\n",
    "- æµé‡å……è¶³ï¼Œå¯ä»¥æ‰¿å—åŠ¨æ€è°ƒæ•´\n",
    "- éœ€è¦å¿«é€Ÿæ‰¾åˆ°æœ€ä¼˜æ–¹æ¡ˆ\n",
    "- æ–¹æ¡ˆæ•°é‡è¾ƒå¤šï¼ˆ>2ï¼‰\n",
    "- æ”¶ç›Šåé¦ˆå¿«é€Ÿ\n",
    "\n",
    "âŒ **ä¸é€‚åˆ MAB**ï¼š\n",
    "- éœ€è¦ä¸¥æ ¼çš„ç»Ÿè®¡æ¨æ–­ï¼ˆç½®ä¿¡åŒºé—´ã€p å€¼ï¼‰\n",
    "- æµé‡æœ‰é™ï¼Œéœ€è¦å›ºå®šæ ·æœ¬é‡\n",
    "- å†³ç­–åæœé‡å¤§ï¼Œä¸èƒ½æ‰¿å—é£é™©\n",
    "- éœ€è¦ç†è§£ã€Œä¸ºä»€ä¹ˆã€ï¼ˆMAB åªå…³æ³¨ã€Œå“ªä¸ªå¥½\"ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: åŸºç¡€ç®—æ³•\n",
    "\n",
    "## 2.1 Epsilon-Greedyï¼šæœ€ç®€å•çš„ç­–ç•¥\n",
    "\n",
    "### ç®—æ³•æ€æƒ³\n",
    "\n",
    "- ä»¥æ¦‚ç‡ $\\epsilon$ éšæœºæ¢ç´¢\n",
    "- ä»¥æ¦‚ç‡ $1 - \\epsilon$ é€‰æ‹©å½“å‰æœ€ä¼˜\n",
    "\n",
    "### ä¼ªä»£ç \n",
    "\n",
    "```\n",
    "for t = 1 to T:\n",
    "    if random() < epsilon:\n",
    "        é€‰æ‹©éšæœºè‡‚ï¼ˆæ¢ç´¢ï¼‰\n",
    "    else:\n",
    "        é€‰æ‹©å½“å‰å¹³å‡æ”¶ç›Šæœ€é«˜çš„è‡‚ï¼ˆåˆ©ç”¨ï¼‰\n",
    "    \n",
    "    è§‚å¯Ÿæ”¶ç›Šï¼Œæ›´æ–°ç»Ÿè®¡é‡\n",
    "```\n",
    "\n",
    "### ä¼˜ç¼ºç‚¹\n",
    "\n",
    "âœ… **ä¼˜ç‚¹**ï¼šç®€å•ã€ç›´è§‚ã€æ˜“å®ç°  \n",
    "âŒ **ç¼ºç‚¹**ï¼šæ¢ç´¢æ˜¯ç›²ç›®çš„ï¼Œä¸è€ƒè™‘ä¸ç¡®å®šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms: è‡‚çš„æ•°é‡\n",
    "            epsilon: æ¢ç´¢æ¦‚ç‡\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)  # æ¯ä¸ªè‡‚è¢«é€‰æ‹©çš„æ¬¡æ•°\n",
    "        self.values = np.zeros(n_arms)  # æ¯ä¸ªè‡‚çš„å¹³å‡æ”¶ç›Š\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"é€‰æ‹©ä¸€ä¸ªè‡‚\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜\n",
    "            return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"æ›´æ–°ç»Ÿè®¡é‡\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        # å¢é‡å¼æ›´æ–°å‡å€¼\n",
    "        self.values[arm] = ((n - 1) / n) * self.values[arm] + (1 / n) * reward\n",
    "    \n",
    "    def run(self, true_rates: np.ndarray, n_rounds: int) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"è¿è¡Œå®éªŒ\"\"\"\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        for t in range(n_rounds):\n",
    "            arm = self.select_arm()\n",
    "            reward = np.random.binomial(1, true_rates[arm])\n",
    "            self.update(arm, reward)\n",
    "            actions.append(arm)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return actions, rewards\n",
    "\n",
    "# è¿è¡Œ Epsilon-Greedy\n",
    "true_rates = np.array([0.05, 0.08, 0.06])\n",
    "n_rounds = 10000\n",
    "\n",
    "eg = EpsilonGreedy(n_arms=3, epsilon=0.1)\n",
    "eg_actions, eg_rewards = eg.run(true_rates, n_rounds)\n",
    "\n",
    "# è®¡ç®—ç´¯ç§¯é—æ†¾\n",
    "best_rate = max(true_rates)\n",
    "eg_regret = []\n",
    "regret = 0\n",
    "for action in eg_actions:\n",
    "    regret += best_rate - true_rates[action]\n",
    "    eg_regret.append(regret)\n",
    "\n",
    "print(f\"\\nğŸ¯ Epsilon-Greedy (Îµ=0.1) ç»“æœï¼š\")\n",
    "print(f\"æ€»é—æ†¾ï¼š{eg_regret[-1]:.2f}\")\n",
    "print(f\"æ¯ä¸ªè‡‚çš„é€‰æ‹©æ¬¡æ•°ï¼š{eg.counts}\")\n",
    "print(f\"ä¼°è®¡çš„æ”¶ç›Šï¼š{eg.values}\")\n",
    "print(f\"çœŸå®æ”¶ç›Šï¼š{true_rates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 UCB (Upper Confidence Bound)ï¼šä¹è§‚æ¢ç´¢\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**\"Optimism in the Face of Uncertainty\"**ï¼ˆé¢å¯¹ä¸ç¡®å®šæ€§ä¿æŒä¹è§‚ï¼‰\n",
    "\n",
    "- ä¸ç¡®å®šæ€§å¤§çš„è‡‚ï¼Œç»™äºˆæ›´é«˜çš„æ¢ç´¢ä¼˜å…ˆçº§\n",
    "- å¹³è¡¡ã€Œä¼°è®¡æ”¶ç›Šã€å’Œã€Œä¸ç¡®å®šæ€§ã€\n",
    "\n",
    "### å…¬å¼\n",
    "\n",
    "é€‰æ‹©å¾—åˆ†æœ€é«˜çš„è‡‚ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_t = \\arg\\max_a \\left[ \\hat{\\mu}_a + \\sqrt{\\frac{2 \\ln t}{n_a}} \\right]\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\hat{\\mu}_a$ï¼šè‡‚ $a$ çš„å¹³å‡æ”¶ç›Šï¼ˆexploitationï¼‰\n",
    "- $\\sqrt{\\frac{2 \\ln t}{n_a}}$ï¼šç½®ä¿¡åŒºé—´åŠå¾„ï¼ˆexploration bonusï¼‰\n",
    "- $t$ï¼šå½“å‰è½®æ•°\n",
    "- $n_a$ï¼šè‡‚ $a$ è¢«é€‰æ‹©çš„æ¬¡æ•°\n",
    "\n",
    "### ç›´è§‚ç†è§£\n",
    "\n",
    "- **ç¬¬ä¸€é¡¹**ï¼ˆ$\\hat{\\mu}_a$ï¼‰ï¼šå·²çŸ¥ä¿¡æ¯ï¼Œé¼“åŠ±åˆ©ç”¨\n",
    "- **ç¬¬äºŒé¡¹**ï¼ˆ$\\sqrt{\\frac{2 \\ln t}{n_a}}$ï¼‰ï¼šä¸ç¡®å®šæ€§å¥–åŠ±ï¼Œé¼“åŠ±æ¢ç´¢\n",
    "  - $n_a$ è¶Šå°ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œbonus è¶Šå¤§\n",
    "  - $t$ è¶Šå¤§ï¼Œéœ€è¦æ›´å¤šè¯æ®æ‰èƒ½ç¡®å®šæŸä¸ªè‡‚æ˜¯å·®çš„\n",
    "\n",
    "### ç”Ÿæ´»ç±»æ¯”\n",
    "\n",
    "æ‹›è˜æ—¶ï¼š\n",
    "- **æœ‰ç»éªŒçš„å€™é€‰äºº**ï¼šèƒ½åŠ›å·²çŸ¥ï¼ˆ$\\hat{\\mu}$ é«˜ï¼‰ï¼Œä½†å¤©èŠ±æ¿ä¹Ÿæ˜ç¡®ï¼ˆbonus å°ï¼‰\n",
    "- **åº”å±Šæ¯•ä¸šç”Ÿ**ï¼šæ½œåŠ›æœªçŸ¥ï¼ˆ$\\hat{\\mu}$ ä½ï¼‰ï¼Œä½†å¯èƒ½æ˜¯å¤©æ‰ï¼ˆbonus å¤§ï¼‰\n",
    "\n",
    "UCB ä¼šç»™åº”å±Šç”Ÿæ›´å¤šæœºä¼šï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    def __init__(self, n_arms: int, c: float = 2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms: è‡‚çš„æ•°é‡\n",
    "            c: æ¢ç´¢ç³»æ•°ï¼ˆé»˜è®¤ sqrt(2) çš„å¹³æ–¹ï¼‰\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.t = 0  # æ€»è½®æ•°\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"é€‰æ‹©ä¸€ä¸ªè‡‚\"\"\"\n",
    "        # åˆå§‹åŒ–ï¼šæ¯ä¸ªè‡‚è‡³å°‘é€‰æ‹©ä¸€æ¬¡\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        # UCB å¾—åˆ†\n",
    "        ucb_values = self.values + np.sqrt(\n",
    "            self.c * np.log(self.t) / self.counts\n",
    "        )\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"æ›´æ–°ç»Ÿè®¡é‡\"\"\"\n",
    "        self.t += 1\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * self.values[arm] + (1 / n) * reward\n",
    "    \n",
    "    def run(self, true_rates: np.ndarray, n_rounds: int) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"è¿è¡Œå®éªŒ\"\"\"\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        for t in range(n_rounds):\n",
    "            arm = self.select_arm()\n",
    "            reward = np.random.binomial(1, true_rates[arm])\n",
    "            self.update(arm, reward)\n",
    "            actions.append(arm)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return actions, rewards\n",
    "\n",
    "# è¿è¡Œ UCB\n",
    "ucb = UCB(n_arms=3)\n",
    "ucb_actions, ucb_rewards = ucb.run(true_rates, n_rounds)\n",
    "\n",
    "# è®¡ç®—ç´¯ç§¯é—æ†¾\n",
    "ucb_regret = []\n",
    "regret = 0\n",
    "for action in ucb_actions:\n",
    "    regret += best_rate - true_rates[action]\n",
    "    ucb_regret.append(regret)\n",
    "\n",
    "print(f\"\\nğŸ¯ UCB ç»“æœï¼š\")\n",
    "print(f\"æ€»é—æ†¾ï¼š{ucb_regret[-1]:.2f}\")\n",
    "print(f\"æ¯ä¸ªè‡‚çš„é€‰æ‹©æ¬¡æ•°ï¼š{ucb.counts}\")\n",
    "print(f\"ä¼°è®¡çš„æ”¶ç›Šï¼š{ucb.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Thompson Samplingï¼šè´å¶æ–¯æ€ç»´\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**\"Probability Matching\"**ï¼ˆæ¦‚ç‡åŒ¹é…ï¼‰\n",
    "\n",
    "- ä¸ºæ¯ä¸ªè‡‚ç»´æŠ¤ä¸€ä¸ªæ”¶ç›Šåˆ†å¸ƒçš„ã€Œä¿¡å¿µã€\n",
    "- ä»æ¯ä¸ªè‡‚çš„ä¿¡å¿µåˆ†å¸ƒä¸­é‡‡æ ·\n",
    "- é€‰æ‹©é‡‡æ ·å€¼æœ€å¤§çš„è‡‚\n",
    "\n",
    "### ç®—æ³•æµç¨‹ï¼ˆä¼¯åŠªåˆ©å¥–åŠ±ï¼‰\n",
    "\n",
    "å¯¹äºæ¯ä¸ªè‡‚ $a$ï¼Œç»´æŠ¤ Beta åˆ†å¸ƒ $\\text{Beta}(\\alpha_a, \\beta_a)$ï¼š\n",
    "- $\\alpha_a$ï¼šæˆåŠŸæ¬¡æ•° + 1\n",
    "- $\\beta_a$ï¼šå¤±è´¥æ¬¡æ•° + 1\n",
    "\n",
    "æ¯è½®ï¼š\n",
    "1. ä»æ¯ä¸ªè‡‚çš„ Beta åˆ†å¸ƒä¸­é‡‡æ ·ï¼š$\\theta_a \\sim \\text{Beta}(\\alpha_a, \\beta_a)$\n",
    "2. é€‰æ‹© $a_t = \\arg\\max_a \\theta_a$\n",
    "3. è§‚å¯Ÿå¥–åŠ± $r_t$ï¼Œæ›´æ–°ï¼š\n",
    "   - å¦‚æœ $r_t = 1$ï¼š$\\alpha_{a_t} \\leftarrow \\alpha_{a_t} + 1$\n",
    "   - å¦‚æœ $r_t = 0$ï¼š$\\beta_{a_t} \\leftarrow \\beta_{a_t} + 1$\n",
    "\n",
    "### ä¸ºä»€ä¹ˆç”¨ Beta åˆ†å¸ƒï¼Ÿ\n",
    "\n",
    "- **Beta åˆ†å¸ƒ**æ˜¯ [0, 1] ä¸Šçš„è¿ç»­åˆ†å¸ƒï¼Œé€‚åˆå»ºæ¨¡æ¦‚ç‡\n",
    "- **å…±è½­å…ˆéªŒ**ï¼šBeta æ˜¯äºŒé¡¹åˆ†å¸ƒçš„å…±è½­å…ˆéªŒï¼Œæ›´æ–°å…¬å¼ç®€å•\n",
    "- **ç›´è§‚è§£é‡Š**ï¼š$\\text{Beta}(\\alpha, \\beta)$ çš„å‡å€¼æ˜¯ $\\frac{\\alpha}{\\alpha + \\beta}$ï¼Œå°±æ˜¯æˆåŠŸç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    def __init__(self, n_arms: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms: è‡‚çš„æ•°é‡\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        # Beta åˆ†å¸ƒå‚æ•°ï¼ˆä½¿ç”¨å‡åŒ€å…ˆéªŒ Beta(1, 1)ï¼‰\n",
    "        self.alpha = np.ones(n_arms)\n",
    "        self.beta = np.ones(n_arms)\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"é€‰æ‹©ä¸€ä¸ªè‡‚\"\"\"\n",
    "        # ä»æ¯ä¸ªè‡‚çš„ Beta åˆ†å¸ƒä¸­é‡‡æ ·\n",
    "        samples = np.random.beta(self.alpha, self.beta)\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"æ›´æ–° Beta åˆ†å¸ƒå‚æ•°\"\"\"\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "    \n",
    "    def run(self, true_rates: np.ndarray, n_rounds: int) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"è¿è¡Œå®éªŒ\"\"\"\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        for t in range(n_rounds):\n",
    "            arm = self.select_arm()\n",
    "            reward = np.random.binomial(1, true_rates[arm])\n",
    "            self.update(arm, reward)\n",
    "            actions.append(arm)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return actions, rewards\n",
    "    \n",
    "    def get_mean_estimates(self) -> np.ndarray:\n",
    "        \"\"\"è·å–æ¯ä¸ªè‡‚çš„æ”¶ç›Šä¼°è®¡\"\"\"\n",
    "        return self.alpha / (self.alpha + self.beta)\n",
    "\n",
    "# è¿è¡Œ Thompson Sampling\n",
    "ts = ThompsonSampling(n_arms=3)\n",
    "ts_actions, ts_rewards = ts.run(true_rates, n_rounds)\n",
    "\n",
    "# è®¡ç®—ç´¯ç§¯é—æ†¾\n",
    "ts_regret = []\n",
    "regret = 0\n",
    "for action in ts_actions:\n",
    "    regret += best_rate - true_rates[action]\n",
    "    ts_regret.append(regret)\n",
    "\n",
    "print(f\"\\nğŸ¯ Thompson Sampling ç»“æœï¼š\")\n",
    "print(f\"æ€»é—æ†¾ï¼š{ts_regret[-1]:.2f}\")\n",
    "print(f\"Alpha å‚æ•°ï¼š{ts.alpha}\")\n",
    "print(f\"Beta å‚æ•°ï¼š{ts.beta}\")\n",
    "print(f\"ä¼°è®¡çš„æ”¶ç›Šï¼š{ts.get_mean_estimates()}\")\n",
    "print(f\"çœŸå®æ”¶ç›Šï¼š{true_rates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è§†åŒ– Thompson Sampling çš„ä¿¡å¿µæ¼”åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°è¿è¡Œ Thompson Samplingï¼Œè®°å½•ä¿¡å¿µæ¼”åŒ–\n",
    "ts_evolve = ThompsonSampling(n_arms=3)\n",
    "snapshots = [100, 500, 1000, 5000, 10000]  # è®°å½•è¿™äº›æ—¶é—´ç‚¹çš„åˆ†å¸ƒ\n",
    "distributions = {t: [] for t in snapshots}\n",
    "\n",
    "for t in range(1, n_rounds + 1):\n",
    "    arm = ts_evolve.select_arm()\n",
    "    reward = np.random.binomial(1, true_rates[arm])\n",
    "    ts_evolve.update(arm, reward)\n",
    "    \n",
    "    if t in snapshots:\n",
    "        distributions[t] = {\n",
    "            'alpha': ts_evolve.alpha.copy(),\n",
    "            'beta': ts_evolve.beta.copy()\n",
    "        }\n",
    "\n",
    "# ç»˜åˆ¶ä¿¡å¿µæ¼”åŒ–\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=[f't = {t}' for t in snapshots],\n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "x = np.linspace(0, 0.15, 1000)\n",
    "arm_names = ['çº¢è‰² (5%)', 'è“è‰² (8%)', 'ç»¿è‰² (6%)']\n",
    "colors_list = [COLORS['red'], COLORS['blue'], COLORS['green']]\n",
    "\n",
    "for idx, t in enumerate(snapshots):\n",
    "    row = idx // 3 + 1\n",
    "    col = idx % 3 + 1\n",
    "    \n",
    "    alpha = distributions[t]['alpha']\n",
    "    beta = distributions[t]['beta']\n",
    "    \n",
    "    for arm in range(3):\n",
    "        y = stats.beta.pdf(x, alpha[arm], beta[arm])\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x, y=y,\n",
    "                mode='lines',\n",
    "                name=arm_names[arm],\n",
    "                line=dict(color=colors_list[arm], width=2),\n",
    "                showlegend=(idx == 0),\n",
    "                legendgroup=f'arm{arm}'\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # æ ‡æ³¨çœŸå®å€¼\n",
    "        fig.add_vline(\n",
    "            x=true_rates[arm],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=colors_list[arm],\n",
    "            line_width=1,\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text=\"è½¬åŒ–ç‡\", range=[0, 0.15])\n",
    "fig.update_yaxes(title_text=\"æ¦‚ç‡å¯†åº¦\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Thompson Sampling ä¿¡å¿µæ¼”åŒ–ï¼ˆè™šçº¿ä¸ºçœŸå®å€¼ï¼‰',\n",
    "    template='plotly_white',\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è§‚å¯Ÿï¼š\")\n",
    "print(\"- éšç€æ—¶é—´æ¨ç§»ï¼Œåˆ†å¸ƒè¶Šæ¥è¶Šçª„ï¼ˆä¸ç¡®å®šæ€§é™ä½ï¼‰\")\n",
    "print(\"- è“è‰²çš„åˆ†å¸ƒæœ€ç»ˆé›†ä¸­åœ¨ 8% é™„è¿‘\")\n",
    "print(\"- çº¢è‰²å’Œç»¿è‰²çš„åˆ†å¸ƒè¢«æ¢ç´¢å¾—æ›´å°‘ï¼Œä½†ä¹Ÿé€æ¸æ”¶æ•›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 ç®—æ³•å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ä¸‰ç§ç®—æ³•å’Œ A/B Testing\n",
    "fig = go.Figure()\n",
    "\n",
    "algorithms = [\n",
    "    ('A/B Testing', cumulative_regret, COLORS['gray']),\n",
    "    ('Epsilon-Greedy', eg_regret, COLORS['yellow']),\n",
    "    ('UCB', ucb_regret, COLORS['purple']),\n",
    "    ('Thompson Sampling', ts_regret, COLORS['blue'])\n",
    "]\n",
    "\n",
    "for name, regret, color in algorithms:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(regret) + 1)),\n",
    "        y=regret,\n",
    "        mode='lines',\n",
    "        name=name,\n",
    "        line=dict(color=color, width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ç®—æ³•å¯¹æ¯”ï¼šç´¯ç§¯é—æ†¾éšæ—¶é—´å˜åŒ–',\n",
    "    xaxis_title='è½®æ•°',\n",
    "    yaxis_title='ç´¯ç§¯é—æ†¾',\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ç»Ÿè®¡è¡¨\n",
    "results_df = pd.DataFrame({\n",
    "    'ç®—æ³•': ['A/B Testing', 'Epsilon-Greedy', 'UCB', 'Thompson Sampling'],\n",
    "    'æ€»é—æ†¾': [\n",
    "        cumulative_regret[-1],\n",
    "        eg_regret[-1],\n",
    "        ucb_regret[-1],\n",
    "        ts_regret[-1]\n",
    "    ],\n",
    "    'å¹³å‡é—æ†¾': [\n",
    "        cumulative_regret[-1] / n_rounds,\n",
    "        eg_regret[-1] / n_rounds,\n",
    "        ucb_regret[-1] / n_rounds,\n",
    "        ts_regret[-1] / n_rounds\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_df['ç›¸å¯¹ A/B Testing'] = (\n",
    "    (results_df['æ€»é—æ†¾'] / results_df.loc[0, 'æ€»é—æ†¾'] - 1) * 100\n",
    ").round(1).astype(str) + '%'\n",
    "\n",
    "print(\"\\nğŸ“Š ç®—æ³•å¯¹æ¯”ç»“æœï¼š\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ¯ ç»“è®ºï¼š\")\n",
    "print(\"- Thompson Sampling è¡¨ç°æœ€å¥½ï¼Œé—æ†¾æœ€å°\")\n",
    "print(\"- UCB æ¬¡ä¹‹ï¼Œç†è®ºä¿è¯å¼º\")\n",
    "print(\"- Epsilon-Greedy ç®€å•ä½†ä¸å¤Ÿé«˜æ•ˆ\")\n",
    "print(\"- A/B Testing é—æ†¾æœ€å¤§ï¼Œå› ä¸ºæ¢ç´¢é˜¶æ®µå›ºå®š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ TODO 1: è°ƒæ•´ Epsilon-Greedy çš„æ¢ç´¢ç‡\n",
    "\n",
    "å°è¯•ä¸åŒçš„ $\\epsilon$ å€¼ï¼ˆ0.05, 0.1, 0.2ï¼‰ï¼Œè§‚å¯Ÿå¯¹é—æ†¾çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç­”æ¡ˆï¼šæµ‹è¯•ä¸åŒçš„ epsilon å€¼\nepsilon_values = [0.05, 0.1, 0.2]\nresults = {}\n\nfor eps in epsilon_values:\n    eg_test = EpsilonGreedy(n_arms=3, epsilon=eps)\n    actions, rewards = eg_test.run(true_rates, n_rounds)\n    \n    # è®¡ç®—ç´¯ç§¯é—æ†¾\n    regret = []\n    cumulative = 0\n    for action in actions:\n        cumulative += best_rate - true_rates[action]\n        regret.append(cumulative)\n    \n    results[eps] = {\n        'regret': regret,\n        'total': regret[-1],\n        'counts': eg_test.counts\n    }\n\n# å¯è§†åŒ–å¯¹æ¯”\nfig = go.Figure()\n\nfor eps in epsilon_values:\n    fig.add_trace(go.Scatter(\n        x=list(range(1, n_rounds + 1)),\n        y=results[eps]['regret'],\n        mode='lines',\n        name=f'Îµ = {eps}',\n        line=dict(width=2)\n    ))\n\nfig.update_layout(\n    title='ä¸åŒ Epsilon å€¼çš„é—æ†¾å¯¹æ¯”',\n    xaxis_title='è½®æ•°',\n    yaxis_title='ç´¯ç§¯é—æ†¾',\n    template='plotly_white',\n    height=500\n)\n\nfig.show()\n\nprint(\"\\nğŸ“Š ä¸åŒ Epsilon å€¼çš„ç»“æœå¯¹æ¯”ï¼š\")\nfor eps in epsilon_values:\n    print(f\"\\nÎµ = {eps}:\")\n    print(f\"  æ€»é—æ†¾: {results[eps]['total']:.2f}\")\n    print(f\"  æ¯ä¸ªè‡‚é€‰æ‹©æ¬¡æ•°: {results[eps]['counts']}\")\n\nprint(\"\\nğŸ’¡ åˆ†æï¼š\")\nprint(\"- Îµ å¤ªå° (0.05): æ¢ç´¢ä¸è¶³ï¼Œå¯èƒ½é”™è¿‡æœ€ä¼˜è‡‚\")\nprint(\"- Îµ å¤ªå¤§ (0.2): è¿‡åº¦æ¢ç´¢ï¼Œæµªè´¹åœ¨æ¬¡ä¼˜è‡‚ä¸Š\")\nprint(\"- Îµ = 0.1: åœ¨æœ¬ä¾‹ä¸­è¡¨ç°è¾ƒå¥½ï¼Œå¹³è¡¡äº†æ¢ç´¢å’Œåˆ©ç”¨\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Contextual Bandit\n",
    "\n",
    "## 3.1 ä» MAB åˆ° Contextual Bandit\n",
    "\n",
    "### ç»å…¸ MAB çš„å±€é™\n",
    "\n",
    "- **å‡è®¾**ï¼šæ‰€æœ‰ç”¨æˆ·ç›¸åŒï¼Œæœ€ä¼˜è‡‚æ˜¯å›ºå®šçš„\n",
    "- **ç°å®**ï¼šä¸åŒç”¨æˆ·åå¥½ä¸åŒ\n",
    "  - å¹´è½»äººå–œæ¬¢è“è‰²æŒ‰é’®\n",
    "  - è€å¹´äººå–œæ¬¢çº¢è‰²æŒ‰é’®\n",
    "\n",
    "### Contextual Bandit\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šæ ¹æ®ä¸Šä¸‹æ–‡ï¼ˆç”¨æˆ·ç‰¹å¾ï¼‰é€‰æ‹©æœ€ä¼˜è‡‚\n",
    "\n",
    "- **è¾“å…¥**ï¼šä¸Šä¸‹æ–‡ $x_t \\in \\mathbb{R}^d$ï¼ˆç”¨æˆ·å¹´é¾„ã€æ€§åˆ«ã€åœ°åŸŸç­‰ï¼‰\n",
    "- **åŠ¨ä½œ**ï¼šé€‰æ‹©è‡‚ $a_t$\n",
    "- **åé¦ˆ**ï¼šè§‚å¯Ÿå¥–åŠ± $r_t$\n",
    "- **ç›®æ ‡**ï¼šå­¦ä¹ ç­–ç•¥ $\\pi(x) \\to a$\n",
    "\n",
    "### åº”ç”¨åœºæ™¯\n",
    "\n",
    "- **ä¸ªæ€§åŒ–æ¨è**ï¼šæ ¹æ®ç”¨æˆ·å†å²æ¨èå•†å“\n",
    "- **æ–°é—»æ¨é€**ï¼šæ ¹æ®é˜…è¯»åå¥½æ¨é€æ–‡ç« \n",
    "- **å¹¿å‘ŠæŠ•æ”¾**ï¼šæ ¹æ®ç”¨æˆ·ç”»åƒå±•ç¤ºå¹¿å‘Š\n",
    "\n",
    "## 3.2 LinUCB ç®—æ³•\n",
    "\n",
    "### æ ¸å¿ƒå‡è®¾\n",
    "\n",
    "å¥–åŠ±ä¸ä¸Šä¸‹æ–‡çš„å…³ç³»æ˜¯**çº¿æ€§çš„**ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r_{t,a} = x_t^\\top \\theta_a + \\epsilon_t\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $x_t \\in \\mathbb{R}^d$ï¼šä¸Šä¸‹æ–‡å‘é‡\n",
    "- $\\theta_a \\in \\mathbb{R}^d$ï¼šè‡‚ $a$ çš„å‚æ•°å‘é‡\n",
    "- $\\epsilon_t$ï¼šå™ªå£°\n",
    "\n",
    "### ç®—æ³•æµç¨‹\n",
    "\n",
    "å¯¹äºæ¯ä¸ªè‡‚ $a$ï¼Œç»´æŠ¤ï¼š\n",
    "- $A_a = I_d + \\sum_{t: a_t=a} x_t x_t^\\top$ ï¼ˆè®¾è®¡çŸ©é˜µï¼‰\n",
    "- $b_a = \\sum_{t: a_t=a} r_t x_t$ ï¼ˆå¥–åŠ±å‘é‡ï¼‰\n",
    "\n",
    "åœ¨æ—¶åˆ» $t$ï¼š\n",
    "1. è®¡ç®—æ¯ä¸ªè‡‚çš„ UCBï¼š\n",
    "\n",
    "$$\n",
    "\\text{UCB}_a(x_t) = \\hat{\\theta}_a^\\top x_t + \\alpha \\sqrt{x_t^\\top A_a^{-1} x_t}\n",
    "$$\n",
    "\n",
    "   å…¶ä¸­ $\\hat{\\theta}_a = A_a^{-1} b_a$ï¼ˆå²­å›å½’ä¼°è®¡ï¼‰\n",
    "\n",
    "2. é€‰æ‹© $a_t = \\arg\\max_a \\text{UCB}_a(x_t)$\n",
    "3. è§‚å¯Ÿå¥–åŠ± $r_t$ï¼Œæ›´æ–° $A_{a_t}$ å’Œ $b_{a_t}$\n",
    "\n",
    "### ç›´è§‚è§£é‡Š\n",
    "\n",
    "- **ç¬¬ä¸€é¡¹**ï¼ˆ$\\hat{\\theta}_a^\\top x_t$ï¼‰ï¼šé¢„æµ‹å¥–åŠ±\n",
    "- **ç¬¬äºŒé¡¹**ï¼ˆ$\\alpha \\sqrt{x_t^\\top A_a^{-1} x_t}$ï¼‰ï¼šä¸ç¡®å®šæ€§ï¼ˆè¶Šå°‘è§çš„ä¸Šä¸‹æ–‡ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB:\n",
    "    def __init__(self, n_arms: int, n_features: int, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms: è‡‚çš„æ•°é‡\n",
    "            n_features: ç‰¹å¾ç»´åº¦\n",
    "            alpha: æ¢ç´¢ç³»æ•°\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªè‡‚åˆå§‹åŒ–å‚æ•°\n",
    "        self.A = [np.identity(n_features) for _ in range(n_arms)]\n",
    "        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n",
    "    \n",
    "    def select_arm(self, context: np.ndarray) -> int:\n",
    "        \"\"\"æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©è‡‚\"\"\"\n",
    "        ucb_values = []\n",
    "        \n",
    "        for arm in range(self.n_arms):\n",
    "            # è®¡ç®—å‚æ•°ä¼°è®¡\n",
    "            A_inv = np.linalg.inv(self.A[arm])\n",
    "            theta = A_inv @ self.b[arm]\n",
    "            \n",
    "            # è®¡ç®— UCB\n",
    "            ucb = theta @ context + self.alpha * np.sqrt(\n",
    "                context @ A_inv @ context\n",
    "            )\n",
    "            ucb_values.append(ucb)\n",
    "        \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm: int, context: np.ndarray, reward: float):\n",
    "        \"\"\"æ›´æ–°å‚æ•°\"\"\"\n",
    "        self.A[arm] += np.outer(context, context)\n",
    "        self.b[arm] += reward * context\n",
    "    \n",
    "    def get_theta(self, arm: int) -> np.ndarray:\n",
    "        \"\"\"è·å–è‡‚çš„å‚æ•°ä¼°è®¡\"\"\"\n",
    "        return np.linalg.inv(self.A[arm]) @ self.b[arm]\n",
    "\n",
    "# æ¨¡æ‹Ÿåœºæ™¯ï¼šä¸åŒå¹´é¾„ç”¨æˆ·å¯¹æŒ‰é’®é¢œè‰²çš„åå¥½\n",
    "# å‡è®¾ç‰¹å¾ï¼š[å¹´é¾„ / 100, æ€§åˆ«(0/1)]\n",
    "# çœŸå®å‚æ•°ï¼š\n",
    "# - çº¢è‰²ï¼šå¹´è½»äººä¸å–œæ¬¢ï¼Œç”·æ€§æ›´å–œæ¬¢\n",
    "# - è“è‰²ï¼šå¹´è½»äººå–œæ¬¢ï¼Œæ€§åˆ«æ— å…³\n",
    "# - ç»¿è‰²ï¼šè€å¹´äººå–œæ¬¢ï¼Œå¥³æ€§æ›´å–œæ¬¢\n",
    "\n",
    "true_theta = np.array([\n",
    "    [-0.2, 0.1],   # çº¢è‰²\n",
    "    [0.3, 0.0],    # è“è‰²\n",
    "    [0.2, -0.1]    # ç»¿è‰²\n",
    "])\n",
    "\n",
    "def generate_context() -> np.ndarray:\n",
    "    \"\"\"ç”Ÿæˆéšæœºä¸Šä¸‹æ–‡\"\"\"\n",
    "    age = np.random.uniform(18, 70) / 100\n",
    "    gender = np.random.randint(2)\n",
    "    return np.array([age, gender])\n",
    "\n",
    "def get_reward(arm: int, context: np.ndarray) -> float:\n",
    "    \"\"\"æ ¹æ®ä¸Šä¸‹æ–‡å’Œè‡‚ç”Ÿæˆå¥–åŠ±\"\"\"\n",
    "    mean_reward = context @ true_theta[arm]\n",
    "    # æ˜ å°„åˆ° [0, 1]\n",
    "    prob = 1 / (1 + np.exp(-mean_reward * 10))\n",
    "    return np.random.binomial(1, prob)\n",
    "\n",
    "# è¿è¡Œ LinUCB\n",
    "n_rounds = 5000\n",
    "linucb = LinUCB(n_arms=3, n_features=2, alpha=0.5)\n",
    "\n",
    "linucb_actions = []\n",
    "linucb_rewards = []\n",
    "contexts = []\n",
    "\n",
    "for t in range(n_rounds):\n",
    "    context = generate_context()\n",
    "    arm = linucb.select_arm(context)\n",
    "    reward = get_reward(arm, context)\n",
    "    \n",
    "    linucb.update(arm, context, reward)\n",
    "    \n",
    "    linucb_actions.append(arm)\n",
    "    linucb_rewards.append(reward)\n",
    "    contexts.append(context)\n",
    "\n",
    "# æ‰“å°å­¦åˆ°çš„å‚æ•°\n",
    "print(\"\\nğŸ¯ LinUCB å­¦ä¹ ç»“æœï¼š\")\n",
    "print(\"\\nçœŸå®å‚æ•° Î¸:\")\n",
    "print(\"çº¢è‰²:\", true_theta[0])\n",
    "print(\"è“è‰²:\", true_theta[1])\n",
    "print(\"ç»¿è‰²:\", true_theta[2])\n",
    "\n",
    "print(\"\\nå­¦ä¹ åˆ°çš„å‚æ•° Î¸:\")\n",
    "for arm in range(3):\n",
    "    arm_names = ['çº¢è‰²', 'è“è‰²', 'ç»¿è‰²']\n",
    "    print(f\"{arm_names[arm]}:\", linucb.get_theta(arm))\n",
    "\n",
    "print(f\"\\nå¹³å‡å¥–åŠ±: {np.mean(linucb_rewards):.3f}\")\n",
    "print(f\"æ¯ä¸ªè‡‚çš„é€‰æ‹©æ¬¡æ•°: {np.bincount(linucb_actions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è§†åŒ–ï¼šä¸åŒç”¨æˆ·ç¾¤ä½“çš„æœ€ä¼˜è‡‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç½‘æ ¼ä¸Šçš„é¢„æµ‹\n",
    "age_range = np.linspace(0.18, 0.70, 50)\n",
    "gender_vals = [0, 1]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['ç”·æ€§ç”¨æˆ·', 'å¥³æ€§ç”¨æˆ·']\n",
    ")\n",
    "\n",
    "arm_names = ['çº¢è‰²', 'è“è‰²', 'ç»¿è‰²']\n",
    "colors_list = [COLORS['red'], COLORS['blue'], COLORS['green']]\n",
    "\n",
    "for gender_idx, gender in enumerate(gender_vals):\n",
    "    for arm in range(3):\n",
    "        theta = linucb.get_theta(arm)\n",
    "        predicted_rewards = []\n",
    "        \n",
    "        for age in age_range:\n",
    "            context = np.array([age, gender])\n",
    "            reward = context @ theta\n",
    "            predicted_rewards.append(reward)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=age_range * 100,\n",
    "                y=predicted_rewards,\n",
    "                mode='lines',\n",
    "                name=arm_names[arm],\n",
    "                line=dict(color=colors_list[arm], width=2),\n",
    "                showlegend=(gender_idx == 0),\n",
    "                legendgroup=f'arm{arm}'\n",
    "            ),\n",
    "            row=1, col=gender_idx + 1\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text=\"å¹´é¾„\")\n",
    "fig.update_yaxes(title_text=\"é¢„æµ‹å¥–åŠ±\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='LinUCB å­¦ä¹ åˆ°çš„ç­–ç•¥ï¼šä¸åŒå¹´é¾„å’Œæ€§åˆ«çš„æœ€ä¼˜æŒ‰é’®',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è§‚å¯Ÿï¼š\")\n",
    "print(\"- å¯¹äºç”·æ€§ç”¨æˆ·ï¼Œå¹´è½»äººæ›´å–œæ¬¢è“è‰²ï¼Œå¹´é•¿è€…åå¥½ç»¿è‰²\")\n",
    "print(\"- å¯¹äºå¥³æ€§ç”¨æˆ·ï¼Œæ•´ä½“åå¥½ç•¥æœ‰ä¸åŒ\")\n",
    "print(\"- è¿™å°±æ˜¯ä¸ªæ€§åŒ–æ¨èçš„åŸºç¡€ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ç¥ç»ç½‘ç»œæ–¹æ³•ï¼ˆNeural Contextual Banditï¼‰\n",
    "\n",
    "### LinUCB çš„å±€é™\n",
    "\n",
    "- **çº¿æ€§å‡è®¾**è¿‡å¼ºï¼šçœŸå®ä¸–ç•Œä¸­ï¼Œå¥–åŠ±ä¸ç‰¹å¾çš„å…³ç³»å¾€å¾€æ˜¯éçº¿æ€§çš„\n",
    "- **ç‰¹å¾å·¥ç¨‹**ï¼šéœ€è¦æ‰‹å·¥è®¾è®¡ç‰¹å¾\n",
    "\n",
    "### Neural Bandit\n",
    "\n",
    "ç”¨ç¥ç»ç½‘ç»œ $f_\\theta(x, a)$ å»ºæ¨¡å¥–åŠ±ï¼š\n",
    "\n",
    "$$\n",
    "r_{t,a} = f_\\theta(x_t, a) + \\epsilon_t\n",
    "$$\n",
    "\n",
    "### æŒ‘æˆ˜ï¼šå¦‚ä½•æ¢ç´¢ï¼Ÿ\n",
    "\n",
    "1. **Epsilon-Greedy + NN**ï¼šç®€å•ä½†ä½æ•ˆ\n",
    "2. **Dropout as Bayesian Approximation**ï¼šç”¨ Dropout ä¼°è®¡ä¸ç¡®å®šæ€§\n",
    "3. **Bootstrap**ï¼šè®­ç»ƒå¤šä¸ªç½‘ç»œï¼Œç”¨åˆ†æ­§åº¦é‡ä¸ç¡®å®šæ€§\n",
    "4. **Neural Linear (NeuralLinear)**ï¼šç»“åˆ NN å’Œ LinUCB\n",
    "\n",
    "### NeuralLinear æ€æƒ³\n",
    "\n",
    "1. ç”¨ç¥ç»ç½‘ç»œæå–ç‰¹å¾ï¼š$\\phi(x) = \\text{NN}(x)$\n",
    "2. åœ¨ç‰¹å¾ç©ºé—´ä¸Šç”¨ LinUCBï¼š$r = \\phi(x)^\\top \\theta_a$\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ TODO 2: å®ç° Epsilon-Greedy + Neural Network\n",
    "\n",
    "ä½¿ç”¨ç®€å•çš„ç¥ç»ç½‘ç»œï¼ˆå¦‚ 2 å±‚ MLPï¼‰é¢„æµ‹å¥–åŠ±ï¼Œç»“åˆ Epsilon-Greedy æ¢ç´¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç­”æ¡ˆï¼šNeural Epsilon-Greedy å®ç°\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nclass NeuralEpsilonGreedy:\n    def __init__(self, n_arms: int, n_features: int, epsilon: float = 0.1,\n                 retrain_interval: int = 100):\n        self.n_arms = n_arms\n        self.n_features = n_features\n        self.epsilon = epsilon\n        self.retrain_interval = retrain_interval\n        \n        # ä¸ºæ¯ä¸ªè‡‚è®­ç»ƒä¸€ä¸ªæ¨¡å‹\n        self.models = [MLPRegressor(hidden_layer_sizes=(32, 16), \n                                    max_iter=100, random_state=42)\n                       for _ in range(n_arms)]\n        self.scalers = [StandardScaler() for _ in range(n_arms)]\n        \n        # å­˜å‚¨è®­ç»ƒæ•°æ®\n        self.data = {i: {'X': [], 'y': []} for i in range(n_arms)}\n        self.t = 0\n    \n    def select_arm(self, context: np.ndarray) -> int:\n        # å‰æœŸéšæœºæ¢ç´¢ä»¥æ”¶é›†åˆå§‹æ•°æ®\n        if self.t < self.n_arms * 10:\n            return self.t % self.n_arms\n        \n        # Epsilon-Greedy\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.n_arms)\n        else:\n            # é¢„æµ‹æ¯ä¸ªè‡‚çš„å¥–åŠ±\n            predictions = []\n            for arm in range(self.n_arms):\n                if len(self.data[arm]['X']) > 10:\n                    X_scaled = self.scalers[arm].transform([context])\n                    pred = self.models[arm].predict(X_scaled)[0]\n                else:\n                    pred = 0\n                predictions.append(pred)\n            return np.argmax(predictions)\n    \n    def update(self, arm: int, context: np.ndarray, reward: float):\n        self.t += 1\n        self.data[arm]['X'].append(context)\n        self.data[arm]['y'].append(reward)\n        \n        # å®šæœŸé‡æ–°è®­ç»ƒ\n        if self.t % self.retrain_interval == 0:\n            for a in range(self.n_arms):\n                if len(self.data[a]['X']) > 10:\n                    X = np.array(self.data[a]['X'])\n                    y = np.array(self.data[a]['y'])\n                    self.scalers[a].fit(X)\n                    X_scaled = self.scalers[a].transform(X)\n                    self.models[a].fit(X_scaled, y)\n\n# è¿è¡Œ Neural Epsilon-Greedy\nn_rounds_neural = 3000\nneural_eg = NeuralEpsilonGreedy(n_arms=3, n_features=2, epsilon=0.1, retrain_interval=100)\n\nneural_actions = []\nneural_rewards = []\n\nfor t in range(n_rounds_neural):\n    context = generate_context()\n    arm = neural_eg.select_arm(context)\n    reward = get_reward(arm, context)\n    neural_eg.update(arm, context, reward)\n    neural_actions.append(arm)\n    neural_rewards.append(reward)\n\nprint(\"\\nğŸ¯ Neural Epsilon-Greedy ç»“æœï¼š\")\nprint(f\"å¹³å‡å¥–åŠ±: {np.mean(neural_rewards):.3f}\")\nprint(f\"æ¯ä¸ªè‡‚é€‰æ‹©æ¬¡æ•°: {np.bincount(neural_actions)}\")\nprint(f\"\\nä¸ LinUCB å¯¹æ¯”:\")\nprint(f\"LinUCB å¹³å‡å¥–åŠ±: {np.mean(linucb_rewards):.3f}\")\nprint(f\"Neural-EG å¹³å‡å¥–åŠ±: {np.mean(neural_rewards):.3f}\")\nprint(\"\\nğŸ’¡ Neural Bandit å¯ä»¥æ•æ‰éçº¿æ€§å…³ç³»ï¼Œä½†éœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—èµ„æº\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: é—æ†¾åˆ†æ (Regret Analysis)\n",
    "\n",
    "## 4.1 Regret çš„å®šä¹‰\n",
    "\n",
    "### ç´¯ç§¯é—æ†¾ï¼ˆCumulative Regretï¼‰\n",
    "\n",
    "$$\n",
    "R_T = \\sum_{t=1}^{T} (\\mu^* - \\mu_{a_t}) = T \\mu^* - \\sum_{t=1}^{T} \\mu_{a_t}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\mu^* = \\max_a \\mu_a$ï¼šæœ€ä¼˜è‡‚çš„æœŸæœ›æ”¶ç›Š\n",
    "- $\\mu_{a_t}$ï¼šç¬¬ $t$ è½®é€‰æ‹©çš„è‡‚çš„æœŸæœ›æ”¶ç›Š\n",
    "\n",
    "### ç›´è§‚ç†è§£\n",
    "\n",
    "Regret è¡¡é‡ã€Œå¦‚æœä¸€ç›´é€‰æœ€ä¼˜è‡‚ã€å’Œã€Œå®é™…é€‰æ‹©ã€çš„å·®è·ã€‚\n",
    "\n",
    "- **æœ€å¥½çš„æƒ…å†µ**ï¼š$R_T = 0$ï¼ˆä¸€ç›´é€‰æœ€ä¼˜è‡‚ï¼‰\n",
    "- **æœ€åçš„æƒ…å†µ**ï¼š$R_T = T \\Delta$ï¼ˆä¸€ç›´é€‰æœ€å·®è‡‚ï¼Œ$\\Delta$ æ˜¯ä¸æœ€ä¼˜è‡‚çš„å·®è·ï¼‰\n",
    "\n",
    "## 4.2 å„ç®—æ³•çš„ Regret Bound\n",
    "\n",
    "### Epsilon-Greedy\n",
    "\n",
    "$$\n",
    "R_T = O(T^{2/3})\n",
    "$$\n",
    "\n",
    "- **çº¿æ€§é—æ†¾**ï¼šæ¢ç´¢æ°¸è¿œä¸åœæ­¢ï¼Œå³ä½¿å·²ç»æ‰¾åˆ°æœ€ä¼˜è‡‚\n",
    "- **æ”¹è¿›**ï¼šEpsilon-Decreasingï¼ˆ$\\epsilon_t = 1/t$ï¼‰å¯ä»¥è¾¾åˆ° $O(\\log T)$\n",
    "\n",
    "### UCB\n",
    "\n",
    "$$\n",
    "R_T = O\\left( \\sqrt{KT \\log T} \\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $K$ æ˜¯è‡‚çš„æ•°é‡ã€‚\n",
    "\n",
    "- **æ¬¡çº¿æ€§é—æ†¾**ï¼šéšæ—¶é—´æ¨ç§»ï¼Œå¹³å‡é—æ†¾ $R_T / T \\to 0$\n",
    "- **ç†è®ºä¿è¯å¼º**ï¼šå¯¹ä»»ä½•å¥–åŠ±åˆ†å¸ƒéƒ½æˆç«‹\n",
    "\n",
    "### Thompson Sampling\n",
    "\n",
    "$$\n",
    "R_T = O\\left( \\sqrt{KT} \\right)\n",
    "$$\n",
    "\n",
    "- **å®è·µä¸­è¡¨ç°æœ€å¥½**\n",
    "- **ç†è®ºåˆ†æè¾ƒéš¾**ï¼ˆä¾èµ–äºå…ˆéªŒåˆ†å¸ƒï¼‰\n",
    "\n",
    "## 4.3 Gap-Dependent Bound\n",
    "\n",
    "å®šä¹‰ **Gap**ï¼š$\\Delta_a = \\mu^* - \\mu_a$ï¼ˆè‡‚ $a$ ä¸æœ€ä¼˜è‡‚çš„å·®è·ï¼‰\n",
    "\n",
    "UCB çš„ Gap-Dependent Boundï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R_T = O\\left( \\sum_{a: \\Delta_a > 0} \\frac{\\log T}{\\Delta_a} \\right)\n",
    "$$\n",
    "\n",
    "**ç›´è§‚è§£é‡Š**ï¼š\n",
    "- Gap è¶Šå¤§çš„è‡‚ï¼ˆ$\\Delta_a$ å¤§ï¼‰ï¼Œè¶Šå®¹æ˜“è¢«è¯†åˆ«ä¸ºæ¬¡ä¼˜ï¼Œæ¢ç´¢æ¬¡æ•°å°‘\n",
    "- Gap è¶Šå°çš„è‡‚ï¼ˆ$\\Delta_a$ å°ï¼‰ï¼Œéœ€è¦æ›´å¤šæ¬¡æ¢ç´¢æ‰èƒ½ç¡®å®š\n",
    "\n",
    "## 4.4 Lower Boundï¼ˆä¸å¯é€¾è¶Šçš„ç•Œé™ï¼‰\n",
    "\n",
    "Lai & Robbins (1985) è¯æ˜ï¼Œ**ä»»ä½•ç®—æ³•**çš„é—æ†¾è‡³å°‘æ˜¯ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R_T = \\Omega\\left( \\sum_{a: \\Delta_a > 0} \\frac{\\log T}{\\Delta_a} \\right)\n",
    "$$\n",
    "\n",
    "è¿™æ„å‘³ç€ UCB å’Œ Thompson Sampling åœ¨ Gap-Dependent æ„ä¹‰ä¸‹æ˜¯**æ¸è¿‘æœ€ä¼˜çš„**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒï¼šéªŒè¯ Regret Bound\n",
    "# è¿è¡Œå¤šæ¬¡å®éªŒï¼Œè§‚å¯Ÿå¹³å‡é—æ†¾çš„å¢é•¿è¶‹åŠ¿\n",
    "\n",
    "n_experiments = 50\n",
    "n_rounds = 10000\n",
    "true_rates = np.array([0.3, 0.5, 0.4])  # Gap æ›´æ˜æ˜¾çš„è®¾ç½®\n",
    "\n",
    "ucb_regrets = []\n",
    "ts_regrets = []\n",
    "\n",
    "for exp in range(n_experiments):\n",
    "    # UCB\n",
    "    ucb = UCB(n_arms=3)\n",
    "    ucb_actions, _ = ucb.run(true_rates, n_rounds)\n",
    "    regret = sum(max(true_rates) - true_rates[a] for a in ucb_actions)\n",
    "    ucb_regrets.append(regret)\n",
    "    \n",
    "    # Thompson Sampling\n",
    "    ts = ThompsonSampling(n_arms=3)\n",
    "    ts_actions, _ = ts.run(true_rates, n_rounds)\n",
    "    regret = sum(max(true_rates) - true_rates[a] for a in ts_actions)\n",
    "    ts_regrets.append(regret)\n",
    "\n",
    "# ç»˜åˆ¶åˆ†å¸ƒ\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    y=ucb_regrets,\n",
    "    name='UCB',\n",
    "    marker_color=COLORS['purple']\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    y=ts_regrets,\n",
    "    name='Thompson Sampling',\n",
    "    marker_color=COLORS['blue']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Regret åˆ†å¸ƒï¼ˆ{n_experiments} æ¬¡å®éªŒï¼Œæ¯æ¬¡ {n_rounds} è½®ï¼‰',\n",
    "    yaxis_title='ç´¯ç§¯é—æ†¾',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š ç»Ÿè®¡ç»“æœï¼ˆ{n_experiments} æ¬¡å®éªŒï¼‰:\")\n",
    "print(f\"UCB å¹³å‡é—æ†¾: {np.mean(ucb_regrets):.2f} Â± {np.std(ucb_regrets):.2f}\")\n",
    "print(f\"Thompson Sampling å¹³å‡é—æ†¾: {np.mean(ts_regrets):.2f} Â± {np.std(ts_regrets):.2f}\")\n",
    "\n",
    "# ç†è®ºé¢„æµ‹\n",
    "K = 3\n",
    "T = n_rounds\n",
    "theoretical_ucb = np.sqrt(K * T * np.log(T))\n",
    "theoretical_ts = np.sqrt(K * T)\n",
    "\n",
    "print(f\"\\nğŸ“ ç†è®º Bound:\")\n",
    "print(f\"UCB: O(âˆš(KT log T)) â‰ˆ {theoretical_ucb:.2f}\")\n",
    "print(f\"Thompson Sampling: O(âˆš(KT)) â‰ˆ {theoretical_ts:.2f}\")\n",
    "print(f\"\\nå®é™…é—æ†¾è¿œå°äºç†è®ºä¸Šç•Œï¼Œè¯´æ˜ Bound æ˜¯å®½æ¾çš„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 å®è·µä¸­çš„é€‰æ‹©\n",
    "\n",
    "### ç®—æ³•é€‰æ‹©æŒ‡å—\n",
    "\n",
    "| åœºæ™¯ | æ¨èç®—æ³• | ç†ç”± |\n",
    "|-----|---------|-----|\n",
    "| **å¿«é€ŸåŸå‹** | Epsilon-Greedy | ç®€å•æ˜“å®ç° |\n",
    "| **éœ€è¦ç†è®ºä¿è¯** | UCB | Bound æ¸…æ™°ï¼Œæ— éœ€å…ˆéªŒ |\n",
    "| **è¿½æ±‚æœ€ä¼˜æ€§èƒ½** | Thompson Sampling | å®è·µä¸­æœ€å¥½ |\n",
    "| **æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯** | LinUCB | ä¸ªæ€§åŒ–æ¨è |\n",
    "| **éçº¿æ€§å…³ç³»** | Neural Bandit | å¤æ‚æ¨¡å¼è¯†åˆ« |\n",
    "\n",
    "### è¶…å‚æ•°è°ƒä¼˜\n",
    "\n",
    "- **Epsilon-Greedy**ï¼š$\\epsilon \\in [0.05, 0.2]$\n",
    "- **UCB**ï¼š$c \\in [1, 3]$ï¼ˆ$c=2$ æ˜¯æ ‡å‡†é€‰æ‹©ï¼‰\n",
    "- **LinUCB**ï¼š$\\alpha \\in [0.1, 2]$ï¼ˆéœ€è¦äº¤å‰éªŒè¯ï¼‰\n",
    "\n",
    "### A/B Testing vs MAB å†³ç­–æ ‘\n",
    "\n",
    "```\n",
    "éœ€è¦ä¸¥æ ¼çš„ç»Ÿè®¡æ¨æ–­ï¼ˆp å€¼ã€ç½®ä¿¡åŒºé—´ï¼‰ï¼Ÿ\n",
    "â”œâ”€ æ˜¯ â†’ A/B Testing\n",
    "â””â”€ å¦ â†’ æµé‡æ˜¯å¦å……è¶³ï¼Ÿ\n",
    "    â”œâ”€ å¦ â†’ A/B Testing\n",
    "    â””â”€ æ˜¯ â†’ æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Ÿ\n",
    "        â”œâ”€ æ˜¯ â†’ Contextual Bandit (LinUCB)\n",
    "        â””â”€ å¦ â†’ Multi-Armed Bandit (Thompson Sampling)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: ä¸šåŠ¡åº”ç”¨\n",
    "\n",
    "## 5.1 æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨\n",
    "\n",
    "### åœºæ™¯ï¼šæ–°é—»æ¨è\n",
    "\n",
    "- **é—®é¢˜**ï¼šç»™ç”¨æˆ·æ¨èå“ªç¯‡æ–‡ç« ï¼Ÿ\n",
    "- **æŒ‘æˆ˜**ï¼š\n",
    "  - æ–‡ç« æ•°é‡å¤šï¼ˆæ•°åƒç¯‡ï¼‰\n",
    "  - ç”¨æˆ·åå¥½ä¸åŒ\n",
    "  - æ–‡ç« æ—¶æ•ˆæ€§å¼ºï¼ˆæ–°æ–‡ç« ä¸æ–­å‡ºç°ï¼‰\n",
    "\n",
    "### è§£å†³æ–¹æ¡ˆï¼šContextual Bandit\n",
    "\n",
    "- **ä¸Šä¸‹æ–‡**ï¼šç”¨æˆ·ç‰¹å¾ï¼ˆå†å²é˜…è¯»ã€å…´è¶£æ ‡ç­¾ï¼‰+ æ–‡ç« ç‰¹å¾ï¼ˆç±»åˆ«ã€æ ‡é¢˜ï¼‰\n",
    "- **åŠ¨ä½œ**ï¼šé€‰æ‹©æ¨èå“ªç¯‡æ–‡ç« \n",
    "- **å¥–åŠ±**ï¼šç‚¹å‡»ã€é˜…è¯»æ—¶é•¿\n",
    "\n",
    "### çœŸå®æ¡ˆä¾‹\n",
    "\n",
    "- **Yahoo! News**ï¼šLinUCB æå‡ CTR 12.5%\n",
    "- **Google News**ï¼šNeural Bandit å¤§è§„æ¨¡åº”ç”¨\n",
    "\n",
    "## 5.2 å¹¿å‘ŠæŠ•æ”¾\n",
    "\n",
    "### åœºæ™¯ï¼šå¹¿å‘Šä½ç«ä»·\n",
    "\n",
    "- **é—®é¢˜**ï¼šç»™è¿™ä¸ªç”¨æˆ·å±•ç¤ºå“ªä¸ªå¹¿å‘Šï¼Ÿ\n",
    "- **ç›®æ ‡**ï¼šæœ€å¤§åŒ–ç‚¹å‡»ç‡ Ã— å‡ºä»·\n",
    "\n",
    "### æŒ‘æˆ˜\n",
    "\n",
    "- **å»¶è¿Ÿåé¦ˆ**ï¼šå¹¿å‘Šç‚¹å‡»åçš„è½¬åŒ–å¯èƒ½å‡ å¤©åæ‰å‘ç”Ÿ\n",
    "- **é¢„ç®—çº¦æŸ**ï¼šæŸäº›å¹¿å‘Šçš„é¢„ç®—æœ‰é™\n",
    "- **éå¹³ç¨³æ€§**ï¼šç”¨æˆ·å…´è¶£éšæ—¶é—´å˜åŒ–\n",
    "\n",
    "### è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "- **Sliding Window**ï¼šåªä½¿ç”¨æœ€è¿‘çš„æ•°æ®\n",
    "- **Discounted UCB**ï¼šç»™è¿‘æœŸæ•°æ®æ›´é«˜æƒé‡\n",
    "- **Budget-Constrained Bandit**ï¼šè€ƒè™‘é¢„ç®—çš„ Bandit\n",
    "\n",
    "## 5.3 åŠ¨æ€å®šä»·\n",
    "\n",
    "### åœºæ™¯ï¼šç”µå•†åŠ¨æ€è°ƒä»·\n",
    "\n",
    "- **é—®é¢˜**ï¼šæŸå•†å“åº”è¯¥å®šä»·å¤šå°‘ï¼Ÿ\n",
    "- **åŠ¨ä½œ**ï¼šé€‰æ‹©ä»·æ ¼åŒºé—´ï¼ˆå¦‚ 99, 109, 119ï¼‰\n",
    "- **å¥–åŠ±**ï¼šé”€é‡ Ã— åˆ©æ¶¦ç‡\n",
    "\n",
    "### æ–¹æ³•\n",
    "\n",
    "- **è¿ç»­è‡‚ Bandit**ï¼šä»·æ ¼æ˜¯è¿ç»­å˜é‡\n",
    "- **GP-UCB (Gaussian Process UCB)**ï¼šç”¨é«˜æ–¯è¿‡ç¨‹å»ºæ¨¡å¥–åŠ±å‡½æ•°\n",
    "\n",
    "## 5.4 ä¸´åºŠè¯•éªŒ\n",
    "\n",
    "### åœºæ™¯ï¼šè¯ç‰©è¯•éªŒ\n",
    "\n",
    "ä¼ ç»Ÿ RCTï¼ˆéšæœºå¯¹ç…§è¯•éªŒï¼‰ï¼š\n",
    "- å‰æœŸå›ºå®šåˆ†é…ï¼ŒåæœŸåˆ†æ\n",
    "- **ä¼¦ç†é—®é¢˜**ï¼šæ˜çŸ¥æŸè¯æ•ˆæœå·®ï¼Œè¿˜è¦ç»™æ‚£è€…ä½¿ç”¨\n",
    "\n",
    "### Adaptive Trial (è‡ªé€‚åº”è¯•éªŒ)\n",
    "\n",
    "ç”¨ Thompson Samplingï¼š\n",
    "- åŠ¨æ€è°ƒæ•´åˆ†é…æ¯”ä¾‹\n",
    "- æ›´å¤šæ‚£è€…æ¥å—æœ‰æ•ˆæ²»ç–—\n",
    "- FDA å·²æ‰¹å‡†å¤šä¸ªè‡ªé€‚åº”è¯•éªŒ\n",
    "\n",
    "### çœŸå®æ¡ˆä¾‹\n",
    "\n",
    "- **BATTLE Trial** (è‚ºç™Œæ²»ç–—)ï¼šç”¨è‡ªé€‚åº”è®¾è®¡ï¼Œæé«˜æ²»ç–—æœ‰æ•ˆç‡\n",
    "- **COVID-19 ç–«è‹—è¯•éªŒ**ï¼šéƒ¨åˆ†é‡‡ç”¨è‡ªé€‚åº”æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¡ˆä¾‹æ¨¡æ‹Ÿï¼šæ–°é—»æ¨èç³»ç»Ÿ\n",
    "\n",
    "class NewsRecommendation:\n",
    "    def __init__(self, n_articles: int = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_articles: æ–‡ç« æ•°é‡\n",
    "        \"\"\"\n",
    "        self.n_articles = n_articles\n",
    "        \n",
    "        # æ¯ç¯‡æ–‡ç« çš„çœŸå®è´¨é‡ï¼ˆç”¨æˆ·ä¸çŸ¥é“ï¼‰\n",
    "        self.true_ctr = np.random.uniform(0.01, 0.10, n_articles)\n",
    "        self.true_ctr[0] = 0.12  # è®¾ç½®ä¸€ç¯‡çˆ†æ¬¾æ–‡ç« \n",
    "        \n",
    "        # ç”¨ Thompson Sampling æ¨è\n",
    "        self.ts = ThompsonSampling(n_articles)\n",
    "        \n",
    "    def simulate(self, n_users: int = 10000):\n",
    "        \"\"\"æ¨¡æ‹Ÿæ¨èè¿‡ç¨‹\"\"\"\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        cumulative_ctr = []\n",
    "        \n",
    "        for t in range(n_users):\n",
    "            # æ¨èæ–‡ç« \n",
    "            article = self.ts.select_arm()\n",
    "            \n",
    "            # ç”¨æˆ·æ˜¯å¦ç‚¹å‡»\n",
    "            click = np.random.binomial(1, self.true_ctr[article])\n",
    "            \n",
    "            # æ›´æ–°æ¨¡å‹\n",
    "            self.ts.update(article, click)\n",
    "            \n",
    "            actions.append(article)\n",
    "            rewards.append(click)\n",
    "            cumulative_ctr.append(np.mean(rewards))\n",
    "        \n",
    "        return actions, rewards, cumulative_ctr\n",
    "\n",
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "news_system = NewsRecommendation(n_articles=10)\n",
    "actions, rewards, cumulative_ctr = news_system.simulate(n_users=10000)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['ç´¯ç§¯ç‚¹å‡»ç‡', 'å„æ–‡ç« æ¨èæ¬¡æ•°']\n",
    ")\n",
    "\n",
    "# ç´¯ç§¯ CTR\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(cumulative_ctr) + 1)),\n",
    "        y=cumulative_ctr,\n",
    "        mode='lines',\n",
    "        line=dict(color=COLORS['blue'], width=2),\n",
    "        name='å®é™… CTR'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# æœ€ä¼˜ CTRï¼ˆå¦‚æœä¸€ç›´æ¨èæœ€å¥½çš„æ–‡ç« ï¼‰\n",
    "best_ctr = max(news_system.true_ctr)\n",
    "fig.add_hline(\n",
    "    y=best_ctr,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=COLORS['green'],\n",
    "    annotation_text=f\"æœ€ä¼˜ CTR: {best_ctr:.3f}\",\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# å„æ–‡ç« æ¨èæ¬¡æ•°\n",
    "article_counts = np.bincount(actions, minlength=news_system.n_articles)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f'æ–‡ç« {i}' for i in range(news_system.n_articles)],\n",
    "        y=article_counts,\n",
    "        marker_color=COLORS['purple'],\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"ç”¨æˆ·æ•°\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"æ–‡ç« \", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"ç‚¹å‡»ç‡\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"æ¨èæ¬¡æ•°\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='æ–°é—»æ¨èç³»ç»Ÿæ¨¡æ‹Ÿ',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨èç³»ç»Ÿç»“æœï¼š\")\n",
    "print(f\"æœ€ç»ˆ CTR: {cumulative_ctr[-1]:.4f}\")\n",
    "print(f\"æœ€ä¼˜ CTR: {best_ctr:.4f}\")\n",
    "print(f\"è¾¾åˆ°æœ€ä¼˜çš„ {cumulative_ctr[-1]/best_ctr*100:.1f}%\")\n",
    "\n",
    "print(\"\\nå„æ–‡ç« çœŸå® CTR å’Œæ¨èæ¬¡æ•°:\")\n",
    "for i in range(news_system.n_articles):\n",
    "    print(f\"æ–‡ç« {i}: CTR={news_system.true_ctr[i]:.4f}, æ¨è{article_counts[i]}æ¬¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ TODO 3: æ¨¡æ‹Ÿå¹¿å‘ŠæŠ•æ”¾åœºæ™¯\n",
    "\n",
    "è®¾è®¡ä¸€ä¸ªå¹¿å‘ŠæŠ•æ”¾ç³»ç»Ÿï¼š\n",
    "- 5 ä¸ªå¹¿å‘Šï¼Œä¸åŒçš„ CTR å’Œå‡ºä»·\n",
    "- ç›®æ ‡ï¼šæœ€å¤§åŒ– CTR Ã— å‡ºä»·\n",
    "- ä½¿ç”¨ UCB æˆ– Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç­”æ¡ˆï¼šå¹¿å‘ŠæŠ•æ”¾æ¨¡æ‹Ÿ\nclass AdPlacement:\n    def __init__(self, n_ads: int = 5):\n        # æ¯ä¸ªå¹¿å‘Šçš„çœŸå® CTR å’Œå‡ºä»·\n        np.random.seed(42)\n        self.true_ctrs = np.random.uniform(0.01, 0.08, n_ads)\n        self.bids = np.random.uniform(0.5, 2.0, n_ads)  # å‡ºä»·ï¼ˆå…ƒï¼‰\n        \n        # çœŸå®æ”¶ç›Š = CTR Ã— å‡ºä»·\n        self.true_revenues = self.true_ctrs * self.bids\n        self.best_ad = np.argmax(self.true_revenues)\n        \n        print(\"ğŸ“Š å¹¿å‘Šè®¾ç½®ï¼š\")\n        for i in range(n_ads):\n            print(f\"å¹¿å‘Š {i}: CTR={self.true_ctrs[i]:.3f}, å‡ºä»·=Â¥{self.bids[i]:.2f}, \"\n                  f\"æœŸæœ›æ”¶ç›Š=Â¥{self.true_revenues[i]:.4f}\")\n        print(f\"\\næœ€ä¼˜å¹¿å‘Š: {self.best_ad} (æœŸæœ›æ”¶ç›Š: Â¥{self.true_revenues[self.best_ad]:.4f})\")\n    \n    def get_reward(self, ad: int) -> float:\n        # ç”¨æˆ·ç‚¹å‡»ä¸å¦\n        click = np.random.binomial(1, self.true_ctrs[ad])\n        # æ”¶ç›Š = ç‚¹å‡» Ã— å‡ºä»·\n        return click * self.bids[ad]\n\n# åˆ›å»ºå¹¿å‘Šç³»ç»Ÿ\nad_system = AdPlacement(n_ads=5)\n\n# ç­–ç•¥ 1: Thompson Sampling\nts_ads = ThompsonSampling(n_arms=5)\nts_actions = []\nts_revenues = []\n\nfor t in range(10000):\n    ad = ts_ads.select_arm()\n    revenue = ad_system.get_reward(ad)\n    # Thompson Sampling éœ€è¦ 0/1 å¥–åŠ±ï¼Œè¿™é‡Œç”¨ç‚¹å‡»ä½œä¸ºå¥–åŠ±\n    click = 1 if revenue > 0 else 0\n    ts_ads.update(ad, click)\n    ts_actions.append(ad)\n    ts_revenues.append(revenue)\n\n# ç­–ç•¥ 2: å‡åŒ€æ¨è\nuniform_revenues = []\nfor t in range(10000):\n    ad = t % 5\n    revenue = ad_system.get_reward(ad)\n    uniform_revenues.append(revenue)\n\n# å¯¹æ¯”ç»“æœ\nprint(\"\\n\\nğŸ“Š å¹¿å‘ŠæŠ•æ”¾ç»“æœå¯¹æ¯”ï¼š\")\nprint(f\"\\nThompson Sampling:\")\nprint(f\"  æ€»æ”¶ç›Š: Â¥{sum(ts_revenues):.2f}\")\nprint(f\"  å¹³å‡æ”¶ç›Š: Â¥{np.mean(ts_revenues):.4f}\")\nprint(f\"  å„å¹¿å‘Šå±•ç¤ºæ¬¡æ•°: {np.bincount(ts_actions)}\")\n\nprint(f\"\\nå‡åŒ€æ¨è:\")\nprint(f\"  æ€»æ”¶ç›Š: Â¥{sum(uniform_revenues):.2f}\")\nprint(f\"  å¹³å‡æ”¶ç›Š: Â¥{np.mean(uniform_revenues):.4f}\")\n\nimprovement = (sum(ts_revenues) - sum(uniform_revenues)) / sum(uniform_revenues) * 100\nprint(f\"\\nğŸ’° æ”¶ç›Šæå‡: {improvement:.1f}%\")\n\n# å¯è§†åŒ–\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=list(range(1, 10001)),\n    y=np.cumsum(ts_revenues),\n    mode='lines',\n    name='Thompson Sampling',\n    line=dict(color=COLORS['blue'], width=2)\n))\nfig.add_trace(go.Scatter(\n    x=list(range(1, 10001)),\n    y=np.cumsum(uniform_revenues),\n    mode='lines',\n    name='å‡åŒ€æ¨è',\n    line=dict(color=COLORS['gray'], width=2)\n))\nfig.update_layout(\n    title='ç´¯ç§¯æ”¶ç›Šå¯¹æ¯”',\n    xaxis_title='å±•ç¤ºæ¬¡æ•°',\n    yaxis_title='ç´¯ç§¯æ”¶ç›Š (Â¥)',\n    template='plotly_white',\n    height=400\n)\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç»ƒä¹ ä¸æ€è€ƒé¢˜\n",
    "\n",
    "## ç»ƒä¹  1: Epsilon-Decreasing\n",
    "\n",
    "å®ç°ä¸€ä¸ª Epsilon-Decreasing ç­–ç•¥ï¼Œä½¿å¾— $\\epsilon_t = \\min(1, c / t)$ï¼Œå¯¹æ¯”ä¸å›ºå®š $\\epsilon$ çš„å·®å¼‚ã€‚\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- ç»˜åˆ¶é—æ†¾æ›²çº¿å¯¹æ¯”\n",
    "- å°è¯•ä¸åŒçš„ $c$ å€¼\n",
    "- åˆ†æï¼šä¸ºä»€ä¹ˆé€’å‡çš„ $\\epsilon$ æ›´å¥½ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  1: Epsilon-Decreasing å®ç°\n\nclass EpsilonDecreasing:\n    def __init__(self, n_arms: int, c: float = 1.0):\n        self.n_arms = n_arms\n        self.c = c\n        self.counts = np.zeros(n_arms)\n        self.values = np.zeros(n_arms)\n        self.t = 0\n    \n    def select_arm(self) -> int:\n        self.t += 1\n        # epsilon_t = min(1, c / t)\n        epsilon = min(1.0, self.c / self.t)\n        \n        if np.random.random() < epsilon:\n            return np.random.randint(self.n_arms)\n        else:\n            return np.argmax(self.values)\n    \n    def update(self, arm: int, reward: float):\n        self.counts[arm] += 1\n        n = self.counts[arm]\n        self.values[arm] = ((n - 1) / n) * self.values[arm] + (1 / n) * reward\n    \n    def run(self, true_rates: np.ndarray, n_rounds: int):\n        actions = []\n        rewards = []\n        for _ in range(n_rounds):\n            arm = self.select_arm()\n            reward = np.random.binomial(1, true_rates[arm])\n            self.update(arm, reward)\n            actions.append(arm)\n            rewards.append(reward)\n        return actions, rewards\n\n# æµ‹è¯•ä¸åŒçš„ c å€¼\nc_values = [1.0, 5.0, 10.0]\nfig = go.Figure()\n\n# æ·»åŠ å›ºå®š epsilon ä½œä¸ºåŸºçº¿\neg_baseline = EpsilonGreedy(n_arms=3, epsilon=0.1)\neg_actions, _ = eg_baseline.run(true_rates, n_rounds)\neg_regret_baseline = []\ncumulative = 0\nfor action in eg_actions:\n    cumulative += best_rate - true_rates[action]\n    eg_regret_baseline.append(cumulative)\n\nfig.add_trace(go.Scatter(\n    x=list(range(1, n_rounds + 1)),\n    y=eg_regret_baseline,\n    mode='lines',\n    name='å›ºå®š Îµ=0.1',\n    line=dict(dash='dash', width=2)\n))\n\n# æµ‹è¯•é€’å‡ epsilon\nfor c in c_values:\n    ed = EpsilonDecreasing(n_arms=3, c=c)\n    actions, _ = ed.run(true_rates, n_rounds)\n    \n    regret = []\n    cumulative = 0\n    for action in actions:\n        cumulative += best_rate - true_rates[action]\n        regret.append(cumulative)\n    \n    fig.add_trace(go.Scatter(\n        x=list(range(1, n_rounds + 1)),\n        y=regret,\n        mode='lines',\n        name=f'é€’å‡ c={c}',\n        line=dict(width=2)\n    ))\n\nfig.update_layout(\n    title='Epsilon-Decreasing vs å›ºå®š Epsilon',\n    xaxis_title='è½®æ•°',\n    yaxis_title='ç´¯ç§¯é—æ†¾',\n    template='plotly_white',\n    height=500\n)\nfig.show()\n\nprint(\"\\nğŸ“Š åˆ†æï¼š\")\nprint(\"- å›ºå®š epsilon: æŒç»­æ¢ç´¢ï¼Œé—æ†¾çº¿æ€§å¢é•¿\")\nprint(\"- é€’å‡ epsilon: åæœŸå‡å°‘æ¢ç´¢ï¼Œé—æ†¾å¢é•¿å˜æ…¢\")\nprint(\"- c å€¼è¶Šå¤§ï¼ŒåˆæœŸæ¢ç´¢è¶Šå¤šï¼ŒåæœŸæ”¶æ•›è¶Šæ…¢\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  2: Batched Thompson Sampling\n",
    "\n",
    "åœ¨çœŸå®ç³»ç»Ÿä¸­ï¼Œæ¨¡å‹ä¸æ˜¯æ¯æ¬¡éƒ½æ›´æ–°ï¼Œè€Œæ˜¯ç§¯ç´¯ä¸€æ‰¹æ•°æ®åæ‰¹é‡æ›´æ–°ã€‚å®ç°ä¸€ä¸ªæ‰¹é‡ç‰ˆæœ¬çš„ Thompson Samplingã€‚\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- æ¯éš” $B$ è½®æ›´æ–°ä¸€æ¬¡æ¨¡å‹ï¼ˆå¦‚ $B=100$ï¼‰\n",
    "- å¯¹æ¯”ä¸å®æ—¶æ›´æ–°çš„å·®å¼‚\n",
    "- åˆ†æï¼šbatch size å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  2: Batched Thompson Sampling\n\nclass BatchedThompsonSampling:\n    def __init__(self, n_arms: int, batch_size: int = 100):\n        self.n_arms = n_arms\n        self.batch_size = batch_size\n        self.alpha = np.ones(n_arms)\n        self.beta = np.ones(n_arms)\n        \n        # æ‰¹æ¬¡ç¼“å­˜\n        self.batch_data = {i: [] for i in range(n_arms)}\n        self.t = 0\n    \n    def select_arm(self) -> int:\n        samples = np.random.beta(self.alpha, self.beta)\n        return np.argmax(samples)\n    \n    def update(self, arm: int, reward: float):\n        self.t += 1\n        self.batch_data[arm].append(reward)\n        \n        # æ¯ batch_size è½®æ›´æ–°ä¸€æ¬¡\n        if self.t % self.batch_size == 0:\n            for a in range(self.n_arms):\n                if len(self.batch_data[a]) > 0:\n                    successes = sum(self.batch_data[a])\n                    failures = len(self.batch_data[a]) - successes\n                    self.alpha[a] += successes\n                    self.beta[a] += failures\n                    self.batch_data[a] = []\n    \n    def run(self, true_rates: np.ndarray, n_rounds: int):\n        actions = []\n        rewards = []\n        for _ in range(n_rounds):\n            arm = self.select_arm()\n            reward = np.random.binomial(1, true_rates[arm])\n            self.update(arm, reward)\n            actions.append(arm)\n            rewards.append(reward)\n        return actions, rewards\n\n# å¯¹æ¯”ä¸åŒ batch size\nbatch_sizes = [1, 50, 100, 500]\nfig = go.Figure()\n\nfor bs in batch_sizes:\n    if bs == 1:\n        # å®æ—¶æ›´æ–° = æ ‡å‡† Thompson Sampling\n        ts_test = ThompsonSampling(n_arms=3)\n        actions, _ = ts_test.run(true_rates, n_rounds)\n        label = 'å®æ—¶æ›´æ–°'\n    else:\n        bts = BatchedThompsonSampling(n_arms=3, batch_size=bs)\n        actions, _ = bts.run(true_rates, n_rounds)\n        label = f'Batch={bs}'\n    \n    regret = []\n    cumulative = 0\n    for action in actions:\n        cumulative += best_rate - true_rates[action]\n        regret.append(cumulative)\n    \n    fig.add_trace(go.Scatter(\n        x=list(range(1, n_rounds + 1)),\n        y=regret,\n        mode='lines',\n        name=label,\n        line=dict(width=2)\n    ))\n\nfig.update_layout(\n    title='Batched vs å®æ—¶æ›´æ–° Thompson Sampling',\n    xaxis_title='è½®æ•°',\n    yaxis_title='ç´¯ç§¯é—æ†¾',\n    template='plotly_white',\n    height=500\n)\nfig.show()\n\nprint(\"\\nğŸ“Š åˆ†æï¼š\")\nprint(\"- Batch size è¶Šå¤§ï¼Œæ›´æ–°è¶Šä¸åŠæ—¶ï¼Œé—æ†¾è¶Šå¤§\")\nprint(\"- ä½†æ‰¹é‡æ›´æ–°å¯ä»¥å‡å°‘è®¡ç®—å¼€é”€\")\nprint(\"- å®é™…åº”ç”¨ä¸­éœ€è¦æƒè¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3: Non-Stationary Bandit\n",
    "\n",
    "æ¨¡æ‹Ÿä¸€ä¸ªéå¹³ç¨³åœºæ™¯ï¼šè‡‚çš„çœŸå®æ”¶ç›Šéšæ—¶é—´å˜åŒ–ï¼ˆå¦‚å­£èŠ‚æ€§æ³¢åŠ¨ï¼‰ã€‚\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- è®¾ç½®æŸä¸ªè‡‚çš„æ”¶ç›Šåœ¨ç¬¬ 5000 è½®æ—¶çªç„¶æé«˜\n",
    "- å¯¹æ¯” UCB å’Œ Thompson Sampling çš„é€‚åº”é€Ÿåº¦\n",
    "- æ€è€ƒï¼šå¦‚ä½•æ”¹è¿›ç®—æ³•ä»¥æ›´å¿«é€‚åº”å˜åŒ–ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  3: Non-Stationary Bandit\n\ndef run_nonstationary_experiment(algorithm_class, n_rounds: int = 10000, change_point: int = 5000):\n    \"\"\"è¿è¡Œéå¹³ç¨³å®éªŒ\"\"\"\n    # åˆå§‹è½¬åŒ–ç‡\n    rates_phase1 = np.array([0.05, 0.08, 0.06])\n    # ç¬¬ 5000 è½®åï¼Œè‡‚ 0 çš„è½¬åŒ–ç‡æå‡\n    rates_phase2 = np.array([0.09, 0.08, 0.06])  # è‡‚ 0 å˜æˆæœ€ä¼˜\n    \n    if algorithm_class == 'UCB':\n        algo = UCB(n_arms=3)\n    else:\n        algo = ThompsonSampling(n_arms=3)\n    \n    actions = []\n    rewards = []\n    \n    for t in range(n_rounds):\n        arm = algo.select_arm()\n        \n        # æ ¹æ®æ—¶é—´é€‰æ‹©è½¬åŒ–ç‡\n        rates = rates_phase1 if t < change_point else rates_phase2\n        reward = np.random.binomial(1, rates[arm])\n        \n        algo.update(arm, reward)\n        actions.append(arm)\n        rewards.append(reward)\n    \n    return actions, rewards\n\n# è¿è¡Œå®éªŒ\nucb_actions, _ = run_nonstationary_experiment('UCB')\nts_actions, _ = run_nonstationary_experiment('TS')\n\n# åˆ†æè‡‚ 0 çš„é€‰æ‹©æ¯”ä¾‹ï¼ˆæ»‘åŠ¨çª—å£ï¼‰\nwindow = 500\n\ndef calculate_arm0_ratio(actions, window_size):\n    ratios = []\n    for i in range(window_size, len(actions)):\n        window_actions = actions[i-window_size:i]\n        ratio = sum(1 for a in window_actions if a == 0) / window_size\n        ratios.append(ratio)\n    return ratios\n\nucb_arm0 = calculate_arm0_ratio(ucb_actions, window)\nts_arm0 = calculate_arm0_ratio(ts_actions, window)\n\n# å¯è§†åŒ–\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=list(range(window, len(ucb_actions))),\n    y=ucb_arm0,\n    mode='lines',\n    name='UCB',\n    line=dict(color=COLORS['purple'], width=2)\n))\n\nfig.add_trace(go.Scatter(\n    x=list(range(window, len(ts_actions))),\n    y=ts_arm0,\n    mode='lines',\n    name='Thompson Sampling',\n    line=dict(color=COLORS['blue'], width=2)\n))\n\n# æ ‡æ³¨å˜åŒ–ç‚¹\nfig.add_vline(x=5000, line_dash=\"dash\", line_color=\"red\",\n              annotation_text=\"è‡‚ 0 å˜ä¸ºæœ€ä¼˜\")\n\nfig.update_layout(\n    title='Non-Stationary Bandit: ç®—æ³•é€‚åº”é€Ÿåº¦å¯¹æ¯”<br><sub>è‡‚ 0 çš„é€‰æ‹©æ¯”ä¾‹ï¼ˆæ»‘åŠ¨çª—å£=500ï¼‰</sub>',\n    xaxis_title='è½®æ•°',\n    yaxis_title='è‡‚ 0 é€‰æ‹©æ¯”ä¾‹',\n    template='plotly_white',\n    height=500\n)\nfig.show()\n\nprint(\"\\nğŸ“Š åˆ†æï¼š\")\nprint(\"- Thompson Sampling é€‚åº”æ›´å¿«ï¼ˆè´å¶æ–¯æ›´æ–°çµæ´»ï¼‰\")\nprint(\"- UCB é€‚åº”è¾ƒæ…¢ï¼ˆå†å²æ•°æ®æƒé‡å¤§ï¼‰\")\nprint(\"\\næ”¹è¿›æ–¹æ³•ï¼š\")\nprint(\"- Discounted UCB: ç»™è¿‘æœŸæ•°æ®æ›´é«˜æƒé‡\")\nprint(\"- Sliding Window: åªä½¿ç”¨æœ€è¿‘ N è½®æ•°æ®\")\nprint(\"- Change Point Detection: æ£€æµ‹ç¯å¢ƒå˜åŒ–å¹¶é‡ç½®\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€è€ƒé¢˜\n",
    "\n",
    "### 1. Exploration vs Exploitation\n",
    "\n",
    "**é—®é¢˜**ï¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥æ›´å¤šæ¢ç´¢ï¼Ÿä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥æ›´å¤šåˆ©ç”¨ï¼Ÿ\n",
    "\n",
    "**æç¤º**ï¼šè€ƒè™‘æ—¶é—´ã€æˆæœ¬ã€é£é™©ç­‰å› ç´ ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2. MAB çš„å±€é™\n",
    "\n",
    "**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆ MAB ä¸èƒ½å®Œå…¨æ›¿ä»£ A/B Testingï¼Ÿ\n",
    "\n",
    "**æç¤º**ï¼šè€ƒè™‘ç»Ÿè®¡æ¨æ–­ã€å› æœè§£é‡Šã€é•¿æœŸæ•ˆåº”ç­‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Contextual Bandit çš„ç‰¹å¾å·¥ç¨‹\n",
    "\n",
    "**é—®é¢˜**ï¼šåœ¨æ¨èç³»ç»Ÿä¸­ï¼Œåº”è¯¥å¦‚ä½•è®¾è®¡ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Ÿ\n",
    "\n",
    "**æç¤º**ï¼šç”¨æˆ·ç‰¹å¾ã€ç‰©å“ç‰¹å¾ã€äº¤å‰ç‰¹å¾ç­‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ä» Bandit åˆ° RL\n",
    "\n",
    "**é—®é¢˜**ï¼šMulti-Armed Bandit å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n",
    "\n",
    "**æç¤º**ï¼šBandit æ˜¯ RL çš„ç‰¹æ®Šæƒ…å†µï¼ˆå•æ­¥å†³ç­–ï¼‰ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ€»ç»“\n",
    "\n",
    "## æœ¬èŠ‚è¦ç‚¹\n",
    "\n",
    "### æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "1. **Explore-Exploit Trade-off**ï¼šæ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡\n",
    "2. **Regret**ï¼šè¡¡é‡ç®—æ³•æ€§èƒ½çš„æ ¸å¿ƒæŒ‡æ ‡\n",
    "3. **Contextual Bandit**ï¼šåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯åšä¸ªæ€§åŒ–å†³ç­–\n",
    "\n",
    "### ç®—æ³•å¯¹æ¯”\n",
    "\n",
    "| ç®—æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|-----|------|------|--------|\n",
    "| **Epsilon-Greedy** | ç®€å•æ˜“å®ç° | æ¢ç´¢ç›²ç›® | å¿«é€ŸåŸå‹ |\n",
    "| **UCB** | ç†è®ºä¿è¯å¼º | éœ€è¦è°ƒå‚ | éœ€è¦ç†è®ºä¿è¯ |\n",
    "| **Thompson Sampling** | æ€§èƒ½æœ€ä¼˜ | ç†è®ºåˆ†æéš¾ | è¿½æ±‚æ€§èƒ½ |\n",
    "| **LinUCB** | åˆ©ç”¨ä¸Šä¸‹æ–‡ | çº¿æ€§å‡è®¾ | ä¸ªæ€§åŒ–æ¨è |\n",
    "| **Neural Bandit** | å»ºæ¨¡èƒ½åŠ›å¼º | è®¡ç®—æˆæœ¬é«˜ | å¤æ‚æ¨¡å¼ |\n",
    "\n",
    "### ç†è®ºç»“æœ\n",
    "\n",
    "- **UCB**: $R_T = O(\\sqrt{KT \\log T})$\n",
    "- **Thompson Sampling**: $R_T = O(\\sqrt{KT})$\n",
    "- **Lower Bound**: $R_T = \\Omega\\left( \\sum_a \\frac{\\log T}{\\Delta_a} \\right)$\n",
    "\n",
    "### å®è·µåº”ç”¨\n",
    "\n",
    "- **æ¨èç³»ç»Ÿ**ï¼šæ–°é—»ã€è§†é¢‘ã€å•†å“æ¨è\n",
    "- **å¹¿å‘ŠæŠ•æ”¾**ï¼šåŠ¨æ€é€‰æ‹©å¹¿å‘Š\n",
    "- **åŠ¨æ€å®šä»·**ï¼šç”µå•†ã€æ‰“è½¦\n",
    "- **ä¸´åºŠè¯•éªŒ**ï¼šè‡ªé€‚åº”è¯•éªŒè®¾è®¡\n",
    "\n",
    "## è¿›é˜¶é˜…è¯»\n",
    "\n",
    "### ç»å…¸è®ºæ–‡\n",
    "\n",
    "1. **Auer et al. (2002)**: \"Finite-time Analysis of the Multiarmed Bandit Problem\" - UCB ç®—æ³•\n",
    "2. **Chapelle & Li (2011)**: \"An Empirical Evaluation of Thompson Sampling\" - Thompson Sampling å®è¯åˆ†æ\n",
    "3. **Li et al. (2010)**: \"A Contextual-Bandit Approach to Personalized News Article Recommendation\" - LinUCB åœ¨ Yahoo! çš„åº”ç”¨\n",
    "\n",
    "### ä¹¦ç±\n",
    "\n",
    "- **Lattimore & SzepesvÃ¡ri (2020)**: \"Bandit Algorithms\" - å…¨é¢çš„ç†è®ºæ•™æ\n",
    "- **Sutton & Barto (2018)**: \"Reinforcement Learning: An Introduction\" - RL å…¥é—¨ï¼ŒåŒ…å« Bandit ç« èŠ‚\n",
    "\n",
    "### åœ¨çº¿èµ„æº\n",
    "\n",
    "- **Vowpal Wabbit**: å·¥ä¸šçº§ Contextual Bandit åº“\n",
    "- **Microsoft Decision Service**: å¾®è½¯çš„ Bandit æœåŠ¡\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‹ä¸€æ­¥\n",
    "\n",
    "- **å®è·µ**ï¼šåœ¨çœŸå®é¡¹ç›®ä¸­åº”ç”¨ MABï¼ˆå¦‚æ¨èç³»ç»Ÿã€A/B æµ‹è¯•ä¼˜åŒ–ï¼‰\n",
    "- **æ·±å…¥**ï¼šå­¦ä¹ å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰ï¼ŒMAB æ˜¯ RL çš„åŸºç¡€\n",
    "- **ç ”ç©¶**ï¼šæ¢ç´¢æœ€æ–°ç ”ç©¶ï¼ˆå¦‚ Neural Banditã€Offline Bandit Evaluationï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "æ­å–œä½ å®Œæˆæœ¬èŠ‚å­¦ä¹ ï¼ğŸ‰\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº† Multi-Armed Bandit çš„æ ¸å¿ƒæ€æƒ³å’Œä¸»æµç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å®é™…ä¸šåŠ¡ä¸­åº”ç”¨è¿™äº›æ–¹æ³•ä¼˜åŒ–å†³ç­–ã€‚\n",
    "\n",
    "è®°ä½ï¼š**æœ€å¥½çš„ç®—æ³•æ˜¯é€‚åˆä½ ä¸šåŠ¡åœºæ™¯çš„ç®—æ³•**ã€‚ä¸è¦ç›²ç›®è¿½æ±‚ç†è®ºæœ€ä¼˜ï¼Œè¦è€ƒè™‘å®æ–½æˆæœ¬ã€å¯è§£é‡Šæ€§ã€å›¢é˜Ÿèƒ½åŠ›ç­‰å› ç´ ã€‚\n",
    "\n",
    "Happy Optimizing! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}