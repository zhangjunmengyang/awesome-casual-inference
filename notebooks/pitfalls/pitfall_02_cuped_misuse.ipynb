{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitfall 02: CUPED é€‚ç”¨æ¡ä»¶è¯¯ç”¨\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ notebook åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. **è¯†åˆ«** CUPED çš„é€‚ç”¨æ¡ä»¶å’Œå¸¸è§è¯¯ç”¨åœºæ™¯\n",
    "2. **è¯Šæ–­** CUPED å¤±æ•ˆçš„åŸå› \n",
    "3. **æŒæ¡** æ­£ç¡®ä½¿ç”¨ CUPED çš„æœ€ä½³å®è·µ\n",
    "4. **åº”å¯¹** é¢è¯•ä¸­å…³äº CUPED çš„æ·±åº¦è¿½é—®\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– èƒŒæ™¯æ•…äº‹\n",
    "\n",
    "> **åœºæ™¯ï¼šå®éªŒå¹³å°çš„å›°æƒ‘**\n",
    ">\n",
    "> ä½ æ˜¯æŸå¤§å‚å®éªŒå¹³å°çš„æ•°æ®ç§‘å­¦å®¶ã€‚æœ€è¿‘äº§å“å›¢é˜ŸæŠ±æ€¨ï¼š\"æˆ‘ä»¬ç”¨äº† CUPEDï¼Œä½†æ–¹å·®ç¼©å‡æ•ˆæœä¸æ˜æ˜¾ï¼Œæœ‰æ—¶ç”šè‡³å˜å·®äº†ï¼\"\n",
    ">\n",
    "> ç»è¿‡æ’æŸ¥ï¼Œä½ å‘ç°å›¢é˜Ÿåœ¨ä»¥ä¸‹åœºæ™¯é”™è¯¯ä½¿ç”¨äº† CUPEDï¼š\n",
    "> - æ–°ç”¨æˆ·å®éªŒï¼ˆæ²¡æœ‰å†å²æ•°æ®ï¼‰\n",
    "> - åå˜é‡ä¸ç»“æœç›¸å…³æ€§å¾ˆä½\n",
    "> - å¤„ç†æ•ˆåº”å½±å“äº†åå˜é‡\n",
    "> - æ ·æœ¬é‡å¤ªå°\n",
    ">\n",
    "> **æœ¬ notebook å°†æ·±å…¥åˆ†æè¿™äº›é™·é˜±ï¼Œå¸®ä½ å½»åº•ç†è§£ CUPEDã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: CUPED åŸç†å¿«é€Ÿå›é¡¾\n",
    "\n",
    "### 1.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "CUPED (Controlled-experiment Using Pre-Experiment Data) çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\n",
    "\n",
    "**åˆ©ç”¨å®éªŒå‰çš„åå˜é‡ X ä¸ç»“æœ Y çš„ç›¸å…³æ€§ï¼Œå‡å°‘ç»“æœå˜é‡çš„æ–¹å·®ã€‚**\n",
    "\n",
    "### 1.2 æ•°å­¦æ¨å¯¼\n",
    "\n",
    "è®¾ï¼š\n",
    "- $Y$ ä¸ºç»“æœå˜é‡\n",
    "- $X$ ä¸ºå®éªŒå‰åå˜é‡ï¼ˆå¦‚å†å²æŒ‡æ ‡ï¼‰\n",
    "\n",
    "CUPED è°ƒæ•´åçš„ç»“æœï¼š\n",
    "$$Y_{adj} = Y - \\theta (X - \\bar{X})$$\n",
    "\n",
    "å…¶ä¸­ $\\theta = \\frac{Cov(Y, X)}{Var(X)}$ æ˜¯æœ€å°åŒ– $Var(Y_{adj})$ çš„æœ€ä¼˜ç³»æ•°ã€‚\n",
    "\n",
    "### 1.3 æ–¹å·®ç¼©å‡\n",
    "\n",
    "$$Var(Y_{adj}) = Var(Y)(1 - \\rho^2_{XY})$$\n",
    "\n",
    "å…¶ä¸­ $\\rho_{XY}$ æ˜¯ $X$ å’Œ $Y$ çš„ç›¸å…³ç³»æ•°ã€‚\n",
    "\n",
    "**å…³é”®æ´è§**ï¼šæ–¹å·®ç¼©å‡ç¨‹åº¦å–å†³äºç›¸å…³æ€§ $\\rho^2$ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuped_adjust(Y, X):\n",
    "    \"\"\"\n",
    "    æ ‡å‡† CUPED è°ƒæ•´\n",
    "    \n",
    "    Args:\n",
    "        Y: ç»“æœå˜é‡\n",
    "        X: åå˜é‡ï¼ˆå®éªŒå‰æ•°æ®ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "        Y_adj: è°ƒæ•´åçš„ç»“æœ\n",
    "        theta: è°ƒæ•´ç³»æ•°\n",
    "        rho: ç›¸å…³ç³»æ•°\n",
    "    \"\"\"\n",
    "    # è®¡ç®— theta\n",
    "    cov_xy = np.cov(Y, X)[0, 1]\n",
    "    var_x = np.var(X, ddof=1)\n",
    "    theta = cov_xy / var_x if var_x > 0 else 0\n",
    "    \n",
    "    # è®¡ç®—ç›¸å…³ç³»æ•°\n",
    "    rho = np.corrcoef(Y, X)[0, 1]\n",
    "    \n",
    "    # CUPED è°ƒæ•´\n",
    "    Y_adj = Y - theta * (X - np.mean(X))\n",
    "    \n",
    "    return Y_adj, theta, rho\n",
    "\n",
    "def run_ab_test(Y_control, Y_treatment, alpha=0.05):\n",
    "    \"\"\"\n",
    "    è¿è¡Œ A/B æµ‹è¯•å¹¶è¿”å›ç»“æœ\n",
    "    \"\"\"\n",
    "    n_c, n_t = len(Y_control), len(Y_treatment)\n",
    "    \n",
    "    # å‡å€¼å·®\n",
    "    diff = np.mean(Y_treatment) - np.mean(Y_control)\n",
    "    \n",
    "    # æ ‡å‡†è¯¯\n",
    "    se = np.sqrt(np.var(Y_control, ddof=1)/n_c + np.var(Y_treatment, ddof=1)/n_t)\n",
    "    \n",
    "    # t ç»Ÿè®¡é‡å’Œ p å€¼\n",
    "    t_stat = diff / se\n",
    "    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n_c + n_t - 2))\n",
    "    \n",
    "    # ç½®ä¿¡åŒºé—´\n",
    "    ci = stats.t.interval(1-alpha, df=n_c+n_t-2, loc=diff, scale=se)\n",
    "    \n",
    "    return {\n",
    "        'diff': diff,\n",
    "        'se': se,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'ci': ci,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "print(\"âœ… å·¥å…·å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: å¤±è´¥æ¨¡å¼ 1 - åå˜é‡ä¸ç»“æœç›¸å…³æ€§ä½\n",
    "\n",
    "### 2.1 é—®é¢˜æè¿°\n",
    "\n",
    "**å¸¸è§è¯¯åŒº**ï¼š\"åªè¦æœ‰å†å²æ•°æ®ï¼Œç”¨ CUPED å°±ä¸€å®šèƒ½å‡å°‘æ–¹å·®\"\n",
    "\n",
    "**çœŸç›¸**ï¼šå¦‚æœ $\\rho_{XY}$ å¾ˆä½ï¼Œæ–¹å·®ç¼©å‡æ•ˆæœå¾®ä¹å…¶å¾®ï¼Œç”šè‡³å¯èƒ½å› ä¸ºä¼°è®¡è¯¯å·®åè€Œå˜å·®ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_low_correlation_cuped(n_samples=1000, true_effect=2.0, rho=0.1, n_simulations=500):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿä½ç›¸å…³æ€§åœºæ™¯ä¸‹çš„ CUPED æ•ˆæœ\n",
    "    \"\"\"\n",
    "    results_raw = []\n",
    "    results_cuped = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # ç”Ÿæˆç›¸å…³çš„ X å’Œ Y\n",
    "        # Y = rho * X + sqrt(1-rho^2) * noise + treatment_effect\n",
    "        X = np.random.randn(n_samples)\n",
    "        noise = np.random.randn(n_samples)\n",
    "        \n",
    "        # æ§åˆ¶ç»„\n",
    "        Y_c = rho * X[:n_samples//2] + np.sqrt(1-rho**2) * noise[:n_samples//2]\n",
    "        X_c = X[:n_samples//2]\n",
    "        \n",
    "        # å®éªŒç»„\n",
    "        Y_t = rho * X[n_samples//2:] + np.sqrt(1-rho**2) * noise[n_samples//2:] + true_effect\n",
    "        X_t = X[n_samples//2:]\n",
    "        \n",
    "        # åŸå§‹ A/B æµ‹è¯•\n",
    "        result_raw = run_ab_test(Y_c, Y_t)\n",
    "        results_raw.append(result_raw)\n",
    "        \n",
    "        # CUPED è°ƒæ•´\n",
    "        Y_c_adj, _, _ = cuped_adjust(Y_c, X_c)\n",
    "        Y_t_adj, _, _ = cuped_adjust(Y_t, X_t)\n",
    "        \n",
    "        result_cuped = run_ab_test(Y_c_adj, Y_t_adj)\n",
    "        results_cuped.append(result_cuped)\n",
    "    \n",
    "    return results_raw, results_cuped\n",
    "\n",
    "# å¯¹æ¯”ä¸åŒç›¸å…³æ€§\n",
    "correlations = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "variance_reduction = []\n",
    "power_improvement = []\n",
    "\n",
    "print(\"æ¨¡æ‹Ÿä¸åŒç›¸å…³æ€§ä¸‹çš„ CUPED æ•ˆæœ...\\n\")\n",
    "\n",
    "for rho in correlations:\n",
    "    results_raw, results_cuped = simulate_low_correlation_cuped(n_samples=500, rho=rho, n_simulations=300)\n",
    "    \n",
    "    # è®¡ç®—æ–¹å·®ç¼©å‡\n",
    "    var_raw = np.mean([r['se']**2 for r in results_raw])\n",
    "    var_cuped = np.mean([r['se']**2 for r in results_cuped])\n",
    "    var_reduction = 1 - var_cuped / var_raw\n",
    "    variance_reduction.append(var_reduction)\n",
    "    \n",
    "    # è®¡ç®— Power æå‡\n",
    "    power_raw = np.mean([r['significant'] for r in results_raw])\n",
    "    power_cuped = np.mean([r['significant'] for r in results_cuped])\n",
    "    power_improvement.append(power_cuped - power_raw)\n",
    "    \n",
    "    print(f\"ç›¸å…³æ€§ Ï = {rho:.1f}:\")\n",
    "    print(f\"  ç†è®ºæ–¹å·®ç¼©å‡: {rho**2:.1%}\")\n",
    "    print(f\"  å®é™…æ–¹å·®ç¼©å‡: {var_reduction:.1%}\")\n",
    "    print(f\"  Power æå‡: {power_cuped - power_raw:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æ–¹å·®ç¼©å‡\n",
    "ax1 = axes[0]\n",
    "theoretical = [rho**2 for rho in correlations]\n",
    "x = np.arange(len(correlations))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, theoretical, width, label='ç†è®ºå€¼', color='#3498db', alpha=0.8)\n",
    "ax1.bar(x + width/2, variance_reduction, width, label='å®é™…å€¼', color='#e74c3c', alpha=0.8)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'Ï={r}' for r in correlations])\n",
    "ax1.set_ylabel('æ–¹å·®ç¼©å‡æ¯”ä¾‹')\n",
    "ax1.set_title('CUPED æ–¹å·®ç¼©å‡ï¼šç†è®º vs å®é™…')\n",
    "ax1.legend()\n",
    "ax1.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='10% é˜ˆå€¼')\n",
    "\n",
    "# Power æå‡\n",
    "ax2 = axes[1]\n",
    "colors = ['#e74c3c' if p < 0.02 else '#2ecc71' for p in power_improvement]\n",
    "ax2.bar(correlations, power_improvement, color=colors, alpha=0.8, width=0.15)\n",
    "ax2.set_xlabel('ç›¸å…³ç³»æ•° Ï')\n",
    "ax2.set_ylabel('Power æå‡')\n",
    "ax2.set_title('CUPED å¸¦æ¥çš„ Power æå‡')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®å‘ç°ï¼š\")\n",
    "print(f\"   å½“ Ï < 0.3 æ—¶ï¼ŒCUPED æ–¹å·®ç¼©å‡ < 10%ï¼Œæ•ˆæœæœ‰é™\")\n",
    "print(f\"   å®é™…æ–¹å·®ç¼©å‡å¯èƒ½ç•¥ä½äºç†è®ºå€¼ï¼ˆå› ä¸º Î¸ éœ€è¦ä¼°è®¡ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 è¯Šæ–­æ–¹æ³•\n",
    "\n",
    "**åœ¨åº”ç”¨ CUPED å‰ï¼Œå…ˆæ£€æŸ¥ç›¸å…³æ€§ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuped_diagnostic(Y, X, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    CUPED è¯Šæ–­å‡½æ•°\n",
    "    åˆ¤æ–­æ˜¯å¦é€‚åˆä½¿ç”¨ CUPED\n",
    "    \"\"\"\n",
    "    # è®¡ç®—ç›¸å…³æ€§\n",
    "    rho, p_value = stats.pearsonr(Y, X)\n",
    "    \n",
    "    # ç†è®ºæ–¹å·®ç¼©å‡\n",
    "    theoretical_reduction = rho ** 2\n",
    "    \n",
    "    # åˆ¤æ–­\n",
    "    recommendations = []\n",
    "    \n",
    "    if abs(rho) < 0.3:\n",
    "        recommendations.append(\"âš ï¸ ç›¸å…³æ€§è¾ƒä½ (|Ï| < 0.3)ï¼ŒCUPED æ•ˆæœæœ‰é™\")\n",
    "    elif abs(rho) < 0.5:\n",
    "        recommendations.append(\"âš¡ ç›¸å…³æ€§ä¸­ç­‰ (0.3 â‰¤ |Ï| < 0.5)ï¼ŒCUPED æœ‰ä¸€å®šæ•ˆæœ\")\n",
    "    else:\n",
    "        recommendations.append(\"âœ… ç›¸å…³æ€§è¾ƒå¼º (|Ï| â‰¥ 0.5)ï¼Œæ¨èä½¿ç”¨ CUPED\")\n",
    "    \n",
    "    if p_value > significance_level:\n",
    "        recommendations.append(\"âš ï¸ ç›¸å…³æ€§ä¸æ˜¾è‘—ï¼ŒCUPED å¯èƒ½å¼•å…¥å™ªå£°\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"CUPED è¯Šæ–­æŠ¥å‘Š\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nç›¸å…³ç³»æ•°: Ï = {rho:.4f}\")\n",
    "    print(f\"ç›¸å…³æ€§ p-value: {p_value:.4f}\")\n",
    "    print(f\"ç†è®ºæ–¹å·®ç¼©å‡: {theoretical_reduction:.2%}\")\n",
    "    print(f\"\\nå»ºè®®ï¼š\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"  {rec}\")\n",
    "    \n",
    "    return {\n",
    "        'rho': rho,\n",
    "        'p_value': p_value,\n",
    "        'theoretical_reduction': theoretical_reduction,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "# ä½ç›¸å…³æ€§åœºæ™¯\n",
    "X_low = np.random.randn(1000)\n",
    "Y_low = 0.1 * X_low + np.random.randn(1000)\n",
    "\n",
    "print(\"åœºæ™¯ 1: ä½ç›¸å…³æ€§\")\n",
    "cuped_diagnostic(Y_low, X_low)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# é«˜ç›¸å…³æ€§åœºæ™¯\n",
    "X_high = np.random.randn(1000)\n",
    "Y_high = 0.8 * X_high + 0.6 * np.random.randn(1000)\n",
    "\n",
    "print(\"åœºæ™¯ 2: é«˜ç›¸å…³æ€§\")\n",
    "cuped_diagnostic(Y_high, X_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: å¤±è´¥æ¨¡å¼ 2 - æ–°ç”¨æˆ·æ²¡æœ‰å†å²æ•°æ®\n",
    "\n",
    "### 3.1 é—®é¢˜æè¿°\n",
    "\n",
    "**åœºæ™¯**ï¼šå¯¹æ–°ç”¨æˆ·è¿›è¡Œå®éªŒï¼Œä½†æ–°ç”¨æˆ·æ²¡æœ‰å†å²è¡Œä¸ºæ•°æ®ã€‚\n",
    "\n",
    "**å¸¸è§é”™è¯¯åšæ³•**ï¼š\n",
    "1. ç”¨ 0 å¡«å……ç¼ºå¤±å€¼\n",
    "2. ç”¨å…¨ä½“å‡å€¼å¡«å……\n",
    "3. å¼ºè¡Œä½¿ç”¨æ³¨å†Œåçš„æ•°æ®ä½œä¸º\"å†å²\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_new_user_cuped(n_samples=2000, new_user_ratio=0.3, true_effect=2.0):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿæ–°ç”¨æˆ·åœºæ™¯ä¸‹çš„ CUPED é—®é¢˜\n",
    "    \"\"\"\n",
    "    n_new = int(n_samples * new_user_ratio)\n",
    "    n_old = n_samples - n_new\n",
    "    \n",
    "    # è€ç”¨æˆ·ï¼šæœ‰å†å²æ•°æ®ï¼Œç›¸å…³æ€§é«˜\n",
    "    X_old = np.random.randn(n_old) * 10 + 50  # å†å² GMV\n",
    "    noise_old = np.random.randn(n_old) * 5\n",
    "    Y_old = 0.7 * X_old + noise_old + 20  # å½“æœŸ GMV\n",
    "    \n",
    "    # æ–°ç”¨æˆ·ï¼šæ— å†å²æ•°æ®\n",
    "    X_new_missing = np.full(n_new, np.nan)\n",
    "    noise_new = np.random.randn(n_new) * 8  # æ–°ç”¨æˆ·æ–¹å·®æ›´å¤§\n",
    "    Y_new = 30 + noise_new  # æ–°ç”¨æˆ·åŸºç¡€å€¼ä¸åŒ\n",
    "    \n",
    "    # åˆå¹¶\n",
    "    X_all = np.concatenate([X_old, X_new_missing])\n",
    "    Y_all = np.concatenate([Y_old, Y_new])\n",
    "    is_new = np.concatenate([np.zeros(n_old), np.ones(n_new)])\n",
    "    \n",
    "    # éšæœºåˆ†ç»„\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    X_all, Y_all, is_new = X_all[perm], Y_all[perm], is_new[perm]\n",
    "    \n",
    "    # åˆ†ä¸ºæ§åˆ¶ç»„å’Œå®éªŒç»„\n",
    "    n_half = n_samples // 2\n",
    "    X_c, Y_c, is_new_c = X_all[:n_half], Y_all[:n_half], is_new[:n_half]\n",
    "    X_t, Y_t, is_new_t = X_all[n_half:], Y_all[n_half:] + true_effect, is_new[n_half:]\n",
    "    \n",
    "    return X_c, Y_c, is_new_c, X_t, Y_t, is_new_t\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "X_c, Y_c, is_new_c, X_t, Y_t, is_new_t = simulate_new_user_cuped(n_samples=2000, new_user_ratio=0.4)\n",
    "\n",
    "print(\"æ•°æ®æ¦‚å†µï¼š\")\n",
    "print(f\"  æ§åˆ¶ç»„: {len(Y_c)} æ ·æœ¬, æ–°ç”¨æˆ· {is_new_c.sum():.0f} ({is_new_c.mean():.1%})\")\n",
    "print(f\"  å®éªŒç»„: {len(Y_t)} æ ·æœ¬, æ–°ç”¨æˆ· {is_new_t.sum():.0f} ({is_new_t.mean():.1%})\")\n",
    "print(f\"  ç¼ºå¤±åå˜é‡: {np.isnan(X_c).sum() + np.isnan(X_t).sum()} ä¸ª\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ä¸åŒå¤„ç†ç¼ºå¤±å€¼çš„æ–¹æ³•\n",
    "\n",
    "def compare_missing_strategies(X_c, Y_c, X_t, Y_t):\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”ä¸åŒçš„ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # ç­–ç•¥ 0: ä¸ä½¿ç”¨ CUPEDï¼ˆåŸºå‡†ï¼‰\n",
    "    result_raw = run_ab_test(Y_c, Y_t)\n",
    "    results['ä¸ä½¿ç”¨ CUPED'] = result_raw\n",
    "    \n",
    "    # ç­–ç•¥ 1: ç”¨ 0 å¡«å……\n",
    "    X_c_zero = np.nan_to_num(X_c, nan=0)\n",
    "    X_t_zero = np.nan_to_num(X_t, nan=0)\n",
    "    Y_c_adj, _, _ = cuped_adjust(Y_c, X_c_zero)\n",
    "    Y_t_adj, _, _ = cuped_adjust(Y_t, X_t_zero)\n",
    "    results['å¡«å…… 0'] = run_ab_test(Y_c_adj, Y_t_adj)\n",
    "    \n",
    "    # ç­–ç•¥ 2: ç”¨å‡å€¼å¡«å……\n",
    "    X_c_mean = np.nan_to_num(X_c, nan=np.nanmean(X_c))\n",
    "    X_t_mean = np.nan_to_num(X_t, nan=np.nanmean(X_t))\n",
    "    Y_c_adj, _, _ = cuped_adjust(Y_c, X_c_mean)\n",
    "    Y_t_adj, _, _ = cuped_adjust(Y_t, X_t_mean)\n",
    "    results['å¡«å……å‡å€¼'] = run_ab_test(Y_c_adj, Y_t_adj)\n",
    "    \n",
    "    # ç­–ç•¥ 3: åªå¯¹æœ‰å†å²æ•°æ®çš„ç”¨æˆ·ä½¿ç”¨ CUPED\n",
    "    mask_c = ~np.isnan(X_c)\n",
    "    mask_t = ~np.isnan(X_t)\n",
    "    \n",
    "    if mask_c.sum() > 0 and mask_t.sum() > 0:\n",
    "        Y_c_old_adj, theta_c, _ = cuped_adjust(Y_c[mask_c], X_c[mask_c])\n",
    "        Y_t_old_adj, theta_t, _ = cuped_adjust(Y_t[mask_t], X_t[mask_t])\n",
    "        \n",
    "        # æ–°ç”¨æˆ·ä¸è°ƒæ•´\n",
    "        Y_c_combined = np.concatenate([Y_c_old_adj, Y_c[~mask_c]])\n",
    "        Y_t_combined = np.concatenate([Y_t_old_adj, Y_t[~mask_t]])\n",
    "        results['åˆ†å±‚ CUPED'] = run_ab_test(Y_c_combined, Y_t_combined)\n",
    "    \n",
    "    # ç­–ç•¥ 4: åªåˆ†æè€ç”¨æˆ·ï¼ˆå¯èƒ½å¼•å…¥é€‰æ‹©åå·®ï¼‰\n",
    "    if mask_c.sum() > 0 and mask_t.sum() > 0:\n",
    "        Y_c_adj, _, _ = cuped_adjust(Y_c[mask_c], X_c[mask_c])\n",
    "        Y_t_adj, _, _ = cuped_adjust(Y_t[mask_t], X_t[mask_t])\n",
    "        results['ä»…è€ç”¨æˆ·+CUPED'] = run_ab_test(Y_c_adj, Y_t_adj)\n",
    "    \n",
    "    return results\n",
    "\n",
    "strategies_results = compare_missing_strategies(X_c, Y_c, X_t, Y_t)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ä¸åŒç¼ºå¤±å€¼å¤„ç†ç­–ç•¥å¯¹æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nçœŸå®æ•ˆåº” = 2.0\")\n",
    "print(f\"\\n{'ç­–ç•¥':<20} {'ä¼°è®¡æ•ˆåº”':<12} {'æ ‡å‡†è¯¯':<12} {'p-value':<12}\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "for strategy, result in strategies_results.items():\n",
    "    print(f\"{strategy:<20} {result['diff']:<12.3f} {result['se']:<12.3f} {result['p_value']:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ç­–ç•¥æ•ˆæœ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "strategies = list(strategies_results.keys())\n",
    "se_values = [strategies_results[s]['se'] for s in strategies]\n",
    "diff_values = [strategies_results[s]['diff'] for s in strategies]\n",
    "\n",
    "# æ ‡å‡†è¯¯å¯¹æ¯”\n",
    "ax1 = axes[0]\n",
    "colors = ['#3498db' if s == 'åˆ†å±‚ CUPED' else '#95a5a6' for s in strategies]\n",
    "bars = ax1.bar(range(len(strategies)), se_values, color=colors)\n",
    "ax1.set_xticks(range(len(strategies)))\n",
    "ax1.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "ax1.set_ylabel('æ ‡å‡†è¯¯')\n",
    "ax1.set_title('ä¸åŒç­–ç•¥çš„æ ‡å‡†è¯¯')\n",
    "ax1.axhline(y=se_values[0], color='red', linestyle='--', alpha=0.5, label='åŸºå‡†')\n",
    "\n",
    "# æ•ˆåº”ä¼°è®¡åå·®\n",
    "ax2 = axes[1]\n",
    "true_effect = 2.0\n",
    "bias = [d - true_effect for d in diff_values]\n",
    "colors = ['#2ecc71' if abs(b) < 0.5 else '#e74c3c' for b in bias]\n",
    "ax2.bar(range(len(strategies)), bias, color=colors)\n",
    "ax2.set_xticks(range(len(strategies)))\n",
    "ax2.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "ax2.set_ylabel('ä¼°è®¡åå·® (ä¼°è®¡å€¼ - çœŸå®å€¼)')\n",
    "ax2.set_title('ä¸åŒç­–ç•¥çš„ä¼°è®¡åå·®')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®å‘ç°ï¼š\")\n",
    "print(\"  1. ç”¨ 0 å¡«å……ä¼šå¼•å…¥ç³»ç»Ÿåå·®ï¼ˆæ–°ç”¨æˆ·çš„ 0 ä¸æ˜¯çœŸå®çš„å†å²å€¼ï¼‰\")\n",
    "print(\"  2. ç”¨å‡å€¼å¡«å……ä¼šå‰Šå¼± CUPED æ•ˆæœï¼ˆé™ä½äº†ç›¸å…³æ€§ï¼‰\")\n",
    "print(\"  3. åˆ†å±‚ CUPEDï¼ˆè€ç”¨æˆ·ç”¨ CUPEDï¼Œæ–°ç”¨æˆ·ä¸ç”¨ï¼‰æ˜¯æœ€ä½³ç­–ç•¥\")\n",
    "print(\"  4. ä»…åˆ†æè€ç”¨æˆ·å¯èƒ½å¼•å…¥é€‰æ‹©åå·®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ­£ç¡®åšæ³•ï¼šåˆ†å±‚ CUPED\n",
    "\n",
    "**æœ€ä½³å®è·µ**ï¼š\n",
    "1. å°†ç”¨æˆ·åˆ†ä¸º\"æœ‰å†å²æ•°æ®\"å’Œ\"æ— å†å²æ•°æ®\"ä¸¤å±‚\n",
    "2. å¯¹æœ‰å†å²æ•°æ®çš„ç”¨æˆ·ä½¿ç”¨ CUPED\n",
    "3. å¯¹æ— å†å²æ•°æ®çš„ç”¨æˆ·ä½¿ç”¨åŸå§‹å€¼\n",
    "4. åˆå¹¶ä¸¤å±‚çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_cuped(Y_c, X_c, Y_t, X_t):\n",
    "    \"\"\"\n",
    "    åˆ†å±‚ CUPED çš„æ­£ç¡®å®ç°\n",
    "    \"\"\"\n",
    "    # è¯†åˆ«æœ‰/æ— å†å²æ•°æ®çš„ç”¨æˆ·\n",
    "    mask_c = ~np.isnan(X_c)\n",
    "    mask_t = ~np.isnan(X_t)\n",
    "    \n",
    "    # æœ‰å†å²æ•°æ®çš„ç”¨æˆ·ï¼šä½¿ç”¨ CUPED\n",
    "    if mask_c.sum() > 0:\n",
    "        Y_c_old_adj, theta_c, rho_c = cuped_adjust(Y_c[mask_c], X_c[mask_c])\n",
    "        print(f\"è€ç”¨æˆ· CUPED å‚æ•°: Î¸={theta_c:.3f}, Ï={rho_c:.3f}\")\n",
    "    else:\n",
    "        Y_c_old_adj = np.array([])\n",
    "    \n",
    "    if mask_t.sum() > 0:\n",
    "        Y_t_old_adj, theta_t, rho_t = cuped_adjust(Y_t[mask_t], X_t[mask_t])\n",
    "    else:\n",
    "        Y_t_old_adj = np.array([])\n",
    "    \n",
    "    # æ— å†å²æ•°æ®çš„ç”¨æˆ·ï¼šä½¿ç”¨åŸå§‹å€¼\n",
    "    Y_c_new = Y_c[~mask_c]\n",
    "    Y_t_new = Y_t[~mask_t]\n",
    "    \n",
    "    # åˆå¹¶\n",
    "    Y_c_final = np.concatenate([Y_c_old_adj, Y_c_new])\n",
    "    Y_t_final = np.concatenate([Y_t_old_adj, Y_t_new])\n",
    "    \n",
    "    print(f\"\\nåˆ†å±‚ç»Ÿè®¡:\")\n",
    "    print(f\"  æ§åˆ¶ç»„: è€ç”¨æˆ· {mask_c.sum()}, æ–°ç”¨æˆ· {(~mask_c).sum()}\")\n",
    "    print(f\"  å®éªŒç»„: è€ç”¨æˆ· {mask_t.sum()}, æ–°ç”¨æˆ· {(~mask_t).sum()}\")\n",
    "    \n",
    "    return Y_c_final, Y_t_final\n",
    "\n",
    "# åº”ç”¨åˆ†å±‚ CUPED\n",
    "print(\"åˆ†å±‚ CUPED åº”ç”¨:\")\n",
    "print(\"=\" * 50)\n",
    "Y_c_final, Y_t_final = stratified_cuped(Y_c, X_c, Y_t, X_t)\n",
    "\n",
    "result = run_ab_test(Y_c_final, Y_t_final)\n",
    "print(f\"\\næ•ˆåº”ä¼°è®¡: {result['diff']:.3f} (çœŸå®å€¼: 2.0)\")\n",
    "print(f\"æ ‡å‡†è¯¯: {result['se']:.3f}\")\n",
    "print(f\"p-value: {result['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: å¤±è´¥æ¨¡å¼ 3 - å¤„ç†æ•ˆåº”å½±å“åå˜é‡\n",
    "\n",
    "### 4.1 é—®é¢˜æè¿°\n",
    "\n",
    "**CUPED çš„æ ¸å¿ƒå‡è®¾**ï¼šåå˜é‡ $X$ åœ¨å®éªŒå‰æµ‹é‡ï¼Œä¸å—å¤„ç†çš„å½±å“ã€‚\n",
    "\n",
    "**è¿ååœºæ™¯**ï¼š\n",
    "- ä½¿ç”¨å®éªŒæœŸé—´çš„æ•°æ®ä½œä¸ºåå˜é‡\n",
    "- åå˜é‡æ˜¯ç»“æœå˜é‡çš„ä¸€éƒ¨åˆ†\n",
    "- åå˜é‡å—åˆ°å¤„ç†çš„é—´æ¥å½±å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_confounded_cuped(n_samples=2000, true_effect=2.0, n_simulations=300):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿåå˜é‡å—å¤„ç†å½±å“çš„åœºæ™¯\n",
    "    \n",
    "    é”™è¯¯åœºæ™¯ï¼šä½¿ç”¨å®éªŒç¬¬ä¸€å‘¨çš„æ•°æ®ä½œä¸º\"åå˜é‡\"æ¥é¢„æµ‹ç¬¬äºŒå‘¨\n",
    "    é—®é¢˜ï¼šç¬¬ä¸€å‘¨çš„æ•°æ®å·²ç»å—åˆ°å¤„ç†çš„å½±å“ï¼\n",
    "    \"\"\"\n",
    "    results_correct = []  # ä½¿ç”¨å®éªŒå‰æ•°æ®\n",
    "    results_wrong = []    # ä½¿ç”¨å®éªŒæœŸé—´æ•°æ®ï¼ˆé”™è¯¯ï¼‰\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # å®éªŒå‰æ•°æ®ï¼ˆæœªå—å¤„ç†å½±å“ï¼‰\n",
    "        X_pre = np.random.randn(n_samples) * 10 + 50\n",
    "        \n",
    "        # åˆ†ç»„\n",
    "        n_half = n_samples // 2\n",
    "        X_pre_c, X_pre_t = X_pre[:n_half], X_pre[n_half:]\n",
    "        \n",
    "        # ç¬¬ä¸€å‘¨ç»“æœï¼ˆå·²å—å¤„ç†å½±å“ï¼‰\n",
    "        Y_week1_c = 0.7 * X_pre_c + np.random.randn(n_half) * 5 + 20\n",
    "        Y_week1_t = 0.7 * X_pre_t + np.random.randn(n_half) * 5 + 20 + true_effect * 0.5  # å—å¤„ç†å½±å“\n",
    "        \n",
    "        # ç¬¬äºŒå‘¨ç»“æœï¼ˆç›®æ ‡å˜é‡ï¼‰\n",
    "        Y_week2_c = 0.6 * X_pre_c + 0.3 * Y_week1_c + np.random.randn(n_half) * 4 + 10\n",
    "        Y_week2_t = 0.6 * X_pre_t + 0.3 * Y_week1_t + np.random.randn(n_half) * 4 + 10 + true_effect  # ä¸»æ•ˆåº”\n",
    "        \n",
    "        # æ­£ç¡®åšæ³•ï¼šä½¿ç”¨å®éªŒå‰æ•°æ®\n",
    "        Y_c_adj, _, _ = cuped_adjust(Y_week2_c, X_pre_c)\n",
    "        Y_t_adj, _, _ = cuped_adjust(Y_week2_t, X_pre_t)\n",
    "        results_correct.append(run_ab_test(Y_c_adj, Y_t_adj))\n",
    "        \n",
    "        # é”™è¯¯åšæ³•ï¼šä½¿ç”¨ç¬¬ä¸€å‘¨æ•°æ®ï¼ˆå·²å—å¤„ç†å½±å“ï¼‰\n",
    "        Y_c_adj_wrong, _, _ = cuped_adjust(Y_week2_c, Y_week1_c)\n",
    "        Y_t_adj_wrong, _, _ = cuped_adjust(Y_week2_t, Y_week1_t)\n",
    "        results_wrong.append(run_ab_test(Y_c_adj_wrong, Y_t_adj_wrong))\n",
    "    \n",
    "    return results_correct, results_wrong\n",
    "\n",
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "print(\"æ¨¡æ‹Ÿåå˜é‡å—å¤„ç†å½±å“çš„åœºæ™¯...\\n\")\n",
    "results_correct, results_wrong = simulate_confounded_cuped(n_simulations=500)\n",
    "\n",
    "# åˆ†æç»“æœ\n",
    "true_effect = 2.0\n",
    "\n",
    "estimates_correct = [r['diff'] for r in results_correct]\n",
    "estimates_wrong = [r['diff'] for r in results_wrong]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"åå˜é‡é€‰æ‹©çš„å½±å“\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nçœŸå®æ•ˆåº”: {true_effect}\")\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'å¹³å‡ä¼°è®¡':<12} {'åå·®':<12} {'æ ‡å‡†å·®':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'æ­£ç¡®: ä½¿ç”¨å®éªŒå‰æ•°æ®':<25} {np.mean(estimates_correct):<12.3f} {np.mean(estimates_correct)-true_effect:<12.3f} {np.std(estimates_correct):<12.3f}\")\n",
    "print(f\"{'é”™è¯¯: ä½¿ç”¨ç¬¬ä¸€å‘¨æ•°æ®':<25} {np.mean(estimates_wrong):<12.3f} {np.mean(estimates_wrong)-true_effect:<12.3f} {np.std(estimates_wrong):<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ä¼°è®¡åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ä¼°è®¡åˆ†å¸ƒå¯¹æ¯”\n",
    "ax1 = axes[0]\n",
    "ax1.hist(estimates_correct, bins=30, alpha=0.6, label='æ­£ç¡®: å®éªŒå‰æ•°æ®', color='#2ecc71')\n",
    "ax1.hist(estimates_wrong, bins=30, alpha=0.6, label='é”™è¯¯: ç¬¬ä¸€å‘¨æ•°æ®', color='#e74c3c')\n",
    "ax1.axvline(x=true_effect, color='black', linestyle='--', linewidth=2, label=f'çœŸå®æ•ˆåº”={true_effect}')\n",
    "ax1.set_xlabel('æ•ˆåº”ä¼°è®¡')\n",
    "ax1.set_ylabel('é¢‘æ•°')\n",
    "ax1.set_title('æ•ˆåº”ä¼°è®¡åˆ†å¸ƒå¯¹æ¯”')\n",
    "ax1.legend()\n",
    "\n",
    "# åå·®åˆ†æ\n",
    "ax2 = axes[1]\n",
    "bias_correct = np.array(estimates_correct) - true_effect\n",
    "bias_wrong = np.array(estimates_wrong) - true_effect\n",
    "\n",
    "data_to_plot = [bias_correct, bias_wrong]\n",
    "bp = ax2.boxplot(data_to_plot, labels=['æ­£ç¡®', 'é”™è¯¯'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('#2ecc71')\n",
    "bp['boxes'][1].set_facecolor('#e74c3c')\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_ylabel('ä¼°è®¡åå·®')\n",
    "ax2.set_title('ä¼°è®¡åå·®åˆ†å¸ƒ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®å‘ç°ï¼š\")\n",
    "print(f\"   ä½¿ç”¨å—å¤„ç†å½±å“çš„åå˜é‡ä¼šå¯¼è‡´ä¼°è®¡åå·®\")\n",
    "print(f\"   åå·®æ–¹å‘ï¼šé€šå¸¸ä½ä¼°å¤„ç†æ•ˆåº”\")\n",
    "print(f\"   åŸå› ï¼šåå˜é‡\"å¸æ”¶\"äº†éƒ¨åˆ†å¤„ç†æ•ˆåº”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: å¤±è´¥æ¨¡å¼ 4 - æ ·æœ¬é‡è¿‡å°\n",
    "\n",
    "### 5.1 é—®é¢˜æè¿°\n",
    "\n",
    "CUPED éœ€è¦ä¼°è®¡ Î¸ = Cov(Y,X)/Var(X)ï¼Œå½“æ ·æœ¬é‡å°æ—¶ï¼š\n",
    "- Î¸ çš„ä¼°è®¡æ–¹å·®å¤§\n",
    "- å¯èƒ½å¼•å…¥é¢å¤–å™ªå£°\n",
    "- æ–¹å·®ç¼©å‡æ•ˆæœä¸ç¨³å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_small_sample_cuped(sample_sizes=[50, 100, 200, 500, 1000, 2000], \n",
    "                                  rho=0.6, true_effect=2.0, n_simulations=500):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿä¸åŒæ ·æœ¬é‡ä¸‹çš„ CUPED æ•ˆæœ\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        var_reductions = []\n",
    "        biases = []\n",
    "        \n",
    "        for _ in range(n_simulations):\n",
    "            # ç”Ÿæˆæ•°æ®\n",
    "            X = np.random.randn(n)\n",
    "            noise = np.random.randn(n)\n",
    "            \n",
    "            n_half = n // 2\n",
    "            Y_c = rho * X[:n_half] + np.sqrt(1-rho**2) * noise[:n_half]\n",
    "            Y_t = rho * X[n_half:] + np.sqrt(1-rho**2) * noise[n_half:] + true_effect\n",
    "            X_c, X_t = X[:n_half], X[n_half:]\n",
    "            \n",
    "            # åŸå§‹æ–¹å·®\n",
    "            var_raw = np.var(Y_c, ddof=1) + np.var(Y_t, ddof=1)\n",
    "            \n",
    "            # CUPED\n",
    "            Y_c_adj, _, _ = cuped_adjust(Y_c, X_c)\n",
    "            Y_t_adj, _, _ = cuped_adjust(Y_t, X_t)\n",
    "            var_cuped = np.var(Y_c_adj, ddof=1) + np.var(Y_t_adj, ddof=1)\n",
    "            \n",
    "            var_reductions.append(1 - var_cuped / var_raw)\n",
    "            \n",
    "            # ä¼°è®¡åå·®\n",
    "            estimate = np.mean(Y_t_adj) - np.mean(Y_c_adj)\n",
    "            biases.append(estimate - true_effect)\n",
    "        \n",
    "        results[n] = {\n",
    "            'var_reduction_mean': np.mean(var_reductions),\n",
    "            'var_reduction_std': np.std(var_reductions),\n",
    "            'bias_mean': np.mean(biases),\n",
    "            'bias_std': np.std(biases)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "print(\"æ¨¡æ‹Ÿä¸åŒæ ·æœ¬é‡çš„ CUPED æ•ˆæœ...\\n\")\n",
    "sample_size_results = simulate_small_sample_cuped()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ä¸åŒæ ·æœ¬é‡ä¸‹çš„ CUPED æ•ˆæœ (ç†è®ºæ–¹å·®ç¼©å‡ = {0.6**2:.1%})\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'æ ·æœ¬é‡':<10} {'æ–¹å·®ç¼©å‡å‡å€¼':<15} {'æ–¹å·®ç¼©å‡æ ‡å‡†å·®':<15} {'ä¼°è®¡åå·®å‡å€¼':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n, r in sample_size_results.items():\n",
    "    print(f\"{n:<10} {r['var_reduction_mean']:<15.2%} {r['var_reduction_std']:<15.2%} {r['bias_mean']:<15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sample_sizes = list(sample_size_results.keys())\n",
    "var_reduction_means = [sample_size_results[n]['var_reduction_mean'] for n in sample_sizes]\n",
    "var_reduction_stds = [sample_size_results[n]['var_reduction_std'] for n in sample_sizes]\n",
    "\n",
    "# æ–¹å·®ç¼©å‡\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(sample_sizes, var_reduction_means, yerr=var_reduction_stds, \n",
    "            fmt='o-', capsize=5, capthick=2, color='#3498db')\n",
    "ax1.axhline(y=0.6**2, color='red', linestyle='--', label=f'ç†è®ºå€¼ ({0.6**2:.1%})')\n",
    "ax1.set_xlabel('æ ·æœ¬é‡')\n",
    "ax1.set_ylabel('æ–¹å·®ç¼©å‡')\n",
    "ax1.set_title('æ ·æœ¬é‡å¯¹æ–¹å·®ç¼©å‡çš„å½±å“')\n",
    "ax1.legend()\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# æ–¹å·®ç¼©å‡çš„ç¨³å®šæ€§\n",
    "ax2 = axes[1]\n",
    "ax2.plot(sample_sizes, var_reduction_stds, 'o-', color='#e74c3c')\n",
    "ax2.set_xlabel('æ ·æœ¬é‡')\n",
    "ax2.set_ylabel('æ–¹å·®ç¼©å‡çš„æ ‡å‡†å·®')\n",
    "ax2.set_title('æ ·æœ¬é‡å¯¹ä¼°è®¡ç¨³å®šæ€§çš„å½±å“')\n",
    "ax2.axhline(y=0.05, color='gray', linestyle='--', alpha=0.5, label='5% é˜ˆå€¼')\n",
    "ax2.set_xscale('log')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®å‘ç°ï¼š\")\n",
    "print(\"   æ ·æœ¬é‡ < 200 æ—¶ï¼Œæ–¹å·®ç¼©å‡ä¼°è®¡ä¸ç¨³å®š\")\n",
    "print(\"   å»ºè®®ï¼šè‡³å°‘ 200 æ ·æœ¬/ç»„æ‰è€ƒè™‘ä½¿ç”¨ CUPED\")\n",
    "print(\"   æ ·æœ¬é‡è¶Šå¤§ï¼ŒCUPED æ•ˆæœè¶Šæ¥è¿‘ç†è®ºå€¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: CUPED æœ€ä½³å®è·µæ¸…å•\n",
    "\n",
    "### ğŸ§ª TODO ç»ƒä¹ : å®ç° CUPED å‰ç½®æ£€æŸ¥\n",
    "\n",
    "è¯·å®ç°ä¸€ä¸ªå®Œæ•´çš„ CUPED å‰ç½®æ£€æŸ¥å‡½æ•°ï¼Œåœ¨åº”ç”¨ CUPED å‰éªŒè¯å„é¡¹æ¡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def cuped_preflight_check(Y_control, X_control, Y_treatment, X_treatment, \n                          min_correlation=0.3, min_sample_size=200, alpha=0.05):\n    \"\"\"\n    CUPED å‰ç½®æ£€æŸ¥\n    \n    æ£€æŸ¥é¡¹ï¼š\n    1. æ ·æœ¬é‡æ˜¯å¦è¶³å¤Ÿ\n    2. ç›¸å…³æ€§æ˜¯å¦è¶³å¤Ÿå¼º\n    3. åå˜é‡æ˜¯å¦æœ‰ç¼ºå¤±å€¼\n    4. åå˜é‡æ˜¯å¦åœ¨å¤„ç†å‰æµ‹é‡ï¼ˆé€šè¿‡æ£€æŸ¥ç»„é—´å¹³è¡¡æ€§ï¼‰\n    \n    Returns:\n        passed: æ˜¯å¦é€šè¿‡æ‰€æœ‰æ£€æŸ¥\n        report: è¯¦ç»†æ£€æŸ¥æŠ¥å‘Š\n    \"\"\"\n    checks = []\n    passed = True\n    \n    # æ£€æŸ¥ 1: æ ·æœ¬é‡\n    n_c, n_t = len(Y_control), len(Y_treatment)\n    sample_size_ok = n_c >= min_sample_size and n_t >= min_sample_size\n    checks.append({\n        'name': 'æ ·æœ¬é‡æ£€æŸ¥',\n        'passed': sample_size_ok,\n        'message': f\"æ§åˆ¶ç»„: {n_c}, å®éªŒç»„: {n_t} (æœ€ä½è¦æ±‚: {min_sample_size})\",\n        'severity': 'error' if not sample_size_ok else 'ok'\n    })\n    if not sample_size_ok:\n        passed = False\n    \n    # æ£€æŸ¥ 2: ç›¸å…³æ€§\n    X_valid_c = X_control[~np.isnan(X_control)]\n    Y_valid_c = Y_control[~np.isnan(X_control)]\n    X_valid_t = X_treatment[~np.isnan(X_treatment)]\n    Y_valid_t = Y_treatment[~np.isnan(X_treatment)]\n    \n    if len(X_valid_c) > 2 and len(X_valid_t) > 2:\n        X_all = np.concatenate([X_valid_c, X_valid_t])\n        Y_all = np.concatenate([Y_valid_c, Y_valid_t])\n        rho, p_val = stats.pearsonr(X_all, Y_all)\n        \n        corr_ok = abs(rho) >= min_correlation\n        checks.append({\n            'name': 'ç›¸å…³æ€§æ£€æŸ¥',\n            'passed': corr_ok,\n            'message': f\"Ï = {rho:.3f} (æœ€ä½è¦æ±‚: {min_correlation}), ç†è®ºæ–¹å·®ç¼©å‡: {rho**2:.1%}\",\n            'severity': 'warning' if not corr_ok else 'ok'\n        })\n    else:\n        checks.append({\n            'name': 'ç›¸å…³æ€§æ£€æŸ¥',\n            'passed': False,\n            'message': 'æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§',\n            'severity': 'error'\n        })\n        passed = False\n    \n    # æ£€æŸ¥ 3: ç¼ºå¤±å€¼\n    missing_c = np.isnan(X_control).sum()\n    missing_t = np.isnan(X_treatment).sum()\n    missing_ratio = (missing_c + missing_t) / (n_c + n_t)\n    \n    missing_ok = missing_ratio < 0.3\n    checks.append({\n        'name': 'ç¼ºå¤±å€¼æ£€æŸ¥',\n        'passed': missing_ok,\n        'message': f\"æ§åˆ¶ç»„ç¼ºå¤±: {missing_c}, å®éªŒç»„ç¼ºå¤±: {missing_t} ({missing_ratio:.1%})\",\n        'severity': 'warning' if missing_ratio > 0.1 else 'ok'\n    })\n    \n    # æ£€æŸ¥ 4: åå˜é‡åˆ†å¸ƒå¹³è¡¡æ€§ï¼ˆå®éªŒå‰åº”è¯¥å¹³è¡¡ï¼‰\n    if len(X_valid_c) > 2 and len(X_valid_t) > 2:\n        t_stat, p_val = stats.ttest_ind(X_valid_c, X_valid_t)\n        balance_ok = p_val > alpha\n        checks.append({\n            'name': 'åå˜é‡å¹³è¡¡æ€§æ£€æŸ¥',\n            'passed': balance_ok,\n            'message': f\"p-value = {p_val:.4f}\" + (\" (ä¸å¹³è¡¡ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜!)\" if not balance_ok else \" (å¹³è¡¡)\"),\n            'severity': 'warning' if not balance_ok else 'ok'\n        })\n    \n    # æ‰“å°æŠ¥å‘Š\n    print(\"=\" * 60)\n    print(\"CUPED å‰ç½®æ£€æŸ¥æŠ¥å‘Š\")\n    print(\"=\" * 60)\n    \n    for check in checks:\n        status = 'âœ…' if check['passed'] else ('âš ï¸' if check['severity'] == 'warning' else 'âŒ')\n        print(f\"\\n{status} {check['name']}\")\n        print(f\"   {check['message']}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    if passed:\n        print(\"âœ… æ‰€æœ‰å…³é”®æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥ä½¿ç”¨ CUPED\")\n    else:\n        print(\"âŒ å­˜åœ¨å…³é”®é—®é¢˜ï¼Œå»ºè®®ä¸ä½¿ç”¨ CUPED æˆ–å…ˆè§£å†³é—®é¢˜\")\n    \n    return passed, checks\n\nprint(\"âœ… CUPED å‰ç½®æ£€æŸ¥å‡½æ•°å·²å®ç°\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ å‚è€ƒç­”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å‚è€ƒç­”æ¡ˆï¼šCUPED å‰ç½®æ£€æŸ¥å®Œæ•´å®ç°\n",
    "# ============================================================\n",
    "\n",
    "def cuped_preflight_check(Y_control, X_control, Y_treatment, X_treatment, \n",
    "                          min_correlation=0.3, min_sample_size=200, alpha=0.05):\n",
    "    \"\"\"\n",
    "    CUPED å‰ç½®æ£€æŸ¥ - å®Œæ•´å®ç°\n",
    "    \"\"\"\n",
    "    checks = []\n",
    "    passed = True\n",
    "    \n",
    "    # æ£€æŸ¥ 1: æ ·æœ¬é‡\n",
    "    n_c, n_t = len(Y_control), len(Y_treatment)\n",
    "    sample_size_ok = n_c >= min_sample_size and n_t >= min_sample_size\n",
    "    checks.append({\n",
    "        'name': 'æ ·æœ¬é‡æ£€æŸ¥',\n",
    "        'passed': sample_size_ok,\n",
    "        'message': f\"æ§åˆ¶ç»„: {n_c}, å®éªŒç»„: {n_t} (æœ€ä½è¦æ±‚: {min_sample_size})\",\n",
    "        'severity': 'error' if not sample_size_ok else 'ok'\n",
    "    })\n",
    "    if not sample_size_ok:\n",
    "        passed = False\n",
    "    \n",
    "    # æ£€æŸ¥ 2: ç›¸å…³æ€§\n",
    "    X_valid_c = X_control[~np.isnan(X_control)]\n",
    "    Y_valid_c = Y_control[~np.isnan(X_control)]\n",
    "    X_valid_t = X_treatment[~np.isnan(X_treatment)]\n",
    "    Y_valid_t = Y_treatment[~np.isnan(X_treatment)]\n",
    "    \n",
    "    if len(X_valid_c) > 2 and len(X_valid_t) > 2:\n",
    "        X_all = np.concatenate([X_valid_c, X_valid_t])\n",
    "        Y_all = np.concatenate([Y_valid_c, Y_valid_t])\n",
    "        rho, p_val = stats.pearsonr(X_all, Y_all)\n",
    "        \n",
    "        corr_ok = abs(rho) >= min_correlation\n",
    "        checks.append({\n",
    "            'name': 'ç›¸å…³æ€§æ£€æŸ¥',\n",
    "            'passed': corr_ok,\n",
    "            'message': f\"Ï = {rho:.3f} (æœ€ä½è¦æ±‚: {min_correlation}), ç†è®ºæ–¹å·®ç¼©å‡: {rho**2:.1%}\",\n",
    "            'severity': 'warning' if not corr_ok else 'ok'\n",
    "        })\n",
    "    else:\n",
    "        checks.append({\n",
    "            'name': 'ç›¸å…³æ€§æ£€æŸ¥',\n",
    "            'passed': False,\n",
    "            'message': 'æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§',\n",
    "            'severity': 'error'\n",
    "        })\n",
    "        passed = False\n",
    "    \n",
    "    # æ£€æŸ¥ 3: ç¼ºå¤±å€¼\n",
    "    missing_c = np.isnan(X_control).sum()\n",
    "    missing_t = np.isnan(X_treatment).sum()\n",
    "    missing_ratio = (missing_c + missing_t) / (n_c + n_t)\n",
    "    \n",
    "    missing_ok = missing_ratio < 0.3\n",
    "    checks.append({\n",
    "        'name': 'ç¼ºå¤±å€¼æ£€æŸ¥',\n",
    "        'passed': missing_ok,\n",
    "        'message': f\"æ§åˆ¶ç»„ç¼ºå¤±: {missing_c}, å®éªŒç»„ç¼ºå¤±: {missing_t} ({missing_ratio:.1%})\",\n",
    "        'severity': 'warning' if missing_ratio > 0.1 else 'ok'\n",
    "    })\n",
    "    \n",
    "    # æ£€æŸ¥ 4: åå˜é‡åˆ†å¸ƒå¹³è¡¡æ€§\n",
    "    if len(X_valid_c) > 2 and len(X_valid_t) > 2:\n",
    "        t_stat, p_val = stats.ttest_ind(X_valid_c, X_valid_t)\n",
    "        balance_ok = p_val > alpha\n",
    "        checks.append({\n",
    "            'name': 'åå˜é‡å¹³è¡¡æ€§æ£€æŸ¥',\n",
    "            'passed': balance_ok,\n",
    "            'message': f\"p-value = {p_val:.4f}\" + (\" (ä¸å¹³è¡¡ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜!)\" if not balance_ok else \" (å¹³è¡¡)\"),\n",
    "            'severity': 'warning' if not balance_ok else 'ok'\n",
    "        })\n",
    "    \n",
    "    # æ‰“å°æŠ¥å‘Š\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CUPED å‰ç½®æ£€æŸ¥æŠ¥å‘Š\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for check in checks:\n",
    "        status = 'âœ…' if check['passed'] else ('âš ï¸' if check['severity'] == 'warning' else 'âŒ')\n",
    "        print(f\"\\n{status} {check['name']}\")\n",
    "        print(f\"   {check['message']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if passed:\n",
    "        print(\"âœ… æ‰€æœ‰å…³é”®æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥ä½¿ç”¨ CUPED\")\n",
    "    else:\n",
    "        print(\"âŒ å­˜åœ¨å…³é”®é—®é¢˜ï¼Œå»ºè®®ä¸ä½¿ç”¨ CUPED æˆ–å…ˆè§£å†³é—®é¢˜\")\n",
    "    \n",
    "    return passed, checks\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"æµ‹è¯•åœºæ™¯ 1: æ­£å¸¸æƒ…å†µ\")\n",
    "X_c = np.random.randn(500)\n",
    "Y_c = 0.7 * X_c + np.random.randn(500) * 0.5\n",
    "X_t = np.random.randn(500)\n",
    "Y_t = 0.7 * X_t + np.random.randn(500) * 0.5 + 2\n",
    "\n",
    "cuped_preflight_check(Y_c, X_c, Y_t, X_t)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"æµ‹è¯•åœºæ™¯ 2: é—®é¢˜åœºæ™¯ï¼ˆæ ·æœ¬é‡å°ã€ç›¸å…³æ€§ä½ï¼‰\")\n",
    "X_c_small = np.random.randn(50)\n",
    "Y_c_small = 0.1 * X_c_small + np.random.randn(50)\n",
    "X_t_small = np.random.randn(50)\n",
    "Y_t_small = 0.1 * X_t_small + np.random.randn(50) + 2\n",
    "\n",
    "cuped_preflight_check(Y_c_small, X_c_small, Y_t_small, X_t_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ æ€è€ƒé¢˜\n",
    "\n",
    "1. **åå˜é‡é€‰æ‹©**ï¼šå¦‚æœæœ‰å¤šä¸ªå€™é€‰åå˜é‡ï¼Œå¦‚ä½•é€‰æ‹©æœ€ä½³çš„ï¼Ÿ\n",
    "   - æç¤ºï¼šè€ƒè™‘ç›¸å…³æ€§ã€ç¨³å®šæ€§ã€ä¸šåŠ¡æ„ä¹‰\n",
    "\n",
    "2. **å¤šåå˜é‡ CUPED**ï¼šå¦‚æœæƒ³åŒæ—¶ä½¿ç”¨å¤šä¸ªåå˜é‡ï¼Œè¯¥å¦‚ä½•å¤„ç†ï¼Ÿ\n",
    "   - æç¤ºï¼šå¤šå…ƒå›å½’ã€CUPAC\n",
    "\n",
    "3. **CUPED çš„è¾¹ç•Œ**ï¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ï¼ŒCUPED å¯èƒ½æ¯”åŸå§‹æ–¹æ³•æ›´å·®ï¼Ÿ\n",
    "   - æç¤ºï¼šå°æ ·æœ¬ã€ä¼°è®¡è¯¯å·®\n",
    "\n",
    "4. **ä¸åˆ†å±‚æŠ½æ ·çš„å…³ç³»**ï¼šCUPED å’Œåˆ†å±‚æŠ½æ ·éƒ½èƒ½å‡å°‘æ–¹å·®ï¼Œä¸¤è€…æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "   - æç¤ºï¼šäº‹å‰ vs äº‹åï¼Œé€‚ç”¨åœºæ™¯\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ æ€»ç»“\n",
    "\n",
    "### CUPED å¤±è´¥æ¨¡å¼æ€»ç»“\n",
    "\n",
    "| å¤±è´¥æ¨¡å¼ | åŸå›  | è¯Šæ–­æ–¹æ³• | è§£å†³æ–¹æ¡ˆ |\n",
    "|---------|------|---------|----------|\n",
    "| ç›¸å…³æ€§ä½ | Ï < 0.3 | è®¡ç®—ç›¸å…³ç³»æ•° | é€‰æ‹©æ›´å¥½çš„åå˜é‡ |\n",
    "| æ–°ç”¨æˆ·æ— æ•°æ® | ç¼ºå¤±å€¼ | æ£€æŸ¥ç¼ºå¤±æ¯”ä¾‹ | åˆ†å±‚ CUPED |\n",
    "| åå˜é‡å—å½±å“ | å®éªŒæœŸé—´æ•°æ® | æ£€æŸ¥æ•°æ®æ—¶é—´ | ä½¿ç”¨å®éªŒå‰æ•°æ® |\n",
    "| æ ·æœ¬é‡è¿‡å° | Î¸ ä¼°è®¡ä¸ç¨³å®š | æ£€æŸ¥æ ·æœ¬é‡ | è‡³å°‘ 200/ç»„ |\n",
    "\n",
    "### æœ€ä½³å®è·µæ¸…å•\n",
    "\n",
    "- [ ] ä½¿ç”¨**å®éªŒå‰**çš„æ•°æ®ä½œä¸ºåå˜é‡\n",
    "- [ ] æ£€æŸ¥ç›¸å…³æ€§ â‰¥ 0.3\n",
    "- [ ] ç¡®ä¿æ ·æœ¬é‡ â‰¥ 200/ç»„\n",
    "- [ ] å¤„ç†ç¼ºå¤±å€¼ï¼ˆåˆ†å±‚ CUPEDï¼‰\n",
    "- [ ] éªŒè¯åå˜é‡åœ¨ç»„é—´å¹³è¡¡\n",
    "- [ ] æŠ¥å‘Šæ–¹å·®ç¼©å‡æ•ˆæœ\n",
    "\n",
    "### é¢è¯•è¦ç‚¹\n",
    "\n",
    "- èƒ½è§£é‡Š CUPED çš„æ•°å­¦åŸç†\n",
    "- èƒ½åˆ—ä¸¾ CUPED çš„é€‚ç”¨æ¡ä»¶\n",
    "- èƒ½è¯Šæ–­ CUPED å¤±æ•ˆçš„åŸå› \n",
    "- èƒ½è®¾è®¡ CUPED çš„å‰ç½®æ£€æŸ¥æµç¨‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}