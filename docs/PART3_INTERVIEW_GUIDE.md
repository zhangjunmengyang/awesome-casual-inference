# Part 3: Quasi-Experiments - é¢è¯•æŒ‡å—

## ç›®å½•

1. [Difference-in-Differences (DID)](#1-difference-in-differences-did)
2. [Synthetic Control Method (SCM)](#2-synthetic-control-method-scm)
3. [Regression Discontinuity Design (RDD)](#3-regression-discontinuity-design-rdd)
4. [Instrumental Variables (IV)](#4-instrumental-variables-iv)
5. [æ–¹æ³•å¯¹æ¯”ä¸é€‰æ‹©](#5-æ–¹æ³•å¯¹æ¯”ä¸é€‰æ‹©)

---

## 1. Difference-in-Differences (DID)

### æ ¸å¿ƒåŸç†

**ä¸€å¥è¯æ€»ç»“**ï¼šé€šè¿‡å¯¹ç…§ç»„çš„å˜åŒ–è¶‹åŠ¿ï¼Œæ¨æ–­å¤„ç†ç»„åœ¨æ²¡æœ‰å¤„ç†æ—¶çš„åäº‹å®è¶‹åŠ¿ã€‚

### é«˜é¢‘é¢è¯•é¢˜

#### Q1: è§£é‡Š DID çš„æ ¸å¿ƒå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•æ£€éªŒï¼Ÿ

**ç­”æ¡ˆ**ï¼š

æ ¸å¿ƒå‡è®¾æ˜¯**å¹³è¡Œè¶‹åŠ¿å‡è®¾ï¼ˆParallel Trends Assumptionï¼‰**ï¼š

- **å®šä¹‰**ï¼šåœ¨æ²¡æœ‰å¤„ç†çš„æƒ…å†µä¸‹ï¼Œå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„ç»“æœå˜é‡ä¼šæœ‰ç›¸åŒçš„æ—¶é—´è¶‹åŠ¿
- **æ•°å­¦è¡¨è¾¾**ï¼šE[Yâ‚â‚œâ½â°â¾ - Yâ‚,â‚œâ‚‹â‚â½â°â¾] = E[Yâ‚€â‚œ - Yâ‚€,â‚œâ‚‹â‚]

**æ£€éªŒæ–¹æ³•**ï¼š

1. **å›¾å½¢åŒ–æ£€éªŒ**
   - ç»˜åˆ¶å¤„ç†ç»„å’Œå¯¹ç…§ç»„åœ¨æ”¿ç­–å‰çš„è¶‹åŠ¿å›¾
   - å¦‚æœä¸¤æ¡çº¿å¹³è¡Œ â†’ æ”¯æŒå‡è®¾

2. **Lead Testï¼ˆæå‰æœŸæ£€éªŒï¼‰**
   ```python
   # åœ¨æ”¿ç­–å‰çš„å„æœŸæ·»åŠ è™šæ‹Ÿå¤„ç†å˜é‡
   for t in pre_periods:
       df[f'lead_{t}'] = treat * (period == t)

   # å›å½’æ£€éªŒç³»æ•°æ˜¯å¦æ˜¾è‘—
   # å¦‚æœä¸æ˜¾è‘— â†’ æ”¯æŒå¹³è¡Œè¶‹åŠ¿
   ```

3. **å®‰æ…°å‰‚æ£€éªŒï¼ˆPlacebo Testï¼‰**
   - å‡è®¾ä¸€ä¸ªå‡çš„æ”¿ç­–æ—¶é—´ç‚¹ï¼ˆåœ¨çœŸå®æ”¿ç­–ä¹‹å‰ï¼‰
   - ä¼°è®¡"å‡"DID æ•ˆåº”
   - å¦‚æœä¸æ˜¾è‘— â†’ æ”¯æŒå¹³è¡Œè¶‹åŠ¿

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- å¼ºè°ƒ"å¹³è¡Œè¶‹åŠ¿æ˜¯ä¸å¯ç›´æ¥æ£€éªŒçš„"ï¼ˆå› ä¸ºè§‚å¯Ÿä¸åˆ°åäº‹å®ï¼‰
- æ‰€æœ‰æ£€éªŒéƒ½åªèƒ½æ£€éªŒæ”¿ç­–å‰çš„è¶‹åŠ¿ï¼Œéœ€è¦å‡è®¾æ”¿ç­–åä¹Ÿæˆç«‹
- å¦‚æœè¿åï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨åˆæˆæ§åˆ¶ã€åŒ¹é…ã€æˆ–æ§åˆ¶è¶‹åŠ¿å·®å¼‚

---

#### Q2: å¹³è¡Œè¶‹åŠ¿å‡è®¾è¿åäº†æ€ä¹ˆåŠï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**æ–¹æ³• 1ï¼šæ§åˆ¶æ—¶é—´è¶‹åŠ¿**
```python
# å…è®¸ä¸åŒç»„æœ‰ä¸åŒçš„çº¿æ€§è¶‹åŠ¿
model = 'Y ~ treat + post + treat_post + treat*time + post*time'
```

**æ–¹æ³• 2ï¼šåŒ¹é… + DID**
- å…ˆç”¨ PSM æ‰¾åˆ°è¶‹åŠ¿ç›¸ä¼¼çš„å¯¹ç…§ç»„
- å†åœ¨åŒ¹é…æ ·æœ¬ä¸Šåš DID

**æ–¹æ³• 3ï¼šæ”¹ç”¨åˆæˆæ§åˆ¶ï¼ˆSynthetic Controlï¼‰**
- ä¸å‡è®¾å¹³è¡Œè¶‹åŠ¿
- é€šè¿‡ä¼˜åŒ–æƒé‡æ‰¾åˆ°æœ€ä½³å¯¹ç…§ç»„åˆ

**æ–¹æ³• 4ï¼šæ§åˆ¶ç»„ç‰¹å®šè¶‹åŠ¿**
```python
# å…è®¸æ¯ä¸ªç»„æœ‰è‡ªå·±çš„å›ºå®šæ•ˆåº”å’Œè¶‹åŠ¿
model = 'Y ~ C(group) + C(time) + treat_post'
```

**æ–¹æ³• 5ï¼šè¿›è¡Œæ•æ„Ÿæ€§åˆ†æ**
- æŠ¥å‘Šä¸åŒè§„æ ¼ä¸‹çš„ç»“æœ
- æ£€æŸ¥ç»“æœçš„ç¨³å¥æ€§

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æ²¡æœ‰é“¶å¼¹ï¼Œéœ€è¦ç»“åˆå…·ä½“åœºæ™¯é€‰æ‹©æ–¹æ³•
- é€æ˜åº¦å¾ˆé‡è¦ï¼ŒæŠ¥å‘Šæ‰€æœ‰æ£€éªŒç»“æœ
- å¦‚æœå¤šç§æ–¹æ³•éƒ½å¾—åˆ°ç›¸åŒç»“è®ºï¼Œç»“æœæ›´å¯ä¿¡

---

#### Q3: äº¤é”™ DIDï¼ˆStaggered DIDï¼‰æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**é—®é¢˜**ï¼šå½“ä¸åŒå•ä½åœ¨ä¸åŒæ—¶é—´æ¥å—å¤„ç†æ—¶ï¼Œä¼ ç»Ÿçš„ TWFEï¼ˆTwo-Way Fixed Effectsï¼‰ä¼°è®¡é‡å¯èƒ½æœ‰åã€‚

**æ ¸å¿ƒåŸå› **ï¼š
1. å·²æ¥å—å¤„ç†çš„å•ä½ä¼šæˆä¸ºåæ¥å—å¤„ç†å•ä½çš„"å¯¹ç…§ç»„"
2. å¦‚æœå¤„ç†æ•ˆåº”éšæ—¶é—´å˜åŒ–ï¼Œä¼šå‡ºç°"è´Ÿæƒé‡"é—®é¢˜
3. Goodman-Bacon åˆ†è§£å®šç†è¡¨æ˜ï¼ŒTWFE æ˜¯å¤šä¸ª 2Ã—2 DID çš„åŠ æƒå¹³å‡ï¼Œä½†æƒé‡å¯èƒ½ä¸ºè´Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ³• 1ï¼šCallaway & Sant'Anna (2021)**
- ç”¨"Never-treated"æˆ–"Not-yet-treated"ä½œä¸ºå¯¹ç…§ç»„
- ä¼°è®¡æ¯ä¸ª cohort Ã— time çš„ ATT
- ç„¶åèšåˆ

**æ–¹æ³• 2ï¼šSun & Abraham (2021) - äº‹ä»¶ç ”ç©¶æ³•**
```python
# ç›¸å¯¹äºå¤„ç†æ—¶é—´çš„äº‹ä»¶æ—¶é—´
df['event_time'] = df['time'] - df['treatment_time']

# ä¼°è®¡æ¯ä¸ªäº‹ä»¶æ—¶é—´çš„æ•ˆåº”
for k in event_times:
    df[f'D_{k}'] = (event_time == k) * treated
```

**æ–¹æ³• 3ï¼šde Chaisemartin & D'Haultfoeuille (2020)**
- æä¾› DID_M ä¼°è®¡é‡
- æ£€æŸ¥è´Ÿæƒé‡çš„æ¯”ä¾‹

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ° Goodman-Bacon åˆ†è§£å®šç†
- æåˆ°"ç¦å¿Œæ¯”è¾ƒ"ï¼ˆForbidden Comparisonï¼‰çš„æ¦‚å¿µ
- çŸ¥é“ `did` R åŒ… æˆ– `csdid` Stata åŒ…

---

#### Q4: å¦‚ä½•å®ç°äº‹ä»¶ç ”ç©¶æ³•ï¼ˆEvent Studyï¼‰ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**ä»£ç å®ç°**ï¼š
```python
import statsmodels.formula.api as smf

# 1. åˆ›å»ºç›¸å¯¹æ—¶é—´å˜é‡
df['rel_time'] = df['time'] - df['treatment_time']

# 2. åˆ›å»ºäº‹ä»¶æ—¶é—´è™šæ‹Ÿå˜é‡ï¼ˆæ’é™¤ -1 ä½œä¸ºåŸºå‡†ï¼‰
for k in range(-5, 6):  # æ”¿ç­–å‰5æœŸåˆ°æ”¿ç­–å5æœŸ
    if k != -1:  # æ’é™¤ -1 ä½œä¸ºåŸºå‡†
        df[f'D_{k}'] = ((df['rel_time'] == k) & (df['treated'] == 1)).astype(int)

# 3. å›å½’
formula = 'Y ~ ' + ' + '.join([f'D_{k}' for k in range(-5, 6) if k != -1])
formula += ' + C(unit) + C(time)'  # å•ä½å’Œæ—¶é—´å›ºå®šæ•ˆåº”

model = smf.ols(formula, data=df).fit(cov_type='cluster', cov_kwds={'groups': df['unit']})

# 4. æå–ç³»æ•°å¹¶å¯è§†åŒ–
coeffs = [model.params[f'D_{k}'] if k != -1 else 0 for k in range(-5, 6)]
ses = [model.bse[f'D_{k}'] if k != -1 else 0 for k in range(-5, 6)]

# ç»˜åˆ¶äº‹ä»¶ç ”ç©¶å›¾
plt.errorbar(range(-5, 6), coeffs, yerr=1.96*np.array(ses))
plt.axhline(0, linestyle='--', color='gray')
plt.axvline(-0.5, linestyle='--', color='red')  # æ”¿ç­–å®æ–½æ—¶ç‚¹
```

**è§£è¯»**ï¼š
- **æ”¿ç­–å‰ï¼ˆk < 0ï¼‰**ï¼šç³»æ•°åº”æ¥è¿‘ 0 ä¸”ä¸æ˜¾è‘—ï¼ˆæ”¯æŒå¹³è¡Œè¶‹åŠ¿ï¼‰
- **æ”¿ç­–åï¼ˆk â‰¥ 0ï¼‰**ï¼šç³»æ•°æ˜¾è‘—åç¦» 0ï¼ˆæ”¿ç­–æœ‰æ•ˆï¼‰
- **åŠ¨æ€æ•ˆåº”**ï¼šå¯ä»¥çœ‹åˆ°æ•ˆåº”å¦‚ä½•éšæ—¶é—´æ¼”å˜

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°"event study æ˜¯ DID åœ¨æ—¶é—´ç»´åº¦çš„åˆ†è§£"
- æåˆ°"å¯ä»¥æ£€éªŒé¢„æœŸæ•ˆåº”ï¼ˆanticipationï¼‰å’Œæ»åæ•ˆåº”ï¼ˆpersistenceï¼‰"
- å¼ºè°ƒèšç±»æ ‡å‡†è¯¯çš„é‡è¦æ€§

---

## 2. Synthetic Control Method (SCM)

### æ ¸å¿ƒåŸç†

**ä¸€å¥è¯æ€»ç»“**ï¼šç”¨å¤šä¸ªå¯¹ç…§å•ä½çš„åŠ æƒç»„åˆï¼Œåˆæˆä¸€ä¸ªä¸å¤„ç†å•ä½æœ€ç›¸ä¼¼çš„"è™šæ‹Ÿ"å¯¹ç…§ç»„ã€‚

### é«˜é¢‘é¢è¯•é¢˜

#### Q5: åˆæˆæ§åˆ¶ vs DIDï¼Œä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ

**ç­”æ¡ˆ**ï¼š

| ç»´åº¦ | DID | åˆæˆæ§åˆ¶ |
|------|-----|----------|
| **å¤„ç†å•ä½æ•°** | å¤šä¸ª | é€šå¸¸ 1 ä¸ª |
| **å¯¹ç…§ç»„æ„å»º** | ç®€å•åˆ†ç»„ | åŠ æƒç»„åˆ |
| **å…³é”®å‡è®¾** | å¹³è¡Œè¶‹åŠ¿ | å¯ä»¥çº¿æ€§ç»„åˆå‡ºåäº‹å® |
| **æ¨æ–­æ–¹æ³•** | æ ‡å‡†è¯¯ã€t æ£€éªŒ | Placebo Tests |
| **é€‚ç”¨åœºæ™¯** | æ”¿ç­–åœ¨å¤šåœ°å®æ–½ | å•ä¸€äº‹ä»¶ï¼ˆæŸåŸå¸‚ã€æŸæ³•æ¡ˆï¼‰ |
| **çµæ´»æ€§** | ä½ï¼ˆç­‰æƒé‡ï¼‰ | é«˜ï¼ˆä¼˜åŒ–æƒé‡ï¼‰ |

**ä½¿ç”¨ DID**ï¼š
- âœ… å¤šä¸ªå¤„ç†å•ä½
- âœ… å¤„ç†æ—¶ç‚¹ä¸€è‡´
- âœ… å¯¹ç…§ç»„å’Œå¤„ç†ç»„è¶‹åŠ¿ç›¸ä¼¼
- âœ… éœ€è¦æ§åˆ¶æ›´å¤šåå˜é‡

**ä½¿ç”¨åˆæˆæ§åˆ¶**ï¼š
- âœ… å•ä¸ªæˆ–å°‘æ•°å¤„ç†å•ä½
- âœ… æ‰¾ä¸åˆ°å®Œç¾çš„å¯¹ç…§
- âœ… å¹³è¡Œè¶‹åŠ¿å‡è®¾å­˜ç–‘
- âœ… å…³æ³¨ç‰¹å®šäº‹ä»¶çš„å› æœæ•ˆåº”

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- DID æ˜¯åˆæˆæ§åˆ¶çš„ç‰¹ä¾‹ï¼ˆç­‰æƒé‡ï¼‰
- å¯ä»¥ç»„åˆä½¿ç”¨ï¼šSynthetic DID
- æåˆ° Abadie (2021) çš„ç»¼è¿°æ–‡ç« 

---

#### Q6: åˆæˆæ§åˆ¶çš„æƒé‡å¦‚ä½•ä¼°è®¡ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**ä¼˜åŒ–ç›®æ ‡**ï¼š
$$
W^* = \arg\min_W \sum_{t=1}^{T_0} \left( Y_{1t} - \sum_{j=2}^{J+1} w_j Y_{jt} \right)^2
$$

**çº¦æŸæ¡ä»¶**ï¼š
- $w_j \geq 0$ ï¼ˆéè´Ÿï¼‰
- $\sum_{j} w_j = 1$ ï¼ˆæƒé‡å’Œä¸º 1ï¼‰

**å®ç°**ï¼š
```python
from scipy.optimize import minimize

def objective(w, treated_pre, donors_pre):
    synthetic = donors_pre @ w
    return np.sum((treated_pre - synthetic) ** 2)

constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
bounds = [(0, 1) for _ in range(n_donors)]
w0 = np.ones(n_donors) / n_donors

result = minimize(objective, w0,
                  args=(treated_pre, donors_pre),
                  method='SLSQP',
                  bounds=bounds,
                  constraints=constraints)

weights = result.x
```

**æ‰©å±•ï¼šåå˜é‡åŒ¹é…**
- ä¸ä»…åŒ¹é…ç»“æœå˜é‡çš„å†å²ï¼Œè¿˜åŒ¹é…åå˜é‡ï¼ˆGDPã€äººå£ç­‰ï¼‰
- ä¼˜åŒ–ç›®æ ‡å˜ä¸ºï¼š$\min_W \|X_1 - X_0 W\|_V^2$
- $V$ æ˜¯æƒé‡çŸ©é˜µï¼Œä½“ç°ä¸åŒç‰¹å¾çš„é‡è¦æ€§

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- è¿™æ˜¯ä¸€ä¸ªäºŒæ¬¡è§„åˆ’é—®é¢˜ï¼ˆQuadratic Programmingï¼‰
- æƒé‡é€šå¸¸å¾ˆç¨€ç–ï¼ˆå¾ˆå¤šä¸º 0ï¼‰
- ç¨€ç–æ€§æ˜¯å¥½äº‹ï¼šé¿å…è¿‡æ‹Ÿåˆï¼Œæ˜“äºè§£é‡Š

---

#### Q7: åˆæˆæ§åˆ¶å¦‚ä½•åšç»Ÿè®¡æ¨æ–­ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**æŒ‘æˆ˜**ï¼š
- åªæœ‰ 1 ä¸ªå¤„ç†å•ä½ â†’ æ— æ³•ç”¨ t æ£€éªŒ
- æ—¶é—´åºåˆ—ç›¸å…³ â†’ æ ‡å‡†è¯¯ä¼°è®¡å›°éš¾

**è§£å†³æ–¹æ¡ˆï¼šPlacebo Testsï¼ˆå®‰æ…°å‰‚æ£€éªŒï¼‰**

**æ“ä½œæ­¥éª¤**ï¼š

1. **å‡è£…æ¯ä¸ªä¾›ä½“éƒ½æ¥å—äº†å¤„ç†**
2. **å¯¹æ¯ä¸ª"å‡å¤„ç†"å•ä½ä¼°è®¡åˆæˆæ§åˆ¶**
3. **è®¡ç®—"å‡æ•ˆåº”"**
4. **æ¯”è¾ƒçœŸå®æ•ˆåº”å’Œå‡æ•ˆåº”çš„åˆ†å¸ƒ**

**ä»£ç **ï¼š
```python
# 1. çœŸå®å¤„ç†å•ä½çš„æ•ˆåº”
att_real = np.mean(Y_treated[T0:] - Y_synthetic[T0:])

# 2. å¯¹æ¯ä¸ªä¾›ä½“åš Placebo
placebo_effects = []
for j in donors:
    # å‡è£…ç¬¬ j ä¸ªä¾›ä½“æ˜¯å¤„ç†å•ä½
    Y_placebo_treated = Y_donors[:, j]
    Y_placebo_donors = np.delete(Y_donors, j, axis=1)

    # ä¼°è®¡åˆæˆæ§åˆ¶
    sc_placebo = fit_synthetic_control(Y_placebo_treated, Y_placebo_donors)
    att_placebo = np.mean(Y_placebo_treated[T0:] - sc_placebo[T0:])

    placebo_effects.append(att_placebo)

# 3. è®¡ç®— p-value
p_value = np.mean([abs(e) >= abs(att_real) for e in placebo_effects])
```

**p-value è§£é‡Š**ï¼š
- æœ‰å¤šå°‘æ¯”ä¾‹çš„å•ä½çš„æ•ˆåº”æ¯”çœŸå®å¤„ç†å•ä½è¿˜å¤§ï¼Ÿ
- p < 0.05 â†’ çœŸå®æ•ˆåº”æ˜¾è‘—

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ° RMSPE æ¯”å€¼æ£€éªŒï¼ˆPost/Preï¼‰
- æåˆ°"Pre-treatment fit"è¿‡æ»¤ï¼ˆåªä¿ç•™æ‹Ÿåˆå¥½çš„ Placeboï¼‰
- çŸ¥é“ Abadie, Diamond & Hainmueller (2010, 2015) çš„å·¥ä½œ

---

## 3. Regression Discontinuity Design (RDD)

### æ ¸å¿ƒåŸç†

**ä¸€å¥è¯æ€»ç»“**ï¼šå¦‚æœå¤„ç†åˆ†é…ç”±æŸä¸ªè¿ç»­å˜é‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼å†³å®šï¼Œé‚£ä¹ˆé˜ˆå€¼é™„è¿‘çš„å•ä½å…·æœ‰å¯æ¯”æ€§ã€‚

### é«˜é¢‘é¢è¯•é¢˜

#### Q8: Sharp RDD vs Fuzzy RDD çš„åŒºåˆ«ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**Sharp RDD**ï¼š
- **å®šä¹‰**ï¼šè·¨è¿‡é˜ˆå€¼ â†’ 100% æ¥å—å¤„ç†
- **è¯†åˆ«**ï¼š$\tau = \lim_{x \downarrow c} E[Y|X=x] - \lim_{x \uparrow c} E[Y|X=x]$
- **ä¼°è®¡**ï¼šå±€éƒ¨çº¿æ€§å›å½’

**Fuzzy RDD**ï¼š
- **å®šä¹‰**ï¼šè·¨è¿‡é˜ˆå€¼ â†’ æ¥å—å¤„ç†çš„æ¦‚ç‡è·³è·ƒï¼ˆä½†ä¸æ˜¯ 100%ï¼‰
- **è¯†åˆ«**ï¼š
  $$\tau = \frac{\lim_{x \downarrow c} E[Y|X=x] - \lim_{x \uparrow c} E[Y|X=x]}{\lim_{x \downarrow c} E[D|X=x] - \lim_{x \uparrow c} E[D|X=x]}$$
- **ä¼°è®¡**ï¼š2SLSï¼Œç”¨"è·¨è¿‡é˜ˆå€¼"ä½œä¸ºå·¥å…·å˜é‡
- **è§£é‡Š**ï¼šLATEï¼ˆLocal Average Treatment Effectï¼‰

**ä¾‹å­**ï¼š
- **Sharp**ï¼šæ»¡ 200 å…ƒ â†’ 100% è·å¾—ä¼˜æƒ åˆ¸
- **Fuzzy**ï¼šæ»¡ 200 å…ƒ â†’ 80% è·å¾—ä¼˜æƒ åˆ¸ï¼ˆæœ‰äº›äººæ‹’ç»ï¼‰

**Fuzzy RDD çš„å®ç°**ï¼š
```python
# ç¬¬ä¸€é˜¶æ®µï¼šD ~ 1{X >= c}
above_cutoff = (X >= cutoff).astype(int)
first_stage = smf.ols('treatment ~ above_cutoff + X + I(X**2)', data=df).fit()
D_hat = first_stage.fittedvalues

# ç¬¬äºŒé˜¶æ®µï¼šY ~ D_hat
second_stage = smf.ols('outcome ~ D_hat + X + I(X**2)', data=df).fit()
tau_fuzzy = second_stage.params['D_hat']
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- Fuzzy RDD æœ¬è´¨ä¸Šæ˜¯ IV ä¼°è®¡
- å·¥å…·å˜é‡ Z = 1{X â‰¥ c}
- ä¼°è®¡çš„æ˜¯ Compliers çš„ LATE

---

#### Q9: RDD çš„å¸¦å®½ï¼ˆBandwidthï¼‰å¦‚ä½•é€‰æ‹©ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**æƒè¡¡ï¼ˆBias-Variance Tradeoffï¼‰**ï¼š
- **å¸¦å®½å° h**ï¼š
  - âœ… åå·®å°ï¼ˆåªç”¨é˜ˆå€¼é™„è¿‘çš„è§‚æµ‹ï¼‰
  - âŒ æ–¹å·®å¤§ï¼ˆæ ·æœ¬é‡å°‘ï¼‰
- **å¸¦å®½å¤§ h**ï¼š
  - âœ… æ–¹å·®å°ï¼ˆæ ·æœ¬é‡å¤šï¼‰
  - âŒ åå·®å¤§ï¼ˆåŒ…å«è¿œç¦»é˜ˆå€¼çš„è§‚æµ‹ï¼‰

**é€‰æ‹©æ–¹æ³•**ï¼š

**1. è§„åˆ™ of Thumbï¼ˆç»éªŒæ³•åˆ™ï¼‰**
```python
h = 1.84 * Ïƒ * n^(-1/5)
```

**2. äº¤å‰éªŒè¯ï¼ˆCross-Validationï¼‰**
- Leave-one-out CV
- K-fold CV

**3. MSE-Optimal å¸¦å®½ï¼ˆImbens & Kalyanaraman, 2012ï¼‰**
```python
from rdd import rdd
bandwidth = rdd.optimal_bandwidth(X, Y, cutoff)
```

**4. CCT å¸¦å®½ï¼ˆCalonico, Cattaneo & Titiunik, 2014ï¼‰**
```python
# ç®€åŒ–å®ç°
def cct_bandwidth(X, Y, cutoff):
    # ä¼°è®¡æ¡ä»¶æ–¹å·®
    sigma2_left = estimate_variance(X[X < cutoff], Y[X < cutoff])
    sigma2_right = estimate_variance(X[X >= cutoff], Y[X >= cutoff])

    # ä¼°è®¡äºŒé˜¶å¯¼æ•°
    m2_left = estimate_second_derivative(X[X < cutoff], Y[X < cutoff])
    m2_right = estimate_second_derivative(X[X >= cutoff], Y[X >= cutoff])

    # MSE-optimal å…¬å¼
    C_K = 3.44  # Kernel å¸¸æ•°
    n = len(X)
    h_opt = C_K * (sigma2 / (n * m2^2))^(1/5)

    return h_opt
```

**å®è·µå»ºè®®**ï¼š
- æŠ¥å‘Šå¤šä¸ªå¸¦å®½ä¸‹çš„ç»“æœï¼ˆæ•æ„Ÿæ€§åˆ†æï¼‰
- å¦‚æœç»“æœå¯¹å¸¦å®½ä¸æ•æ„Ÿ â†’ ç¨³å¥
- CCT å¸¦å®½æ˜¯ç†è®ºæœ€ä¼˜ï¼Œä½†å®è·µä¸­å¯èƒ½ä¸ç¨³å®š

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°"undersmoothing"ï¼ˆæ•…æ„ç”¨å°å¸¦å®½å‡å°‘åå·®ï¼‰
- çŸ¥é“ `rdrobust` R åŒ…
- æåˆ° robust bias-corrected æ¨æ–­

---

#### Q10: RDD çš„æœ‰æ•ˆæ€§æ£€æŸ¥ï¼ˆValidity Checksï¼‰æœ‰å“ªäº›ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**1. è¿ç»­æ€§æ£€æŸ¥ï¼ˆContinuity Checksï¼‰**

**a) åå˜é‡çš„è¿ç»­æ€§**
- åœ¨é˜ˆå€¼å¤„ï¼Œåå˜é‡ä¸åº”è¯¥è·³è·ƒ
- å¦‚æœè·³è·ƒ â†’ å¯èƒ½æœ‰å…¶ä»–æœºåˆ¶åœ¨èµ·ä½œç”¨

```python
for covariate in ['age', 'income', 'education']:
    rdd = RDD(cutoff=200, bandwidth=30)
    rdd.fit(X, df[covariate])
    print(f"{covariate} åœ¨é˜ˆå€¼å¤„çš„è·³è·ƒ: {rdd.tau_} (p={rdd.pvalue_})")
    # åº”è¯¥éƒ½ä¸æ˜¾è‘—
```

**b) å¯†åº¦çš„è¿ç»­æ€§ï¼ˆMcCrary Testï¼‰**
- å¦‚æœäººä»¬å¯ä»¥æ“çºµ Running Variable â†’ å¯†åº¦ä¼šåœ¨é˜ˆå€¼å¤„è·³è·ƒ
- ç”¨ McCrary (2008) æ£€éªŒ

```python
from rdd import mccrary_test
p_value = mccrary_test(X, cutoff=200)
# p > 0.05 â†’ å¯†åº¦è¿ç»­ï¼Œæ²¡æœ‰æ“çºµ
```

**2. Placebo æ£€éªŒ**

**a) Placebo æˆªæ–­ç‚¹**
- åœ¨éçœŸå®é˜ˆå€¼å¤„è¿›è¡Œ RDD ä¼°è®¡
- åº”è¯¥æ²¡æœ‰æ•ˆåº”

```python
placebo_cutoffs = [150, 170, 190, 210, 230, 250]
for c in placebo_cutoffs:
    rdd = RDD(cutoff=c, bandwidth=30)
    rdd.fit(X, Y)
    # åº”è¯¥éƒ½ä¸æ˜¾è‘—
```

**b) Placebo ç»“æœå˜é‡**
- ç”¨ä¸åº”è¯¥å—å½±å“çš„ç»“æœå˜é‡
- åº”è¯¥æ²¡æœ‰æ•ˆåº”

**3. ç”œç”œåœˆ RDDï¼ˆDonut RDDï¼‰**
- å»æ‰é˜ˆå€¼æ­£ä¸Šæ–¹å’Œæ­£ä¸‹æ–¹çš„è§‚æµ‹ï¼ˆå¯èƒ½è¢«æ“çºµï¼‰
- å¦‚æœç»“æœç¨³å¥ â†’ æ›´å¯ä¿¡

```python
# å»æ‰ [c-Î´, c+Î´] åŒºé—´çš„è§‚æµ‹
donut_df = df[(df['score'] < cutoff - 5) | (df['score'] > cutoff + 5)]
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- çŸ¥é“ Lee (2008) çš„è¾¹ç•Œè®ºæ–‡
- æåˆ°"local randomization"çš„è§†è§’
- çŸ¥é“ä»€ä¹ˆæƒ…å†µä¸‹ RDD å¯èƒ½å¤±æ•ˆï¼ˆæ“çºµã€ååº”æ€§ï¼‰

---

## 4. Instrumental Variables (IV)

### æ ¸å¿ƒåŸç†

**ä¸€å¥è¯æ€»ç»“**ï¼šæ‰¾ä¸€ä¸ªåªå½±å“ç»“æœå˜é‡ Y é€šè¿‡å¤„ç†å˜é‡ X çš„å¤–ç”Ÿå˜é‡ Zï¼Œåˆ©ç”¨ Z çš„å˜åŒ–è¯†åˆ« X å¯¹ Y çš„å› æœæ•ˆåº”ã€‚

### é«˜é¢‘é¢è¯•é¢˜

#### Q11: å·¥å…·å˜é‡çš„ä¸‰ä¸ªå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•æ£€éªŒï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**ä¸‰ä¸ªå‡è®¾**ï¼š

**1. ç›¸å…³æ€§ï¼ˆRelevanceï¼‰**
- **å®šä¹‰**ï¼šZ å’Œ X ç›¸å…³
- **æ•°å­¦**ï¼šCov(Z, X) â‰  0
- **æ£€éªŒ**ï¼š
  ```python
  # ç¬¬ä¸€é˜¶æ®µ F ç»Ÿè®¡é‡
  first_stage = smf.ols('X ~ Z', data=df).fit()
  f_stat = first_stage.fvalue

  # åˆ¤æ–­æ ‡å‡†ï¼šF > 10 â†’ å¼ºå·¥å…·å˜é‡
  ```
- **å¯æ£€éªŒ**ï¼šâœ… å¯ä»¥ç›´æ¥æ£€éªŒ

**2. æ’é™¤æ€§ï¼ˆExclusion Restrictionï¼‰**
- **å®šä¹‰**ï¼šZ åªé€šè¿‡ X å½±å“ Yï¼ˆZ ä¸ç›´æ¥å½±å“ Yï¼‰
- **æ•°å­¦**ï¼šZ âŠ¥ Y | X
- **æ£€éªŒ**ï¼šâŒ **ä¸å¯æ£€éªŒ**ï¼ˆéœ€è¦ç†è®ºæ”¯æŒï¼‰
- **ä¾‹å¤–**ï¼šå¦‚æœæœ‰å¤šä¸ª IVï¼Œå¯ä»¥ç”¨è¿‡åº¦è¯†åˆ«æ£€éªŒï¼ˆHansen J Testï¼‰

**3. å¤–ç”Ÿæ€§ï¼ˆExogeneityï¼‰**
- **å®šä¹‰**ï¼šZ ä¸æœªè§‚æµ‹æ··æ·†å› å­ U æ— å…³
- **æ•°å­¦**ï¼šCov(Z, U) = 0
- **æ£€éªŒ**ï¼šâŒ **ä¸å¯æ£€éªŒ**ï¼ˆU ä¸å¯è§‚æµ‹ï¼‰
- **ä¾èµ–**ï¼šç†è®ºã€åˆ¶åº¦çŸ¥è¯†ã€è‡ªç„¶å®éªŒ

**æ£€éªŒå·¥å…·**ï¼š

| å‡è®¾ | å¯æ£€éªŒæ€§ | æ£€éªŒæ–¹æ³• |
|------|---------|---------|
| Relevance | âœ… | F ç»Ÿè®¡é‡ > 10 |
| Exclusion | âš ï¸ | è¿‡åº¦è¯†åˆ«æ£€éªŒï¼ˆéœ€å¤šä¸ª IVï¼‰ |
| Exogeneity | âŒ | ç†è®ºè®ºè¯ |

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- å¼ºè°ƒ"æ’é™¤æ€§æ˜¯æœ€éš¾æ»¡è¶³çš„"
- æåˆ°"å¥½çš„ IV æ¥è‡ªè‡ªç„¶å®éªŒã€éšæœºåŒ–ã€åˆ¶åº¦ç‰¹å¾"
- çŸ¥é“ä»€ä¹ˆæ˜¯å¼±å·¥å…·å˜é‡é—®é¢˜ï¼ˆF < 10ï¼‰

---

#### Q12: 2SLS çš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Ÿæ‰‹åŠ¨å®ç°ä¸€éã€‚

**ç­”æ¡ˆ**ï¼š

**ç›´è§‰**ï¼š
1. **ç¬¬ä¸€é˜¶æ®µ**ï¼šæŠŠ X åˆ†è§£æˆ"å¤–ç”Ÿéƒ¨åˆ†"å’Œ"å†…ç”Ÿéƒ¨åˆ†"
   - $\hat{X} = f(Z)$ â† å¤–ç”Ÿéƒ¨åˆ†ï¼ˆåªç”± Z å†³å®šï¼‰
   - $X - \hat{X}$ â† å†…ç”Ÿéƒ¨åˆ†ï¼ˆä¸ U ç›¸å…³ï¼‰

2. **ç¬¬äºŒé˜¶æ®µ**ï¼šç”¨å¤–ç”Ÿéƒ¨åˆ† $\hat{X}$ å›å½’ Y
   - å› ä¸º $\hat{X}$ ä¸ U æ— å…³ï¼Œæ‰€ä»¥ä¼°è®¡æ˜¯æ— åçš„

**æ‰‹åŠ¨å®ç°**ï¼š

```python
import numpy as np
from sklearn.linear_model import LinearRegression

def two_stage_least_squares(Z, X, Y):
    """
    æ‰‹åŠ¨å®ç° 2SLS

    å‚æ•°:
        Z: å·¥å…·å˜é‡ (n,)
        X: å†…ç”Ÿå˜é‡ (n,)
        Y: ç»“æœå˜é‡ (n,)
    """
    # ç¬¬ä¸€é˜¶æ®µï¼šX ~ Z
    first_stage = LinearRegression()
    first_stage.fit(Z.reshape(-1, 1), X)
    X_hat = first_stage.predict(Z.reshape(-1, 1))

    # æ£€æŸ¥ç¬¬ä¸€é˜¶æ®µå¼ºåº¦
    r2 = first_stage.score(Z.reshape(-1, 1), X)
    f_stat = (r2 / (1 - r2)) * (len(X) - 2)
    print(f"First-stage F = {f_stat:.2f}")

    # ç¬¬äºŒé˜¶æ®µï¼šY ~ X_hat
    second_stage = LinearRegression()
    second_stage.fit(X_hat.reshape(-1, 1), Y)
    beta_2sls = second_stage.coef_[0]

    # Wald ä¼°è®¡é‡ï¼ˆç­‰ä»·ï¼‰
    beta_wald = np.cov(Z, Y)[0,1] / np.cov(Z, X)[0,1]

    return {
        'beta_2sls': beta_2sls,
        'beta_wald': beta_wald,
        'first_stage_f': f_stat
    }
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

OLS ä¼°è®¡ X â†’ Y æ—¶ï¼š
$$\beta_{OLS} = \frac{Cov(X, Y)}{Var(X)} = \beta + \frac{Cov(X, U)}{Var(X)} \quad \text{(æœ‰åï¼)}$$

2SLS ä¼°è®¡ï¼š
$$\beta_{2SLS} = \frac{Cov(\hat{X}, Y)}{Var(\hat{X})} = \frac{Cov(Z, Y)}{Cov(Z, X)} = \beta \quad \text{(æ— å)}$$

å› ä¸º $Cov(Z, U) = 0$ï¼ˆå¤–ç”Ÿæ€§å‡è®¾ï¼‰ã€‚

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- 2SLS çš„æ ‡å‡†è¯¯éœ€è¦è°ƒæ•´ï¼ˆä¸èƒ½ç›´æ¥ç”¨ç¬¬äºŒé˜¶æ®µçš„ SEï¼‰
- æ­£ç¡®çš„ SE éœ€è¦è€ƒè™‘ç¬¬ä¸€é˜¶æ®µçš„ä¸ç¡®å®šæ€§
- å®è·µä¸­ç”¨ `ivreg` æˆ– `linearmodels.IV2SLS`

---

#### Q13: ä»€ä¹ˆæ˜¯ LATEï¼ˆå±€éƒ¨å¹³å‡å¤„ç†æ•ˆåº”ï¼‰ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**å®šä¹‰**ï¼š
IV ä¼°è®¡çš„æ˜¯ **Compliers** çš„å¹³å‡å¤„ç†æ•ˆåº”ï¼ˆATEï¼‰ï¼Œä¸æ˜¯æ‰€æœ‰äººçš„ ATEã€‚

**äººç¾¤åˆ†ç±»ï¼ˆImbens & Angrist, 1994ï¼‰**ï¼š

æ ¹æ® $(D_i(Z=0), D_i(Z=1))$ï¼Œå¯ä»¥åˆ†ä¸º 4 ç±»äººï¼š

| ç±»å‹ | $D(Z=0)$ | $D(Z=1)$ | æè¿° | ä¾‹å­ï¼ˆå…µå½¹ IVï¼‰ |
|------|----------|----------|------|----------------|
| **Never-takers** | 0 | 0 | Z ä¸å½±å“ D | æœ‰å¥åº·é—®é¢˜ï¼Œæ— æ³•å…¥ä¼ |
| **Compliers** | 0 | 1 | Z å†³å®š D | æŠ½ä¸­å°±å»ï¼ŒæœªæŠ½ä¸­ä¸å» |
| **Always-takers** | 1 | 1 | æ€»æ˜¯ D=1 | å¿—æ„¿å…¥ä¼ |
| **Defiers** | 1 | 0 | Z åå‘å½±å“ D | ï¼ˆé€šå¸¸å‡è®¾ä¸å­˜åœ¨ï¼‰ |

**IV ä¼°è®¡çš„æ˜¯ä»€ä¹ˆï¼Ÿ**

$$\tau_{IV} = E[Y_i(1) - Y_i(0) | \text{Complier}] = \text{LATE}$$

**ç›´è§‰**ï¼š
- Always-takers: Z çš„å˜åŒ–ä¸å½±å“ä»–ä»¬çš„ Dï¼Œæ‰€ä»¥æ— æ³•è¯†åˆ«æ•ˆåº”
- Never-takers: åŒä¸Š
- Compliers: Z çš„å˜åŒ–æ”¹å˜äº†ä»–ä»¬çš„ Dï¼Œæ‰€ä»¥èƒ½è¯†åˆ«æ•ˆåº”
- IV ä¼°è®¡çš„æ˜¯ Compliers çš„ ATE

**å¤–æ¨æ€§é—®é¢˜**ï¼š
- LATE â‰  ATEï¼ˆé™¤éæ‰€æœ‰äººéƒ½æ˜¯ Compliersï¼‰
- å¦‚æœ Compliers å¾ˆç‰¹æ®Šï¼ŒLATE å¯èƒ½ä¸èƒ½æ¨å¹¿åˆ°æ€»ä½“

**ä¾‹å­ï¼šå¾å…µæŠ½ç­¾ & æ•™è‚²å›æŠ¥ç‡**
- **Z**ï¼šæ˜¯å¦è¢«æŠ½ä¸­å¾å…µ
- **X**ï¼šæ•™è‚²å¹´é™
- **Y**ï¼šæ”¶å…¥
- **Compliers**ï¼šè¢«æŠ½ä¸­å°±å»ï¼ˆä¸­æ–­æ•™è‚²ï¼‰ï¼ŒæœªæŠ½ä¸­å°±ç»§ç»­è¯»ä¹¦çš„äºº
- **LATE**ï¼šè¿™éƒ¨åˆ†äººå¤šè¯»ä¸€å¹´ä¹¦çš„æ”¶å…¥å›æŠ¥
- **æ³¨æ„**ï¼šè¿™å¯èƒ½ä¸æ˜¯"æ‰€æœ‰äºº"çš„æ•™è‚²å›æŠ¥ç‡

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ° Monotonicity Assumptionï¼ˆå•è°ƒæ€§ï¼šæ²¡æœ‰ Defiersï¼‰
- çŸ¥é“ LATE çš„å¤–éƒ¨æœ‰æ•ˆæ€§å±€é™
- æåˆ° Fuzzy RDD ä¼°è®¡çš„ä¹Ÿæ˜¯ LATE

---

#### Q14: å¼±å·¥å…·å˜é‡ï¼ˆWeak IVï¼‰æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå¦‚ä½•æ£€æµ‹ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**é—®é¢˜**ï¼š

1. **æœ‰é™æ ·æœ¬åå·®**ï¼š
   - 2SLS ä¼°è®¡é‡å‘ OLS åç§»
   - å³ä½¿ n â†’ âˆï¼Œåå·®ä¹Ÿä¸ä¸€å®šæ¶ˆå¤±ï¼ˆå¦‚æœ F å¤ªå°ï¼‰

2. **æ ‡å‡†è¯¯å¤±æ•ˆ**ï¼š
   - æ¸è¿‘æ ‡å‡†è¯¯ä¸¥é‡ä½ä¼°çœŸå®æ ‡å‡†è¯¯
   - ç½®ä¿¡åŒºé—´è¦†ç›–ç‡è¿œä½äºåä¹‰æ°´å¹³ï¼ˆå¦‚ 95%ï¼‰

3. **æ£€éªŒå¤±æ•ˆ**ï¼š
   - t æ£€éªŒæ‹’ç»ç‡è¿œé«˜äºåä¹‰æ°´å¹³
   - å®¹æ˜“å‡ºç°å‡é˜³æ€§

**æ£€æµ‹æ–¹æ³•**ï¼š

**1. First-Stage F ç»Ÿè®¡é‡**
```python
first_stage = smf.ols('X ~ Z + controls', data=df).fit()
f_stat = first_stage.fvalue

# åˆ¤æ–­æ ‡å‡†ï¼ˆStock & Yogo, 2005ï¼‰
if f_stat > 10:
    print("âœ“ å¼ºå·¥å…·å˜é‡")
elif f_stat > 5:
    print("âš  ä¸­ç­‰å¼ºåº¦ï¼Œéœ€è°¨æ…")
else:
    print("âœ— å¼±å·¥å…·å˜é‡ï¼Œç»“æœä¸å¯ä¿¡")
```

**2. Cragg-Donald ç»Ÿè®¡é‡ï¼ˆå¤šä¸ªå†…ç”Ÿå˜é‡ï¼‰**
- æ¨å¹¿çš„ F ç»Ÿè®¡é‡
- ä¸´ç•Œå€¼è¡¨ï¼šStock & Yogo (2005)

**è§£å†³æ–¹æ¡ˆ**ï¼š

**1. Anderson-Rubin æ£€éªŒ**
- åœ¨å¼± IV ä¸‹ä»ç„¶æœ‰æ•ˆï¼ˆä¸ä¾èµ–æ¸è¿‘ç†è®ºï¼‰
- ä½†åŠŸæ•ˆè¾ƒä½ï¼ˆæ›´ä¿å®ˆï¼‰

```python
def anderson_rubin_test(Z, X, Y, beta_0):
    """æ£€éªŒ H0: beta = beta_0"""
    Y_tilde = Y - beta_0 * X
    model = smf.ols('Y_tilde ~ Z', data=df).fit()
    f_stat = model.fvalue
    p_value = model.f_pvalue
    return p_value
```

**2. LIMLï¼ˆLimited Information Maximum Likelihoodï¼‰**
- æ¯” 2SLS æ›´ç¨³å¥ï¼ˆåœ¨å¼± IV ä¸‹åå·®æ›´å°ï¼‰
- ä½†æ–¹å·®æ›´å¤§

**3. æ‰¾æ›´å¼ºçš„å·¥å…·å˜é‡**
- å¢åŠ å·¥å…·å˜é‡çš„æ•°é‡
- æ‰¾ä¸ X ç›¸å…³æ€§æ›´å¼ºçš„ Z

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- æåˆ°"Many weak instruments"é—®é¢˜ï¼ˆå¾ˆå¤šå¼± IV ä¹Ÿæ— æµäºäº‹ï¼‰
- çŸ¥é“ Staiger & Stock (1997) çš„ä¸´ç•Œå€¼ 3.84
- æåˆ°æœ‰æ•ˆä¼°è®¡ï¼šJIVE, UJIVE, MBTSLS

---

## 5. æ–¹æ³•å¯¹æ¯”ä¸é€‰æ‹©

### Q15: è¿™å››ç§æ–¹æ³•å¦‚ä½•é€‰æ‹©ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | æ ¸å¿ƒå‡è®¾ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|---------|------|------|
| **DID** | â€¢ æ”¿ç­–åœ¨å¤šä¸ªå•ä½å®æ–½<br>â€¢ æœ‰æ¸…æ™°çš„å‰/åã€å¤„ç†/å¯¹ç…§ | å¹³è¡Œè¶‹åŠ¿ | â€¢ ç®€å•ç›´è§‚<br>â€¢ æ˜“äºå®æ–½<br>â€¢ å¯æ§åˆ¶åå˜é‡ | â€¢ å‡è®¾å¼º<br>â€¢ å¯¹è¶‹åŠ¿æ•æ„Ÿ |
| **SCM** | â€¢ å•ä¸€äº‹ä»¶<br>â€¢ æ‰¾ä¸åˆ°å®Œç¾å¯¹ç…§<br>â€¢ æœ‰å¤šä¸ªæ½œåœ¨å¯¹ç…§å•ä½ | å¯çº¿æ€§ç»„åˆ | â€¢ çµæ´»ï¼ˆä¼˜åŒ–æƒé‡ï¼‰<br>â€¢ ç›´è§‚å¯è§†åŒ–<br>â€¢ ä¸ä¾èµ–å¹³è¡Œè¶‹åŠ¿ | â€¢ éœ€è¦å¤šä¸ªå¯¹ç…§<br>â€¢ å¤§æ ·æœ¬æ¨æ–­å›°éš¾ |
| **RDD** | â€¢ å¤„ç†ç”±é˜ˆå€¼å†³å®š<br>â€¢ é˜ˆå€¼é™„è¿‘ä¸å¯æ“çºµ<br>â€¢ è¿è¡Œå˜é‡è¿ç»­ | é˜ˆå€¼é™„è¿‘å¯æ¯” | â€¢ è¯†åˆ«å¼º<br>â€¢ å†…éƒ¨æ•ˆåº¦é«˜<br>â€¢ ä¸éœ€è¦éšæœºåŒ– | â€¢ å¤–éƒ¨æ•ˆåº¦å¼±ï¼ˆLATEï¼‰<br>â€¢ å¸¦å®½æ•æ„Ÿ<br>â€¢ éœ€è¦å¤§æ ·æœ¬ |
| **IV** | â€¢ å­˜åœ¨æ··æ·†<br>â€¢ æœ‰åˆç†çš„å¤–ç”Ÿå†²å‡»<br>â€¢ å¯ä»¥è®ºè¯æ’é™¤æ€§ | Relevance<br>Exclusion<br>Exogeneity | â€¢ å¤„ç†å†…ç”Ÿæ€§<br>â€¢ ç†è®ºåŸºç¡€å¼º | â€¢ æ‰¾å¥½ IV å¾ˆéš¾<br>â€¢ å¼± IV é—®é¢˜<br>â€¢ ä¼°è®¡ LATE |

**å†³ç­–æ ‘**ï¼š

```
START
â”œâ”€ æœ‰éšæœºå®éªŒï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ ä¸éœ€è¦å‡†å®éªŒï¼Œç›´æ¥æ¯”è¾ƒå‡å€¼
â”‚  â””â”€ å¦ â†“
â”‚
â”œâ”€ å¤„ç†åˆ†é…ç”±é˜ˆå€¼å†³å®šï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ ç”¨ RDD
â”‚  â””â”€ å¦ â†“
â”‚
â”œâ”€ æœ‰æ˜ç¡®çš„å‰/åã€å¤„ç†/å¯¹ç…§ï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†“
â”‚  â”‚  â”œâ”€ å¤šä¸ªå¤„ç†å•ä½ + è¶‹åŠ¿ç›¸ä¼¼ï¼Ÿ
â”‚  â”‚  â”‚  â”œâ”€ æ˜¯ â†’ ç”¨ DID
â”‚  â”‚  â”‚  â””â”€ å¦ â†“
â”‚  â”‚  â””â”€ å•ä¸ªå¤„ç†å•ä½ + å¤šä¸ªæ½œåœ¨å¯¹ç…§ï¼Ÿ
â”‚  â”‚     â””â”€ æ˜¯ â†’ ç”¨ Synthetic Control
â”‚  â””â”€ å¦ â†“
â”‚
â””â”€ æœ‰åˆç†çš„å·¥å…·å˜é‡ï¼Ÿ
   â”œâ”€ æ˜¯ â†’ ç”¨ IV
   â””â”€ å¦ â†’ è€ƒè™‘è§‚å¯Ÿæ€§æ–¹æ³•ï¼ˆMatching, IPW ç­‰ï¼‰
```

**é¢è¯•åŠ åˆ†ç‚¹**ï¼š
- å¯ä»¥ç»„åˆä½¿ç”¨ï¼ˆå¦‚ DID + Matching, SCM + Placebo Testsï¼‰
- æœ€å¥½ç”¨å¤šç§æ–¹æ³•éªŒè¯ï¼ˆRobustness Checkï¼‰
- æ¯ç§æ–¹æ³•éƒ½æœ‰å±€é™æ€§ï¼Œå…³é”®æ˜¯è®ºè¯å‡è®¾çš„åˆç†æ€§

---

## 6. ä»é›¶å®ç°æ ¸å¿ƒç®—æ³•

### DID ä¼°è®¡å™¨

```python
import numpy as np
import pandas as pd

def did_estimator(df, outcome, treat_col, post_col):
    """
    æ‰‹åŠ¨å®ç° DID ä¼°è®¡

    å‚æ•°:
        df: DataFrame
        outcome: ç»“æœå˜é‡åˆ—å
        treat_col: å¤„ç†ç»„æŒ‡ç¤ºå˜é‡
        post_col: æ”¿ç­–åæŒ‡ç¤ºå˜é‡
    """
    # è®¡ç®—å››ä¸ªå‡å€¼
    y_treat_post = df[df[treat_col] & df[post_col]][outcome].mean()
    y_treat_pre = df[df[treat_col] & ~df[post_col]][outcome].mean()
    y_control_post = df[~df[treat_col] & df[post_col]][outcome].mean()
    y_control_pre = df[~df[treat_col] & ~df[post_col]][outcome].mean()

    # DID ä¼°è®¡é‡
    did = (y_treat_post - y_treat_pre) - (y_control_post - y_control_pre)

    # æ ‡å‡†è¯¯ï¼ˆå‡è®¾åŒæ–¹å·®ï¼‰
    # ä½¿ç”¨ delta method çš„ç®€åŒ–ç‰ˆæœ¬
    n_treat = df[df[treat_col]].shape[0]
    n_control = df[~df[treat_col]].shape[0]

    var_treat = df[df[treat_col]][outcome].var()
    var_control = df[~df[treat_col]][outcome].var()

    se = np.sqrt(var_treat / n_treat + var_control / n_control)

    return {
        'DIDä¼°è®¡': did,
        'æ ‡å‡†è¯¯': se,
        'tç»Ÿè®¡é‡': did / se,
        'på€¼': 2 * (1 - stats.norm.cdf(abs(did / se)))
    }
```

### åˆæˆæ§åˆ¶ä¼°è®¡å™¨

```python
from scipy.optimize import minimize

def synthetic_control(treated, donors, treatment_period):
    """
    æ‰‹åŠ¨å®ç°åˆæˆæ§åˆ¶

    å‚æ•°:
        treated: å¤„ç†å•ä½æ—¶é—´åºåˆ— (T,)
        donors: ä¾›ä½“æ± æ—¶é—´åºåˆ—çŸ©é˜µ (T, J)
        treatment_period: å¤„ç†å¼€å§‹æ—¶é—´ç´¢å¼•
    """
    # å‰å¤„ç†æœŸæ•°æ®
    treated_pre = treated[:treatment_period]
    donors_pre = donors[:treatment_period, :]

    # ä¼˜åŒ–ç›®æ ‡
    def objective(w):
        synthetic = donors_pre @ w
        return np.sum((treated_pre - synthetic) ** 2)

    # çº¦æŸ
    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
    bounds = [(0, 1) for _ in range(donors.shape[1])]
    w0 = np.ones(donors.shape[1]) / donors.shape[1]

    # æ±‚è§£
    result = minimize(objective, w0,
                      method='SLSQP',
                      bounds=bounds,
                      constraints=constraints)

    # ç”Ÿæˆåˆæˆæ§åˆ¶
    synthetic = donors @ result.x

    # ATT
    att = np.mean(treated[treatment_period:] - synthetic[treatment_period:])

    return {
        'æƒé‡': result.x,
        'åˆæˆæ§åˆ¶': synthetic,
        'ATT': att
    }
```

### RDD ä¼°è®¡å™¨

```python
def rdd_estimator(X, Y, cutoff, bandwidth, polynomial_order=1):
    """
    æ‰‹åŠ¨å®ç° Sharp RDD

    å‚æ•°:
        X: Running variable
        Y: Outcome
        cutoff: é˜ˆå€¼
        bandwidth: å¸¦å®½
        polynomial_order: å¤šé¡¹å¼é˜¶æ•°
    """
    # ä¸­å¿ƒåŒ–
    X_centered = X - cutoff

    # é€‰æ‹©å¸¦å®½å†…çš„è§‚æµ‹
    mask = abs(X_centered) <= bandwidth
    X_bw = X_centered[mask]
    Y_bw = Y[mask]

    # å·¦å³ä¸¤ä¾§
    left_mask = X_bw < 0
    right_mask = X_bw >= 0

    # å¤šé¡¹å¼ç‰¹å¾
    from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(degree=polynomial_order)

    # å·¦ä¾§å›å½’
    X_left = poly.fit_transform(X_bw[left_mask].reshape(-1, 1))
    model_left = LinearRegression().fit(X_left, Y_bw[left_mask])
    y_left_0 = model_left.predict(poly.transform([[0]]))[0]

    # å³ä¾§å›å½’
    X_right = poly.fit_transform(X_bw[right_mask].reshape(-1, 1))
    model_right = LinearRegression().fit(X_right, Y_bw[right_mask])
    y_right_0 = model_right.predict(poly.transform([[0]]))[0]

    # RDD ä¼°è®¡é‡
    tau = y_right_0 - y_left_0

    # æ ‡å‡†è¯¯ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ç”¨ robust SEï¼‰
    resid_left = Y_bw[left_mask] - model_left.predict(X_left)
    resid_right = Y_bw[right_mask] - model_right.predict(X_right)

    sigma2_left = np.var(resid_left)
    sigma2_right = np.var(resid_right)

    n_left = left_mask.sum()
    n_right = right_mask.sum()

    se = np.sqrt(sigma2_left / n_left + sigma2_right / n_right)

    return {
        'å¤„ç†æ•ˆåº”': tau,
        'æ ‡å‡†è¯¯': se,
        'tç»Ÿè®¡é‡': tau / se,
        'på€¼': 2 * (1 - stats.norm.cdf(abs(tau / se)))
    }
```

### 2SLS ä¼°è®¡å™¨

```python
def two_stage_least_squares(Z, X, Y):
    """
    æ‰‹åŠ¨å®ç° 2SLS

    å‚æ•°:
        Z: å·¥å…·å˜é‡ (n,) æˆ– (n, k)
        X: å†…ç”Ÿå˜é‡ (n,)
        Y: ç»“æœå˜é‡ (n,)
    """
    # ç¡®ä¿æ˜¯çŸ©é˜µå½¢å¼
    if Z.ndim == 1:
        Z = Z.reshape(-1, 1)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    if Y.ndim == 1:
        Y = Y.reshape(-1, 1)

    n = len(Y)

    # ç¬¬ä¸€é˜¶æ®µï¼šX ~ Z
    # X = Z * gamma + v
    # gamma = (Z'Z)^{-1} Z'X
    gamma = np.linalg.inv(Z.T @ Z) @ Z.T @ X
    X_hat = Z @ gamma

    # ç¬¬ä¸€é˜¶æ®µ F ç»Ÿè®¡é‡
    ss_res = np.sum((X - X_hat)**2)
    ss_tot = np.sum((X - np.mean(X))**2)
    r2 = 1 - ss_res / ss_tot
    f_stat = (r2 / (1 - r2)) * (n - Z.shape[1] - 1) / Z.shape[1]

    # ç¬¬äºŒé˜¶æ®µï¼šY ~ X_hat
    # Y = X_hat * beta + epsilon
    # beta = (X_hat'X_hat)^{-1} X_hat'Y
    beta_2sls = np.linalg.inv(X_hat.T @ X_hat) @ X_hat.T @ Y

    # è®¡ç®—æ®‹å·®
    Y_hat = X_hat @ beta_2sls
    residuals = Y - Y_hat

    # 2SLS æ ‡å‡†è¯¯ï¼ˆéœ€è¦è°ƒæ•´ï¼‰
    # Var(beta) = sigma^2 * (X'P_Z X)^{-1}
    # å…¶ä¸­ P_Z = Z(Z'Z)^{-1}Z'
    sigma2 = np.sum(residuals**2) / (n - X.shape[1])

    P_Z = Z @ np.linalg.inv(Z.T @ Z) @ Z.T
    var_beta = sigma2 * np.linalg.inv(X.T @ P_Z @ X)
    se_2sls = np.sqrt(np.diag(var_beta))

    return {
        'beta_2sls': beta_2sls[0, 0],
        'se': se_2sls[0],
        't_stat': beta_2sls[0, 0] / se_2sls[0],
        'first_stage_f': f_stat,
        'weak_iv': f_stat < 10
    }
```

---

## 7. çœŸå®é¢è¯•é¢˜ç¤ºä¾‹

### æ¡ˆä¾‹é¢˜ 1ï¼šç¾å›¢å¤–å–æ–°åŠŸèƒ½ä¸Šçº¿

**èƒŒæ™¯**ï¼š
ç¾å›¢åœ¨ 2023 å¹´ 7 æœˆåœ¨åŒ—äº¬å’Œä¸Šæµ·è¯•ç‚¹äº†ã€Œæ— æ¥è§¦é…é€ã€åŠŸèƒ½ã€‚ä½ æ˜¯æ•°æ®ç§‘å­¦å®¶ï¼Œéœ€è¦è¯„ä¼°è¿™ä¸ªåŠŸèƒ½å¯¹è®¢å•é‡çš„å½±å“ã€‚

**å¯ç”¨æ•°æ®**ï¼š
- 2023 å¹´ 1-12 æœˆï¼Œ20 ä¸ªåŸå¸‚çš„æœˆåº¦è®¢å•é‡
- åŒ—äº¬ã€ä¸Šæµ·åœ¨ 7 æœˆä¸Šçº¿åŠŸèƒ½
- å…¶ä»– 18 ä¸ªåŸå¸‚æœªä¸Šçº¿

**é—®é¢˜**ï¼š
1. ä½ ä¼šç”¨ä»€ä¹ˆæ–¹æ³•ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. æ ¸å¿ƒå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•æ£€éªŒï¼Ÿ
3. å¦‚æœå‡è®¾ä¸æ»¡è¶³æ€ä¹ˆåŠï¼Ÿ

**å‚è€ƒç­”æ¡ˆ**ï¼š

**1. æ–¹æ³•é€‰æ‹©**

æˆ‘ä¼šå…ˆå°è¯• **DID**ï¼Œå¦‚æœå¹³è¡Œè¶‹åŠ¿ä¸æ»¡è¶³ï¼Œå†è€ƒè™‘ **åˆæˆæ§åˆ¶**ã€‚

ç†ç”±ï¼š
- âœ… æœ‰ä¸¤ä¸ªå¤„ç†å•ä½ï¼ˆåŒ—äº¬ã€ä¸Šæµ·ï¼‰
- âœ… æœ‰æ¸…æ™°çš„å‰/åæ—¶é—´ç‚¹
- âœ… æœ‰ 18 ä¸ªæ½œåœ¨å¯¹ç…§åŸå¸‚
- âœ… é¢æ¿æ•°æ®ç»“æ„

**2. æ ¸å¿ƒå‡è®¾ä¸æ£€éªŒ**

**DID å‡è®¾ï¼šå¹³è¡Œè¶‹åŠ¿**

æ£€éªŒæ–¹æ³•ï¼š
```python
# æ–¹æ³• 1ï¼šå›¾å½¢åŒ–æ£€éªŒ
# ç»˜åˆ¶ 1-6 æœˆçš„è®¢å•é‡è¶‹åŠ¿
fig, ax = plt.subplots()
for city in ['åŒ—äº¬', 'ä¸Šæµ·']:
    df_city = df[df['city'] == city]
    ax.plot(df_city['month'][:6], df_city['orders'][:6], label=city)

for city in other_cities:
    df_city = df[df['city'] == city]
    ax.plot(df_city['month'][:6], df_city['orders'][:6], alpha=0.3, color='gray')

# å¦‚æœè¶‹åŠ¿å¹³è¡Œ â†’ æ”¯æŒå‡è®¾

# æ–¹æ³• 2ï¼šEvent Study
# ä¼°è®¡æ”¿ç­–å‰å„æœˆçš„"å‡æ•ˆåº”"
for month in range(1, 7):
    df[f'lead_{month}'] = df['treat'] * (df['month'] == month)

model = smf.ols('orders ~ treat + C(month) + ' +
                ' + '.join([f'lead_{m}' for m in range(1, 6)]),
                data=df).fit()

# å¦‚æœ lead ç³»æ•°ä¸æ˜¾è‘— â†’ æ”¯æŒå¹³è¡Œè¶‹åŠ¿
```

**3. å‡è®¾ä¸æ»¡è¶³çš„åº”å¯¹**

**æ–¹æ¡ˆ Aï¼šåˆæˆæ§åˆ¶**
```python
# ç”¨ 18 ä¸ªå¯¹ç…§åŸå¸‚çš„åŠ æƒç»„åˆä½œä¸º"åˆæˆåŒ—äº¬/ä¸Šæµ·"
from synthetic_control import SyntheticControl

sc_beijing = SyntheticControl(treatment_time=7)
sc_beijing.fit(
    treated=df[df['city']=='åŒ—äº¬']['orders'],
    donors=df[df['city'].isin(other_cities)].pivot(
        index='month', columns='city', values='orders'
    )
)

# ä¼˜ç‚¹ï¼šä¸ä¾èµ–å¹³è¡Œè¶‹åŠ¿ï¼Œæƒé‡æ˜¯æ•°æ®é©±åŠ¨çš„
# ç¼ºç‚¹ï¼šåªæœ‰ 2 ä¸ªå¤„ç†å•ä½ï¼Œæ¨æ–­è¾ƒå¼±
```

**æ–¹æ¡ˆ Bï¼šæ§åˆ¶åŸå¸‚ç‰¹å®šè¶‹åŠ¿**
```python
# å…è®¸æ¯ä¸ªåŸå¸‚æœ‰ä¸åŒçš„çº¿æ€§è¶‹åŠ¿
model = smf.ols('orders ~ treat + C(month) + C(city) + ' +
                'treat*month + city*month + treat_post',
                data=df).fit()
```

**æ–¹æ¡ˆ Cï¼šåŒ¹é… + DID**
```python
# å…ˆç”¨ PSM æ‰¾åˆ°ä¸åŒ—äº¬ã€ä¸Šæµ·æœ€ç›¸ä¼¼çš„åŸå¸‚
# å†åœ¨åŒ¹é…æ ·æœ¬ä¸Šåš DID
from sklearn.neighbors import NearestNeighbors

# åŒ¹é…åå˜é‡ï¼šäººå‡ GDPã€äººå£ã€æ¶ˆè´¹æ°´å¹³ç­‰
nn = NearestNeighbors(n_neighbors=5)
nn.fit(df_covariates[df['city'].isin(other_cities)])

matched_cities = nn.kneighbors(df_covariates[df['city'].isin(['åŒ—äº¬', 'ä¸Šæµ·'])])
```

---

### æ¡ˆä¾‹é¢˜ 2ï¼šä¼šå‘˜ç­‰çº§é—¨æ§›çš„æ•ˆåº”

**èƒŒæ™¯**ï¼š
æŸç”µå•†å¹³å°çš„ä¼šå‘˜ä½“ç³»ï¼š
- ç´¯è®¡æ¶ˆè´¹ < 1000 å…ƒï¼šæ™®é€šä¼šå‘˜
- ç´¯è®¡æ¶ˆè´¹ â‰¥ 1000 å…ƒï¼šé‡‘å¡ä¼šå‘˜ï¼ˆäº«å—æŠ˜æ‰£ï¼‰

ä½ éœ€è¦è¯„ä¼°é‡‘å¡ä¼šå‘˜èµ„æ ¼å¯¹åç»­æ¶ˆè´¹çš„å½±å“ã€‚

**é—®é¢˜**ï¼š
1. ç›´æ¥æ¯”è¾ƒé‡‘å¡å’Œæ™®é€šä¼šå‘˜çš„æ¶ˆè´¹å·®å¼‚æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
2. ä½ ä¼šç”¨ä»€ä¹ˆæ–¹æ³•ï¼Ÿ
3. éœ€è¦å“ªäº› validity checksï¼Ÿ

**å‚è€ƒç­”æ¡ˆ**ï¼š

**1. ç›´æ¥æ¯”è¾ƒçš„é—®é¢˜**

```python
# é”™è¯¯åšæ³•
avg_gold = df[df['total_spending'] >= 1000]['future_spending'].mean()
avg_regular = df[df['total_spending'] < 1000]['future_spending'].mean()
effect = avg_gold - avg_regular  # âŒ æœ‰åï¼
```

**é—®é¢˜**ï¼š
- **é€‰æ‹©åå·®**ï¼šæ¶ˆè´¹è¶…è¿‡ 1000 çš„äººæœ¬èº«å°±æ›´çˆ±æ¶ˆè´¹ï¼ˆèƒ½åŠ›ã€åå¥½ä¸åŒï¼‰
- **æ··æ·†å› ç´ **ï¼šæ”¶å…¥ã€å¹´é¾„ã€åœ°åŸŸç­‰
- **åå‘å› æœ**ï¼šå¯èƒ½æ˜¯å› ä¸ºä»–ä»¬æœ¬æ¥å°±ä¼šå¤šæ¶ˆè´¹ï¼Œæ‰€ä»¥æ‰è¾¾åˆ° 1000

**2. æ–¹æ³•é€‰æ‹©ï¼šRDD**

è¿™æ˜¯å…¸å‹çš„ **Regression Discontinuity Design** åœºæ™¯ï¼š
- å¤„ç†åˆ†é…ç”±é˜ˆå€¼å†³å®šï¼ˆ1000 å…ƒï¼‰
- é˜ˆå€¼é™„è¿‘çš„äººåº”è¯¥æ˜¯ç›¸ä¼¼çš„ï¼ˆæ¶ˆè´¹ 995 vs 1005ï¼‰

```python
from rdd import RDD

# 1. å‡†å¤‡æ•°æ®
df['above_1000'] = (df['total_spending'] >= 1000).astype(int)
df['running_var'] = df['total_spending'] - 1000  # ä¸­å¿ƒåŒ–

# 2. RDD ä¼°è®¡
rdd = RDD(cutoff=0, bandwidth=200, polynomial_order=1)
rdd.fit(df['running_var'], df['future_spending'])

print(f"é‡‘å¡æ•ˆåº”: {rdd.tau_:.2f} å…ƒ")
print(f"p-value: {rdd.pvalue_:.4f}")

# 3. å¯è§†åŒ–
rdd.plot()
```

**ç›´è§‰**ï¼š
- æ¶ˆè´¹ 999 å…ƒçš„äººå’Œæ¶ˆè´¹ 1001 å…ƒçš„äººåº”è¯¥éå¸¸ç›¸ä¼¼
- å”¯ä¸€çš„åŒºåˆ«æ˜¯åè€…è·å¾—äº†é‡‘å¡
- ä¸¤è€…åç»­æ¶ˆè´¹çš„å·®å¼‚å¯ä»¥å½’å› äºé‡‘å¡èµ„æ ¼

**3. Validity Checks**

**Check 1ï¼šå¯†åº¦æ£€éªŒï¼ˆMcCrary Testï¼‰**
```python
# æ£€æŸ¥æ˜¯å¦æœ‰äººæ•…æ„æ“çºµåˆ° 1000 ä»¥ä¸Š
from rdd import mccrary_test

p_value = mccrary_test(df['running_var'], cutoff=0)

if p_value < 0.05:
    print("âš ï¸ å¯†åº¦åœ¨é˜ˆå€¼å¤„ä¸è¿ç»­ï¼Œå¯èƒ½æœ‰æ“çºµ")
else:
    print("âœ“ å¯†åº¦è¿ç»­ï¼Œæ²¡æœ‰æ“çºµè¯æ®")
```

**Check 2ï¼šåå˜é‡è¿ç»­æ€§**
```python
# åœ¨é˜ˆå€¼å¤„ï¼Œåå˜é‡ä¸åº”è¯¥è·³è·ƒ
for covar in ['age', 'income', 'city_tier']:
    rdd_covar = RDD(cutoff=0, bandwidth=200)
    rdd_covar.fit(df['running_var'], df[covar])

    print(f"{covar} åœ¨é˜ˆå€¼å¤„çš„è·³è·ƒ: {rdd_covar.tau_:.3f} (p={rdd_covar.pvalue_:.3f})")
    # åº”è¯¥éƒ½ä¸æ˜¾è‘—
```

**Check 3ï¼šPlacebo æˆªæ–­ç‚¹**
```python
# åœ¨éçœŸå®é˜ˆå€¼å¤„ä¸åº”è¯¥æœ‰è·³è·ƒ
placebo_cutoffs = [-500, -200, 200, 500]

for c in placebo_cutoffs:
    rdd_placebo = RDD(cutoff=c, bandwidth=200)
    rdd_placebo.fit(df['running_var'], df['future_spending'])

    print(f"Placebo cutoff {c}: tau={rdd_placebo.tau_:.2f}, p={rdd_placebo.pvalue_:.3f}")
    # åº”è¯¥éƒ½ä¸æ˜¾è‘—
```

**Check 4ï¼šå¸¦å®½æ•æ„Ÿæ€§**
```python
# ç»“æœåº”è¯¥å¯¹å¸¦å®½é€‰æ‹©ä¸å¤ªæ•æ„Ÿ
bandwidths = [100, 150, 200, 250, 300]

for h in bandwidths:
    rdd_h = RDD(cutoff=0, bandwidth=h)
    rdd_h.fit(df['running_var'], df['future_spending'])

    print(f"Bandwidth {h}: tau={rdd_h.tau_:.2f}")

# å¦‚æœå˜åŒ–ä¸å¤§ â†’ ç¨³å¥
```

---

## 8. å¸¸è§é™·é˜±ä¸è¯¯åŒº

### é™·é˜± 1ï¼šæ··æ·† ATT å’Œ ATE

**é”™è¯¯**ï¼š
> "DID ä¼°è®¡çš„æ˜¯ ATE"

**æ­£ç¡®**ï¼š
- DID ä¼°è®¡çš„æ˜¯ **ATT**ï¼ˆAverage Treatment Effect on the Treatedï¼‰
- åªæœ‰å½“å¤„ç†æ•ˆåº”åŒè´¨æ—¶ï¼ŒATT = ATE

**ä¾‹å­**ï¼š
```python
# çœŸå®çš„ DGP
def simulate_heterogeneous():
    # å¤„ç†æ•ˆåº”å–å†³äºåŸºçº¿æ°´å¹³
    baseline = np.random.normal(100, 20, 1000)

    # é«˜åŸºçº¿çš„äººè¢«é€‰å…¥å¤„ç†ç»„
    treated = baseline > 110

    # å¤„ç†æ•ˆåº”ï¼šå¯¹åŸºçº¿é«˜çš„äººæ•ˆåº”æ›´å¤§
    effect = np.where(treated, 0.1 * baseline, 0)

    y = baseline + effect + np.random.normal(0, 5, 1000)

    return {
        'ATT': effect[treated].mean(),  # åªé’ˆå¯¹å¤„ç†ç»„
        'ATE': effect.mean(),           # å…¨ä½“å¹³å‡
        'å·®å¼‚': effect[treated].mean() - effect.mean()
    }

# ATT â‰  ATE when effect is heterogeneous
```

---

### é™·é˜± 2ï¼šåå¤„ç†åå·®ï¼ˆPost-treatment Biasï¼‰

**é”™è¯¯**ï¼š
> "åœ¨ DID ä¸­æ§åˆ¶æ”¿ç­–åæ‰å‡ºç°çš„å˜é‡"

**ä¾‹å­**ï¼š
```python
# âŒ é”™è¯¯ï¼šæ§åˆ¶äº† post-treatment å˜é‡
model = smf.ols(
    'revenue ~ treat + post + treat_post + new_feature_usage',
    data=df
).fit()

# new_feature_usage æ˜¯æ”¿ç­–å®æ–½åæ‰äº§ç”Ÿçš„
# å®ƒæœ¬èº«å¯èƒ½æ˜¯æ”¿ç­–æ•ˆåº”çš„ä¸€éƒ¨åˆ†ï¼
```

**æ­£ç¡®åšæ³•**ï¼š
- åªæ§åˆ¶ **pre-treatment** å˜é‡
- Post-treatment å˜é‡å¯èƒ½æ˜¯ä¸­ä»‹å˜é‡ï¼ˆmediatorï¼‰
- å¦‚æœè¦åˆ†ææœºåˆ¶ï¼Œç”¨ mediation analysis

---

### é™·é˜± 3ï¼šè¿‡æ‹Ÿåˆçš„åˆæˆæ§åˆ¶

**é”™è¯¯**ï¼š
> "åœ¨æ•´ä¸ªæ—¶é—´æ®µï¼ˆåŒ…æ‹¬æ”¿ç­–åï¼‰ä¸Šæ‹Ÿåˆåˆæˆæ§åˆ¶"

**æ­£ç¡®**ï¼š
- **åªåœ¨å‰å¤„ç†æœŸæ‹Ÿåˆæƒé‡**
- æ”¿ç­–åçš„æ•°æ®ç”¨æ¥è¯„ä¼°æ•ˆåº”ï¼Œä¸èƒ½ç”¨æ¥æ‹Ÿåˆ

```python
# âŒ é”™è¯¯
sc = SyntheticControl(treatment_period=T0)
sc.fit(treated, donors)  # é»˜è®¤ç”¨å…¨éƒ¨æ•°æ®

# âœ“ æ­£ç¡®
sc = SyntheticControl(treatment_period=T0)
sc.fit(treated[:T0], donors[:T0, :])  # åªç”¨å‰å¤„ç†æœŸ
```

---

### é™·é˜± 4ï¼šå¿½ç•¥ RDD çš„ LATE æ€§è´¨

**é”™è¯¯**ï¼š
> "RDD ä¼°è®¡çš„æ˜¯å…¨ä½“çš„ ATE"

**æ­£ç¡®**ï¼š
- RDD ä¼°è®¡çš„æ˜¯ **é˜ˆå€¼é™„è¿‘**çš„ LATE
- ä¸èƒ½æ¨å¹¿åˆ°è¿œç¦»é˜ˆå€¼çš„äººç¾¤

**ä¾‹å­**ï¼š
```python
# ä¼šå‘˜é—¨æ§› RDD
# ä¼°è®¡çš„æ˜¯"æ¥è¿‘ 1000 å…ƒæ¶ˆè´¹"çš„äººè·å¾—é‡‘å¡çš„æ•ˆåº”
# ä¸æ˜¯"æ‰€æœ‰äºº"è·å¾—é‡‘å¡çš„æ•ˆåº”

# æ¶ˆè´¹ 10000 å…ƒçš„äººå³ä½¿æ²¡æœ‰é‡‘å¡ï¼Œä¹Ÿä¼šç»§ç»­é«˜æ¶ˆè´¹
# ä»–ä»¬çš„å¤„ç†æ•ˆåº”å¯èƒ½å®Œå…¨ä¸åŒ
```

---

## 9. æ¨èèµ„æº

### å¿…è¯»è®ºæ–‡

**DID**:
1. Bertrand, Duflo & Mullainathan (2004) - "How Much Should We Trust DID?"
2. Goodman-Bacon (2021) - "Difference-in-differences with variation in treatment timing"
3. Callaway & Sant'Anna (2021) - "Difference-in-Differences with multiple time periods"

**Synthetic Control**:
1. Abadie & Gardeazabal (2003) - "The Economic Costs of Conflict"
2. Abadie, Diamond & Hainmueller (2010) - "Synthetic Control Methods"
3. Abadie (2021) - "Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects"

**RDD**:
1. Lee & Lemieux (2010) - "Regression Discontinuity Designs in Economics"
2. Imbens & Lemieux (2008) - "Regression discontinuity designs: A guide to practice"
3. Cattaneo, Idrobo & Titiunik (2019) - *A Practical Introduction to RDD* (ä¹¦)

**IV**:
1. Angrist & Krueger (1991) - "Does Compulsory School Attendance Affect Schooling and Earnings?"
2. Angrist, Imbens & Rubin (1996) - "Identification of Causal Effects Using IV"
3. Stock & Yogo (2005) - "Testing for Weak Instruments in Linear IV Regression"

### Python åŒ…

- `linearmodels`: IV ä¼°è®¡
- `pyfixest`: é«˜æ€§èƒ½é¢æ¿æ•°æ®å›å½’
- `causalimpact`: æ—¶é—´åºåˆ—å› æœæ¨æ–­ï¼ˆGoogle çš„è´å¶æ–¯æ–¹æ³•ï¼‰
- `statsmodels`: DID, å›å½’, æ—¶é—´åºåˆ—

### åœ¨çº¿è¯¾ç¨‹

- Scott Cunningham - *Causal Inference: The Mixtape*
- Matheus Facure - *Causal Inference for The Brave and True*
- Nick Huntington-Klein - *The Effect*

---

**Good luck with your interview! ğŸ‰**
