"""
Datasets Module Demo - æ•°æ®é›†æ¨¡å—æ¼”ç¤º

å±•ç¤º datasets æ¨¡å—çš„ä¸»è¦åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# å¯¼å…¥æ•°æ®é›†æ¨¡å—
from datasets import (
    load_lalonde,
    generate_ihdp_semi_synthetic,
    generate_linear_dgp,
    generate_nonlinear_dgp,
    generate_heterogeneous_dgp,
    train_test_split_causal,
    describe_dataset
)

from datasets.utils import (
    check_covariate_balance,
    compute_propensity_score,
    plot_dataset_overview,
    plot_propensity_overlap
)

from datasets.synthetic import generate_marketing_dgp


def demo_lalonde():
    """æ¼”ç¤º LaLonde æ•°æ®é›†"""
    print("="*80)
    print("DEMO 1: LaLonde Dataset - å°±ä¸šåŸ¹è®­æ•°æ®")
    print("="*80)

    # åŠ è½½ä¸‰ä¸ªç‰ˆæœ¬
    nsw_df = load_lalonde('nsw')
    psid_df = load_lalonde('psid')
    cps_df = load_lalonde('cps')

    print("\n1.1 æ•°æ®é›†è§„æ¨¡å¯¹æ¯”")
    print("-"*80)
    print(f"NSW (RCT):             n = {len(nsw_df):,}")
    print(f"PSID (Observational):  n = {len(psid_df):,}")
    print(f"CPS (Observational):   n = {len(cps_df):,}")

    print("\n1.2 æœ´ç´  ATE ä¼°è®¡")
    print("-"*80)
    for name, df in [('NSW', nsw_df), ('PSID', psid_df), ('CPS', cps_df)]:
        ate = df[df['treat']==1]['re78'].mean() - df[df['treat']==0]['re78'].mean()
        print(f"{name:5s}: ${ate:,.2f}")

    print("\nğŸ’¡ è§£è¯»:")
    print("   - NSW (RCT) çš„ä¼°è®¡çº¦ä¸º $1,900 (çœŸå®å› æœæ•ˆåº”)")
    print("   - PSID/CPS çš„ä¼°è®¡ä¸ºè´Ÿå€¼ï¼Œå­˜åœ¨ä¸¥é‡é€‰æ‹©åå·®!")
    print("   - è¿™æ­£æ˜¯ LaLonde (1986) çš„ç»å…¸å‘ç°")

    # åå˜é‡å¹³è¡¡æ£€æŸ¥
    print("\n1.3 åå˜é‡å¹³è¡¡æ€§æ£€æŸ¥")
    print("-"*80)
    feature_cols = ['age', 'education', 're74', 're75']

    for name, df in [('NSW', nsw_df), ('PSID', psid_df)]:
        balance = check_covariate_balance(
            df[feature_cols].values,
            df['treat'].values,
            feature_names=feature_cols,
            threshold=0.1
        )
        imbalanced = balance[balance['SMD'] > 0.1]
        print(f"\n{name}: {len(imbalanced)}/{len(feature_cols)} features imbalanced")
        if len(imbalanced) > 0:
            print(balance.head(3).to_string(index=False))


def demo_ihdp():
    """æ¼”ç¤º IHDP æ•°æ®é›†"""
    print("\n" + "="*80)
    print("DEMO 2: IHDP Dataset - å©´å„¿å¥åº·å‘å±•è®¡åˆ’")
    print("="*80)

    print("\n2.1 ç”Ÿæˆ IHDP åŠåˆæˆæ•°æ® (è®¾ç½® A)")
    print("-"*80)
    X, T, Y, true_ite = generate_ihdp_semi_synthetic(n_samples=747, setting='A', seed=42)

    print(f"æ ·æœ¬é‡: {len(T)}")
    print(f"ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"å¤„ç†ç‡: {T.mean():.2%}")
    print(f"\nçœŸå® ATE: {true_ite.mean():.3f}")
    print(f"ITE æ ‡å‡†å·®: {true_ite.std():.3f}")
    print(f"ITE èŒƒå›´: [{true_ite.min():.3f}, {true_ite.max():.3f}]")

    # å¯¹æ¯”æœ´ç´ ä¼°è®¡
    naive_ate = Y[T==1].mean() - Y[T==0].mean()
    bias = abs(naive_ate - true_ite.mean())
    print(f"\næœ´ç´  ATE: {naive_ate:.3f}")
    print(f"åå·®: {bias:.3f}")

    print("\n2.2 è®¾ç½® A vs è®¾ç½® B å¯¹æ¯”")
    print("-"*80)
    for setting in ['A', 'B']:
        X_s, T_s, Y_s, ite_s = generate_ihdp_semi_synthetic(
            n_samples=747,
            setting=setting,
            seed=42
        )
        print(f"\nè®¾ç½® {setting}:")
        print(f"  ATE: {ite_s.mean():.3f} Â± {ite_s.std():.3f}")
        print(f"  å¼‚è´¨æ€§ç³»æ•° (ITE_std/ATE): {ite_s.std()/ite_s.mean():.2f}")

    print("\nğŸ’¡ ç”¨é€”: IHDP æ˜¯è¯„ä¼° CATE æ–¹æ³•çš„é»„é‡‘æ ‡å‡†")


def demo_synthetic():
    """æ¼”ç¤ºåˆæˆæ•°æ®ç”Ÿæˆå™¨"""
    print("\n" + "="*80)
    print("DEMO 3: Synthetic Data Generators - åˆæˆæ•°æ®ç”Ÿæˆå™¨")
    print("="*80)

    print("\n3.1 çº¿æ€§ DGP - æ··æ·†æ•ˆåº”æ¼”ç¤º")
    print("-"*80)

    results = []
    for confounding in [False, True]:
        X, T, Y, true_ite = generate_linear_dgp(
            n_samples=1000,
            confounding=confounding,
            treatment_effect=2.0,
            seed=42
        )

        naive_ate = Y[T==1].mean() - Y[T==0].mean()
        bias = abs(naive_ate - true_ite.mean())

        results.append({
            'Confounding': 'Yes' if confounding else 'No',
            'True ATE': f"{true_ite.mean():.3f}",
            'Naive ATE': f"{naive_ate:.3f}",
            'Bias': f"{bias:.3f}"
        })

    print(pd.DataFrame(results).to_string(index=False))
    print("\nğŸ’¡ æ··æ·†å¯¼è‡´æœ´ç´ ä¼°è®¡ä¸¥é‡åå·®!")

    print("\n3.2 éçº¿æ€§ DGP - å¤æ‚åº¦å¯¹æ¯”")
    print("-"*80)

    for complexity in ['low', 'medium', 'high']:
        X, T, Y, true_ite = generate_nonlinear_dgp(
            n_samples=1000,
            complexity=complexity,
            seed=42
        )

        print(f"\n{complexity.upper()}:")
        print(f"  ATE: {true_ite.mean():.3f}")
        print(f"  ITE std: {true_ite.std():.3f}")
        print(f"  ITE range: [{true_ite.min():.3f}, {true_ite.max():.3f}]")

    print("\n3.3 å¼‚è´¨æ€§ DGP - ä¸åŒå¼‚è´¨æ€§æ¨¡å¼")
    print("-"*80)

    for het_type in ['linear', 'interaction', 'threshold', 'complex']:
        X, T, Y, true_ite = generate_heterogeneous_dgp(
            n_samples=1000,
            heterogeneity_type=het_type,
            seed=42
        )

        print(f"\n{het_type.upper()}:")
        print(f"  ATE: {true_ite.mean():.3f}")
        print(f"  ITE std: {true_ite.std():.3f}")
        print(f"  å¼‚è´¨æ€§ç³»æ•°: {true_ite.std()/abs(true_ite.mean()):.2f}")


def demo_marketing():
    """æ¼”ç¤ºè¥é”€åœºæ™¯æ•°æ®"""
    print("\n" + "="*80)
    print("DEMO 4: Marketing Scenarios - è¥é”€åœºæ™¯æ•°æ®")
    print("="*80)

    scenarios = ['coupon', 'email', 'recommendation']

    for scenario in scenarios:
        df, true_uplift = generate_marketing_dgp(
            n_samples=5000,
            scenario=scenario,
            seed=42
        )

        outcome_col = {
            'coupon': 'conversion',
            'email': 'click',
            'recommendation': 'purchase'
        }[scenario]

        treated_rate = df[df['treatment']==1][outcome_col].mean()
        control_rate = df[df['treatment']==0][outcome_col].mean()
        observed_uplift = treated_rate - control_rate

        print(f"\n{scenario.upper()} åœºæ™¯:")
        print(f"  æ ·æœ¬é‡: {len(df):,}")
        print(f"  å¤„ç†ç»„{outcome_col}ç‡: {treated_rate:.2%}")
        print(f"  å¯¹ç…§ç»„{outcome_col}ç‡: {control_rate:.2%}")
        print(f"  è§‚æµ‹ Uplift: {observed_uplift:.4f}")
        print(f"  çœŸå®å¹³å‡ Uplift: {true_uplift.mean():.4f}")


def demo_utils():
    """æ¼”ç¤ºå·¥å…·å‡½æ•°"""
    print("\n" + "="*80)
    print("DEMO 5: Utility Functions - å·¥å…·å‡½æ•°")
    print("="*80)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    X, T, Y, true_ite = generate_heterogeneous_dgp(
        n_samples=1000,
        heterogeneity_type='linear',
        seed=42
    )

    print("\n5.1 æ•°æ®é›†æè¿°")
    print("-"*80)
    stats = describe_dataset(X, T, Y, true_ite)
    print(stats.to_string(index=False))

    print("\n5.2 å› æœæ•°æ®åˆ’åˆ†")
    print("-"*80)
    X_tr, X_te, T_tr, T_te, Y_tr, Y_te, ite_tr, ite_te = train_test_split_causal(
        X, T, Y, true_ite,
        test_size=0.3,
        stratify_treatment=True
    )

    print(f"è®­ç»ƒé›†: {len(T_tr)} (å¤„ç†ç‡: {T_tr.mean():.2%})")
    print(f"æµ‹è¯•é›†: {len(T_te)} (å¤„ç†ç‡: {T_te.mean():.2%})")
    print(f"å¤„ç†ç‡å·®å¼‚: {abs(T_tr.mean() - T_te.mean()):.4f}")

    print("\n5.3 åå˜é‡å¹³è¡¡æ£€æŸ¥")
    print("-"*80)
    balance = check_covariate_balance(X, T, threshold=0.1)
    print(balance.head(5).to_string(index=False))

    imbalanced = balance[balance['SMD'] > 0.1]
    if len(imbalanced) > 0:
        print(f"\nâš  {len(imbalanced)} features are imbalanced")
    else:
        print("\nâœ“ All features are balanced")

    print("\n5.4 å€¾å‘å¾—åˆ†")
    print("-"*80)
    ps = compute_propensity_score(X, T, method='logistic')
    print(f"å€¾å‘å¾—åˆ†èŒƒå›´: [{ps.min():.3f}, {ps.max():.3f}]")
    print(f"å¤„ç†ç»„å¹³å‡ PS: {ps[T==1].mean():.3f}")
    print(f"å¯¹ç…§ç»„å¹³å‡ PS: {ps[T==0].mean():.3f}")


def demo_cate_evaluation():
    """æ¼”ç¤º CATE æ–¹æ³•è¯„ä¼°"""
    print("\n" + "="*80)
    print("DEMO 6: CATE Method Evaluation - CATE æ–¹æ³•è¯„ä¼°ç¤ºä¾‹")
    print("="*80)

    # ç”Ÿæˆæ•°æ®
    X, T, Y, true_ite = generate_ihdp_semi_synthetic(setting='A', seed=42)

    # åˆ’åˆ†æ•°æ®
    X_tr, X_te, T_tr, T_te, Y_tr, Y_te, ite_tr, ite_te = train_test_split_causal(
        X, T, Y, true_ite, test_size=0.3
    )

    print("\n6.1 è®­ç»ƒ T-Learner")
    print("-"*80)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error, r2_score

    # T-Learner
    model_0 = RandomForestRegressor(n_estimators=100, random_state=42)
    model_1 = RandomForestRegressor(n_estimators=100, random_state=42)

    model_0.fit(X_tr[T_tr==0], Y_tr[T_tr==0])
    model_1.fit(X_tr[T_tr==1], Y_tr[T_tr==1])

    # é¢„æµ‹ CATE
    pred_ite = model_1.predict(X_te) - model_0.predict(X_te)

    # è¯„ä¼°
    mse = mean_squared_error(ite_te, pred_ite)
    rmse = np.sqrt(mse)
    r2 = r2_score(ite_te, pred_ite)

    print(f"æµ‹è¯•é›†æ ·æœ¬: {len(ite_te)}")
    print(f"çœŸå® ATE: {ite_te.mean():.3f}")
    print(f"é¢„æµ‹ ATE: {pred_ite.mean():.3f}")
    print(f"\nCATE RMSE: {rmse:.3f}")
    print(f"CATE RÂ²: {r2:.3f}")

    # è¯¯å·®åˆ†æ
    abs_error = np.abs(pred_ite - ite_te)
    print(f"\nç»å¯¹è¯¯å·®ç»Ÿè®¡:")
    print(f"  Mean: {abs_error.mean():.3f}")
    print(f"  Median: {np.median(abs_error):.3f}")
    print(f"  90th percentile: {np.percentile(abs_error, 90):.3f}")


def demo_visualizations():
    """æ¼”ç¤ºå¯è§†åŒ–åŠŸèƒ½"""
    print("\n" + "="*80)
    print("DEMO 7: Visualizations - å¯è§†åŒ–ç¤ºä¾‹")
    print("="*80)

    # ç”Ÿæˆæ•°æ®
    X, T, Y, true_ite = generate_heterogeneous_dgp(
        n_samples=1000,
        heterogeneity_type='threshold',
        seed=42
    )

    print("\n7.1 æ•°æ®é›†æ¦‚è§ˆå›¾")
    print("-"*80)
    fig1 = plot_dataset_overview(X, T, Y, true_ite)
    print("âœ“ å›¾è¡¨å·²ç”Ÿæˆ (åŒ…å«: å¤„ç†åˆ†å¸ƒ, ç»“æœåˆ†å¸ƒ, ITEåˆ†å¸ƒ, åå˜é‡æ•£ç‚¹)")

    print("\n7.2 å€¾å‘å¾—åˆ†é‡å å›¾")
    print("-"*80)
    fig2 = plot_propensity_overlap(X, T)
    print("âœ“ å›¾è¡¨å·²ç”Ÿæˆ (æ£€æŸ¥å…±åŒæ”¯æ’‘å‡è®¾)")

    print("\nğŸ’¡ åœ¨ Jupyter ä¸­ä½¿ç”¨ fig.show() æŸ¥çœ‹äº¤äº’å¼å›¾è¡¨")


def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("\n" + "="*80)
    print("ğŸ¯ DATASETS MODULE COMPREHENSIVE DEMO")
    print("="*80)
    print("\næœ¬æ¼”ç¤ºå±•ç¤º datasets æ¨¡å—çš„å®Œæ•´åŠŸèƒ½")
    print("åŒ…æ‹¬: ç»å…¸æ•°æ®é›†ã€åˆæˆæ•°æ®ã€å·¥å…·å‡½æ•°ã€è¯„ä¼°ç¤ºä¾‹\n")

    # è¿è¡Œå„ä¸ªæ¼”ç¤º
    demo_lalonde()
    demo_ihdp()
    demo_synthetic()
    demo_marketing()
    demo_utils()
    demo_cate_evaluation()
    demo_visualizations()

    # æ€»ç»“
    print("\n" + "="*80)
    print("âœ… DEMO COMPLETE - æ¼”ç¤ºå®Œæˆ")
    print("="*80)
    print("\nä¸»è¦åŠŸèƒ½:")
    print("  âœ“ LaLonde: è§‚æµ‹æ•°æ® vs RCT å¯¹æ¯”")
    print("  âœ“ IHDP: CATE è¯„ä¼°åŸºå‡†")
    print("  âœ“ Synthetic: å¤šç§å› æœæ¨¡å‹ç”Ÿæˆ")
    print("  âœ“ Marketing: å®é™…åœºæ™¯æ•°æ®")
    print("  âœ“ Utils: å®Œæ•´å·¥å…·é“¾")
    print("  âœ“ Visualization: äº¤äº’å¼å¯è§†åŒ–")

    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹ datasets/README.md äº†è§£è¯¦ç»†æ–‡æ¡£")
    print("  2. åœ¨ Jupyter ä¸­è¿è¡Œç¤ºä¾‹ä»£ç ")
    print("  3. å°†æ•°æ®é›†é›†æˆåˆ°å› æœæ¨æ–­æ¨¡å‹ä¸­")

    print("\n" + "="*80)


if __name__ == "__main__":
    main()
