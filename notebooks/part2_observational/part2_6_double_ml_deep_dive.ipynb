{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Double Machine Learning æ·±åº¦å‰–æ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¬ çœŸå®åœºæ™¯ï¼šå½“ç»æµå­¦å®¶é‡ä¸Šæœºå™¨å­¦ä¹ \n",
    "\n",
    "æƒ³è±¡ä½ æ˜¯æŸç”µå•†å¹³å°çš„æ•°æ®ç§‘å­¦å®¶ï¼Œè€æ¿é—®ä½ ï¼š\n",
    "\n",
    "> ã€Œæˆ‘ä»¬çš„ VIP ä¼šå‘˜æƒç›Šåˆ°åº•å€¼å¤šå°‘é’±ï¼Ÿèƒ½ä¸èƒ½ç”¨å› æœæ¨æ–­ç®—å‡ºæ¥ï¼Ÿã€\n",
    "\n",
    "ä½ ä¿¡å¿ƒæ»¡æ»¡åœ°è¯´ï¼šã€Œæ²¡é—®é¢˜ï¼æˆ‘æœ‰ 500 ä¸ªç”¨æˆ·ç‰¹å¾ï¼Œç”¨ XGBoost ä¼°è®¡å€¾å‘å¾—åˆ†ï¼Œå†ç”¨ DR ä¼°è®¡æ•ˆåº”ï¼ã€\n",
    "\n",
    "ç»“æœå‘¢ï¼Ÿ\n",
    "\n",
    "- ğŸ”´ **æ•ˆåº”ä¼°è®¡å¿½é«˜å¿½ä½**ï¼Œç½®ä¿¡åŒºé—´å·¨å¤§\n",
    "- ğŸ”´ **æ¢ä¸ªæ¨¡å‹ç»“æœå°±å˜**ï¼Œè€æ¿ä¸æ•¢ä¿¡\n",
    "- ğŸ”´ **p-value ä¸€ç›´å¾ˆæ˜¾è‘—**ï¼Œä½†ä½ å¿ƒé‡Œæ²¡åº•\n",
    "\n",
    "é—®é¢˜å‡ºåœ¨å“ªï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "### é«˜ç»´æ•°æ®çš„è¯…å’’ ğŸ‘»\n",
    "\n",
    "å½“ç‰¹å¾å¾ˆå¤šï¼ˆé«˜ç»´ï¼‰æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•ä¼šé‡åˆ°ä¸¤ä¸ªè‡´å‘½é—®é¢˜ï¼š\n",
    "\n",
    "| é—®é¢˜ | ä¼ ç»Ÿæ–¹æ³•çš„è¡¨ç° | åæœ |\n",
    "|------|---------------|------|\n",
    "| **æ­£åˆ™åŒ–åå·®** | ML æ¨¡å‹éœ€è¦æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ | æ­£åˆ™åŒ–ä¼šæŠŠå› æœæ•ˆåº”ä¹Ÿã€Œç¼©å°ã€äº† |\n",
    "| **è¿‡æ‹Ÿåˆåå·®** | ç”¨åŒä¸€ä»½æ•°æ®è®­ç»ƒå’Œé¢„æµ‹ | æ®‹å·®è¢«ä½ä¼°ï¼Œæ ‡å‡†è¯¯å¤±æ•ˆ |\n",
    "\n",
    "**Double Machine Learning (DML)** å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜è€Œç”Ÿçš„ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç»ƒä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. ğŸ¯ ç†è§£**æ­£åˆ™åŒ–åå·®**å’Œ**è¿‡æ‹Ÿåˆåå·®**çš„æ¥æº\n",
    "2. ğŸ¯ æŒæ¡ **Neyman æ­£äº¤æ€§**çš„æ ¸å¿ƒæ€æƒ³\n",
    "3. ğŸ¯ **ä»é›¶å®ç°** Cross-fitting DMLï¼ˆä¸ç”¨ä»»ä½•å› æœåº“ï¼‰\n",
    "4. ğŸ¯ ç†è§£ DML çš„**æ¸è¿‘æ­£æ€æ€§**å’Œç½®ä¿¡åŒºé—´æ„é€ \n",
    "5. ğŸ¯ åœ¨é¢è¯•ä¸­æ¸…æ™°åœ°è§£é‡Š DML åŸç†\n",
    "\n",
    "> âš ï¸ **é‡è¦**: è¿™æ˜¯å¤§å‚é¢è¯•çš„é«˜é¢‘è€ƒç‚¹ï¼å¾ˆå¤šé¢è¯•å®˜ä¼šé—®ã€ŒDML å’Œæ™®é€š DR æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦ Cross-fittingï¼Ÿã€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Dict, Callable, List\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæ¯•ï¼\")\n",
    "print(\"ğŸ“¦ æœ¬èŠ‚æˆ‘ä»¬å°†ä»é›¶å®ç° Double MLï¼Œä¸ä¾èµ– econml ç­‰åº“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§  Part 1: ä¸ºä»€ä¹ˆéœ€è¦ Double MLï¼Ÿ\n",
    "\n",
    "### 1.1 æ­£åˆ™åŒ–åå·®é—®é¢˜\n",
    "\n",
    "å‡è®¾çœŸå®çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ (DGP) æ˜¯ï¼š\n",
    "\n",
    "$$Y = \\tau \\cdot T + g(X) + \\epsilon$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\tau$ æ˜¯æˆ‘ä»¬è¦ä¼°è®¡çš„**å› æœæ•ˆåº”**\n",
    "- $g(X)$ æ˜¯åå˜é‡å¯¹ç»“æœçš„å½±å“ï¼ˆå¯èƒ½éå¸¸å¤æ‚ï¼‰\n",
    "- $T$ æ˜¯å¤„ç†å˜é‡\n",
    "\n",
    "**é—®é¢˜æ¥äº†**ï¼šå¦‚æœæˆ‘ä»¬ç›´æ¥ç”¨å¸¦æ­£åˆ™åŒ–çš„å›å½’ï¼š\n",
    "\n",
    "$$\\min_{\\tau, \\beta} \\sum_i (Y_i - \\tau T_i - X_i'\\beta)^2 + \\lambda ||\\beta||$$\n",
    "\n",
    "æ­£åˆ™åŒ–é¡¹ $\\lambda ||\\beta||$ ä¼šã€Œé¡ºä¾¿ã€æŠŠ $\\tau$ ä¹Ÿå¾€ 0 å‹ç¼©ï¼\n",
    "\n",
    "è¿™å°±æ˜¯**æ­£åˆ™åŒ–åå·® (Regularization Bias)**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_regularization_bias():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºæ­£åˆ™åŒ–åå·®ï¼šå½“ç‰¹å¾å¾ˆå¤šæ—¶ï¼ŒLasso ä¼šæŠŠå› æœæ•ˆåº”ä¹Ÿå‹ç¼©\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n = 1000\n",
    "    p = 200  # é«˜ç»´ï¼š200 ä¸ªç‰¹å¾\n",
    "    \n",
    "    # ç”Ÿæˆé«˜ç»´ç‰¹å¾\n",
    "    X = np.random.randn(n, p)\n",
    "    \n",
    "    # çœŸå®çš„ç¨€ç–ç³»æ•°ï¼ˆåªæœ‰å‰ 10 ä¸ªæœ‰ç”¨ï¼‰\n",
    "    true_beta = np.zeros(p)\n",
    "    true_beta[:10] = np.random.randn(10)\n",
    "    \n",
    "    # å¤„ç†å˜é‡ï¼ˆä¸ X ç›¸å…³ï¼‰\n",
    "    propensity_logit = X[:, 0] + 0.5 * X[:, 1]\n",
    "    T = (np.random.rand(n) < 1/(1 + np.exp(-propensity_logit))).astype(float)\n",
    "    \n",
    "    # çœŸå®å› æœæ•ˆåº”\n",
    "    TRUE_ATE = 2.0\n",
    "    \n",
    "    # ç”Ÿæˆç»“æœ\n",
    "    Y = TRUE_ATE * T + X @ true_beta + np.random.randn(n) * 0.5\n",
    "    \n",
    "    # æ–¹æ³• 1: OLSï¼ˆæ— æ­£åˆ™åŒ–ï¼Œä½†åœ¨é«˜ç»´ä¸‹ä¸ç¨³å®šï¼‰\n",
    "    X_with_T = np.column_stack([T, X])\n",
    "    try:\n",
    "        ols_coef = np.linalg.lstsq(X_with_T, Y, rcond=None)[0]\n",
    "        ols_ate = ols_coef[0]\n",
    "    except:\n",
    "        ols_ate = np.nan\n",
    "    \n",
    "    # æ–¹æ³• 2: Lassoï¼ˆæœ‰æ­£åˆ™åŒ–åå·®ï¼‰\n",
    "    lasso = LassoCV(cv=5, max_iter=10000)\n",
    "    lasso.fit(X_with_T, Y)\n",
    "    lasso_ate = lasso.coef_[0]\n",
    "    \n",
    "    # æ–¹æ³• 3: æœ´ç´ ä¼°è®¡\n",
    "    naive_ate = Y[T==1].mean() - Y[T==0].mean()\n",
    "    \n",
    "    print(\"ğŸ”¬ æ­£åˆ™åŒ–åå·®æ¼”ç¤º\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"è®¾ç½®: n={n}, p={p} (é«˜ç»´æ•°æ®)\")\n",
    "    print(f\"çœŸå® ATE = {TRUE_ATE}\")\n",
    "    print()\n",
    "    print(f\"æ–¹æ³•              | ATE ä¼°è®¡  | åå·®\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"æœ´ç´ ä¼°è®¡          | {naive_ate:8.4f} | {naive_ate - TRUE_ATE:+.4f}\")\n",
    "    if not np.isnan(ols_ate):\n",
    "        print(f\"OLS (æ— æ­£åˆ™åŒ–)    | {ols_ate:8.4f} | {ols_ate - TRUE_ATE:+.4f}\")\n",
    "    print(f\"Lasso (æ­£åˆ™åŒ–)    | {lasso_ate:8.4f} | {lasso_ate - TRUE_ATE:+.4f} âš ï¸\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ è§‚å¯Ÿ: Lasso çš„ä¼°è®¡è¢«å‹ç¼©å‘ 0ï¼Œè¿™å°±æ˜¯æ­£åˆ™åŒ–åå·®ï¼\")\n",
    "    \n",
    "    return TRUE_ATE, naive_ate, lasso_ate\n",
    "\n",
    "demonstrate_regularization_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 è¿‡æ‹Ÿåˆåå·®é—®é¢˜\n",
    "\n",
    "å³ä½¿æˆ‘ä»¬ç”¨ AIPWï¼Œå¦‚æœç”¨**åŒä¸€ä»½æ•°æ®**è®­ç»ƒæ¨¡å‹å’Œè®¡ç®—æ•ˆåº”ï¼Œè¿˜ä¼šæœ‰**è¿‡æ‹Ÿåˆåå·®**ï¼š\n",
    "\n",
    "$$\\text{AIPW} = \\frac{1}{n}\\sum_i \\left[ \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{T_i(Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}_0(X_i))}{1-\\hat{e}(X_i)} \\right]$$\n",
    "\n",
    "é—®é¢˜åœ¨äºï¼š$\\hat{\\mu}(X_i)$ æ˜¯ç”¨åŒ…å«ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æ•°æ®è®­ç»ƒçš„ï¼Œæ‰€ä»¥ $Y_i - \\hat{\\mu}(X_i)$ ä¼šè¢«**ä½ä¼°**ï¼\n",
    "\n",
    "è¿™ä¸ªã€Œè‡ªå·±é¢„æµ‹è‡ªå·±ã€çš„é—®é¢˜ï¼Œä¼šå¯¼è‡´ï¼š\n",
    "- æ ‡å‡†è¯¯è¢«ä½ä¼°\n",
    "- ç½®ä¿¡åŒºé—´è¿‡çª„\n",
    "- p-value è¿‡äºæ˜¾è‘—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ğŸ“ Part 2: DML çš„æ ¸å¿ƒæ€æƒ³\n\n### 2.1 Partially Linear Model (PLR)\n\nDML æœ€ç»å…¸çš„åº”ç”¨åœºæ™¯æ˜¯**éƒ¨åˆ†çº¿æ€§æ¨¡å‹**ï¼š\n\n$$Y = \\tau \\cdot T + g(X) + \\epsilon, \\quad E[\\epsilon|X, T] = 0$$\n$$T = m(X) + \\eta, \\quad E[\\eta|X] = 0$$\n\nå…¶ä¸­ï¼š\n- $\\tau$ æ˜¯ä½ç»´å‚æ•°ï¼ˆæˆ‘ä»¬å…³å¿ƒçš„å› æœæ•ˆåº”ï¼‰\n- $g(X)$, $m(X)$ æ˜¯é«˜ç»´ nuisance å‚æ•°ï¼ˆçƒ¦äººä½†å¿…é¡»å¤„ç†çš„ï¼‰\n\n### 2.2 Neyman æ­£äº¤æ€§ï¼ˆæ ¸å¿ƒï¼ï¼‰\n\nDML çš„æ ¸å¿ƒåˆ›æ–°æ˜¯æ„é€ ä¸€ä¸ª**å¯¹ nuisance å‚æ•°ä¸æ•æ„Ÿ**çš„çŸ©ä¼°è®¡ã€‚\n\n**å…³é”®æ´å¯Ÿ**ï¼šå…ˆæŠŠ $g(X)$ å’Œ $m(X)$ ä» $Y$ å’Œ $T$ ä¸­ã€Œå»æ‰ã€ï¼Œå¾—åˆ°æ®‹å·®ï¼š\n\n$$\\tilde{Y} = Y - g(X) = \\tau \\cdot T + \\epsilon$$\n$$\\tilde{T} = T - m(X) = \\eta$$\n\nç„¶åç”¨æ®‹å·®åšå›å½’ï¼š\n\n$$\\tilde{Y} = \\tau \\cdot \\tilde{T} + \\epsilon$$\n\n$$\\hat{\\tau} = \\frac{\\sum_i \\tilde{Y}_i \\tilde{T}_i}{\\sum_i \\tilde{T}_i^2} = \\frac{E[\\tilde{Y} \\cdot \\tilde{T}]}{E[\\tilde{T}^2]}$$\n\n### ğŸ“ æ ¸å¿ƒå…¬å¼\n\n$$\\hat{\\tau}_{DML} = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{g}(X_i))(T_i - \\hat{m}(X_i))}{\\frac{1}{n}\\sum_{i=1}^{n}(T_i - \\hat{m}(X_i))^2}$$\n\n| ç¬¦å· | å«ä¹‰ |\n|------|------|\n| $\\hat{g}(X)$ | ç»“æœæ¨¡å‹ï¼š$E[Y|X]$ çš„ä¼°è®¡ |\n| $\\hat{m}(X)$ | å€¾å‘å¾—åˆ†ï¼š$E[T|X]$ çš„ä¼°è®¡ |\n| $Y - \\hat{g}(X)$ | ç»“æœæ®‹å·® |\n| $T - \\hat{m}(X)$ | å¤„ç†æ®‹å·® |\n\n**ç›´è§‚ç†è§£**ï¼šå°±åƒåšä¸€ä¸ªã€ŒåŒé‡æ®‹å·®å›å½’ã€â€”â€”å…ˆç”¨ ML æ¨¡å‹æŠŠ $X$ çš„å½±å“å»æ‰ï¼Œå†ä¼°è®¡ $T$ å¯¹ $Y$ çš„æ•ˆåº”ã€‚\n\n### 2.3 ä¸ºä»€ä¹ˆå«ã€ŒDoubleã€ï¼Ÿ\n\nã€ŒDoubleã€æœ‰ä¸¤å±‚å«ä¹‰ï¼š\n\n1. **Double Residualization**ï¼šå¯¹ $Y$ å’Œ $T$ éƒ½åšæ®‹å·®åŒ–\n2. **Double Robustness**ï¼šå¦‚æœ $\\hat{g}$ æˆ– $\\hat{m}$ ä¹‹ä¸€æ­£ç¡®ï¼Œä¼°è®¡å°±æ˜¯ä¸€è‡´çš„\n\nè¿™ä¸ AIPW çš„åŒé‡ç¨³å¥æ€§æ˜¯ç›¸é€šçš„ï¼\n\n---\n\n## ğŸ“Š Neyman æ­£äº¤æ€§çš„æ•°å­¦æ¨å¯¼\n\n**Neyman æ­£äº¤æ¡ä»¶**: çŸ©å‡½æ•°å¯¹ nuisance å‚æ•°çš„å¯¼æ•°ä¸º 0\n\nDML ä½¿ç”¨çš„çŸ©å‡½æ•°ï¼š\n\n$$\\psi(W; \\tau, \\eta) = (Y - \\tau \\cdot T - g(X))(T - m(X))$$\n\nå…¶ä¸­ $\\eta = (g, m)$ æ˜¯ nuisance å‚æ•°ã€‚\n\n**æ­£äº¤æ€§æ¡ä»¶**:\n\n$$\\frac{\\partial}{\\partial \\eta} E[\\psi(W; \\tau_0, \\eta)] \\Big|_{\\eta=\\eta_0} = 0$$\n\n**è¯æ˜**:\n\n$$E[\\psi(W; \\tau, \\eta)] = E[(Y - \\tau T - g(X))(T - m(X))]$$\n\nå¯¹ $g$ æ±‚åå¯¼ï¼š\n\n$$\\frac{\\partial}{\\partial g} E[(Y - \\tau T - g(X))(T - m(X))] = -E[T - m(X)] = 0$$\n\nï¼ˆå› ä¸º $E[T|X] = m(X)$ï¼Œæ‰€ä»¥ $E[T - m(X)] = 0$ï¼‰\n\nå¯¹ $m$ æ±‚åå¯¼ï¼š\n\n$$\\frac{\\partial}{\\partial m} E[(Y - \\tau T - g(X))(T - m(X))] = -E[Y - \\tau T - g(X)] = 0$$\n\nï¼ˆå› ä¸ºå½“ $\\tau = \\tau_0$, $g = g_0$ æ—¶ï¼Œ$E[Y - \\tau_0 T - g_0(X)] = 0$ï¼‰\n\n**å…³é”®æ´å¯Ÿ**ï¼šçŸ©å‡½æ•°åœ¨çœŸå®å‚æ•°å¤„å¯¹ nuisance å‚æ•°ä¸æ•æ„Ÿï¼\n\nè¿™æ„å‘³ç€ï¼š\n- å³ä½¿ $\\hat{g}$, $\\hat{m}$ æœ‰å°è¯¯å·®\n- å¯¹ $\\hat{\\tau}$ çš„å½±å“æ˜¯äºŒé˜¶çš„ï¼ˆ$O(||\\hat{\\eta} - \\eta||^2)$ï¼‰\n- è€Œä¸æ˜¯ä¸€é˜¶çš„ï¼\n\nè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ DML åœ¨é«˜ç»´æ•°æ®ä¸‹ä»èƒ½å¾—åˆ° $\\sqrt{n}$-ä¸€è‡´çš„ä¼°è®¡å’Œæœ‰æ•ˆæ¨æ–­ï¼\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Part 3: Cross-fittingï¼ˆäº¤å‰æ‹Ÿåˆï¼‰\n",
    "\n",
    "### 3.1 ä¸ºä»€ä¹ˆéœ€è¦ Cross-fittingï¼Ÿ\n",
    "\n",
    "é—®é¢˜ï¼šå¦‚æœç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ $\\hat{g}$ å’Œ $\\hat{m}$ï¼Œç„¶åç”¨åŒä¸€æ‰¹æ•°æ®è®¡ç®—æ®‹å·®ï¼š\n",
    "\n",
    "- æ®‹å·® $Y_i - \\hat{g}(X_i)$ ä¼šåå°ï¼ˆå› ä¸ºæ¨¡å‹ã€Œè§è¿‡ã€è¿™ä¸ªæ ·æœ¬ï¼‰\n",
    "- å¯¼è‡´ $\\hat{\\tau}$ æœ‰å\n",
    "\n",
    "### 3.2 Cross-fitting çš„è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "```\n",
    "æ•°æ®åˆ†æˆ K æŠ˜ï¼š[Fold 1] [Fold 2] [Fold 3] ... [Fold K]\n",
    "\n",
    "å¯¹äº Fold k:\n",
    "  - ç”¨å…¶ä»– K-1 æŠ˜è®­ç»ƒæ¨¡å‹ g_k, m_k\n",
    "  - å¯¹ Fold k çš„æ ·æœ¬è®¡ç®—æ®‹å·®\n",
    "\n",
    "æ±‡æ€»æ‰€æœ‰æŠ˜çš„æ®‹å·®ï¼Œè®¡ç®—æœ€ç»ˆä¼°è®¡\n",
    "```\n",
    "\n",
    "è¿™æ ·ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ®‹å·®éƒ½æ˜¯ç”¨ã€Œæœªè§è¿‡å®ƒçš„æ¨¡å‹ã€è®¡ç®—çš„ï¼\n",
    "\n",
    "### 3.3 Cross-fitting ä¼ªä»£ç \n",
    "\n",
    "```python\n",
    "def cross_fit_dml(X, T, Y, n_folds=5):\n",
    "    Y_residuals = np.zeros(n)\n",
    "    T_residuals = np.zeros(n)\n",
    "    \n",
    "    for train_idx, test_idx in KFold(n_folds).split(X):\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹\n",
    "        g_model.fit(X[train_idx], Y[train_idx])\n",
    "        m_model.fit(X[train_idx], T[train_idx])\n",
    "        \n",
    "        # åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—æ®‹å·®\n",
    "        Y_residuals[test_idx] = Y[test_idx] - g_model.predict(X[test_idx])\n",
    "        T_residuals[test_idx] = T[test_idx] - m_model.predict_proba(X[test_idx])[:, 1]\n",
    "    \n",
    "    # ç”¨æ®‹å·®åšå›å½’\n",
    "    tau_hat = (Y_residuals * T_residuals).sum() / (T_residuals ** 2).sum()\n",
    "    return tau_hat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» Part 4: ä»é›¶å®ç° DML\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬**ä»é›¶å®ç°**ä¸€ä¸ªå®Œæ•´çš„ DML ä¼°è®¡å™¨ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_high_dimensional_data(\n",
    "    n: int = 2000,\n",
    "    p: int = 100,\n",
    "    true_ate: float = 2.0,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆé«˜ç»´æ•°æ®ç”¨äºæµ‹è¯• DML\n",
    "    \n",
    "    DGP:\n",
    "        T = sigmoid(X @ beta_t) + noise\n",
    "        Y = true_ate * T + X @ beta_y + noise\n",
    "    \n",
    "    åœºæ™¯ï¼šVIP ä¼šå‘˜æƒç›Šè¯„ä¼°\n",
    "    - X: ç”¨æˆ·ç‰¹å¾ï¼ˆè´­ä¹°å†å²ã€æµè§ˆè¡Œä¸ºã€äººå£ç»Ÿè®¡ç­‰ï¼‰\n",
    "    - T: æ˜¯å¦æˆä¸º VIP\n",
    "    - Y: åç»­æ¶ˆè´¹é‡‘é¢\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç”Ÿæˆåå˜é‡\n",
    "    X = np.random.randn(n, p)\n",
    "    \n",
    "    # ç¨€ç–ç³»æ•°ï¼ˆåªæœ‰å‰ 10 ä¸ªç‰¹å¾æœ‰ç”¨ï¼‰\n",
    "    beta_t = np.zeros(p)\n",
    "    beta_t[:10] = np.array([0.8, 0.6, 0.5, 0.4, 0.3, -0.3, -0.4, 0.2, 0.1, -0.2])\n",
    "    \n",
    "    beta_y = np.zeros(p)\n",
    "    beta_y[:10] = np.array([1.5, 1.2, 0.8, 0.6, 0.4, -0.5, -0.3, 0.2, 0.3, -0.1])\n",
    "    \n",
    "    # å¤„ç†å˜é‡ï¼ˆäºŒå…ƒï¼‰\n",
    "    propensity_logit = X @ beta_t\n",
    "    propensity = 1 / (1 + np.exp(-propensity_logit))\n",
    "    T = np.random.binomial(1, propensity)\n",
    "    \n",
    "    # ç»“æœå˜é‡\n",
    "    Y = true_ate * T + X @ beta_y + np.random.randn(n) * 0.5\n",
    "    \n",
    "    print(f\"ğŸ“Š é«˜ç»´æ•°æ®ç”Ÿæˆå®Œæˆ\")\n",
    "    print(f\"   æ ·æœ¬é‡: n = {n}\")\n",
    "    print(f\"   ç‰¹å¾æ•°: p = {p}\")\n",
    "    print(f\"   çœŸå® ATE: {true_ate}\")\n",
    "    print(f\"   å¤„ç†ç»„æ¯”ä¾‹: {T.mean():.1%}\")\n",
    "    \n",
    "    return X, T, Y, true_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ•°æ®\n",
    "X, T, Y, TRUE_ATE = generate_high_dimensional_data(n=2000, p=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: å®ç° Cross-fitting DML\n",
    "# è¿™æ˜¯æœ¬èŠ‚æœ€é‡è¦çš„ç»ƒä¹ ï¼\n",
    "\n",
    "def double_ml_plr(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    n_folds: int = 5,\n",
    "    outcome_model = None,\n",
    "    propensity_model = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å®ç° Double Machine Learning for Partially Linear Model\n",
    "    \n",
    "    æ ¸å¿ƒæ­¥éª¤ï¼š\n",
    "    1. Cross-fitting: å°†æ•°æ®åˆ†æˆ K æŠ˜\n",
    "    2. å¯¹æ¯ä¸€æŠ˜ï¼Œç”¨å…¶ä»–æŠ˜è®­ç»ƒ g(X) å’Œ m(X)\n",
    "    3. è®¡ç®—æ®‹å·® Y_res = Y - g(X), T_res = T - m(X)\n",
    "    4. ç”¨æ®‹å·®åšå›å½’ä¼°è®¡ tau\n",
    "    5. è®¡ç®—æ ‡å‡†è¯¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: åå˜é‡çŸ©é˜µ (n, p)\n",
    "        T: å¤„ç†å˜é‡ (n,)\n",
    "        Y: ç»“æœå˜é‡ (n,)\n",
    "        n_folds: äº¤å‰æ‹Ÿåˆçš„æŠ˜æ•°\n",
    "        outcome_model: ç»“æœæ¨¡å‹ï¼ˆé»˜è®¤ Ridgeï¼‰\n",
    "        propensity_model: å€¾å‘å¾—åˆ†æ¨¡å‹ï¼ˆé»˜è®¤ LogisticRegressionï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å« tau, se, ci_lower, ci_upper, pvalue çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    \n",
    "    # é»˜è®¤æ¨¡å‹\n",
    "    if outcome_model is None:\n",
    "        outcome_model = Ridge(alpha=1.0)\n",
    "    if propensity_model is None:\n",
    "        propensity_model = LogisticRegression(max_iter=1000, C=0.1)\n",
    "    \n",
    "    # å­˜å‚¨æ®‹å·®\n",
    "    Y_residuals = np.zeros(n)\n",
    "    T_residuals = np.zeros(n)\n",
    "    \n",
    "    # TODO: å®ç° Cross-fitting\n",
    "    # æç¤ºï¼šä½¿ç”¨ KFold å°†æ•°æ®åˆ†æˆ n_folds æŠ˜\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # === ä½ çš„ä»£ç å¼€å§‹ ===\n",
    "        \n",
    "        # Step 1: åˆ†å‰²æ•°æ®\n",
    "        X_train, X_test = None, None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "        T_train, T_test = None, None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "        Y_train, Y_test = None, None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "        \n",
    "        # Step 2: è®­ç»ƒç»“æœæ¨¡å‹ g(X) å¹¶é¢„æµ‹\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œé¢„æµ‹çš„æ˜¯ E[Y|X]ï¼Œä¸æ˜¯ E[Y|X,T]\n",
    "        # æ‰€ä»¥ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼ˆä¸åŒºåˆ†å¤„ç†ç»„ï¼‰\n",
    "        g_model = Ridge(alpha=1.0)\n",
    "        # ğŸ‘ˆ ä½ çš„ä»£ç : g_model.fit(X_train, Y_train)\n",
    "        g_pred = None  # ğŸ‘ˆ ä½ çš„ä»£ç : g_model.predict(X_test)\n",
    "        \n",
    "        # Step 3: è®­ç»ƒå€¾å‘å¾—åˆ†æ¨¡å‹ m(X) å¹¶é¢„æµ‹\n",
    "        m_model = LogisticRegression(max_iter=1000, C=0.1)\n",
    "        # ğŸ‘ˆ ä½ çš„ä»£ç : m_model.fit(X_train, T_train)\n",
    "        m_pred = None  # ğŸ‘ˆ ä½ çš„ä»£ç : m_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Step 4: è®¡ç®—æ®‹å·®\n",
    "        Y_residuals[test_idx] = None  # ğŸ‘ˆ ä½ çš„ä»£ç : Y_test - g_pred\n",
    "        T_residuals[test_idx] = None  # ğŸ‘ˆ ä½ çš„ä»£ç : T_test - m_pred\n",
    "        \n",
    "        # === ä½ çš„ä»£ç ç»“æŸ ===\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å®Œæˆäº† TODO\n",
    "    if Y_residuals.sum() == 0 or T_residuals.sum() == 0:\n",
    "        print(\"âŒ è¯·å®Œæˆä¸Šé¢çš„ TODOï¼\")\n",
    "        return None\n",
    "    \n",
    "    # Step 5: ç”¨æ®‹å·®åšå›å½’ä¼°è®¡ tau\n",
    "    # tau = Cov(Y_res, T_res) / Var(T_res)\n",
    "    #     = sum(Y_res * T_res) / sum(T_res^2)\n",
    "    tau_hat = (Y_residuals * T_residuals).sum() / (T_residuals ** 2).sum()\n",
    "    \n",
    "    # Step 6: è®¡ç®—æ ‡å‡†è¯¯\n",
    "    # åŸºäº influence function çš„æ ‡å‡†è¯¯\n",
    "    psi = (Y_residuals - tau_hat * T_residuals) * T_residuals\n",
    "    J = (T_residuals ** 2).mean()\n",
    "    var_tau = (psi ** 2).mean() / (n * J ** 2)\n",
    "    se = np.sqrt(var_tau)\n",
    "    \n",
    "    # ç½®ä¿¡åŒºé—´å’Œ p-value\n",
    "    ci_lower = tau_hat - 1.96 * se\n",
    "    ci_upper = tau_hat + 1.96 * se\n",
    "    z_stat = tau_hat / se\n",
    "    pvalue = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    return {\n",
    "        'tau': tau_hat,\n",
    "        'se': se,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'pvalue': pvalue,\n",
    "        'Y_residuals': Y_residuals,\n",
    "        'T_residuals': T_residuals\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<details>\n<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n```python\n# Step 1: åˆ†å‰²æ•°æ®\nX_train, X_test = X[train_idx], X[test_idx]\nT_train, T_test = T[train_idx], T[test_idx]\nY_train, Y_test = Y[train_idx], Y[test_idx]\n\n# Step 2: è®­ç»ƒç»“æœæ¨¡å‹ g(X) å¹¶é¢„æµ‹\ng_model.fit(X_train, Y_train)\ng_pred = g_model.predict(X_test)\n\n# Step 3: è®­ç»ƒå€¾å‘å¾—åˆ†æ¨¡å‹ m(X) å¹¶é¢„æµ‹\nm_model.fit(X_train, T_train)\nm_pred = m_model.predict_proba(X_test)[:, 1]\n\n# Step 4: è®¡ç®—æ®‹å·®\nY_residuals[test_idx] = Y_test - g_pred\nT_residuals[test_idx] = T_test - m_pred\n```\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´å®ç°ï¼ˆç”¨äºåç»­æµ‹è¯•ï¼‰\n",
    "def double_ml_plr_complete(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    n_folds: int = 5,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ DML å®ç°\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    Y_residuals = np.zeros(n)\n",
    "    T_residuals = np.zeros(n)\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        T_train, T_test = T[train_idx], T[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        # è®­ç»ƒç»“æœæ¨¡å‹\n",
    "        g_model = Ridge(alpha=1.0)\n",
    "        g_model.fit(X_train, Y_train)\n",
    "        g_pred = g_model.predict(X_test)\n",
    "        \n",
    "        # è®­ç»ƒå€¾å‘å¾—åˆ†æ¨¡å‹\n",
    "        m_model = LogisticRegression(max_iter=1000, C=0.1)\n",
    "        m_model.fit(X_train, T_train)\n",
    "        m_pred = m_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # è®¡ç®—æ®‹å·®\n",
    "        Y_residuals[test_idx] = Y_test - g_pred\n",
    "        T_residuals[test_idx] = T_test - m_pred\n",
    "    \n",
    "    # ä¼°è®¡ tau\n",
    "    tau_hat = (Y_residuals * T_residuals).sum() / (T_residuals ** 2).sum()\n",
    "    \n",
    "    # æ ‡å‡†è¯¯\n",
    "    psi = (Y_residuals - tau_hat * T_residuals) * T_residuals\n",
    "    J = (T_residuals ** 2).mean()\n",
    "    var_tau = (psi ** 2).mean() / (n * J ** 2)\n",
    "    se = np.sqrt(var_tau)\n",
    "    \n",
    "    ci_lower = tau_hat - 1.96 * se\n",
    "    ci_upper = tau_hat + 1.96 * se\n",
    "    z_stat = tau_hat / se\n",
    "    pvalue = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    return {\n",
    "        'tau': tau_hat,\n",
    "        'se': se,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'pvalue': pvalue,\n",
    "        'Y_residuals': Y_residuals,\n",
    "        'T_residuals': T_residuals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œ DML\n",
    "dml_result = double_ml_plr_complete(X, T, Y, n_folds=5)\n",
    "\n",
    "print(\"ğŸ”¬ Double ML ä¼°è®¡ç»“æœ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"çœŸå® ATE: {TRUE_ATE}\")\n",
    "print(f\"DML ATE:  {dml_result['tau']:.4f} Â± {dml_result['se']:.4f}\")\n",
    "print(f\"95% CI:   [{dml_result['ci_lower']:.4f}, {dml_result['ci_upper']:.4f}]\")\n",
    "print(f\"P-value:  {dml_result['pvalue']:.4e}\")\n",
    "print(f\"åå·®:     {dml_result['tau'] - TRUE_ATE:+.4f}\")\n",
    "print()\n",
    "print(f\"âœ… çœŸå®å€¼ {TRUE_ATE} {'åœ¨' if dml_result['ci_lower'] <= TRUE_ATE <= dml_result['ci_upper'] else 'ä¸åœ¨'} ç½®ä¿¡åŒºé—´å†…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Part 5: æ®‹å·®å¯è§†åŒ–\n",
    "\n",
    "è®©æˆ‘ä»¬å¯è§†åŒ– DML çš„æ ¸å¿ƒâ€”â€”æ®‹å·®åŒ–è¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ®‹å·®\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Y æ®‹å·®åˆ†å¸ƒ\n",
    "axes[0].hist(dml_result['Y_residuals'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Y æ®‹å·® (Y - g(X))', fontsize=12)\n",
    "axes[0].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[0].set_title('ç»“æœæ®‹å·®åˆ†å¸ƒ', fontsize=14)\n",
    "\n",
    "# 2. T æ®‹å·®åˆ†å¸ƒ\n",
    "axes[1].hist(dml_result['T_residuals'], bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('T æ®‹å·® (T - m(X))', fontsize=12)\n",
    "axes[1].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[1].set_title('å¤„ç†æ®‹å·®åˆ†å¸ƒ', fontsize=14)\n",
    "\n",
    "# 3. æ®‹å·®æ•£ç‚¹å›¾ + å›å½’çº¿\n",
    "axes[2].scatter(dml_result['T_residuals'], dml_result['Y_residuals'], \n",
    "               alpha=0.3, s=10, c='gray')\n",
    "\n",
    "# æ‹Ÿåˆçº¿\n",
    "T_res = dml_result['T_residuals']\n",
    "Y_res = dml_result['Y_residuals']\n",
    "x_line = np.linspace(T_res.min(), T_res.max(), 100)\n",
    "y_line = dml_result['tau'] * x_line\n",
    "axes[2].plot(x_line, y_line, 'r-', linewidth=2, label=f'Ï„ = {dml_result[\"tau\"]:.3f}')\n",
    "axes[2].plot(x_line, TRUE_ATE * x_line, 'g--', linewidth=2, label=f'çœŸå® Ï„ = {TRUE_ATE}')\n",
    "\n",
    "axes[2].set_xlabel('T æ®‹å·®', fontsize=12)\n",
    "axes[2].set_ylabel('Y æ®‹å·®', fontsize=12)\n",
    "axes[2].set_title('æ®‹å·®å›å½’ï¼šDML çš„æ ¸å¿ƒ', fontsize=14)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ è§£è¯»:\")\n",
    "print(\"   - å·¦å›¾: Y æ®‹å·®åº”è¯¥å‡å€¼æ¥è¿‘ 0\")\n",
    "print(\"   - ä¸­å›¾: T æ®‹å·®ä¹Ÿåº”è¯¥å‡å€¼æ¥è¿‘ 0\")\n",
    "print(\"   - å³å›¾: æ–œç‡å°±æ˜¯ DML ä¼°è®¡çš„å› æœæ•ˆåº” Ï„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Part 6: å¯¹æ¯”æœ‰æ—  Cross-fitting\n",
    "\n",
    "è®©æˆ‘ä»¬éªŒè¯ Cross-fitting çš„é‡è¦æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml_without_crossfitting(X, T, Y):\n",
    "    \"\"\"\n",
    "    æ²¡æœ‰ Cross-fitting çš„ DMLï¼ˆé”™è¯¯åšæ³•ï¼ï¼‰\n",
    "    ç”¨åŒä¸€ä»½æ•°æ®è®­ç»ƒå’Œé¢„æµ‹\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    \n",
    "    # ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ\n",
    "    g_model = Ridge(alpha=1.0)\n",
    "    g_model.fit(X, Y)\n",
    "    g_pred = g_model.predict(X)  # ç”¨åŒä¸€ä»½æ•°æ®é¢„æµ‹ï¼\n",
    "    \n",
    "    m_model = LogisticRegression(max_iter=1000, C=0.1)\n",
    "    m_model.fit(X, T)\n",
    "    m_pred = m_model.predict_proba(X)[:, 1]  # ç”¨åŒä¸€ä»½æ•°æ®é¢„æµ‹ï¼\n",
    "    \n",
    "    # è®¡ç®—æ®‹å·®\n",
    "    Y_residuals = Y - g_pred\n",
    "    T_residuals = T - m_pred\n",
    "    \n",
    "    # ä¼°è®¡ tau\n",
    "    tau_hat = (Y_residuals * T_residuals).sum() / (T_residuals ** 2).sum()\n",
    "    \n",
    "    # æ ‡å‡†è¯¯ï¼ˆä¼šè¢«ä½ä¼°ï¼ï¼‰\n",
    "    psi = (Y_residuals - tau_hat * T_residuals) * T_residuals\n",
    "    J = (T_residuals ** 2).mean()\n",
    "    var_tau = (psi ** 2).mean() / (n * J ** 2)\n",
    "    se = np.sqrt(var_tau)\n",
    "    \n",
    "    return {'tau': tau_hat, 'se': se}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šæ¬¡æ¨¡æ‹Ÿå¯¹æ¯”\n",
    "n_simulations = 50\n",
    "results_with_cf = []\n",
    "results_without_cf = []\n",
    "\n",
    "print(\"è¿è¡Œæ¨¡æ‹Ÿå®éªŒ...\")\n",
    "for seed in range(n_simulations):\n",
    "    X_sim, T_sim, Y_sim, _ = generate_high_dimensional_data(n=1000, p=50, seed=seed)\n",
    "    \n",
    "    # æœ‰ Cross-fitting\n",
    "    result_cf = double_ml_plr_complete(X_sim, T_sim, Y_sim, n_folds=5)\n",
    "    results_with_cf.append(result_cf['tau'])\n",
    "    \n",
    "    # æ—  Cross-fitting\n",
    "    result_no_cf = dml_without_crossfitting(X_sim, T_sim, Y_sim)\n",
    "    results_without_cf.append(result_no_cf['tau'])\n",
    "\n",
    "print(\"æ¨¡æ‹Ÿå®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ç›´æ–¹å›¾å¯¹æ¯”\n",
    "axes[0].hist(results_with_cf, bins=20, alpha=0.7, label='æœ‰ Cross-fitting', color='green', edgecolor='black')\n",
    "axes[0].hist(results_without_cf, bins=20, alpha=0.7, label='æ—  Cross-fitting', color='red', edgecolor='black')\n",
    "axes[0].axvline(TRUE_ATE, color='black', linestyle='--', linewidth=2, label=f'çœŸå® ATE = {TRUE_ATE}')\n",
    "axes[0].set_xlabel('ATE ä¼°è®¡', fontsize=12)\n",
    "axes[0].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[0].set_title('Cross-fitting çš„å½±å“', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# ç®±çº¿å›¾å¯¹æ¯”\n",
    "data = [results_with_cf, results_without_cf]\n",
    "bp = axes[1].boxplot(data, labels=['æœ‰ CF', 'æ—  CF'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightgreen')\n",
    "bp['boxes'][1].set_facecolor('lightcoral')\n",
    "axes[1].axhline(TRUE_ATE, color='black', linestyle='--', linewidth=2, label=f'çœŸå® ATE = {TRUE_ATE}')\n",
    "axes[1].set_ylabel('ATE ä¼°è®¡', fontsize=12)\n",
    "axes[1].set_title('ä¼°è®¡åˆ†å¸ƒå¯¹æ¯”', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ç»Ÿè®¡å¯¹æ¯”\n",
    "print(\"\\nğŸ“Š ç»Ÿè®¡å¯¹æ¯”\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"çœŸå® ATE: {TRUE_ATE}\")\n",
    "print(f\"\\næœ‰ Cross-fitting:\")\n",
    "print(f\"   å‡å€¼: {np.mean(results_with_cf):.4f}\")\n",
    "print(f\"   åå·®: {np.mean(results_with_cf) - TRUE_ATE:+.4f}\")\n",
    "print(f\"   æ ‡å‡†å·®: {np.std(results_with_cf):.4f}\")\n",
    "print(f\"\\næ—  Cross-fitting:\")\n",
    "print(f\"   å‡å€¼: {np.mean(results_without_cf):.4f}\")\n",
    "print(f\"   åå·®: {np.mean(results_without_cf) - TRUE_ATE:+.4f}\")\n",
    "print(f\"   æ ‡å‡†å·®: {np.std(results_without_cf):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ç»“è®º: Cross-fitting èƒ½æ˜¾è‘—å‡å°‘åå·®ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ† Part 7: æ–¹æ³•å¤§æ¯”æ‹¼\n",
    "\n",
    "è®©æˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒå„ç§æ–¹æ³•åœ¨é«˜ç»´æ•°æ®ä¸‹çš„è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_methods(X, T, Y, true_ate):\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”æ‰€æœ‰å› æœæ¨æ–­æ–¹æ³•\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n = len(Y)\n",
    "    \n",
    "    # 1. æœ´ç´ ä¼°è®¡\n",
    "    naive_ate = Y[T==1].mean() - Y[T==0].mean()\n",
    "    results.append({\n",
    "        'æ–¹æ³•': 'æœ´ç´ ä¼°è®¡',\n",
    "        'ATE': naive_ate,\n",
    "        'åå·®': naive_ate - true_ate,\n",
    "        'è¯´æ˜': 'ç›´æ¥å¯¹æ¯”å‡å€¼'\n",
    "    })\n",
    "    \n",
    "    # 2. OLSï¼ˆå¸¦ Tï¼‰\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    X_with_T = np.column_stack([T, X])\n",
    "    ols = LinearRegression()\n",
    "    ols.fit(X_with_T, Y)\n",
    "    ols_ate = ols.coef_[0]\n",
    "    results.append({\n",
    "        'æ–¹æ³•': 'OLS',\n",
    "        'ATE': ols_ate,\n",
    "        'åå·®': ols_ate - true_ate,\n",
    "        'è¯´æ˜': 'é«˜ç»´ä¸‹ä¸ç¨³å®š'\n",
    "    })\n",
    "    \n",
    "    # 3. Lassoï¼ˆæœ‰æ­£åˆ™åŒ–åå·®ï¼‰\n",
    "    lasso = LassoCV(cv=5, max_iter=10000)\n",
    "    lasso.fit(X_with_T, Y)\n",
    "    lasso_ate = lasso.coef_[0]\n",
    "    results.append({\n",
    "        'æ–¹æ³•': 'Lasso',\n",
    "        'ATE': lasso_ate,\n",
    "        'åå·®': lasso_ate - true_ate,\n",
    "        'è¯´æ˜': 'æ­£åˆ™åŒ–åå·®'\n",
    "    })\n",
    "    \n",
    "    # 4. IPW\n",
    "    lr = LogisticRegression(max_iter=1000, C=0.1)\n",
    "    lr.fit(X, T)\n",
    "    propensity = lr.predict_proba(X)[:, 1]\n",
    "    propensity = np.clip(propensity, 0.01, 0.99)\n",
    "    \n",
    "    weights = T / propensity + (1 - T) / (1 - propensity)\n",
    "    y1_ipw = (Y * weights * T).sum() / (weights * T).sum()\n",
    "    y0_ipw = (Y * weights * (1 - T)).sum() / (weights * (1 - T)).sum()\n",
    "    ipw_ate = y1_ipw - y0_ipw\n",
    "    results.append({\n",
    "        'æ–¹æ³•': 'IPW',\n",
    "        'ATE': ipw_ate,\n",
    "        'åå·®': ipw_ate - true_ate,\n",
    "        'è¯´æ˜': 'ä¾èµ–å€¾å‘å¾—åˆ†'\n",
    "    })\n",
    "    \n",
    "    # 5. AIPWï¼ˆæ—  Cross-fittingï¼‰\n",
    "    g1 = Ridge(alpha=1.0)\n",
    "    g0 = Ridge(alpha=1.0)\n",
    "    g1.fit(X[T==1], Y[T==1])\n",
    "    g0.fit(X[T==0], Y[T==0])\n",
    "    mu1 = g1.predict(X)\n",
    "    mu0 = g0.predict(X)\n",
    "    \n",
    "    aipw_scores = (mu1 - mu0) + T * (Y - mu1) / propensity - (1 - T) * (Y - mu0) / (1 - propensity)\n",
    "    aipw_ate = aipw_scores.mean()\n",
    "    results.append({\n",
    "        'æ–¹æ³•': 'AIPW (æ—  CF)',\n",
    "        'ATE': aipw_ate,\n",
    "        'åå·®': aipw_ate - true_ate,\n",
    "        'è¯´æ˜': 'åŒé‡ç¨³å¥ä½†æ— äº¤å‰æ‹Ÿåˆ'\n",
    "    })\n",
    "    \n",
    "    # 6. DMLï¼ˆæœ‰ Cross-fittingï¼‰\n",
    "    dml_result = double_ml_plr_complete(X, T, Y, n_folds=5)\n",
    "    results.append({\n",
    "        'æ–¹æ³•': 'DML (æœ‰ CF)',\n",
    "        'ATE': dml_result['tau'],\n",
    "        'åå·®': dml_result['tau'] - true_ate,\n",
    "        'è¯´æ˜': 'âœ… æ¨è'\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œå¯¹æ¯”\n",
    "comparison_df = compare_all_methods(X, T, Y, TRUE_ATE)\n",
    "\n",
    "print(\"ğŸ† æ–¹æ³•å¤§æ¯”æ‹¼\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"çœŸå® ATE = {TRUE_ATE}\")\n",
    "print(f\"æ•°æ®ç»´åº¦: n={len(Y)}, p={X.shape[1]}\")\n",
    "print()\n",
    "display(comparison_df)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['gray', 'gray', 'red', 'blue', 'orange', 'green']\n",
    "bars = ax.barh(comparison_df['æ–¹æ³•'], comparison_df['ATE'], color=colors, alpha=0.7)\n",
    "ax.axvline(TRUE_ATE, color='black', linestyle='--', linewidth=2, label=f'çœŸå® ATE = {TRUE_ATE}')\n",
    "ax.set_xlabel('ATE ä¼°è®¡', fontsize=12)\n",
    "ax.set_title('å„æ–¹æ³• ATE ä¼°è®¡å¯¹æ¯”', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# æ ‡æ³¨åå·®\n",
    "for bar, bias in zip(bars, comparison_df['åå·®']):\n",
    "    ax.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "           f'åå·®: {bias:+.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Part 8: ä½¿ç”¨æ›´å¼ºçš„ ML æ¨¡å‹\n",
    "\n",
    "DML çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å¯ä»¥ä½¿ç”¨ä»»æ„ ML æ¨¡å‹ï¼è®©æˆ‘ä»¬è¯•è¯•éšæœºæ£®æ—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ml_with_flexible_models(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    n_folds: int = 5,\n",
    "    outcome_model_class = RandomForestRegressor,\n",
    "    propensity_model_class = RandomForestClassifier,\n",
    "    outcome_params: dict = None,\n",
    "    propensity_params: dict = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ä»»æ„ ML æ¨¡å‹çš„ DML\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    Y_residuals = np.zeros(n)\n",
    "    T_residuals = np.zeros(n)\n",
    "    \n",
    "    outcome_params = outcome_params or {'n_estimators': 100, 'max_depth': 5, 'random_state': 42}\n",
    "    propensity_params = propensity_params or {'n_estimators': 100, 'max_depth': 5, 'random_state': 42}\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        T_train, T_test = T[train_idx], T[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        # ç»“æœæ¨¡å‹\n",
    "        g_model = outcome_model_class(**outcome_params)\n",
    "        g_model.fit(X_train, Y_train)\n",
    "        g_pred = g_model.predict(X_test)\n",
    "        \n",
    "        # å€¾å‘å¾—åˆ†æ¨¡å‹\n",
    "        m_model = propensity_model_class(**propensity_params)\n",
    "        m_model.fit(X_train, T_train)\n",
    "        m_pred = m_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        Y_residuals[test_idx] = Y_test - g_pred\n",
    "        T_residuals[test_idx] = T_test - m_pred\n",
    "    \n",
    "    tau_hat = (Y_residuals * T_residuals).sum() / (T_residuals ** 2).sum()\n",
    "    \n",
    "    psi = (Y_residuals - tau_hat * T_residuals) * T_residuals\n",
    "    J = (T_residuals ** 2).mean()\n",
    "    var_tau = (psi ** 2).mean() / (n * J ** 2)\n",
    "    se = np.sqrt(var_tau)\n",
    "    \n",
    "    return {\n",
    "        'tau': tau_hat,\n",
    "        'se': se,\n",
    "        'ci_lower': tau_hat - 1.96 * se,\n",
    "        'ci_upper': tau_hat + 1.96 * se\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ä¸åŒ ML æ¨¡å‹\n",
    "print(\"ğŸ¯ DML + ä¸åŒ ML æ¨¡å‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ridge (çº¿æ€§)\n",
    "dml_ridge = double_ml_plr_complete(X, T, Y)\n",
    "print(f\"Ridge:          Ï„ = {dml_ridge['tau']:.4f} Â± {dml_ridge['se']:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "dml_rf = double_ml_with_flexible_models(\n",
    "    X, T, Y,\n",
    "    outcome_model_class=RandomForestRegressor,\n",
    "    propensity_model_class=RandomForestClassifier\n",
    ")\n",
    "print(f\"Random Forest:  Ï„ = {dml_rf['tau']:.4f} Â± {dml_rf['se']:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "dml_gb = double_ml_with_flexible_models(\n",
    "    X, T, Y,\n",
    "    outcome_model_class=GradientBoostingRegressor,\n",
    "    propensity_model_class=RandomForestClassifier,\n",
    "    outcome_params={'n_estimators': 100, 'max_depth': 3, 'random_state': 42}\n",
    ")\n",
    "print(f\"Gradient Boost: Ï„ = {dml_gb['tau']:.4f} Â± {dml_gb['se']:.4f}\")\n",
    "\n",
    "print(f\"\\nçœŸå® ATE: {TRUE_ATE}\")\n",
    "print(\"\\nğŸ’¡ è§‚å¯Ÿ: ä¸åŒ ML æ¨¡å‹ç»™å‡ºç›¸ä¼¼çš„ç»“æœï¼Œè¯´æ˜ DML å¾ˆç¨³å¥ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ Part 9: æ€è€ƒé¢˜\n",
    "\n",
    "### é—®é¢˜ 1: DML å’Œæ™®é€š AIPW/DR æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "\n",
    "**æç¤º**: æƒ³æƒ³ Cross-fitting çš„ä½œç”¨\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 2: ä¸ºä»€ä¹ˆ DML éœ€è¦å¯¹ Y å’Œ T éƒ½åšæ®‹å·®åŒ–ï¼ˆPartialling Outï¼‰ï¼Ÿ\n",
    "\n",
    "**æç¤º**: æƒ³æƒ³å¦‚æœåªå¯¹ Y åšæ®‹å·®åŒ–ä¼šæ€æ ·\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 3: Cross-fitting çš„æŠ˜æ•° K å¦‚ä½•é€‰æ‹©ï¼ŸK=2 å’Œ K=10 æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "\n",
    "**æç¤º**: æƒ³æƒ³åå·®-æ–¹å·®æƒè¡¡\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 4: DML çš„å…³é”®å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ DML ä¼šå¤±æ•ˆï¼Ÿ\n",
    "\n",
    "**æç¤º**: æƒ³æƒ³ unconfoundednessã€overlapã€model specification\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 5: é¢è¯•é¢˜ï¼šè¯·ç”¨ä¸€åˆ†é’Ÿå‘é¢è¯•å®˜è§£é‡Š DML çš„æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„å›ç­”...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ“ å‚è€ƒç­”æ¡ˆ\n\n<details>\n<summary>ç‚¹å‡»å±•å¼€</summary>\n\n**é—®é¢˜ 1**: DML å’Œ AIPW çš„æ ¸å¿ƒåŒºåˆ«åœ¨äº **Cross-fitting**ã€‚AIPW ç”¨åŒä¸€ä»½æ•°æ®è®­ç»ƒæ¨¡å‹å’Œè®¡ç®—æ•ˆåº”ï¼Œä¼šæœ‰è¿‡æ‹Ÿåˆåå·®ï¼›DML é€šè¿‡ Cross-fitting ç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹éƒ½æ¥è‡ªã€Œæœªè§è¿‡å®ƒã€çš„æ¨¡å‹ï¼Œæ¶ˆé™¤è¿‡æ‹Ÿåˆåå·®ã€‚\n\n**é—®é¢˜ 2**: å¦‚æœåªå¯¹ Y åšæ®‹å·®åŒ–ï¼ˆ$Y - g(X)$ï¼‰ï¼Œé‚£ T å’Œ X çš„ç›¸å…³æ€§ä»ç„¶å­˜åœ¨ï¼ŒX ä½œä¸ºæ··æ·†å˜é‡ä»ä¼šå½±å“æ•ˆåº”ä¼°è®¡ã€‚åªæœ‰åŒæ—¶å¯¹ Y å’Œ T æ®‹å·®åŒ–ï¼Œæ‰èƒ½å½»åº•ã€Œå»é™¤ã€X çš„å½±å“ï¼Œè¿™å°±æ˜¯ Frisch-Waugh-Lovell å®šç†çš„å»¶ä¼¸ã€‚\n\n**é—®é¢˜ 3**: \n- K=2: æ¯æŠ˜åªç”¨ 50% æ•°æ®è®­ç»ƒï¼Œæ¨¡å‹æ–¹å·®å¤§ï¼Œä½†åå·®å°\n- K=10: æ¯æŠ˜ç”¨ 90% æ•°æ®è®­ç»ƒï¼Œæ¨¡å‹æ›´ç¨³å®šï¼Œä½†è®¡ç®—é‡å¤§\n- é€šå¸¸ K=5 æ˜¯ä¸€ä¸ªå¥½çš„å¹³è¡¡ç‚¹\n\n**é—®é¢˜ 4**: DML çš„å…³é”®å‡è®¾ï¼š\n1. **Unconfoundedness**: æ— æœªè§‚æµ‹æ··æ·†\n2. **Overlap**: å€¾å‘å¾—åˆ†ä¸èƒ½å¤ªæç«¯\n3. **æ¨¡å‹æ”¶æ•›é€Ÿåº¦**: g(X) å’Œ m(X) çš„ä¼°è®¡è¯¯å·®è¡°å‡è¶³å¤Ÿå¿«ï¼ˆ$o(n^{-1/4})$ï¼‰\n\n**é—®é¢˜ 5**: é¢è¯•å›ç­”æ¨¡æ¿ï¼š\n\nã€ŒDML æ˜¯ä¸€ç§ç”¨æœºå™¨å­¦ä¹ ä¼°è®¡å› æœæ•ˆåº”çš„æ–¹æ³•ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\n1. å…ˆç”¨ ML æ¨¡å‹æŠŠåå˜é‡ X å¯¹ Y å’Œ T çš„å½±å“å»æ‰ï¼Œå¾—åˆ°æ®‹å·®\n2. ç”¨æ®‹å·®åšå›å½’ï¼Œä¼°è®¡ T å¯¹ Y çš„å› æœæ•ˆåº”\n3. é€šè¿‡ Cross-fitting é¿å…è¿‡æ‹Ÿåˆåå·®\n\nè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼šå¯ä»¥ç”¨ä»»æ„ ML æ¨¡å‹å¤„ç†é«˜ç»´æ•°æ®ï¼ŒåŒæ—¶ä¿è¯å› æœæ•ˆåº”ä¼°è®¡çš„æ— åæ€§å’Œæœ‰æ•ˆæ¨æ–­ã€‚ã€\n\n</details>\n\n---\n\n## ğŸ¤ é«˜é¢‘é¢è¯•é¢˜è¯¦è§£\n\n### é¢è¯•é¢˜ 1: DML å’Œæ™®é€š DR/AIPW æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦ Cross-fittingï¼Ÿ\n\n**æ ‡å‡†å›ç­”**:\n\n**æ ¸å¿ƒåŒºåˆ«**: Cross-fitting\n\n**é—®é¢˜æ¥æº**: \nå½“æˆ‘ä»¬ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯çµæ´»çš„éå‚æ•°æ¨¡å‹ï¼‰æ—¶ï¼Œå¦‚æœç”¨**åŒä¸€ä»½æ•°æ®**è®­ç»ƒæ¨¡å‹å’Œé¢„æµ‹ï¼š\n\n$$\\text{æ®‹å·®} = Y_i - \\hat{g}(X_i)$$\n\nå…¶ä¸­ $\\hat{g}$ åœ¨è®­ç»ƒæ—¶\"è§è¿‡\" $Y_i$ï¼Œä¼šå¯¼è‡´ï¼š\n- æ®‹å·®è¢«**ä½ä¼°**ï¼ˆoverfitting biasï¼‰\n- æ ‡å‡†è¯¯å¤±æ•ˆ\n- ç½®ä¿¡åŒºé—´è¿‡çª„\n\n**Cross-fitting çš„è§£å†³æ–¹æ¡ˆ**:\n1. æŠŠæ•°æ®åˆ†æˆ K æŠ˜\n2. å¯¹æ¯ä¸€æŠ˜ï¼Œç”¨å…¶ä»– K-1 æŠ˜è®­ç»ƒæ¨¡å‹\n3. ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹å½“å‰æŠ˜\n\nè¿™æ ·æ¯ä¸ª $Y_i$ çš„é¢„æµ‹éƒ½æ¥è‡ªã€Œæ²¡è§è¿‡å®ƒã€çš„æ¨¡å‹ï¼\n\n**å…³é”®æ´å¯Ÿ**: è¿™ç±»ä¼¼ cross-validationï¼Œä½†ç›®çš„ä¸æ˜¯é€‰æ¨¡å‹ï¼Œè€Œæ˜¯**å»é™¤è¿‡æ‹Ÿåˆåå·®**ã€‚\n\n---\n\n### é¢è¯•é¢˜ 2: ä»€ä¹ˆæ˜¯ Neyman æ­£äº¤æ€§ï¼Ÿä¸ºä»€ä¹ˆå®ƒé‡è¦ï¼Ÿ\n\n**æ ‡å‡†å›ç­”**:\n\n**å®šä¹‰**: Neyman æ­£äº¤æ€§æ˜¯æŒ‡çŸ©å‡½æ•°å¯¹ nuisance å‚æ•°çš„å¯¼æ•°ä¸º 0ï¼š\n\n$$\\frac{\\partial}{\\partial \\eta} E[\\psi(W; \\tau_0, \\eta)] \\Big|_{\\eta=\\eta_0} = 0$$\n\n**é‡è¦æ€§**: \n- åœ¨ç»å…¸æ–¹æ³•ä¸­ï¼Œnuisance å‚æ•°ä¼°è®¡è¯¯å·®å¯¹ç›®æ ‡å‚æ•°ä¼°è®¡çš„å½±å“æ˜¯**ä¸€é˜¶çš„** $O(||\\hat{\\eta} - \\eta||)$\n- åœ¨ Neyman æ­£äº¤çš„æ–¹æ³•ä¸­ï¼Œå½±å“æ˜¯**äºŒé˜¶çš„** $O(||\\hat{\\eta} - \\eta||^2)$\n\n**ç›´è§‚æ¯”å–»**:\n- æ™®é€šæ–¹æ³•ï¼šå°çŸ³å­ç ¸åˆ°è½¦ï¼ˆä¸€é˜¶å½±å“ï¼‰\n- Neyman æ­£äº¤ï¼šå°çŸ³å­è¢«å¼¹å¼€ï¼ˆäºŒé˜¶å½±å“ï¼Œå‡ ä¹æ²¡å½±å“ï¼‰\n\n**å®é™…æ„ä¹‰**:\n- å…è®¸ä½¿ç”¨æ­£åˆ™åŒ– ML æ¨¡å‹ï¼ˆLasso, Ridge, RFï¼‰\n- å³ä½¿æ¨¡å‹æœ‰åå·®ï¼Œå¯¹å› æœæ•ˆåº”ä¼°è®¡å½±å“å¾ˆå°\n- å¯ä»¥è¾¾åˆ° $\\sqrt{n}$-ä¸€è‡´æ€§å’Œæ¸è¿‘æ­£æ€æ€§\n\n---\n\n### é¢è¯•é¢˜ 3: DML çš„å…³é”®å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ä¼šå¤±æ•ˆï¼Ÿ\n\n**æ ‡å‡†å›ç­”**:\n\n**å…³é”®å‡è®¾**:\n\n1. **Unconfoundedness (æ— æ··æ·†)**:\n   $$(Y(0), Y(1)) \\perp T | X$$\n   - å¦‚æœæœ‰é‡è¦çš„æœªè§‚æµ‹æ··æ·†ï¼ŒDML æ— æ³•è§£å†³\n   \n2. **Overlap/Positivity (é‡å )**:\n   $$0 < P(T=1|X) < 1$$\n   - å¦‚æœæŸäº› X å€¼ä¸‹ T å‡ ä¹ç¡®å®šï¼Œæ— æ³•ä¼°è®¡æ•ˆåº”\n   \n3. **æ¨¡å‹æ”¶æ•›é€Ÿåº¦**:\n   $$||\\hat{g} - g|| = o_p(n^{-1/4}), \\quad ||\\hat{m} - m|| = o_p(n^{-1/4})$$\n   - Nuisance æ¨¡å‹ä¸èƒ½å¤ªå·®\n\n**å¤±æ•ˆåœºæ™¯**:\n\n| åœºæ™¯ | åæœ | è§£å†³æ–¹æ¡ˆ |\n|------|------|----------|\n| æœ‰æœªè§‚æµ‹æ··æ·†å˜é‡ | ä¼°è®¡æœ‰å | å·¥å…·å˜é‡ã€DIDã€RDD |\n| å€¾å‘å¾—åˆ†æ¥è¿‘ 0 æˆ– 1 | ä¼°è®¡ä¸ç¨³å®š | ä¿®å‰ªæ ·æœ¬ã€é™åˆ¶æ¨æ–­èŒƒå›´ |\n| æ ·æœ¬é‡å¤ªå°ï¼ˆn < 500ï¼‰ | æ–¹å·®å¤§ã€æ¨æ–­ä¸å‡† | ç”¨å‚æ•°æ–¹æ³•æˆ–æ”¶é›†æ›´å¤šæ•°æ® |\n| æ¨¡å‹æ”¶æ•›å¤ªæ…¢ | æ¸è¿‘æ­£æ€æ€§ä¸æˆç«‹ | ç”¨æ›´çµæ´»çš„æ¨¡å‹æˆ–ç‰¹å¾å·¥ç¨‹ |\n\n---\n\n### é¢è¯•é¢˜ 4: Cross-fitting çš„æŠ˜æ•° K å¦‚ä½•é€‰æ‹©ï¼Ÿ\n\n**æ ‡å‡†å›ç­”**:\n\n**æƒè¡¡**:\n\n| Kå€¼ | è®­ç»ƒé›†å¤§å° | æ¨¡å‹è´¨é‡ | è®¡ç®—æˆæœ¬ | æ¨èåœºæ™¯ |\n|-----|-----------|---------|---------|----------|\n| K=2 | 50% | æ–¹å·®å¤§ | ä½ | å¿«é€ŸåŸå‹ |\n| K=5 | 80% | âœ… å¹³è¡¡ | ä¸­ | **æ¨è** |\n| K=10 | 90% | æ–¹å·®å° | é«˜ | å¤§æ•°æ®é›† |\n| K=n (LOOCV) | ~100% | æœ€å°æ–¹å·® | å¾ˆé«˜ | å°æ•°æ®ï¼ˆn<500ï¼‰ |\n\n**ç†è®ºè€ƒè™‘**:\n- K å¤ªå°ï¼šæ¯æŠ˜è®­ç»ƒé›†å°ï¼Œ$\\hat{g}$, $\\hat{m}$ ä¼°è®¡ä¸å‡†\n- K å¤ªå¤§ï¼šè®¡ç®—é‡å¤§ï¼Œäº¤å‰éªŒè¯æ–¹å·®å¢åŠ \n\n**å®è·µå»ºè®®**:\n- **é»˜è®¤ K=5**: åœ¨ç†è®ºæ€§è´¨å’Œè®¡ç®—æ•ˆç‡é—´çš„æœ€ä½³å¹³è¡¡\n- n < 1000 æ—¶å¯ä»¥ç”¨ K=10\n- n > 10000 æ—¶ K=2 æˆ– K=3 å°±å¤Ÿäº†\n\n**é¢è¯•åŠ åˆ†ç‚¹**: æåˆ° repeated cross-fittingï¼ˆé‡å¤å¤šæ¬¡ cross-fitting å–å¹³å‡ï¼‰å¯ä»¥è¿›ä¸€æ­¥é™ä½æ–¹å·®ã€‚\n\n---\n\n### é¢è¯•é¢˜ 5: ä¸ºä»€ä¹ˆ DML éœ€è¦å¯¹ Y å’Œ T éƒ½åšæ®‹å·®åŒ–ï¼Ÿ\n\n**æ ‡å‡†å›ç­”**:\n\nè¿™æ¶‰åŠ **Frisch-Waugh-Lovell (FWL) å®šç†**ï¼\n\n**FWL å®šç†**: åœ¨å¤šå…ƒå›å½’ä¸­\n$$Y = \\tau T + X'\\beta + \\epsilon$$\n\n$\\tau$ çš„ OLS ä¼°è®¡ç­‰ä»·äºï¼š\n1. å¯¹ Y å’Œ T åˆ†åˆ«å¯¹ X å›å½’ï¼Œå¾—åˆ°æ®‹å·® $\\tilde{Y}$, $\\tilde{T}$\n2. ç”¨ $\\tilde{Y}$ å¯¹ $\\tilde{T}$ åšç®€å•å›å½’\n\n**ä¸ºä»€ä¹ˆä¸¤ä¸ªéƒ½è¦æ®‹å·®åŒ–ï¼Ÿ**\n\n- **åªæ®‹å·®åŒ– Y**: $\\tilde{Y} = Y - g(X)$\n  - ä½† T å’Œ X ä»ç›¸å…³ï¼ŒX ä»æ˜¯æ··æ·†å˜é‡ï¼\n  - ç›¸å½“äºåªåšäº†ä¸€åŠçš„æ§åˆ¶\n\n- **åªæ®‹å·®åŒ– T**: $\\tilde{T} = T - m(X)$  \n  - Y ä¸­ä»åŒ…å« X çš„å½±å“\n  - æ— æ³•åˆ†ç¦» T çš„çº¯æ•ˆåº”\n\n- **ä¸¤ä¸ªéƒ½æ®‹å·®åŒ–**:\n  - $\\tilde{Y}$ å»é™¤äº† X å¯¹ç»“æœçš„ç›´æ¥å½±å“\n  - $\\tilde{T}$ å»é™¤äº† X å¯¹å¤„ç†çš„å½±å“  \n  - æ®‹å·®é—´çš„å…³ç³»å°±æ˜¯ T å¯¹ Y çš„å› æœæ•ˆåº”ï¼\n\n**ç›´è§‚æ¯”å–»**: \nè¦æµ‹é‡ã€Œå’–å•¡ã€å¯¹ã€Œå·¥ä½œæ•ˆç‡ã€çš„å½±å“ï¼š\n- åªæ§åˆ¶ Yï¼šå»é™¤ç¡çœ å¯¹æ•ˆç‡çš„å½±å“ï¼Œä½†æ²¡æ§åˆ¶ç¡çœ å¯¹å–å’–å•¡çš„å½±å“\n- åªæ§åˆ¶ Tï¼šå»é™¤ç¡çœ å¯¹å–å’–å•¡çš„å½±å“ï¼Œä½†æ²¡æ§åˆ¶ç¡çœ å¯¹æ•ˆç‡çš„ç›´æ¥å½±å“\n- ä¸¤ä¸ªéƒ½æ§åˆ¶ï¼šå®Œå…¨åˆ†ç¦»å‡ºå’–å•¡çš„çº¯æ•ˆåº”ï¼\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "**DML for Partially Linear Model:**\n",
    "\n",
    "$$\\hat{\\tau}_{DML} = \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{g}_{-k(i)}(X_i))(T_i - \\hat{m}_{-k(i)}(X_i))}{\\sum_{i=1}^{n}(T_i - \\hat{m}_{-k(i)}(X_i))^2}$$\n",
    "\n",
    "å…¶ä¸­ $\\hat{g}_{-k(i)}$ è¡¨ç¤ºåœ¨ä¸åŒ…å«ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æŠ˜ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚\n",
    "\n",
    "### å…³é”®æ¦‚å¿µ\n",
    "\n",
    "| æ¦‚å¿µ | å«ä¹‰ | ä¸ºä»€ä¹ˆé‡è¦ |\n",
    "|------|------|------------|\n",
    "| æ­£åˆ™åŒ–åå·® | ML æ¨¡å‹çš„æ­£åˆ™åŒ–ä¼šå‹ç¼©å› æœæ•ˆåº” | é«˜ç»´æ•°æ®ä¸‹çš„æ ¸å¿ƒé—®é¢˜ |\n",
    "| Neyman æ­£äº¤ | æ„é€ å¯¹ nuisance å‚æ•°ä¸æ•æ„Ÿçš„ä¼°è®¡ | DML çš„ç†è®ºåŸºç¡€ |\n",
    "| Cross-fitting | ç”¨ä¸åŒæŠ˜çš„æ•°æ®è®­ç»ƒå’Œé¢„æµ‹ | æ¶ˆé™¤è¿‡æ‹Ÿåˆåå·® |\n",
    "| Double Residualization | å¯¹ Y å’Œ T éƒ½åšæ®‹å·®åŒ– | å»é™¤ X çš„å½±å“ |\n",
    "\n",
    "### DML vs å…¶ä»–æ–¹æ³•\n",
    "\n",
    "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| OLS | ç®€å• | é«˜ç»´ä¸‹ä¸ç¨³å®š | ä½ç»´ |\n",
    "| Lasso | é«˜ç»´å¯ç”¨ | æ­£åˆ™åŒ–åå·® | çº¯é¢„æµ‹ |\n",
    "| IPW | æ— éœ€ç»“æœæ¨¡å‹ | æƒé‡æç«¯ | ä¸­ç­‰ç»´åº¦ |\n",
    "| AIPW | åŒé‡ç¨³å¥ | è¿‡æ‹Ÿåˆåå·® | ä¸­ç­‰ç»´åº¦ |\n",
    "| **DML** | é«˜ç»´ + æ— å + æœ‰æ•ˆæ¨æ–­ | è®¡ç®—é‡å¤§ | **é«˜ç»´å› æœæ¨æ–­** |\n",
    "\n",
    "### é¢è¯•è¦ç‚¹\n",
    "\n",
    "1. DML è§£å†³**é«˜ç»´æ•°æ®**ä¸‹çš„å› æœæ¨æ–­é—®é¢˜\n",
    "2. æ ¸å¿ƒæ˜¯ **Cross-fitting** + **åŒé‡æ®‹å·®åŒ–**\n",
    "3. å¯ä»¥ä½¿ç”¨**ä»»æ„ ML æ¨¡å‹**ä½œä¸º nuisance ä¼°è®¡å™¨\n",
    "4. ä¿è¯æ¸è¿‘æ­£æ€æ€§ï¼Œå¯ä»¥åš**æœ‰æ•ˆç»Ÿè®¡æ¨æ–­**\n",
    "\n",
    "---\n",
    "\n",
    "**ã€ŒDML å°±åƒåšé¥­å‰å…ˆæŠŠé£Ÿææ´—å¹²å‡€ï¼ˆå»é™¤ X çš„å½±å“ï¼‰ï¼Œå†å¼€å§‹çƒ¹é¥ªï¼ˆä¼°è®¡å› æœæ•ˆåº”ï¼‰ã€‚ã€**\n",
    "\n",
    "ğŸ‰ æ­å–œå®Œæˆ Double ML æ·±åº¦å‰–æï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}