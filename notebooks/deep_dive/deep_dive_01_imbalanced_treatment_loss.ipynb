{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”§ Deep Dive 01: å¤„ç†ä¸å¹³è¡¡çš„è‡ªå®šä¹‰ Loss\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¬ çœŸå®åœºæ™¯ï¼šå‘åˆ¸æ•°æ®çš„å™©æ¢¦\n",
    "\n",
    "ä½ æ˜¯æŸç”µå•†å…¬å¸çš„ç®—æ³•å·¥ç¨‹å¸ˆï¼Œåˆšåˆšæ¥æ‰‹ä¸€ä¸ªæ™ºèƒ½å‘åˆ¸é¡¹ç›®ã€‚\n",
    "\n",
    "æ‰“å¼€æ•°æ®ä¸€çœ‹ï¼Œå‚»çœ¼äº†ï¼š\n",
    "\n",
    "```\n",
    "æ€»ç”¨æˆ·æ•°: 1,000,000\n",
    "å‘åˆ¸ç”¨æˆ·: 50,000 (5%)\n",
    "æœªå‘åˆ¸ç”¨æˆ·: 950,000 (95%)\n",
    "```\n",
    "\n",
    "ä½ ç”¨æ ‡å‡†çš„ IPW ä¼°è®¡æ•ˆåº”ï¼Œç»“æœï¼š\n",
    "\n",
    "- ğŸ”´ å€¾å‘å¾—åˆ†æ¨¡å‹é¢„æµ‹å‡ ä¹å…¨æ˜¯ 0\n",
    "- ğŸ”´ æƒé‡æœ‰äº›é«˜è¾¾ 100+ï¼Œæ–¹å·®çˆ†ç‚¸\n",
    "- ğŸ”´ æ•ˆåº”ä¼°è®¡å¿½æ­£å¿½è´Ÿï¼Œå®Œå…¨ä¸å¯é \n",
    "\n",
    "è€æ¿é—®ï¼šã€Œè¿™ä¸ªå‘åˆ¸æ•ˆæœåˆ°åº•æ˜¯å¤šå°‘ï¼Ÿã€ä½ æ”¯æ”¯å¾å¾è¯´ä¸æ¸…æ¥š...\n",
    "\n",
    "---\n",
    "\n",
    "### é—®é¢˜è¯Šæ–­\n",
    "\n",
    "**å¤„ç†ä¸å¹³è¡¡ (Treatment Imbalance)** æ˜¯å› æœæ¨æ–­ä¸­æœ€å¸¸è§çš„é—®é¢˜ä¹‹ä¸€ï¼š\n",
    "\n",
    "| é—®é¢˜ | è¡¨ç° | åæœ |\n",
    "|------|------|------|\n",
    "| å€¾å‘å¾—åˆ†æç«¯ | $e(x) \\to 0$ æˆ– $e(x) \\to 1$ | æƒé‡çˆ†ç‚¸ |\n",
    "| æ ·æœ¬ä¸å¹³è¡¡ | å°‘æ•°ç±»æ ·æœ¬å¤ªå°‘ | æ¨¡å‹å­¦ä¸å¥½ |\n",
    "| å…±åŒæ”¯æ’‘å·® | ä¸¤ç»„ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¤§ | æ— æ³•å¤–æ¨ |\n",
    "\n",
    "**æœ¬èŠ‚ç›®æ ‡ï¼šä»åŸç†å‡ºå‘ï¼Œè®¾è®¡è‡ªå®šä¹‰ Loss å‡½æ•°æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç»ƒä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. ğŸ¯ è¯Šæ–­å¤„ç†ä¸å¹³è¡¡é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦\n",
    "2. ğŸ¯ ç†è§£å¹¶å®ç° **Focal Loss** ç”¨äºå€¾å‘å¾—åˆ†ä¼°è®¡\n",
    "3. ğŸ¯ è®¾è®¡ **è‡ªå®šä¹‰ Propensity-weighted Loss**\n",
    "4. ğŸ¯ å¯¹æ¯” Trimmingã€Overlap Weightsã€Stabilized Weights ç­‰æ–¹æ³•\n",
    "5. ğŸ¯ åœ¨çœŸå®æ•°æ®ä¸Šåº”ç”¨è¿™äº›æ–¹æ³•\n",
    "\n",
    "> âš ï¸ **Deep Dive ç³»åˆ—** çš„ç›®æ ‡æ˜¯ä»ã€Œè°ƒåŒ…ã€åˆ°ã€Œæ”¹æ¶æ„ã€ï¼ŒçœŸæ­£æŒæ¡æ¨¡å‹åŸç†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from typing import Tuple, Dict, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæ¯•ï¼\")\n",
    "print(\"ğŸ”§ Deep Dive: ä»Šå¤©æˆ‘ä»¬è¦ä»åŸç†æ”¹è¿›æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¬ Part 1: ç”Ÿæˆä¸å¹³è¡¡æ•°æ®\n",
    "\n",
    "é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªçœŸå®çš„ä¸å¹³è¡¡å‘åˆ¸åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_imbalanced_voucher_data(\n",
    "    n: int = 50000,\n",
    "    treatment_rate: float = 0.05,  # åªæœ‰ 5% å‘åˆ¸\n",
    "    true_ate: float = 15.0,        # çœŸå®æ•ˆåº”: å‘åˆ¸å¹³å‡æå‡ 15 å…ƒ\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆä¸¥é‡ä¸å¹³è¡¡çš„å‘åˆ¸æ•°æ®\n",
    "    \n",
    "    åœºæ™¯ï¼šç”µå•†å‘åˆ¸\n",
    "    - X: ç”¨æˆ·ç‰¹å¾ï¼ˆè´­ä¹°åŠ›ã€æ´»è·ƒåº¦ã€å†å²æ¶ˆè´¹ï¼‰\n",
    "    - T: æ˜¯å¦å‘åˆ¸ï¼ˆä¸¥é‡ä¸å¹³è¡¡ï¼ï¼‰\n",
    "    - Y: æ¶ˆè´¹é‡‘é¢\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç”¨æˆ·ç‰¹å¾\n",
    "    purchase_power = np.random.exponential(2, n)      # è´­ä¹°åŠ›\n",
    "    activity = np.random.beta(2, 5, n)                 # æ´»è·ƒåº¦ (åä½)\n",
    "    history_amount = np.random.lognormal(4, 1, n)      # å†å²æ¶ˆè´¹\n",
    "    \n",
    "    X = np.column_stack([\n",
    "        purchase_power,\n",
    "        activity,\n",
    "        history_amount,\n",
    "        purchase_power * activity,  # äº¤äº’é¡¹\n",
    "        np.log1p(history_amount)    # éçº¿æ€§å˜æ¢\n",
    "    ])\n",
    "    \n",
    "    # å‘åˆ¸ç­–ç•¥ï¼šåå‘é«˜è´­ä¹°åŠ›ã€é«˜æ´»è·ƒåº¦ç”¨æˆ·\n",
    "    # è¿™ä¼šå¯¼è‡´ä¸¥é‡çš„é€‰æ‹©åå·®ï¼\n",
    "    propensity_logit = (\n",
    "        -3.5 +                          # åŸºç¡€å¾ˆä½ï¼Œä¿è¯æ•´ä½“å‘åˆ¸ç‡çº¦ 5%\n",
    "        0.5 * purchase_power +          # é«˜è´­ä¹°åŠ›æ›´å¯èƒ½å‘åˆ¸\n",
    "        2.0 * activity +                # é«˜æ´»è·ƒåº¦æ›´å¯èƒ½å‘åˆ¸\n",
    "        0.0005 * history_amount         # å†å²æ¶ˆè´¹å½±å“å°\n",
    "    )\n",
    "    propensity = 1 / (1 + np.exp(-propensity_logit))\n",
    "    T = np.random.binomial(1, propensity)\n",
    "    \n",
    "    # çœŸå®çš„å¼‚è´¨æ€§æ•ˆåº”\n",
    "    # é«˜è´­ä¹°åŠ›ç”¨æˆ·æ•ˆåº”æ›´å¤§ï¼Œä½æ´»è·ƒåº¦ç”¨æˆ·æ•ˆåº”æ›´å¤§ï¼ˆè¾¹é™…æ•ˆåº”ï¼‰\n",
    "    cate = (\n",
    "        true_ate +\n",
    "        3 * purchase_power +            # è´­ä¹°åŠ›é«˜ï¼Œæ•ˆåº”å¤§\n",
    "        -10 * activity +                # æ´»è·ƒåº¦é«˜ï¼Œè¾¹é™…æ•ˆåº”å°\n",
    "        np.random.randn(n) * 5          # å™ªå£°\n",
    "    )\n",
    "    cate = np.clip(cate, 0, 50)  # æ•ˆåº”åœ¨ [0, 50] ä¹‹é—´\n",
    "    \n",
    "    # ç»“æœï¼šæ¶ˆè´¹é‡‘é¢\n",
    "    baseline_spending = (\n",
    "        50 +\n",
    "        20 * purchase_power +\n",
    "        30 * activity +\n",
    "        0.01 * history_amount +\n",
    "        np.random.randn(n) * 20\n",
    "    )\n",
    "    Y = baseline_spending + T * cate\n",
    "    Y = np.clip(Y, 0, None)  # æ¶ˆè´¹ä¸èƒ½ä¸ºè´Ÿ\n",
    "    \n",
    "    # è®¡ç®—çœŸå® ATEï¼ˆå¤„ç†ç»„çš„å¹³å‡æ•ˆåº”ï¼‰\n",
    "    true_ate_actual = cate.mean()\n",
    "    true_att = cate[T == 1].mean()\n",
    "    \n",
    "    print(f\"ğŸ“Š ä¸å¹³è¡¡æ•°æ®ç”Ÿæˆå®Œæˆ\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"æ ·æœ¬é‡: {n:,}\")\n",
    "    print(f\"å‘åˆ¸ç”¨æˆ·: {T.sum():,} ({T.mean():.1%})\")\n",
    "    print(f\"æœªå‘åˆ¸ç”¨æˆ·: {(1-T).sum():,} ({1-T.mean():.1%})\")\n",
    "    print(f\"\\nå€¾å‘å¾—åˆ†åˆ†å¸ƒ:\")\n",
    "    print(f\"   æœ€å°å€¼: {propensity.min():.4f}\")\n",
    "    print(f\"   æœ€å¤§å€¼: {propensity.max():.4f}\")\n",
    "    print(f\"   å‡å€¼: {propensity.mean():.4f}\")\n",
    "    print(f\"\\nçœŸå®æ•ˆåº”:\")\n",
    "    print(f\"   ATE: {true_ate_actual:.2f}\")\n",
    "    print(f\"   ATT: {true_att:.2f}\")\n",
    "    \n",
    "    return X, T, Y, true_att, propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ•°æ®\n",
    "X, T, Y, TRUE_ATT, true_propensity = generate_imbalanced_voucher_data(n=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ä¸å¹³è¡¡ç¨‹åº¦\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. å¤„ç†åˆ†å¸ƒ\n",
    "axes[0].bar(['æœªå‘åˆ¸', 'å‘åˆ¸'], [(1-T).sum(), T.sum()], color=['steelblue', 'coral'])\n",
    "axes[0].set_ylabel('æ ·æœ¬æ•°', fontsize=12)\n",
    "axes[0].set_title(f'å¤„ç†åˆ†å¸ƒ (å‘åˆ¸ç‡: {T.mean():.1%})', fontsize=14)\n",
    "for i, v in enumerate([(1-T).sum(), T.sum()]):\n",
    "    axes[0].text(i, v + 1000, f'{v:,}', ha='center', fontsize=12)\n",
    "\n",
    "# 2. çœŸå®å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "axes[1].hist(true_propensity, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1].axvline(true_propensity.mean(), color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: {true_propensity.mean():.3f}')\n",
    "axes[1].set_xlabel('çœŸå®å€¾å‘å¾—åˆ†', fontsize=12)\n",
    "axes[1].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[1].set_title('çœŸå®å€¾å‘å¾—åˆ†åˆ†å¸ƒ', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. æŒ‰å¤„ç†åˆ†ç»„çš„å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "axes[2].hist(true_propensity[T==0], bins=50, alpha=0.5, label='æœªå‘åˆ¸', color='steelblue')\n",
    "axes[2].hist(true_propensity[T==1], bins=50, alpha=0.5, label='å‘åˆ¸', color='coral')\n",
    "axes[2].set_xlabel('çœŸå®å€¾å‘å¾—åˆ†', fontsize=12)\n",
    "axes[2].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[2].set_title('æŒ‰å¤„ç†åˆ†ç»„çš„å€¾å‘å¾—åˆ†', fontsize=14)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ é—®é¢˜è¯Šæ–­:\")\n",
    "print(f\"   1. å‘åˆ¸ç‡åªæœ‰ {T.mean():.1%}ï¼Œä¸¥é‡ä¸å¹³è¡¡\")\n",
    "print(f\"   2. å¤§éƒ¨åˆ†ç”¨æˆ·çš„å€¾å‘å¾—åˆ†å¾ˆä½ï¼ˆ< 0.1ï¼‰\")\n",
    "print(f\"   3. ä¸¤ç»„çš„å€¾å‘å¾—åˆ†åˆ†å¸ƒæœ‰æ˜æ˜¾å·®å¼‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš¨ Part 2: é—®é¢˜æ¼”ç¤º - æ ‡å‡† IPW å¤±æ•ˆ\n",
    "\n",
    "è®©æˆ‘ä»¬çœ‹çœ‹æ ‡å‡† IPW åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šçš„è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_ipw(X, T, Y):\n",
    "    \"\"\"\n",
    "    æ ‡å‡† IPW ä¼°è®¡\n",
    "    \"\"\"\n",
    "    # ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X, T)\n",
    "    propensity = lr.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # è®¡ç®—æƒé‡\n",
    "    weights = T / propensity + (1 - T) / (1 - propensity)\n",
    "    \n",
    "    # IPW ä¼°è®¡\n",
    "    y1 = (Y * T / propensity).sum() / (T / propensity).sum()\n",
    "    y0 = (Y * (1-T) / (1-propensity)).sum() / ((1-T) / (1-propensity)).sum()\n",
    "    ate = y1 - y0\n",
    "    \n",
    "    return {\n",
    "        'ate': ate,\n",
    "        'propensity': propensity,\n",
    "        'weights': weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ ‡å‡† IPW\n",
    "ipw_result = standard_ipw(X, T, Y)\n",
    "\n",
    "print(\"ğŸš¨ æ ‡å‡† IPW ç»“æœ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"çœŸå® ATT: {TRUE_ATT:.2f}\")\n",
    "print(f\"IPW ä¼°è®¡: {ipw_result['ate']:.2f}\")\n",
    "print(f\"åå·®: {ipw_result['ate'] - TRUE_ATT:+.2f}\")\n",
    "print()\n",
    "print(f\"å€¾å‘å¾—åˆ†ç»Ÿè®¡:\")\n",
    "print(f\"   æœ€å°å€¼: {ipw_result['propensity'].min():.6f}\")\n",
    "print(f\"   æœ€å¤§å€¼: {ipw_result['propensity'].max():.4f}\")\n",
    "print(f\"   å‡å€¼: {ipw_result['propensity'].mean():.4f}\")\n",
    "print()\n",
    "print(f\"æƒé‡ç»Ÿè®¡:\")\n",
    "print(f\"   æœ€å°å€¼: {ipw_result['weights'].min():.2f}\")\n",
    "print(f\"   æœ€å¤§å€¼: {ipw_result['weights'].max():.2f} âš ï¸\")\n",
    "print(f\"   å‡å€¼: {ipw_result['weights'].mean():.2f}\")\n",
    "print(f\"   æ ‡å‡†å·®: {ipw_result['weights'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–é—®é¢˜\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. ä¼°è®¡çš„å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "axes[0].hist(ipw_result['propensity'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[0].axvline(0.05, color='red', linestyle='--', linewidth=2, label='çœŸå®å‘åˆ¸ç‡')\n",
    "axes[0].set_xlabel('ä¼°è®¡çš„å€¾å‘å¾—åˆ†', fontsize=12)\n",
    "axes[0].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[0].set_title('å€¾å‘å¾—åˆ†ä¼°è®¡', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. æƒé‡åˆ†å¸ƒ\n",
    "axes[1].hist(ipw_result['weights'], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].axvline(ipw_result['weights'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('IPW æƒé‡', fontsize=12)\n",
    "axes[1].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[1].set_title(f'æƒé‡åˆ†å¸ƒ (max={ipw_result[\"weights\"].max():.1f})', fontsize=14)\n",
    "axes[1].set_xlim([0, min(50, ipw_result['weights'].max())])\n",
    "\n",
    "# 3. æç«¯æƒé‡çš„å½±å“\n",
    "weight_percentiles = [50, 90, 95, 99, 99.9, 100]\n",
    "weight_values = [np.percentile(ipw_result['weights'], p) for p in weight_percentiles]\n",
    "axes[2].bar([str(p) for p in weight_percentiles], weight_values, color='coral', alpha=0.7)\n",
    "axes[2].set_xlabel('ç™¾åˆ†ä½æ•°', fontsize=12)\n",
    "axes[2].set_ylabel('æƒé‡å€¼', fontsize=12)\n",
    "axes[2].set_title('æƒé‡çš„ç™¾åˆ†ä½åˆ†å¸ƒ', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ é—®é¢˜æ ¹æº:\")\n",
    "print(f\"   1. å€¾å‘å¾—åˆ†ä¼°è®¡åå‘ 0ï¼Œå¯¼è‡´ 1/e(x) æƒé‡çˆ†ç‚¸\")\n",
    "print(f\"   2. å°‘æ•°æç«¯æƒé‡ä¸»å¯¼äº†æ•´ä¸ªä¼°è®¡\")\n",
    "print(f\"   3. æœ‰æ•ˆæ ·æœ¬é‡ (ESS) è¿œå°äºå®é™…æ ·æœ¬é‡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\n",
    "def effective_sample_size(weights):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    ESS = (sum(w))^2 / sum(w^2)\n",
    "    \"\"\"\n",
    "    return (weights.sum() ** 2) / (weights ** 2).sum()\n",
    "\n",
    "ess = effective_sample_size(ipw_result['weights'])\n",
    "print(f\"ğŸ“Š æœ‰æ•ˆæ ·æœ¬é‡åˆ†æ\")\n",
    "print(f\"   å®é™…æ ·æœ¬é‡: {len(Y):,}\")\n",
    "print(f\"   æœ‰æ•ˆæ ·æœ¬é‡ (ESS): {ess:.0f}\")\n",
    "print(f\"   ESS å æ¯”: {ess/len(Y):.1%}\")\n",
    "print(f\"\\nâš ï¸ åªæœ‰ {ess/len(Y):.1%} çš„æ ·æœ¬åœ¨æœ‰æ•ˆåœ°è´¡çŒ®ä¿¡æ¯ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Part 3: è§£å†³æ–¹æ¡ˆ A - æ”¹è¿›å€¾å‘å¾—åˆ†ä¼°è®¡\n",
    "\n",
    "### 3.1 Focal Loss\n",
    "\n",
    "**Focal Loss** æœ€åˆç”¨äºç›®æ ‡æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\n",
    "\n",
    "> å¯¹å®¹æ˜“åˆ†ç±»çš„æ ·æœ¬ï¼ˆæ¦‚ç‡é«˜çš„ï¼‰é™ä½æƒé‡ï¼Œå¯¹éš¾åˆ†ç±»çš„æ ·æœ¬ï¼ˆæ¦‚ç‡ä½çš„ï¼‰å¢åŠ æƒé‡\n",
    "\n",
    "**æ ‡å‡†äº¤å‰ç†µ:**\n",
    "$$L_{CE} = -y \\log(p) - (1-y) \\log(1-p)$$\n",
    "\n",
    "**Focal Loss:**\n",
    "$$L_{FL} = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $p_t = p$ if $y=1$ else $1-p$\n",
    "- $\\alpha$: ç±»åˆ«æƒé‡\n",
    "- $\\gamma$: èšç„¦å‚æ•°ï¼ˆ$\\gamma > 0$ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO 1: å®ç° Focal Loss\n\ndef focal_loss(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    gamma: float = 2.0,\n    alpha: float = 0.25\n) -> float:\n    \"\"\"\n    è®¡ç®— Focal Loss\n    \n    å‚æ•°:\n        y_true: çœŸå®æ ‡ç­¾ (0 æˆ– 1)\n        y_pred: é¢„æµ‹æ¦‚ç‡\n        gamma: èšç„¦å‚æ•°ï¼Œè¶Šå¤§å¯¹éš¾åˆ†ç±»æ ·æœ¬å…³æ³¨è¶Šå¤š\n        alpha: æ­£ç±»æƒé‡\n    \n    è¿”å›:\n        loss: Focal Loss å€¼\n    \"\"\"\n    # è£å‰ªé¢„æµ‹å€¼ï¼Œé¿å… log(0)\n    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    \n    # å®ç° Focal Loss\n    # 1. è®¡ç®— p_t: å¦‚æœ y=1 åˆ™ p_t=pï¼Œå¦åˆ™ p_t=1-p\n    p_t = np.where(y_true == 1, y_pred, 1 - y_pred)\n    \n    # 2. è®¡ç®— alpha_t: å¦‚æœ y=1 åˆ™ alpha_t=alphaï¼Œå¦åˆ™ alpha_t=1-alpha\n    alpha_t = np.where(y_true == 1, alpha, 1 - alpha)\n    \n    # 3. focal_weight = (1 - p_t)^gamma\n    focal_weight = (1 - p_t) ** gamma\n    \n    # 4. loss = -alpha_t * focal_weight * log(p_t)\n    loss = -alpha_t * focal_weight * np.log(p_t)\n    \n    return loss.mean()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ å‚è€ƒç­”æ¡ˆ\n",
    "\n",
    "def focal_loss_solution(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    gamma: float = 2.0,\n",
    "    alpha: float = 0.25\n",
    ") -> float:\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # p_t: æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡\n",
    "    p_t = np.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "    \n",
    "    # alpha_t: ç±»åˆ«æƒé‡\n",
    "    alpha_t = np.where(y_true == 1, alpha, 1 - alpha)\n",
    "    \n",
    "    # focal weight: éš¾åˆ†ç±»æ ·æœ¬æƒé‡æ›´é«˜\n",
    "    focal_weight = (1 - p_t) ** gamma\n",
    "    \n",
    "    # Focal Loss\n",
    "    loss = -alpha_t * focal_weight * np.log(p_t)\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– Focal Loss vs Cross Entropy\n",
    "\n",
    "def visualize_focal_loss():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    p = np.linspace(0.01, 0.99, 100)\n",
    "    \n",
    "    # æ­£ç±» (y=1)\n",
    "    ce_loss = -np.log(p)\n",
    "    fl_gamma1 = -(1-p)**1 * np.log(p)\n",
    "    fl_gamma2 = -(1-p)**2 * np.log(p)\n",
    "    fl_gamma5 = -(1-p)**5 * np.log(p)\n",
    "    \n",
    "    axes[0].plot(p, ce_loss, 'b-', linewidth=2, label='CE (Î³=0)')\n",
    "    axes[0].plot(p, fl_gamma1, 'g--', linewidth=2, label='FL (Î³=1)')\n",
    "    axes[0].plot(p, fl_gamma2, 'r-.', linewidth=2, label='FL (Î³=2)')\n",
    "    axes[0].plot(p, fl_gamma5, 'm:', linewidth=2, label='FL (Î³=5)')\n",
    "    axes[0].set_xlabel('é¢„æµ‹æ¦‚ç‡ p (æ­£ç±»)', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('æ­£ç±» (y=1) çš„ Loss', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim([0, 5])\n",
    "    \n",
    "    # è´Ÿç±» (y=0)\n",
    "    ce_loss_neg = -np.log(1-p)\n",
    "    fl_gamma1_neg = -p**1 * np.log(1-p)\n",
    "    fl_gamma2_neg = -p**2 * np.log(1-p)\n",
    "    fl_gamma5_neg = -p**5 * np.log(1-p)\n",
    "    \n",
    "    axes[1].plot(p, ce_loss_neg, 'b-', linewidth=2, label='CE (Î³=0)')\n",
    "    axes[1].plot(p, fl_gamma1_neg, 'g--', linewidth=2, label='FL (Î³=1)')\n",
    "    axes[1].plot(p, fl_gamma2_neg, 'r-.', linewidth=2, label='FL (Î³=2)')\n",
    "    axes[1].plot(p, fl_gamma5_neg, 'm:', linewidth=2, label='FL (Î³=5)')\n",
    "    axes[1].set_xlabel('é¢„æµ‹æ¦‚ç‡ p (æ­£ç±»)', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('è´Ÿç±» (y=0) çš„ Loss', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim([0, 5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¡ è§‚å¯Ÿ:\")\n",
    "    print(\"   - Î³ è¶Šå¤§ï¼Œå¯¹é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„æƒ©ç½šè¶Šå°\")\n",
    "    print(\"   - è¿™è¿«ä½¿æ¨¡å‹æ›´å…³æ³¨éš¾åˆ†ç±»çš„æ ·æœ¬\")\n",
    "    print(\"   - å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼Œå°‘æ•°ç±»å¾€å¾€æ˜¯éš¾åˆ†ç±»çš„\")\n",
    "\n",
    "visualize_focal_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ä½¿ç”¨ Focal Loss è®­ç»ƒå€¾å‘å¾—åˆ†æ¨¡å‹\n",
    "\n",
    "ç”±äº sklearn ä¸ç›´æ¥æ”¯æŒè‡ªå®šä¹‰ Lossï¼Œæˆ‘ä»¬ç”¨ GradientBoosting çš„ sample_weight æ¥è¿‘ä¼¼å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_propensity_with_focal_weights(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    gamma: float = 2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Focal Loss é£æ ¼çš„æƒé‡è®­ç»ƒå€¾å‘å¾—åˆ†æ¨¡å‹\n",
    "    \n",
    "    æ€è·¯ï¼šå…ˆç”¨æ ‡å‡†æ¨¡å‹é¢„æµ‹ï¼Œå†æ ¹æ®éš¾åº¦è°ƒæ•´æƒé‡é‡æ–°è®­ç»ƒ\n",
    "    \"\"\"\n",
    "    # ç¬¬ä¸€è½®ï¼šæ ‡å‡†è®­ç»ƒ\n",
    "    model_init = LogisticRegression(max_iter=1000)\n",
    "    model_init.fit(X, T)\n",
    "    p_init = model_init.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # è®¡ç®— Focal Weight\n",
    "    p_t = np.where(T == 1, p_init, 1 - p_init)\n",
    "    focal_weights = (1 - p_t) ** gamma\n",
    "    \n",
    "    # ç¬¬äºŒè½®ï¼šä½¿ç”¨ Focal Weight è®­ç»ƒ\n",
    "    model_focal = LogisticRegression(max_iter=1000)\n",
    "    model_focal.fit(X, T, sample_weight=focal_weights)\n",
    "    p_focal = model_focal.predict_proba(X)[:, 1]\n",
    "    \n",
    "    return p_focal, focal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”æ ‡å‡†ä¼°è®¡å’Œ Focal ä¼°è®¡\n",
    "p_focal, focal_weights = train_propensity_with_focal_weights(X, T, gamma=2.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å¯¹æ¯”å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "axes[0].hist(ipw_result['propensity'], bins=50, alpha=0.5, label='æ ‡å‡† LR', color='blue')\n",
    "axes[0].hist(p_focal, bins=50, alpha=0.5, label='Focal åŠ æƒ', color='green')\n",
    "axes[0].set_xlabel('å€¾å‘å¾—åˆ†', fontsize=12)\n",
    "axes[0].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[0].set_title('å€¾å‘å¾—åˆ†åˆ†å¸ƒå¯¹æ¯”', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# Focal æƒé‡åˆ†å¸ƒ\n",
    "axes[1].hist(focal_weights[T==0], bins=50, alpha=0.5, label='æœªå‘åˆ¸', color='steelblue')\n",
    "axes[1].hist(focal_weights[T==1], bins=50, alpha=0.5, label='å‘åˆ¸', color='coral')\n",
    "axes[1].set_xlabel('Focal æƒé‡', fontsize=12)\n",
    "axes[1].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[1].set_title('Focal æƒé‡åˆ†å¸ƒ', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Part 4: è§£å†³æ–¹æ¡ˆ B - ä¿®æ”¹ IPW æƒé‡\n",
    "\n",
    "é™¤äº†æ”¹è¿›å€¾å‘å¾—åˆ†ä¼°è®¡ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç›´æ¥ä¿®æ”¹æƒé‡ã€‚\n",
    "\n",
    "### 4.1 Trimming (æˆªæ–­)\n",
    "\n",
    "æœ€ç®€å•çš„æ–¹æ³•ï¼šåˆ é™¤å€¾å‘å¾—åˆ†å¤ªæç«¯çš„æ ·æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_with_trimming(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    propensity: np.ndarray,\n",
    "    lower: float = 0.01,\n",
    "    upper: float = 0.99\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Trimmed IPW: åˆ é™¤æç«¯å€¾å‘å¾—åˆ†çš„æ ·æœ¬\n",
    "    \"\"\"\n",
    "    # æ‰¾å‡ºåœ¨ [lower, upper] èŒƒå›´å†…çš„æ ·æœ¬\n",
    "    mask = (propensity >= lower) & (propensity <= upper)\n",
    "    \n",
    "    X_trim = X[mask]\n",
    "    T_trim = T[mask]\n",
    "    Y_trim = Y[mask]\n",
    "    propensity_trim = propensity[mask]\n",
    "    \n",
    "    # è®¡ç®— IPW\n",
    "    y1 = (Y_trim * T_trim / propensity_trim).sum() / (T_trim / propensity_trim).sum()\n",
    "    y0 = (Y_trim * (1-T_trim) / (1-propensity_trim)).sum() / ((1-T_trim) / (1-propensity_trim)).sum()\n",
    "    ate = y1 - y0\n",
    "    \n",
    "    return {\n",
    "        'ate': ate,\n",
    "        'n_original': len(Y),\n",
    "        'n_trimmed': mask.sum(),\n",
    "        'pct_kept': mask.mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Overlap Weights (é‡å æƒé‡)\n",
    "\n",
    "Overlap Weights çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**æœ€å¤§åŒ–å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„é‡å **ã€‚\n",
    "\n",
    "$$w_i = \\begin{cases} 1 - e(X_i) & \\text{if } T_i = 1 \\\\ e(X_i) & \\text{if } T_i = 0 \\end{cases}$$\n",
    "\n",
    "ç›´è§‰ï¼š\n",
    "- å¯¹äºå‘åˆ¸ç”¨æˆ·ï¼ˆ$T=1$ï¼‰ï¼Œå¦‚æœä»–çš„å€¾å‘å¾—åˆ†å¾ˆé«˜ï¼ˆæœ¬æ¥å°±å¾ˆå¯èƒ½å‘åˆ¸ï¼‰ï¼Œæƒé‡å°\n",
    "- å¯¹äºæœªå‘åˆ¸ç”¨æˆ·ï¼ˆ$T=0$ï¼‰ï¼Œå¦‚æœä»–çš„å€¾å‘å¾—åˆ†å¾ˆä½ï¼ˆæœ¬æ¥å°±ä¸å¤ªå¯èƒ½å‘åˆ¸ï¼‰ï¼Œæƒé‡ä¹Ÿå°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO 2: å®ç° Overlap Weights\n\ndef ipw_with_overlap_weights(\n    T: np.ndarray,\n    Y: np.ndarray,\n    propensity: np.ndarray\n) -> Dict:\n    \"\"\"\n    ä½¿ç”¨ Overlap Weights çš„ IPW\n    \n    Overlap Weights:\n        - å¦‚æœ T=1: w = 1 - e(X)\n        - å¦‚æœ T=0: w = e(X)\n    \"\"\"\n    # è®¡ç®— Overlap Weights\n    # w = T * (1 - propensity) + (1 - T) * propensity\n    overlap_weights = T * (1 - propensity) + (1 - T) * propensity\n    \n    # åŠ æƒä¼°è®¡\n    y1 = (Y * T * overlap_weights).sum() / (T * overlap_weights).sum()\n    y0 = (Y * (1-T) * overlap_weights).sum() / ((1-T) * overlap_weights).sum()\n    ate = y1 - y0\n    \n    return {\n        'ate': ate,\n        'weights': overlap_weights,\n        'ess': effective_sample_size(overlap_weights)\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ å‚è€ƒç­”æ¡ˆ\n",
    "\n",
    "def ipw_with_overlap_weights_solution(\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    propensity: np.ndarray\n",
    ") -> Dict:\n",
    "    # Overlap Weights\n",
    "    overlap_weights = T * (1 - propensity) + (1 - T) * propensity\n",
    "    \n",
    "    # åŠ æƒä¼°è®¡\n",
    "    y1 = (Y * T * overlap_weights).sum() / (T * overlap_weights).sum()\n",
    "    y0 = (Y * (1-T) * overlap_weights).sum() / ((1-T) * overlap_weights).sum()\n",
    "    ate = y1 - y0\n",
    "    \n",
    "    return {\n",
    "        'ate': ate,\n",
    "        'weights': overlap_weights,\n",
    "        'ess': effective_sample_size(overlap_weights)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Stabilized Weights (ç¨³å®šåŒ–æƒé‡)\n",
    "\n",
    "ç¨³å®šåŒ–æƒé‡åœ¨åˆ†å­åŠ å…¥è¾¹é™…å¤„ç†æ¦‚ç‡ï¼Œå‡å°‘æ–¹å·®ï¼š\n",
    "\n",
    "$$w_i^{stab} = \\frac{P(T)}{e(X_i)} \\cdot T_i + \\frac{1-P(T)}{1-e(X_i)} \\cdot (1-T_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_with_stabilized_weights(\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    propensity: np.ndarray\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ç¨³å®šåŒ–æƒé‡çš„ IPW\n",
    "    \"\"\"\n",
    "    # è¾¹é™…å¤„ç†æ¦‚ç‡\n",
    "    p_t = T.mean()\n",
    "    \n",
    "    # ç¨³å®šåŒ–æƒé‡\n",
    "    stabilized_weights = T * p_t / propensity + (1 - T) * (1 - p_t) / (1 - propensity)\n",
    "    \n",
    "    # è£å‰ªæç«¯æƒé‡\n",
    "    stabilized_weights = np.clip(stabilized_weights, 0.1, 10)\n",
    "    \n",
    "    # åŠ æƒä¼°è®¡\n",
    "    y1 = (Y * T * stabilized_weights).sum() / (T * stabilized_weights).sum()\n",
    "    y0 = (Y * (1-T) * stabilized_weights).sum() / ((1-T) * stabilized_weights).sum()\n",
    "    ate = y1 - y0\n",
    "    \n",
    "    return {\n",
    "        'ate': ate,\n",
    "        'weights': stabilized_weights,\n",
    "        'ess': effective_sample_size(stabilized_weights)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ† Part 5: æ–¹æ³•å¤§æ¯”æ‹¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X, T)\n",
    "propensity_est = lr.predict_proba(X)[:, 1]\n",
    "\n",
    "# å„ç§æ–¹æ³•\n",
    "results = []\n",
    "\n",
    "# 1. æœ´ç´ ä¼°è®¡\n",
    "naive_ate = Y[T==1].mean() - Y[T==0].mean()\n",
    "results.append({'æ–¹æ³•': 'æœ´ç´ ä¼°è®¡', 'ATE': naive_ate, 'ESS': len(Y)})\n",
    "\n",
    "# 2. æ ‡å‡† IPW\n",
    "ipw_std = standard_ipw(X, T, Y)\n",
    "results.append({'æ–¹æ³•': 'æ ‡å‡† IPW', 'ATE': ipw_std['ate'], 'ESS': effective_sample_size(ipw_std['weights'])})\n",
    "\n",
    "# 3. Trimmed IPW\n",
    "ipw_trim = ipw_with_trimming(X, T, Y, propensity_est, lower=0.01, upper=0.99)\n",
    "results.append({'æ–¹æ³•': 'Trimmed IPW', 'ATE': ipw_trim['ate'], 'ESS': ipw_trim['n_trimmed']})\n",
    "\n",
    "# 4. Overlap Weights\n",
    "ipw_overlap = ipw_with_overlap_weights_solution(T, Y, propensity_est)\n",
    "results.append({'æ–¹æ³•': 'Overlap Weights', 'ATE': ipw_overlap['ate'], 'ESS': ipw_overlap['ess']})\n",
    "\n",
    "# 5. Stabilized Weights\n",
    "ipw_stab = ipw_with_stabilized_weights(T, Y, propensity_est)\n",
    "results.append({'æ–¹æ³•': 'Stabilized Weights', 'ATE': ipw_stab['ate'], 'ESS': ipw_stab['ess']})\n",
    "\n",
    "# æ±‡æ€»\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['åå·®'] = results_df['ATE'] - TRUE_ATT\n",
    "results_df['|åå·®|'] = abs(results_df['åå·®'])\n",
    "\n",
    "print(\"ğŸ† æ–¹æ³•å¤§æ¯”æ‹¼\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"çœŸå® ATT: {TRUE_ATT:.2f}\")\n",
    "print()\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ATE ä¼°è®¡å¯¹æ¯”\n",
    "colors = ['gray', 'red', 'orange', 'green', 'blue']\n",
    "bars = axes[0].barh(results_df['æ–¹æ³•'], results_df['ATE'], color=colors, alpha=0.7)\n",
    "axes[0].axvline(TRUE_ATT, color='black', linestyle='--', linewidth=2, label=f'çœŸå® ATT = {TRUE_ATT:.1f}')\n",
    "axes[0].set_xlabel('ATE ä¼°è®¡', fontsize=12)\n",
    "axes[0].set_title('å„æ–¹æ³• ATE ä¼°è®¡', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# ESS å¯¹æ¯”\n",
    "axes[1].barh(results_df['æ–¹æ³•'], results_df['ESS'], color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('æœ‰æ•ˆæ ·æœ¬é‡ (ESS)', fontsize=12)\n",
    "axes[1].set_title('æœ‰æ•ˆæ ·æœ¬é‡å¯¹æ¯”', fontsize=14)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "print(\"   1. Overlap Weights é€šå¸¸ç»™å‡ºæœ€ç¨³å¥çš„ä¼°è®¡\")\n",
    "print(\"   2. Stabilized Weights å¯ä»¥æ˜¾è‘—å¢åŠ  ESS\")\n",
    "print(\"   3. æ ‡å‡† IPW åœ¨æç«¯ä¸å¹³è¡¡æ—¶ä¼šå¤±æ•ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ Part 6: æ€è€ƒé¢˜\n",
    "\n",
    "### é—®é¢˜ 1: Focal Loss çš„ Î³ å‚æ•°å¦‚ä½•é€‰æ‹©ï¼Ÿå¤ªå¤§æˆ–å¤ªå°ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 2: Overlap Weights ä¼°è®¡çš„æ˜¯ä»€ä¹ˆ estimandï¼Ÿå’Œ IPW ä¼°è®¡çš„ ATE æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "\n",
    "**æç¤º**: æƒ³æƒ³ Overlap Weights å…³æ³¨çš„æ˜¯å“ªä¸ªäººç¾¤\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 3: åœ¨å®é™…ä¸šåŠ¡ä¸­ï¼Œä½ ä¼šå¦‚ä½•é€‰æ‹©å¤„ç†ä¸å¹³è¡¡çš„æ–¹æ³•ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 4: Trimming ä¼šå¸¦æ¥ä»€ä¹ˆé—®é¢˜ï¼Ÿå¦‚ä½•å†³å®šæˆªæ–­é˜ˆå€¼ï¼Ÿ\n",
    "\n",
    "**æç¤º**: æƒ³æƒ³å¤–æ¨æ€§å’Œæ ·æœ¬é‡çš„æƒè¡¡\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ æ€»ç»“\n",
    "\n",
    "### å¤„ç†ä¸å¹³è¡¡çš„æ–¹æ³•\n",
    "\n",
    "| æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
    "|------|---------|------|------|\n",
    "| **Focal Loss** | å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬ | æ”¹å–„å€¾å‘å¾—åˆ†ä¼°è®¡ | éœ€è¦è‡ªå®šä¹‰è®­ç»ƒ |\n",
    "| **Trimming** | åˆ é™¤æç«¯æ ·æœ¬ | ç®€å•ç›´è§‚ | ä¸¢å¼ƒæ ·æœ¬ï¼Œæ”¹å˜ç›®æ ‡äººç¾¤ |\n",
    "| **Overlap Weights** | æœ€å¤§åŒ–é‡å  | æƒé‡æ¸©å’Œï¼ŒESS é«˜ | ä¼°è®¡çš„æ˜¯ overlap äººç¾¤æ•ˆåº” |\n",
    "| **Stabilized Weights** | é™¤ä»¥è¾¹é™…æ¦‚ç‡ | å‡å°‘æ–¹å·® | å¯èƒ½ä»æœ‰æç«¯æƒé‡ |\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "1. **å…ˆè¯Šæ–­å†æ²»ç–—**: æ£€æŸ¥å€¾å‘å¾—åˆ†åˆ†å¸ƒã€æƒé‡åˆ†å¸ƒã€ESS\n",
    "2. **å¤šæ–¹æ³•å¯¹æ¯”**: å¦‚æœä¸åŒæ–¹æ³•ç»“æœå·®å¼‚å¤§ï¼Œè¦å°å¿ƒ\n",
    "3. **ä¸šåŠ¡ç†è§£**: ä¸åŒæ–¹æ³•å¯¹åº”ä¸åŒçš„ç›®æ ‡äººç¾¤\n",
    "4. **æ•æ„Ÿæ€§åˆ†æ**: æ”¹å˜æˆªæ–­é˜ˆå€¼ã€æƒé‡æ–¹æ¡ˆï¼Œçœ‹ç»“æœç¨³å®šæ€§\n",
    "\n",
    "### å…³é”®å…¬å¼\n",
    "\n",
    "**Focal Loss:**\n",
    "$$L_{FL} = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "**Overlap Weights:**\n",
    "$$w_i = T_i \\cdot (1-e(X_i)) + (1-T_i) \\cdot e(X_i)$$\n",
    "\n",
    "**Stabilized Weights:**\n",
    "$$w_i^{stab} = T_i \\cdot \\frac{P(T)}{e(X_i)} + (1-T_i) \\cdot \\frac{1-P(T)}{1-e(X_i)}$$\n",
    "\n",
    "---\n",
    "\n",
    "**ã€Œå¤„ç†ä¸å¹³è¡¡ä¸å¯æ€•ï¼Œå¯æ€•çš„æ˜¯ä¸çŸ¥é“è‡ªå·±é‡åˆ°äº†ä¸å¹³è¡¡ã€‚ã€**\n",
    "\n",
    "ğŸ‰ æ­å–œå®Œæˆ Deep Dive 01ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ¤ Part 7: é¢è¯•æ¨¡æ‹Ÿç¯èŠ‚\n\n### é—®é¢˜ 1: è¯Šæ–­å¤„ç†ä¸å¹³è¡¡\n\n**é¢è¯•å®˜**: ä½ æ‹¿åˆ°ä¸€ä¸ªå› æœæ¨æ–­é¡¹ç›®ï¼Œå¤„ç†ç»„åªæœ‰ 5% çš„æ ·æœ¬ï¼Œä½ ä¼šå¦‚ä½•è¯Šæ–­æ˜¯å¦å­˜åœ¨é—®é¢˜ï¼Ÿ\n\n**å‚è€ƒç­”æ¡ˆ**:\n\næˆ‘ä¼šä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¯Šæ–­ï¼š\n\n```python\ndef diagnose_treatment_imbalance(T, e, weights):\n    \"\"\"è¯Šæ–­å¤„ç†ä¸å¹³è¡¡é—®é¢˜\"\"\"\n    print(\"1. åŸºç¡€ç»Ÿè®¡:\")\n    print(f\"   å¤„ç†ç‡: {T.mean():.2%}\")\n    print(f\"   å¤„ç†ç»„æ ·æœ¬: {T.sum()}, æ§åˆ¶ç»„æ ·æœ¬: {(1-T).sum()}\")\n    \n    print(\"\\n2. å€¾å‘å¾—åˆ†åˆ†å¸ƒ:\")\n    print(f\"   e(X) èŒƒå›´: [{e.min():.4f}, {e.max():.4f}]\")\n    print(f\"   e(X) < 0.1: {(e < 0.1).sum()} ({(e < 0.1).mean():.1%})\")\n    print(f\"   e(X) > 0.9: {(e > 0.9).sum()} ({(e > 0.9).mean():.1%})\")\n    \n    print(\"\\n3. IPW æƒé‡è¯Šæ–­:\")\n    print(f\"   æœ€å¤§æƒé‡: {weights.max():.1f}\")\n    print(f\"   æƒé‡ > 10 çš„æ ·æœ¬: {(weights > 10).sum()}\")\n    \n    print(\"\\n4. æœ‰æ•ˆæ ·æœ¬é‡:\")\n    ess = (weights.sum()**2) / (weights**2).sum()\n    print(f\"   ESS: {ess:.0f} / {len(weights)} ({ess/len(weights):.1%})\")\n    \n    print(\"\\n5. å…±åŒæ”¯æ’‘:\")\n    # æ£€æŸ¥ä¸¤ç»„åœ¨ç‰¹å¾ç©ºé—´çš„é‡å \n    print(\"   (éœ€è¦å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ)\")\n    \n    if ess/len(weights) < 0.5:\n        print(\"\\nâš ï¸ è¯Šæ–­: å­˜åœ¨ä¸¥é‡çš„å¤„ç†ä¸å¹³è¡¡é—®é¢˜!\")\n        print(\"   å»ºè®®: ä½¿ç”¨ Overlap Weights æˆ– Trimming\")\n```\n\n**å…³é”®ç‚¹**:\n- å¤„ç†ç‡æœ¬èº«ä¸æ˜¯é—®é¢˜ï¼Œå…³é”®çœ‹å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n- æ£€æŸ¥æƒé‡åˆ†å¸ƒï¼Œè­¦æƒ•æç«¯æƒé‡\n- è®¡ç®— ESSï¼ŒæŸå¤±è¶…è¿‡ 50% éœ€è¦å¤„ç†\n- å¯è§†åŒ–å…±åŒæ”¯æ’‘åŒºåŸŸ\n\n---\n\n### é—®é¢˜ 2: Focal Loss vs Class Weights\n\n**é¢è¯•å®˜**: é™¤äº† Focal Lossï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡ï¼Œä¸¤è€…æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ\n\n**å‚è€ƒç­”æ¡ˆ**:\n\n| æ–¹æ³• | æœºåˆ¶ | é€‚ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |\n|------|------|---------|--------|\n| **Class Weights** | å¯¹å°‘æ•°ç±»æ ·æœ¬LossåŠ æƒ | å…¨å±€ä¸å¹³è¡¡ | ç®€å•ï¼Œä½†å¯¹æ‰€æœ‰å°‘æ•°ç±»æ ·æœ¬ä¸€è§†åŒä» |\n| **Focal Loss** | å¯¹éš¾åˆ†ç±»æ ·æœ¬åŠ æƒ | å­˜åœ¨éš¾åˆ†ç±»æ ·æœ¬ | åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä½†éœ€è¦è°ƒå‚Î³ |\n\n**ä»£ç å¯¹æ¯”**:\n```python\n# Class Weights\nlr = LogisticRegression(class_weight='balanced')\n# ç­‰ä»·äºæ‰‹åŠ¨è®¾ç½®: w_0 = n/(2*n_0), w_1 = n/(2*n_1)\n\n# Focal Loss\n# Loss = -(1-p)^Î³ * log(p)\n# Î³ è¶Šå¤§ï¼Œå¯¹ç®€å•æ ·æœ¬çš„æƒ©ç½šè¶Šå°\n```\n\n**é€‰æ‹©å»ºè®®**:\n- **æ•°æ®ä¸å¹³è¡¡ + æ ·æœ¬è´¨é‡å‡åŒ€**: Class Weights\n- **æ•°æ®ä¸å¹³è¡¡ + æœ‰éš¾åˆ†ç±»æ ·æœ¬**: Focal Loss\n- **æç«¯ä¸å¹³è¡¡ (1:100)**: å…ˆç”¨ Focal Lossï¼Œå†é…åˆ Class Weights\n\n**å®æˆ˜ç»éªŒ**:\n- Focal Loss çš„ Î³ é€šå¸¸åœ¨ [1, 5] ä¹‹é—´\n- Î³=2 æ˜¯ä¸€ä¸ªä¸é”™çš„èµ·ç‚¹\n- å¯ä»¥é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹© Î³\n\n---\n\n### é—®é¢˜ 3: Overlap Weights ä¼°è®¡çš„æ˜¯ä»€ä¹ˆï¼Ÿ\n\n**é¢è¯•å®˜**: Overlap Weights ä¼°è®¡çš„æ˜¯ ATE å—ï¼Ÿå’Œæ ‡å‡† IPW æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n\n**å‚è€ƒç­”æ¡ˆ**:\n\n**NOï¼Overlap Weights ä¼°è®¡çš„æ˜¯ ATO (Average Treatment Effect on the Overlap population)**\n\n$$w_{OW} = e(X)(1-e(X))$$\n\n**ä¼°è®¡ç›®æ ‡å¯¹æ¯”**:\n\n| Estimand | å®šä¹‰ | æƒé‡ | å…³æ³¨äººç¾¤ |\n|----------|------|------|---------|\n| ATE | E[Y(1) - Y(0)] | $w=1$ (å‡åŒ€) | å…¨ä½“ |\n| ATT | E[Y(1) - Y(0) \\| T=1] | $w=1$ for T=1 | å¤„ç†ç»„ |\n| ATO | E[Y(1) - Y(0) \\| \\text{overlap}] | $w=e(X)(1-e(X))$ | é‡å åŒºåŸŸ |\n\n**ç›´è§‰ç†è§£**:\n```python\n# e(X) = 0.5 (æœ€å¤§é‡å )  â†’ w = 0.25 (æœ€é«˜æƒé‡)\n# e(X) = 0.1 æˆ– 0.9      â†’ w = 0.09 (ä½æƒé‡)\n# e(X) â†’ 0 æˆ– 1          â†’ w â†’ 0   (æ¥è¿‘é›¶æƒé‡)\n```\n\n**ä¸šåŠ¡å«ä¹‰**:\n- **ATO å…³æ³¨**: å¤„ç†ç»„å’Œæ§åˆ¶ç»„éƒ½æœ‰ä»£è¡¨æ€§çš„äººç¾¤\n- **å®é™…åº”ç”¨**: å¦‚æœåªåœ¨é‡å äººç¾¤æ¨å¹¿ç­–ç•¥ï¼ŒATO æ›´æœ‰æ„ä¹‰\n- **å¤–æ¨æ€§**: ATO ä¸èƒ½å¤–æ¨åˆ°æç«¯äººç¾¤\n\n**ä»€ä¹ˆæ—¶å€™ç”¨ ATO**:\n1. æç«¯å€¾å‘å¾—åˆ†æ ·æœ¬ä¸å¯ä¿¡ï¼ˆæƒé‡çˆ†ç‚¸ï¼‰\n2. ä¸šåŠ¡åªå…³å¿ƒæœ‰å¯¹æ¯”ä»·å€¼çš„äººç¾¤\n3. è¿½æ±‚ä¼°è®¡çš„ç¨³å®šæ€§è€Œéè¦†ç›–å…¨ä½“\n\n---\n\n### é—®é¢˜ 4: å®æˆ˜åœºæ™¯ - å‘åˆ¸ç­–ç•¥\n\n**é¢è¯•å®˜**: ä½ è´Ÿè´£ä¸€ä¸ªå‘åˆ¸é¡¹ç›®ï¼Œå‘ç°é«˜ä»·å€¼ç”¨æˆ·å‡ ä¹éƒ½è¢«å‘åˆ¸ï¼ˆe(X)â†’1ï¼‰ï¼Œä½ä»·å€¼ç”¨æˆ·å‡ ä¹ä¸å‘åˆ¸ï¼ˆe(X)â†’0ï¼‰ã€‚ä½ ä¼šæ€ä¹ˆåŠï¼Ÿ\n\n**å‚è€ƒç­”æ¡ˆ**:\n\n**Step 1: ç†è§£ä¸šåŠ¡é€»è¾‘**\n```\nä¸ºä»€ä¹ˆå‡ºç°æç«¯å€¾å‘å¾—åˆ†ï¼Ÿ\n- å¯èƒ½æ˜¯æœ‰æ„ä¸ºä¹‹çš„ä¸šåŠ¡ç­–ç•¥ï¼ˆé«˜ä»·å€¼ç”¨æˆ·ä¼˜å…ˆï¼‰\n- ä¹Ÿå¯èƒ½æ˜¯å†å²ç³»ç»Ÿçš„Bug\n```\n\n**Step 2: ç¡®å®šåˆ†æç›®æ ‡**\n\n| ç›®æ ‡ | æ–¹æ³• | è¯´æ˜ |\n|------|------|------|\n| è¯„ä¼°å½“å‰ç­–ç•¥æ•ˆæœ | ATT (å¤„ç†ç»„æ•ˆåº”) | åªçœ‹è¢«å‘åˆ¸ç”¨æˆ· |\n| ä¼˜åŒ–å‘åˆ¸ç­–ç•¥ | ATE | éœ€è¦å¤–æ¨åˆ°æ‰€æœ‰ç”¨æˆ· |\n| åœ¨å¯æ§èŒƒå›´ä¼˜åŒ– | ATO | åªåœ¨é‡å åŒºåŸŸä¼˜åŒ– |\n\n**Step 3: æŠ€æœ¯æ–¹æ¡ˆ**\n\n```python\n# æ–¹æ¡ˆA: å¦‚æœç›®æ ‡æ˜¯ ATT (è¯„ä¼°å½“å‰ç­–ç•¥)\n# åªéœ€è¦å¤„ç†ç»„å’Œå¯æ¯”çš„æ§åˆ¶ç»„\nate_att = ipw_with_trimming(Y, T, e, trim_threshold=0.1)\n\n# æ–¹æ¡ˆB: å¦‚æœç›®æ ‡æ˜¯ ATE (ä¼˜åŒ–ç­–ç•¥)\n# éœ€è¦å¤–æ¨ï¼Œä½†æç«¯æ ·æœ¬ä¸å¯ä¿¡\n# é€‰é¡¹1: Trimming (ä¸¢å¼ƒæç«¯æ ·æœ¬ï¼Œå¯èƒ½æœ‰å)\n# é€‰é¡¹2: Overlap Weights (åªä¼°è®¡é‡å åŒºåŸŸ)\n# é€‰é¡¹3: åšä¸€ä¸ª RCT è¡¥å……æç«¯åŒºåŸŸçš„æ•°æ® â­\n\n# æ–¹æ¡ˆC: åŒç¨³å¥æ–¹æ³•\n# åŒæ—¶å»ºæ¨¡å€¾å‘å¾—åˆ†å’Œç»“æœæ¨¡å‹\nfrom econml.dr import DRLearner\n```\n\n**Step 4: å‘ä¸šåŠ¡æ–¹æ±‡æŠ¥**\n\n\"å½“å‰æ•°æ®åªèƒ½å¯é ä¼°è®¡ä¸­ç­‰ä»·å€¼ç”¨æˆ·çš„æ•ˆåº”ã€‚å¦‚æœè¦è¯„ä¼°å¯¹é«˜/ä½ä»·å€¼ç”¨æˆ·çš„æ•ˆåº”ï¼Œå»ºè®®ï¼š\n1. çŸ­æœŸï¼šä½¿ç”¨ Overlap Weights ä¼°è®¡é‡å äººç¾¤æ•ˆåº”\n2. é•¿æœŸï¼šè®¾è®¡ RCT è¡¥å……æç«¯ç”¨æˆ·æ•°æ®\"\n\n**å…³é”®è¦ç‚¹**:\n- æŠ€æœ¯æ–¹æ¡ˆè¦é…åˆä¸šåŠ¡ç›®æ ‡\n- è¯šå®æ²Ÿé€šæ–¹æ³•çš„å±€é™æ€§\n- æå‡ºæ•°æ®æ”¶é›†å»ºè®®",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}