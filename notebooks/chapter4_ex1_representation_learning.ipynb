{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Exercise 1: è¡¨ç¤ºå­¦ä¹ åŸºç¡€ - ä¸ºæ·±åº¦å› æœæ¨¡å‹é“ºè·¯\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ è¡¨ç¤º (Representation Learning)\n",
    "2. æŒæ¡ç®€å•çš„ç¥ç»ç½‘ç»œç‰¹å¾æå–\n",
    "3. ç†è§£å¤„ç†ç»„å’Œå¯¹ç…§ç»„è¡¨ç¤ºçš„å·®å¼‚\n",
    "4. ä¸ºæ·±åº¦å› æœæ¨¡å‹æ‰“ä¸‹åŸºç¡€\n",
    "\n",
    "---\n",
    "\n",
    "## ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°æ·±åº¦å› æœæ¨¡å‹\n",
    "\n",
    "### ä¼ ç»Ÿæ–¹æ³•çš„å±€é™\n",
    "\n",
    "æˆ‘ä»¬ä¹‹å‰å­¦ä¹ çš„ S-Learnerã€T-Learner ç­‰æ–¹æ³•éƒ½ä¾èµ–äº **æ‰‹å·¥ç‰¹å¾**ã€‚ä½†ç°å®ä¸–ç•Œä¸­ï¼š\n",
    "\n",
    "- ç‰¹å¾å¯èƒ½æ˜¯éçº¿æ€§çš„\n",
    "- çœŸæ­£æœ‰ç”¨çš„ä¿¡æ¯å¯èƒ½éšè—åœ¨ç‰¹å¾çš„å¤æ‚ç»„åˆä¸­\n",
    "- é«˜ç»´æ•°æ® (å¦‚å›¾åƒã€æ–‡æœ¬) æ— æ³•ç›´æ¥ä½¿ç”¨\n",
    "\n",
    "### è¡¨ç¤ºå­¦ä¹ çš„é­”åŠ›\n",
    "\n",
    "**è¡¨ç¤ºå­¦ä¹  (Representation Learning)** è®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æœ€æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºï¼š\n",
    "\n",
    "$$X \\xrightarrow{\\text{ç¥ç»ç½‘ç»œ}} \\Phi(X) \\xrightarrow{\\text{å› æœæ¨¡å‹}} Y(0), Y(1)$$\n",
    "\n",
    "è¿™ä¸ª $\\Phi(X)$ å°±æ˜¯å­¦ä¹ åˆ°çš„ \"è¡¨ç¤º\"ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæ´»åŒ–ç±»æ¯”ï¼šç¿»è¯‘å™¨\n",
    "\n",
    "æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªåªæ‡‚ä¸­æ–‡çš„äººï¼Œè¦åˆ¤æ–­ä¸€ç¯‡è‹±æ–‡è®ºæ–‡çš„è´¨é‡ã€‚\n",
    "\n",
    "**ä¼ ç»Ÿæ–¹æ³•**ï¼š\n",
    "- æ‰‹å·¥æå–ç‰¹å¾ï¼šå•è¯æ•°ã€å¥å­é•¿åº¦ã€æ ‡ç‚¹ç¬¦å·æ•°...\n",
    "- è¿™äº›ç‰¹å¾å¯èƒ½å’Œè®ºæ–‡è´¨é‡å…³ç³»ä¸å¤§\n",
    "\n",
    "**è¡¨ç¤ºå­¦ä¹ æ–¹æ³•**ï¼š\n",
    "- å…ˆç”¨ç¥ç»ç½‘ç»œæŠŠè‹±æ–‡ \"ç¿»è¯‘\" æˆä¸€ç§é€šç”¨è¡¨ç¤º\n",
    "- è¿™ä¸ªè¡¨ç¤ºæ•è·äº†è®ºæ–‡çš„è¯­ä¹‰ä¿¡æ¯\n",
    "- ç„¶åç”¨è¿™ä¸ªè¡¨ç¤ºæ¥é¢„æµ‹è®ºæ–‡è´¨é‡\n",
    "\n",
    "ç¥ç»ç½‘ç»œå°±åƒä¸€ä¸ª **æ™ºèƒ½ç¿»è¯‘å™¨**ï¼ŒæŠŠåŸå§‹æ•°æ®ç¿»è¯‘æˆå¯¹ä»»åŠ¡æœ€æœ‰ç”¨çš„å½¢å¼ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: éçº¿æ€§æ•°æ®çš„æŒ‘æˆ˜\n",
    "\n",
    "### ä¸ºä»€ä¹ˆçº¿æ€§æ¨¡å‹ä¸å¤Ÿç”¨?\n",
    "\n",
    "å‡è®¾çœŸå®çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹æ˜¯éçº¿æ€§çš„ï¼š\n",
    "\n",
    "- åŸå§‹ç‰¹å¾: $X_1, X_2 \\sim N(0, 1)$\n",
    "- çœŸæ­£æœ‰ç”¨çš„ç‰¹å¾: $\\Phi_1 = \\sin(X_1)$, $\\Phi_2 = X_1 \\times X_2$\n",
    "- å¤„ç†åˆ†é…: $P(T=1|X) = \\sigma(\\Phi_1 + \\Phi_2)$\n",
    "- ç»“æœ: $Y = 1 + 2T + \\Phi_1 + 0.5\\Phi_2 + \\epsilon$\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬åªç”¨ $X_1, X_2$ å»ºæ¨¡ï¼Œä¼šé”™è¿‡å…³é”®çš„éçº¿æ€§å…³ç³»ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.1: ç”Ÿæˆéçº¿æ€§æ•°æ®\n",
    "\n",
    "def generate_nonlinear_data(\n",
    "    n: int = 1000,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆéçº¿æ€§æ•°æ®: åŸå§‹ç‰¹å¾æ— æ³•ç›´æ¥æ•è·å› æœå…³ç³»\n",
    "    \n",
    "    DGP (Data Generating Process):\n",
    "    - X1, X2 ~ N(0, 1)\n",
    "    - çœŸå®æœ‰ç”¨ç‰¹å¾: Phi1 = sin(X1), Phi2 = X1 * X2\n",
    "    - T ~ Bernoulli(sigmoid(Phi1 + Phi2))\n",
    "    - Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise\n",
    "    \n",
    "    TODO: å®Œæˆæ•°æ®ç”Ÿæˆ\n",
    "    \n",
    "    Returns:\n",
    "        (X, T, Y) - ç‰¹å¾çŸ©é˜µã€å¤„ç†ã€ç»“æœ\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # TODO: ç”ŸæˆåŸå§‹ç‰¹å¾ X1, X2\n",
    "    X1 = None  # ä½ çš„ä»£ç : np.random.randn(n)\n",
    "    X2 = None  # ä½ çš„ä»£ç : np.random.randn(n)\n",
    "    \n",
    "    # TODO: è®¡ç®—æœ‰ç”¨ç‰¹å¾\n",
    "    # Phi1 = sin(X1), Phi2 = X1 * X2\n",
    "    Phi1 = None  # ä½ çš„ä»£ç : np.sin(X1)\n",
    "    Phi2 = None  # ä½ çš„ä»£ç : X1 * X2\n",
    "    \n",
    "    # TODO: ç”Ÿæˆå¤„ç† T (é€šè¿‡ sigmoid å‡½æ•°)\n",
    "    # logit = Phi1 + 0.5*Phi2\n",
    "    # P(T=1) = sigmoid(logit) = 1 / (1 + exp(-logit))\n",
    "    logit = None  # ä½ çš„ä»£ç \n",
    "    propensity = None  # ä½ çš„ä»£ç : 1 / (1 + np.exp(-logit))\n",
    "    T = None  # ä½ çš„ä»£ç : np.random.binomial(1, propensity, n)\n",
    "    \n",
    "    # TODO: ç”Ÿæˆç»“æœ Y\n",
    "    # Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise (noise ~ N(0, 0.5))\n",
    "    noise = np.random.randn(n) * 0.5\n",
    "    Y = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    X = np.column_stack([X1, X2])\n",
    "    \n",
    "    return X, T, Y\n",
    "\n",
    "# æµ‹è¯•\n",
    "X, T, Y = generate_nonlinear_data()\n",
    "if X is not None and X[0, 0] is not None:\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}, T={T.shape}, Y={Y.shape}\")\n",
    "    print(f\"å¤„ç†æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "    print(f\"å¹³å‡ç»“æœ: {Y.mean():.4f}\")\n",
    "    print(f\"\\nçœŸå® ATE = 2.0 (è¿™æ˜¯æˆ‘ä»¬è®¾å®šçš„)\")\n",
    "else:\n",
    "    print(\"[æœªå®Œæˆ] è¯·å®Œæˆ generate_nonlinear_data å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.1 ç»­: æœ´ç´ çº¿æ€§ä¼°è®¡\n",
    "\n",
    "def naive_linear_estimation(X, T, Y):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨çº¿æ€§å›å½’ä¼°è®¡ ATE (ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾)\n",
    "    \n",
    "    TODO: å®ç°çº¿æ€§å›å½’ä¼°è®¡\n",
    "    \n",
    "    æç¤º: Y = beta0 + beta1*T + beta2*X1 + beta3*X2 + epsilon\n",
    "          beta1 å°±æ˜¯ ATE ä¼°è®¡\n",
    "    \"\"\"\n",
    "    # TODO: æ„é€ ç‰¹å¾çŸ©é˜µ [T, X]\n",
    "    features = None  # ä½ çš„ä»£ç : np.column_stack([T, X])\n",
    "    \n",
    "    if features is None:\n",
    "        return None\n",
    "    \n",
    "    # TODO: è®­ç»ƒçº¿æ€§å›å½’\n",
    "    model = LinearRegression()\n",
    "    model.fit(features, Y)\n",
    "    \n",
    "    # TODO: è¿”å› T çš„ç³»æ•°ä½œä¸º ATE ä¼°è®¡\n",
    "    ate_estimate = model.coef_[0]  # T æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾\n",
    "    \n",
    "    return ate_estimate\n",
    "\n",
    "# æµ‹è¯•\n",
    "if X is not None:\n",
    "    ate_linear = naive_linear_estimation(X, T, Y)\n",
    "    if ate_linear is not None:\n",
    "        print(f\"çº¿æ€§å›å½’ä¼°è®¡çš„ ATE: {ate_linear:.4f}\")\n",
    "        print(f\"çœŸå® ATE: 2.0\")\n",
    "        print(f\"è¯¯å·®: {abs(ate_linear - 2.0):.4f}\")\n",
    "        print(f\"\\nç”±äºå¿½ç•¥äº†éçº¿æ€§å…³ç³»ï¼Œçº¿æ€§ä¼°è®¡å¯èƒ½æœ‰åå·®!\")\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ naive_linear_estimation å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: ç®€å•çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n",
    "\n",
    "### ç¥ç»ç½‘ç»œæ¶æ„\n",
    "\n",
    "```\n",
    "åŸå§‹ç‰¹å¾ X (2ç»´)\n",
    "     |\n",
    "     â†“\n",
    "[Hidden Layer] (20 neurons, ReLU)\n",
    "     |\n",
    "     â†“\n",
    "è¡¨ç¤º Î¦(X) (10ç»´)\n",
    "```\n",
    "\n",
    "è¿™ä¸ªç½‘ç»œå°† 2 ç»´çš„åŸå§‹ç‰¹å¾è½¬æ¢æˆ 10 ç»´çš„ \"è¡¨ç¤º\"ã€‚å¸Œæœ›è¿™ä¸ªè¡¨ç¤ºèƒ½è‡ªåŠ¨æ•è·éçº¿æ€§å…³ç³»ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.2: ç®€å•çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n",
    "\n",
    "class SimpleRepresentation(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€å•çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n",
    "    \n",
    "    X -> [Hidden Layer] -> Phi(X) (å­¦ä¹ åˆ°çš„è¡¨ç¤º)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, repr_dim: int = 10, hidden_dim: int = 20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: å®šä¹‰ç½‘ç»œå±‚\n",
    "        # æç¤º: Input -> Linear -> ReLU -> Linear -> Output\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç :\n",
    "            # nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, repr_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def train_representation(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    repr_dim: int = 10,\n",
    "    n_epochs: int = 100\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    è®­ç»ƒè¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n",
    "    \n",
    "    ç›®æ ‡: å­¦ä¹ èƒ½é¢„æµ‹ Y çš„è¡¨ç¤º Phi(X)\n",
    "    \n",
    "    TODO: å®Œæˆè®­ç»ƒè¿‡ç¨‹\n",
    "    \n",
    "    è®­ç»ƒç­–ç•¥:\n",
    "    1. Phi(X) -> [Linear] -> Y_pred\n",
    "    2. æœ€å°åŒ– MSE(Y, Y_pred)\n",
    "    3. å­¦åˆ°çš„ Phi(X) å¯ç”¨äºåç»­å› æœæ¨æ–­\n",
    "    \"\"\"\n",
    "    \n",
    "    # è½¬æ¢ä¸º Tensor\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    Y_tensor = torch.FloatTensor(Y).unsqueeze(1)\n",
    "    \n",
    "    # æ¨¡å‹\n",
    "    repr_model = SimpleRepresentation(input_dim=X.shape[1], repr_dim=repr_dim)\n",
    "    \n",
    "    # TODO: å®šä¹‰é¢„æµ‹å¤´ (Phi(X) -> Y)\n",
    "    prediction_head = None  # ä½ çš„ä»£ç : nn.Linear(repr_dim, 1)\n",
    "    \n",
    "    if prediction_head is None:\n",
    "        return repr_model  # è¿”å›æœªè®­ç»ƒçš„æ¨¡å‹\n",
    "    \n",
    "    # TODO: å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "    params = list(repr_model.parameters()) + list(prediction_head.parameters())\n",
    "    optimizer = None  # ä½ çš„ä»£ç : optim.Adam(params, lr=0.01)\n",
    "    criterion = None  # ä½ çš„ä»£ç : nn.MSELoss()\n",
    "    \n",
    "    # TODO: è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(n_epochs):\n",
    "        # ä½ çš„ä»£ç :\n",
    "        # optimizer.zero_grad()\n",
    "        # phi = repr_model(X_tensor)\n",
    "        # y_pred = prediction_head(phi)\n",
    "        # loss = criterion(y_pred, Y_tensor)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        pass\n",
    "    \n",
    "    return repr_model\n",
    "\n",
    "# æµ‹è¯•\n",
    "if X is not None:\n",
    "    repr_model = train_representation(X, T, Y, repr_dim=5, n_epochs=50)\n",
    "    print(\"è¡¨ç¤ºå­¦ä¹ æ¨¡å‹åˆ›å»ºå®Œæˆ\")\n",
    "    print(f\"æ¨¡å‹ç»“æ„:\")\n",
    "    print(repr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: å¯è§†åŒ–è¡¨ç¤ºç©ºé—´\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¦å¯è§†åŒ–?\n",
    "\n",
    "é€šè¿‡å¯è§†åŒ–å­¦åˆ°çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ï¼š\n",
    "\n",
    "1. **è¡¨ç¤ºæ˜¯å¦æœ‰æ„ä¹‰?** ç›¸ä¼¼çš„æ ·æœ¬åº”è¯¥åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ¥è¿‘\n",
    "2. **å¤„ç†ç»„å’Œæ§åˆ¶ç»„æ˜¯å¦å¯åŒºåˆ†?** å¦‚æœå®Œå…¨åˆ†ç¦»ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜\n",
    "3. **è¡¨ç¤ºæ˜¯å¦æ•è·äº†é‡è¦ä¿¡æ¯?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.3: å¯è§†åŒ–è¡¨ç¤ºç©ºé—´\n",
    "\n",
    "def visualize_representation(\n",
    "    repr_model: nn.Module,\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å­¦åˆ°çš„è¡¨ç¤ºç©ºé—´\n",
    "    \n",
    "    TODO:\n",
    "    1. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æå–è¡¨ç¤º Phi(X)\n",
    "    2. å¦‚æœç»´åº¦ > 2, ä½¿ç”¨ PCA é™ç»´åˆ° 2D\n",
    "    3. è¿”å› 2D è¡¨ç¤ºç”¨äºç»˜å›¾\n",
    "    \n",
    "    Returns:\n",
    "        (phi_2d, T) - 2D è¡¨ç¤ºå’Œå¤„ç†æ ‡ç­¾\n",
    "    \"\"\"\n",
    "    \n",
    "    repr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        \n",
    "        # TODO: æå–è¡¨ç¤º\n",
    "        phi = None  # ä½ çš„ä»£ç : repr_model(X_tensor)\n",
    "        \n",
    "        if phi is None:\n",
    "            return None, T\n",
    "        \n",
    "        phi = phi.numpy()\n",
    "    \n",
    "    # TODO: å¦‚æœç»´åº¦ > 2, ä½¿ç”¨ PCA é™ç»´\n",
    "    if phi.shape[1] > 2:\n",
    "        # ä½ çš„ä»£ç :\n",
    "        # pca = PCA(n_components=2)\n",
    "        # phi = pca.fit_transform(phi)\n",
    "        pass\n",
    "    \n",
    "    return phi, T\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "if X is not None and repr_model is not None:\n",
    "    phi_2d, _ = visualize_representation(repr_model, X, T)\n",
    "    \n",
    "    if phi_2d is not None:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # åŸå§‹ç‰¹å¾ç©ºé—´\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(X[T==0, 0], X[T==0, 1], alpha=0.5, label='Control', s=10)\n",
    "        plt.scatter(X[T==1, 0], X[T==1, 1], alpha=0.5, label='Treated', s=10)\n",
    "        plt.xlabel('X1')\n",
    "        plt.ylabel('X2')\n",
    "        plt.title('Original Feature Space')\n",
    "        plt.legend()\n",
    "        \n",
    "        # è¡¨ç¤ºç©ºé—´\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if phi_2d.shape[1] >= 2:\n",
    "            plt.scatter(phi_2d[T==0, 0], phi_2d[T==0, 1], alpha=0.5, label='Control', s=10)\n",
    "            plt.scatter(phi_2d[T==1, 0], phi_2d[T==1, 1], alpha=0.5, label='Treated', s=10)\n",
    "            plt.xlabel('Phi1')\n",
    "            plt.ylabel('Phi2')\n",
    "        plt.title('Learned Representation Space')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ visualize_representation å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: è¡¨ç¤ºå¹³è¡¡æ€§æ£€æŸ¥\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯è¡¨ç¤ºå¹³è¡¡?\n",
    "\n",
    "åœ¨å› æœæ¨æ–­ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å¤„ç†ç»„å’Œæ§åˆ¶ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­çš„åˆ†å¸ƒå°½å¯èƒ½ç›¸ä¼¼ã€‚\n",
    "\n",
    "å¦‚æœä¸¤ç»„åˆ†å¸ƒå·®å¼‚å¤ªå¤§ï¼Œè¯´æ˜å­˜åœ¨ **åå˜é‡ä¸å¹³è¡¡**ï¼Œå¯èƒ½å¯¼è‡´ä¼°è®¡åå·®ã€‚\n",
    "\n",
    "### å¹³è¡¡æ€§æŒ‡æ ‡\n",
    "\n",
    "1. **SMD (æ ‡å‡†åŒ–å‡å€¼å·®)**: $\\frac{|\\bar{\\Phi}_T - \\bar{\\Phi}_C|}{\\sigma_{\\Phi}}$\n",
    "   - SMD < 0.1 é€šå¸¸è¢«è®¤ä¸ºæ˜¯è‰¯å¥½å¹³è¡¡\n",
    "\n",
    "2. **MMD (æœ€å¤§å‡å€¼å·®å¼‚)**: $||\\bar{\\Phi}_T - \\bar{\\Phi}_C||^2$\n",
    "   - è¡¡é‡ä¸¤ç»„åˆ†å¸ƒçš„æ•´ä½“å·®å¼‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.4: è¡¨ç¤ºå¹³è¡¡æ€§æ£€æŸ¥\n",
    "\n",
    "def check_representation_balance(\n",
    "    phi: np.ndarray,\n",
    "    T: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥è¡¨ç¤ºåœ¨å¤„ç†ç»„å’Œå¯¹ç…§ç»„ä¹‹é—´çš„å¹³è¡¡æ€§\n",
    "    \n",
    "    å¹³è¡¡æ€§æŒ‡æ ‡:\n",
    "    1. æ ‡å‡†åŒ–å‡å€¼å·® (SMD): |mean(Phi_T) - mean(Phi_C)| / std(Phi)\n",
    "    2. MMD (Maximum Mean Discrepancy): ||mean(Phi_T) - mean(Phi_C)||^2\n",
    "    \n",
    "    TODO: è®¡ç®—å¹³è¡¡æ€§æŒ‡æ ‡\n",
    "    \n",
    "    Returns:\n",
    "        dict with balance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # åˆ†ç¦»å¤„ç†ç»„å’Œå¯¹ç…§ç»„\n",
    "    phi_treated = phi[T == 1]\n",
    "    phi_control = phi[T == 0]\n",
    "    \n",
    "    # TODO: è®¡ç®— SMD (å¯¹æ¯ä¸ªç»´åº¦)\n",
    "    # SMD_j = |mean(Phi_T[:, j]) - mean(Phi_C[:, j])| / std(Phi[:, j])\n",
    "    mean_t = phi_treated.mean(axis=0)\n",
    "    mean_c = phi_control.mean(axis=0)\n",
    "    std_all = phi.std(axis=0) + 1e-8  # é¿å…é™¤é›¶\n",
    "    \n",
    "    smd = None  # ä½ çš„ä»£ç : np.abs(mean_t - mean_c) / std_all\n",
    "    \n",
    "    # TODO: è®¡ç®— MMD (ç®€åŒ–ç‰ˆ: æ¬§æ°è·ç¦»)\n",
    "    # MMD = ||mean(Phi_T) - mean(Phi_C)||^2\n",
    "    mmd = None  # ä½ çš„ä»£ç : np.sum((mean_t - mean_c)**2)\n",
    "    \n",
    "    return {\n",
    "        'smd_mean': np.mean(smd) if smd is not None else None,\n",
    "        'smd_max': np.max(smd) if smd is not None else None,\n",
    "        'mmd': mmd\n",
    "    }\n",
    "\n",
    "# æµ‹è¯•\n",
    "if X is not None and repr_model is not None:\n",
    "    repr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        phi = repr_model(torch.FloatTensor(X)).numpy()\n",
    "    \n",
    "    balance = check_representation_balance(phi, T)\n",
    "    if balance['mmd'] is not None:\n",
    "        print(f\"å¹³å‡ SMD: {balance['smd_mean']:.4f}\")\n",
    "        print(f\"æœ€å¤§ SMD: {balance['smd_max']:.4f}\")\n",
    "        print(f\"MMD: {balance['mmd']:.4f}\")\n",
    "        print(f\"\\næç¤º: SMD < 0.1 è¡¨ç¤ºè‰¯å¥½å¹³è¡¡\")\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ check_representation_balance å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: å¯¹æ¯”å®éªŒ - çº¿æ€§ vs è¡¨ç¤ºå­¦ä¹ \n",
    "\n",
    "è®©æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹ï¼šä½¿ç”¨è¡¨ç¤ºå­¦ä¹ æ˜¯å¦èƒ½æ”¹å–„ ATE ä¼°è®¡ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.5: å¯¹æ¯”çº¿æ€§æ–¹æ³•å’Œè¡¨ç¤ºå­¦ä¹ \n",
    "\n",
    "def compare_linear_vs_learned(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    true_ate: float = 2.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”çº¿æ€§æ–¹æ³•å’Œè¡¨ç¤ºå­¦ä¹ æ–¹æ³•\n",
    "    \n",
    "    TODO:\n",
    "    1. ä½¿ç”¨çº¿æ€§å›å½’ (åŸå§‹ç‰¹å¾) ä¼°è®¡ ATE\n",
    "    2. è®­ç»ƒè¡¨ç¤ºå­¦ä¹ æ¨¡å‹\n",
    "    3. ä½¿ç”¨å­¦åˆ°çš„è¡¨ç¤ºä¼°è®¡ ATE\n",
    "    4. æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„è¯¯å·®\n",
    "    \n",
    "    Returns:\n",
    "        dict with comparison results\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. çº¿æ€§ä¼°è®¡\n",
    "    linear_ate = naive_linear_estimation(X, T, Y)\n",
    "    \n",
    "    # 2. è¡¨ç¤ºå­¦ä¹ \n",
    "    repr_model = train_representation(X, T, Y, repr_dim=10, n_epochs=200)\n",
    "    \n",
    "    # 3. æå–è¡¨ç¤ºå¹¶ä¼°è®¡ ATE\n",
    "    repr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        phi = repr_model(torch.FloatTensor(X)).numpy()\n",
    "    \n",
    "    # ç”¨ Phi æ›¿ä»£ X è¿›è¡Œçº¿æ€§å›å½’\n",
    "    features_phi = np.column_stack([T, phi])\n",
    "    model = LinearRegression()\n",
    "    model.fit(features_phi, Y)\n",
    "    learned_ate = model.coef_[0]  # T çš„ç³»æ•°\n",
    "    \n",
    "    return {\n",
    "        'linear_ate': linear_ate,\n",
    "        'linear_error': abs(linear_ate - true_ate) if linear_ate else None,\n",
    "        'learned_ate': learned_ate,\n",
    "        'learned_error': abs(learned_ate - true_ate) if learned_ate else None,\n",
    "        'true_ate': true_ate\n",
    "    }\n",
    "\n",
    "# è¿è¡Œå¯¹æ¯”\n",
    "if X is not None:\n",
    "    results = compare_linear_vs_learned(X, T, Y, true_ate=2.0)\n",
    "    if results['linear_ate'] is not None:\n",
    "        print(\"=\" * 40)\n",
    "        print(\"çº¿æ€§æ–¹æ³• vs è¡¨ç¤ºå­¦ä¹  å¯¹æ¯”\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"çœŸå® ATE: {results['true_ate']:.4f}\")\n",
    "        print(f\"\\nçº¿æ€§ä¼°è®¡: {results['linear_ate']:.4f} (è¯¯å·®: {results['linear_error']:.4f})\")\n",
    "        if results['learned_ate'] is not None:\n",
    "            print(f\"è¡¨ç¤ºå­¦ä¹ ä¼°è®¡: {results['learned_ate']:.4f} (è¯¯å·®: {results['learned_error']:.4f})\")\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆæ‰€æœ‰å‰åºç»ƒä¹ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒé¢˜\n",
    "\n",
    "### 1. ä¸ºä»€ä¹ˆçº¿æ€§æ¨¡å‹åœ¨éçº¿æ€§æ•°æ®ä¸Šè¡¨ç°ä¸å¥½?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 2. è¡¨ç¤ºå­¦ä¹ å¦‚ä½•å¸®åŠ©å› æœæ¨æ–­?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 3. ä»€ä¹ˆæ˜¯ \"è¡¨ç¤ºå¹³è¡¡\" (Representation Balance)? ä¸ºä»€ä¹ˆé‡è¦?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 4. åœ¨æ·±åº¦å› æœæ¨¡å‹ä¸­ï¼Œå…±äº«è¡¨ç¤ºå±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆ?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 5. å¦‚æœå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¡¨ç¤ºåˆ†å¸ƒå®Œå…¨ä¸é‡å ï¼Œä¼šæœ‰ä»€ä¹ˆé—®é¢˜?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "| æ¦‚å¿µ | è¦ç‚¹ |\n",
    "|------|------|\n",
    "| **è¡¨ç¤ºå­¦ä¹ ** | è®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤º |\n",
    "| **éçº¿æ€§å…³ç³»** | åŸå§‹ç‰¹å¾çš„å¤æ‚ç»„åˆå¯èƒ½æ‰æ˜¯çœŸæ­£æœ‰ç”¨çš„ |\n",
    "| **è¡¨ç¤ºå¹³è¡¡** | å¤„ç†ç»„å’Œæ§åˆ¶ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­åº”è¯¥åˆ†å¸ƒç›¸ä¼¼ |\n",
    "| **SMD** | è¡¡é‡å•ä¸ªç»´åº¦çš„ä¸å¹³è¡¡ç¨‹åº¦ |\n",
    "| **MMD** | è¡¡é‡æ•´ä½“åˆ†å¸ƒå·®å¼‚ |\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦?\n",
    "\n",
    "è¡¨ç¤ºå­¦ä¹ æ˜¯æ·±åº¦å› æœæ¨¡å‹çš„åŸºç¡€ã€‚åœ¨ä¸‹ä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  **TARNet** â€”â€” ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå› æœæ¨æ–­è®¾è®¡çš„ç¥ç»ç½‘ç»œï¼\n",
    "\n",
    "---\n",
    "\n",
    "*æ­å–œä½ å®Œæˆäº†è¡¨ç¤ºå­¦ä¹ åŸºç¡€çš„å­¦ä¹ ï¼* ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
