{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å› æœå‘ç° - ä»æ•°æ®ä¸­å­¦ä¹ å› æœç»“æ„\n",
    "\n",
    "## ä¸šåŠ¡åœºæ™¯ï¼šç”µå•†å¹³å°ç”¨æˆ·æµå¤±é¢„è­¦\n",
    "\n",
    "ä½ æ˜¯æŸç”µå•†å¹³å°çš„æ•°æ®ç§‘å­¦å®¶ã€‚äº§å“å›¢é˜Ÿæ”¶é›†äº†ç”¨æˆ·çš„å¤šä¸ªè¡Œä¸ºç‰¹å¾ï¼š\n",
    "- æµè§ˆæ—¶é•¿ (BrowseTime)\n",
    "- è´­ä¹°é¢‘æ¬¡ (PurchaseFreq)\n",
    "- å®¢æœæŠ•è¯‰ (Complaints)\n",
    "- ä¼˜æƒ åˆ¸ä½¿ç”¨ (CouponUsage)\n",
    "- ä¼šå‘˜ç­‰çº§ (MemberLevel)\n",
    "- æ˜¯å¦æµå¤± (Churn)\n",
    "\n",
    "äº§å“ç»ç†é—®ä½ ï¼š**\"è¿™äº›å› ç´ ä¹‹é—´åˆ°åº•æ˜¯ä»€ä¹ˆå› æœå…³ç³»ï¼Ÿåº”è¯¥ä¼˜å…ˆæ”¹å–„å“ªä¸ªç¯èŠ‚ï¼Ÿ\"**\n",
    "\n",
    "ä¼ ç»Ÿå›å½’åªèƒ½å‘Šè¯‰ä½ ç›¸å…³æ€§ï¼Œè€Œ**å› æœå‘ç°**èƒ½å¸®ä½ æ‰¾åˆ°çœŸæ­£çš„å› æœç»“æ„ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£å› æœå‘ç°ä¸å› æœæ¨æ–­çš„åŒºåˆ«\n",
    "2. æŒæ¡ PC ç®—æ³•çš„åŸç†å’Œå®ç°\n",
    "3. äº†è§£ LiNGAMã€GES ç­‰å…¶ä»–æ–¹æ³•\n",
    "4. å­¦ä¼šè¯„ä¼°å’Œè§£é‡Šå› æœå›¾\n",
    "5. ç†è§£å› æœå‘ç°çš„å±€é™æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å› æœå‘ç°åº“\n",
    "try:\n",
    "    from causallearn.search.ConstraintBased.PC import pc\n",
    "    from causallearn.search.ScoreBased.GES import ges\n",
    "    from causallearn.utils.GraphUtils import GraphUtils\n",
    "    from causallearn.utils.cit import chisq, fisherz\n",
    "except ImportError:\n",
    "    print(\"æ­£åœ¨å®‰è£… causal-learn...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install causal-learn -q\n",
    "    from causallearn.search.ConstraintBased.PC import pc\n",
    "    from causallearn.search.ScoreBased.GES import ges\n",
    "    from causallearn.utils.GraphUtils import GraphUtils\n",
    "    from causallearn.utils.cit import chisq, fisherz\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotly é…ç½®\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: å› æœå‘ç°çš„é—®é¢˜\n",
    "\n",
    "## 1.1 ä»ç›¸å…³åˆ°å› æœ\n",
    "\n",
    "### é—®é¢˜å®šä¹‰\n",
    "\n",
    "**å› æœæ¨æ–­ (Causal Inference)**ï¼š\n",
    "- å·²çŸ¥å› æœå›¾ $G$\n",
    "- ä¼°è®¡å› æœæ•ˆåº” $\\tau = E[Y(1) - Y(0)]$\n",
    "\n",
    "**å› æœå‘ç° (Causal Discovery)**ï¼š\n",
    "- æœªçŸ¥å› æœå›¾\n",
    "- ä»æ•°æ® $D$ ä¸­å­¦ä¹ å› æœç»“æ„ $G$\n",
    "\n",
    "### æ ¸å¿ƒæŒ‘æˆ˜\n",
    "\n",
    "**ç›¸å…³æ€§ä¸ç­‰äºå› æœæ€§**ï¼š\n",
    "$$\n",
    "Corr(X, Y) \\neq 0 \\text{ å¯èƒ½å› ä¸º:}\n",
    "\\begin{cases}\n",
    "X \\rightarrow Y & \\text{(X å¯¼è‡´ Y)} \\\\\n",
    "Y \\rightarrow X & \\text{(Y å¯¼è‡´ X)} \\\\\n",
    "X \\leftarrow Z \\rightarrow Y & \\text{(å…±åŒåŸå› )} \\\\\n",
    "X \\rightarrow Z \\leftarrow Y & \\text{(å…±åŒç»“æœ)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### å¯è¯†åˆ«æ€§\n",
    "\n",
    "**é©¬å°”å¯å¤«ç­‰ä»·ç±» (Markov Equivalence Class)**ï¼š\n",
    "- ç›¸åŒæ¡ä»¶ç‹¬ç«‹æ€§çš„å› æœå›¾é›†åˆ\n",
    "- åªèƒ½å­¦åˆ° CPDAG (Completed Partially DAG)\n",
    "- æŸäº›è¾¹çš„æ–¹å‘æ— æ³•ç¡®å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºï¼šä¸‰ç§ç›¸å…³å…³ç³»\n",
    "def generate_correlation_scenarios():\n",
    "    n = 1000\n",
    "    \n",
    "    # Scenario 1: X -> Y (å› æœ)\n",
    "    X1 = np.random.randn(n)\n",
    "    Y1 = 2 * X1 + np.random.randn(n) * 0.5\n",
    "    \n",
    "    # Scenario 2: X <- Z -> Y (æ··æ·†)\n",
    "    Z = np.random.randn(n)\n",
    "    X2 = Z + np.random.randn(n) * 0.3\n",
    "    Y2 = Z + np.random.randn(n) * 0.3\n",
    "    \n",
    "    # Scenario 3: X -> Z <- Y (å¯¹æ’)\n",
    "    X3 = np.random.randn(n)\n",
    "    Y3 = np.random.randn(n)\n",
    "    Z3 = X3 + Y3 + np.random.randn(n) * 0.3\n",
    "    \n",
    "    return [(X1, Y1, \"X â†’ Y (Causation)\"),\n",
    "            (X2, Y2, \"X â† Z â†’ Y (Confounding)\"),\n",
    "            (X3, Y3, \"X â†’ Z â† Y (Collider)\")]\n",
    "\n",
    "scenarios = generate_correlation_scenarios()\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=[s[2] for s in scenarios]\n",
    ")\n",
    "\n",
    "for i, (X, Y, title) in enumerate(scenarios, 1):\n",
    "    corr = np.corrcoef(X, Y)[0, 1]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X, y=Y, mode='markers',\n",
    "            marker=dict(size=4, color='#2D9CDB', opacity=0.5),\n",
    "            name=f\"Corr = {corr:.3f}\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "    \n",
    "    # æ·»åŠ æ³¨é‡Š\n",
    "    fig.add_annotation(\n",
    "        text=f\"Correlation: {corr:.3f}\",\n",
    "        xref=f\"x{i}\", yref=f\"y{i}\",\n",
    "        x=X.mean(), y=Y.max(),\n",
    "        showarrow=False,\n",
    "        bgcolor=\"lightyellow\"\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"ç›¸å…³æ€§çš„ä¸‰ç§æ¥æº - ä¸ºä»€ä¹ˆéœ€è¦å› æœå‘ç°?\",\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_xaxes(title_text=\"X\")\n",
    "fig.update_yaxes(title_text=\"Y\")\n",
    "fig.show()\n",
    "\n",
    "print(\"ğŸ’¡ å…³é”®æ´å¯Ÿï¼š\")\n",
    "print(\"ä¸‰ä¸ªåœºæ™¯éƒ½æœ‰æ˜¾è‘—ç›¸å…³æ€§ï¼Œä½†å› æœç»“æ„å®Œå…¨ä¸åŒï¼\")\n",
    "print(\"åªæœ‰å› æœå‘ç°æ‰èƒ½åŒºåˆ†å®ƒä»¬ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 å‡†å¤‡ä¸šåŠ¡æ•°æ®\n",
    "\n",
    "è®©æˆ‘ä»¬ç”Ÿæˆç”µå•†ç”¨æˆ·æµå¤±æ•°æ®ï¼Œå…¶ä¸­éšå«äº†çœŸå®çš„å› æœç»“æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ecommerce_data(n=2000):\n",
    "    \"\"\"\n",
    "    çœŸå®å› æœç»“æ„:\n",
    "    MemberLevel -> PurchaseFreq -> BrowseTime\n",
    "                \\                    â†“\n",
    "                 -> CouponUsage -> Churn\n",
    "                         â†‘           â†‘\n",
    "                    Complaints ------â”˜\n",
    "    \"\"\"\n",
    "    # å¤–ç”Ÿå˜é‡\n",
    "    member_level = np.random.choice([1, 2, 3], n, p=[0.5, 0.3, 0.2])\n",
    "    \n",
    "    # ä¼šå‘˜ç­‰çº§å½±å“è´­ä¹°é¢‘æ¬¡\n",
    "    purchase_freq = 2 * member_level + np.random.randn(n) * 1.5\n",
    "    purchase_freq = np.clip(purchase_freq, 0, 15)\n",
    "    \n",
    "    # è´­ä¹°é¢‘æ¬¡å½±å“æµè§ˆæ—¶é•¿\n",
    "    browse_time = 10 + 5 * purchase_freq + np.random.randn(n) * 10\n",
    "    browse_time = np.clip(browse_time, 0, 120)\n",
    "    \n",
    "    # ç‹¬ç«‹çš„æŠ•è¯‰\n",
    "    complaints = np.random.poisson(lam=0.5, size=n)\n",
    "    \n",
    "    # ä¼˜æƒ åˆ¸ä½¿ç”¨å—ä¼šå‘˜ç­‰çº§å’ŒæŠ•è¯‰å½±å“\n",
    "    coupon_usage = 3 * member_level - 2 * complaints + np.random.randn(n) * 2\n",
    "    coupon_usage = np.clip(coupon_usage, 0, 15)\n",
    "    \n",
    "    # æµå¤±å—æµè§ˆæ—¶é•¿ã€ä¼˜æƒ åˆ¸ã€æŠ•è¯‰å½±å“\n",
    "    logit = -2 - 0.05 * browse_time - 0.3 * coupon_usage + 0.8 * complaints\n",
    "    churn_prob = 1 / (1 + np.exp(-logit))\n",
    "    churn = (np.random.rand(n) < churn_prob).astype(int)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'MemberLevel': member_level,\n",
    "        'PurchaseFreq': purchase_freq,\n",
    "        'BrowseTime': browse_time,\n",
    "        'Complaints': complaints,\n",
    "        'CouponUsage': coupon_usage,\n",
    "        'Churn': churn\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "df = generate_ecommerce_data(2000)\n",
    "\n",
    "print(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "print(df.head())\n",
    "print(\"\\nåŸºæœ¬ç»Ÿè®¡:\")\n",
    "print(df.describe())\n",
    "print(f\"\\næµå¤±ç‡: {df['Churn'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›¸å…³æ€§çŸ©é˜µ (è¿™æ˜¯æˆ‘ä»¬èƒ½ç›´æ¥è§‚å¯Ÿåˆ°çš„)\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=corr_matrix.values,\n",
    "    texttemplate='%{text:.2f}',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"Correlation\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"å˜é‡ç›¸å…³æ€§çŸ©é˜µ - ä½†å› æœå…³ç³»æ˜¯ä»€ä¹ˆ?\",\n",
    "    height=500,\n",
    "    xaxis={'side': 'bottom'}\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"â“ é—®é¢˜ï¼š\")\n",
    "print(\"ä»ç›¸å…³æ€§çŸ©é˜µï¼Œä½ èƒ½åˆ¤æ–­å‡ºå› æœæ–¹å‘å—ï¼Ÿ\")\n",
    "print(\"MemberLevel å’Œ PurchaseFreq ç›¸å…³ 0.67ï¼Œä½†è°å¯¼è‡´è°ï¼Ÿ\")\n",
    "print(\"\\nè¿™å°±æ˜¯å› æœå‘ç°è¦è§£å†³çš„é—®é¢˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: åŸºäºçº¦æŸçš„æ–¹æ³• - PC ç®—æ³•\n",
    "\n",
    "## 2.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "PC (Peter-Clark) ç®—æ³•åŸºäº**æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ**å­¦ä¹ å› æœç»“æ„ã€‚\n",
    "\n",
    "### ç†è®ºåŸºç¡€\n",
    "\n",
    "**d-åˆ†ç¦» (d-separation)**ï¼š\n",
    "- å¦‚æœ $X \\perp\\!\\!\\!\\perp Y | Z$ (åœ¨ DAG $G$ ä¸­)\n",
    "- åˆ™ $P(X, Y | Z) = P(X | Z) P(Y | Z)$ (åœ¨æ•°æ®åˆ†å¸ƒ $P$ ä¸­)\n",
    "\n",
    "**é©¬å°”å¯å¤«æ¡ä»¶**ï¼š\n",
    "$$\n",
    "X_i \\perp\\!\\!\\!\\perp \\text{NonDesc}(X_i) | \\text{Pa}(X_i)\n",
    "$$\n",
    "\n",
    "### ç®—æ³•æµç¨‹\n",
    "\n",
    "**Step 1: æ„å»ºå®Œå…¨æ— å‘å›¾**\n",
    "- æ‰€æœ‰å˜é‡ä¸¤ä¸¤ç›¸è¿\n",
    "\n",
    "**Step 2: åˆ è¾¹ (åŸºäºæ¡ä»¶ç‹¬ç«‹æ€§)**\n",
    "- å¯¹æ¯æ¡è¾¹ $X - Y$ï¼Œå¯»æ‰¾æ¡ä»¶é›† $S$\n",
    "- å¦‚æœ $X \\perp\\!\\!\\!\\perp Y | S$ï¼Œåˆ é™¤è¯¥è¾¹\n",
    "\n",
    "**Step 3: å®šå‘è¾¹ (åŸºäº v-ç»“æ„)**\n",
    "- æ‰¾åˆ° $X - Z - Y$ ä¸” $X, Y$ ä¸ç›¸é‚»\n",
    "- å¦‚æœ $Z \\notin S_{XY}$ï¼Œåˆ™ $X \\rightarrow Z \\leftarrow Y$ (å¯¹æ’ç»“æ„)\n",
    "\n",
    "**Step 4: ä¼ æ’­æ–¹å‘**\n",
    "- é¿å…äº§ç”Ÿæ–°çš„ v-ç»“æ„\n",
    "- é¿å…äº§ç”Ÿç¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨å®ç°ç®€åŒ–ç‰ˆ PC ç®—æ³•ï¼ˆæ•™å­¦ç”¨ï¼‰\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from itertools import combinations\n",
    "\n",
    "def independence_test(data, X, Y, Z=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ: X âŠ¥ Y | Z\n",
    "    ä½¿ç”¨åç›¸å…³ç³»æ•°\n",
    "    \"\"\"\n",
    "    if Z is None or len(Z) == 0:\n",
    "        # è¾¹ç¼˜ç‹¬ç«‹æ€§\n",
    "        corr, pval = pearsonr(data[X], data[Y])\n",
    "        return pval > alpha\n",
    "    else:\n",
    "        # æ¡ä»¶ç‹¬ç«‹æ€§ (ç®€åŒ–ï¼šç”¨åç›¸å…³)\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        # æ®‹å·®åŒ–\n",
    "        reg_X = LinearRegression().fit(data[Z], data[X])\n",
    "        reg_Y = LinearRegression().fit(data[Z], data[Y])\n",
    "        \n",
    "        res_X = data[X] - reg_X.predict(data[Z])\n",
    "        res_Y = data[Y] - reg_Y.predict(data[Z])\n",
    "        \n",
    "        corr, pval = pearsonr(res_X, res_Y)\n",
    "        return pval > alpha\n",
    "\n",
    "def simple_pc_skeleton(data, alpha=0.05, max_cond_size=2):\n",
    "    \"\"\"\n",
    "    PC ç®—æ³• Step 1-2: å­¦ä¹ éª¨æ¶\n",
    "    \"\"\"\n",
    "    variables = list(data.columns)\n",
    "    n_vars = len(variables)\n",
    "    \n",
    "    # åˆå§‹åŒ–å®Œå…¨å›¾\n",
    "    adjacency = {var: set(variables) - {var} for var in variables}\n",
    "    separation_sets = {}\n",
    "    \n",
    "    # é€æ­¥å¢åŠ æ¡ä»¶é›†å¤§å°\n",
    "    for cond_size in range(max_cond_size + 1):\n",
    "        for X in variables:\n",
    "            for Y in list(adjacency[X]):\n",
    "                if Y not in adjacency[X]:\n",
    "                    continue\n",
    "                \n",
    "                # å€™é€‰æ¡ä»¶é›†\n",
    "                candidates = adjacency[X] - {Y}\n",
    "                \n",
    "                if len(candidates) < cond_size:\n",
    "                    continue\n",
    "                \n",
    "                # æ£€éªŒæ‰€æœ‰å¤§å°ä¸º cond_size çš„æ¡ä»¶é›†\n",
    "                for Z in combinations(candidates, cond_size):\n",
    "                    Z_list = list(Z)\n",
    "                    if independence_test(data, X, Y, Z_list, alpha):\n",
    "                        # æ‰¾åˆ°åˆ†ç¦»é›†ï¼Œåˆ è¾¹\n",
    "                        adjacency[X].discard(Y)\n",
    "                        adjacency[Y].discard(X)\n",
    "                        separation_sets[(X, Y)] = Z_list\n",
    "                        separation_sets[(Y, X)] = Z_list\n",
    "                        break\n",
    "    \n",
    "    return adjacency, separation_sets\n",
    "\n",
    "# è¿è¡Œç®€åŒ–ç‰ˆ PC\n",
    "print(\"ğŸ” è¿è¡Œ PC ç®—æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰...\")\n",
    "adjacency, sep_sets = simple_pc_skeleton(df, alpha=0.01, max_cond_size=2)\n",
    "\n",
    "print(\"\\nå­¦åˆ°çš„é‚»æ¥å…³ç³»:\")\n",
    "for var, neighbors in adjacency.items():\n",
    "    if neighbors:\n",
    "        print(f\"{var}: {neighbors}\")\n",
    "\n",
    "print(\"\\næ‰¾åˆ°çš„åˆ†ç¦»é›†:\")\n",
    "for (X, Y), S in sep_sets.items():\n",
    "    if X < Y:  # åªæ‰“å°ä¸€æ¬¡\n",
    "        print(f\"{X} âŠ¥ {Y} | {S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ä½¿ç”¨ causal-learn åº“\n",
    "\n",
    "å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆç†Ÿçš„åº“è¿›è¡Œå› æœå‘ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡æ•°æ®\n",
    "data_array = df.values\n",
    "var_names = list(df.columns)\n",
    "\n",
    "# è¿è¡Œ PC ç®—æ³•\n",
    "print(\"ğŸ” è¿è¡Œ PC ç®—æ³• (causal-learn)...\")\n",
    "cg = pc(\n",
    "    data_array,\n",
    "    alpha=0.05,\n",
    "    indep_test='fisherz',  # Fisher's Z æ£€éªŒ\n",
    "    stable=True,\n",
    "    uc_rule=0,\n",
    "    uc_priority=2\n",
    ")\n",
    "\n",
    "# æå–å›¾ç»“æ„\n",
    "graph = cg.G\n",
    "print(\"\\nâœ… PC ç®—æ³•å®Œæˆ\")\n",
    "print(f\"èŠ‚ç‚¹æ•°: {graph.num_vars}\")\n",
    "\n",
    "# å¯è§†åŒ–ï¼ˆä½¿ç”¨ networkxï¼‰\n",
    "def visualize_causal_graph(graph, var_names, title=\"Learned Causal Graph\"):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å› æœå›¾\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(var_names)\n",
    "    \n",
    "    # æå–è¾¹\n",
    "    edges = graph.graph\n",
    "    directed_edges = []\n",
    "    undirected_edges = []\n",
    "    \n",
    "    for i in range(len(var_names)):\n",
    "        for j in range(i + 1, len(var_names)):\n",
    "            edge = edges[i, j]\n",
    "            if edge == -1 and edges[j, i] == 1:\n",
    "                # i -> j\n",
    "                directed_edges.append((var_names[i], var_names[j]))\n",
    "                G.add_edge(var_names[i], var_names[j])\n",
    "            elif edge == 1 and edges[j, i] == -1:\n",
    "                # i <- j\n",
    "                directed_edges.append((var_names[j], var_names[i]))\n",
    "                G.add_edge(var_names[j], var_names[i])\n",
    "            elif edge == -1 and edges[j, i] == -1:\n",
    "                # i - j (undirected)\n",
    "                undirected_edges.append((var_names[i], var_names[j]))\n",
    "    \n",
    "    # ä½¿ç”¨ Plotly ç»˜åˆ¶\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # è¾¹\n",
    "    edge_trace = []\n",
    "    for edge in directed_edges:\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace.append(\n",
    "            go.Scatter(\n",
    "                x=[x0, x1, None],\n",
    "                y=[y0, y1, None],\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color='#2D9CDB'),\n",
    "                hoverinfo='none',\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "        # ç®­å¤´\n",
    "        edge_trace.append(\n",
    "            go.Scatter(\n",
    "                x=[x1],\n",
    "                y=[y1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=15,\n",
    "                    color='#2D9CDB',\n",
    "                    symbol='arrow',\n",
    "                    angle=np.arctan2(y1 - y0, x1 - x0) * 180 / np.pi\n",
    "                ),\n",
    "                hoverinfo='none',\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for edge in undirected_edges:\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace.append(\n",
    "            go.Scatter(\n",
    "                x=[x0, x1, None],\n",
    "                y=[y0, y1, None],\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color='#95A5A6', dash='dash'),\n",
    "                hoverinfo='none',\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # èŠ‚ç‚¹\n",
    "    node_x = [pos[node][0] for node in var_names]\n",
    "    node_y = [pos[node][1] for node in var_names]\n",
    "    \n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=var_names,\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=30,\n",
    "            color='#27AE60',\n",
    "            line=dict(width=2, color='white')\n",
    "        ),\n",
    "        hoverinfo='text',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # ç»„åˆ\n",
    "    fig = go.Figure(data=edge_trace + [node_trace])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = visualize_causal_graph(graph, var_names, \"PC ç®—æ³•å­¦åˆ°çš„å› æœå›¾\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nå›¾ä¾‹:\")\n",
    "print(\"ğŸ”µ å®çº¿ç®­å¤´: ç¡®å®šçš„å› æœæ–¹å‘\")\n",
    "print(\"âšª è™šçº¿: æ— æ³•ç¡®å®šæ–¹å‘ï¼ˆé©¬å°”å¯å¤«ç­‰ä»·ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’» TODO 1: ç†è§£æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ\n",
    "\n",
    "å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ£€éªŒç»™å®šæ¡ä»¶é›†ä¸‹çš„ç‹¬ç«‹æ€§ï¼Œå¹¶è§£é‡Šç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ’» ç»ƒä¹  1: å®ç°æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ\n# æç¤ºï¼š\n# 1. è¾¹ç¼˜ç‹¬ç«‹æ€§ï¼šä½¿ç”¨ pearsonr(data[X], data[Y])\n# 2. æ¡ä»¶ç‹¬ç«‹æ€§ï¼šè®¡ç®—åç›¸å…³\n#    - ç”¨ Z é¢„æµ‹ Xï¼Œå¾—åˆ°æ®‹å·® res_X\n#    - ç”¨ Z é¢„æµ‹ Yï¼Œå¾—åˆ°æ®‹å·® res_Y\n#    - è®¡ç®— res_X å’Œ res_Y çš„ç›¸å…³æ€§\n\ndef analyze_conditional_independence(data, X, Y, Z_list=None):\n    \"\"\"\n    åˆ†æ X å’Œ Y åœ¨ä¸åŒæ¡ä»¶é›†ä¸‹çš„ç‹¬ç«‹æ€§\n    \n    å‚æ•°:\n        data: DataFrame\n        X, Y: å˜é‡å\n        Z_list: æ¡ä»¶å˜é‡åˆ—è¡¨çš„åˆ—è¡¨ (ä¾‹å¦‚ [[], ['A'], ['A', 'B']])\n    \n    è¿”å›:\n        ç»“æœ DataFrame\n    \"\"\"\n    if Z_list is None:\n        Z_list = [[]]\n    \n    results = []\n    \n    # TODO: å®ç°ç‹¬ç«‹æ€§æ£€éªŒ\n    # å¯¹æ¯ä¸ªæ¡ä»¶é›† Z in Z_list:\n    #   1. å¦‚æœ Z ä¸ºç©ºï¼Œè®¡ç®—è¾¹ç¼˜ç›¸å…³\n    #   2. å¦åˆ™ï¼Œè®¡ç®—åç›¸å…³ï¼ˆæ§åˆ¶ Z åçš„ X-Y ç›¸å…³æ€§ï¼‰\n    #   3. è®°å½•ç›¸å…³ç³»æ•°ã€på€¼ã€æ˜¯å¦ç‹¬ç«‹\n    \n    pass  # æ›¿æ¢ä¸ºä½ çš„å®ç°\n\n# æµ‹è¯•ä½ çš„å®ç°\n# result = analyze_conditional_independence(\n#     df,\n#     'PurchaseFreq',\n#     'BrowseTime',\n#     Z_list=[[], ['MemberLevel'], ['MemberLevel', 'Churn']]\n# )\n# print(result)"
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“– å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef analyze_conditional_independence(data, X, Y, Z_list=None):\n    \"\"\"\n    åˆ†æ X å’Œ Y åœ¨ä¸åŒæ¡ä»¶é›†ä¸‹çš„ç‹¬ç«‹æ€§\n    \"\"\"\n    if Z_list is None:\n        Z_list = [[]]\n    \n    results = []\n    \n    from scipy.stats import pearsonr\n    from sklearn.linear_model import LinearRegression\n    \n    for Z in Z_list:\n        if len(Z) == 0:\n            # è¾¹ç¼˜ç›¸å…³\n            corr, pval = pearsonr(data[X], data[Y])\n        else:\n            # åç›¸å…³ï¼šæ§åˆ¶ Z å X å’Œ Y çš„ç›¸å…³æ€§\n            reg_X = LinearRegression().fit(data[Z], data[X])\n            reg_Y = LinearRegression().fit(data[Z], data[Y])\n            res_X = data[X] - reg_X.predict(data[Z])\n            res_Y = data[Y] - reg_Y.predict(data[Z])\n            corr, pval = pearsonr(res_X, res_Y)\n        \n        results.append({\n            'Condition': str(Z) if Z else 'Marginal',\n            'Correlation': corr,\n            'P-value': pval,\n            'Independent (Î±=0.05)': pval > 0.05\n        })\n    \n    return pd.DataFrame(results)\n\n# æµ‹è¯•\nresult = analyze_conditional_independence(\n    df,\n    'PurchaseFreq',\n    'BrowseTime',\n    Z_list=[[], ['MemberLevel'], ['MemberLevel', 'Churn']]\n)\nprint(result)\n```\n\n**ğŸ’¡ è§‚å¯Ÿ**: æ§åˆ¶ä¸åŒå˜é‡åï¼Œç›¸å…³æ€§å¦‚ä½•å˜åŒ–ï¼Ÿ\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: åŸºäºå¾—åˆ†çš„æ–¹æ³• - GES ç®—æ³•\n",
    "\n",
    "## 3.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "GES (Greedy Equivalence Search) é€šè¿‡**ä¼˜åŒ–è¯„åˆ†å‡½æ•°**æœç´¢æœ€ä¼˜å› æœå›¾ã€‚\n",
    "\n",
    "### è¯„åˆ†å‡½æ•°\n",
    "\n",
    "**BIC (Bayesian Information Criterion)**:\n",
    "$$\n",
    "\\text{BIC}(G) = \\log L(D | G, \\hat{\\theta}) - \\frac{k}{2} \\log n\n",
    "$$\n",
    "\n",
    "- $L(D | G, \\hat{\\theta})$: ä¼¼ç„¶å‡½æ•°\n",
    "- $k$: å‚æ•°ä¸ªæ•°\n",
    "- $n$: æ ·æœ¬é‡\n",
    "\n",
    "**ç›®æ ‡**: $\\arg\\max_G \\text{BIC}(G)$\n",
    "\n",
    "### ç®—æ³•æµç¨‹\n",
    "\n",
    "**Forward Phase (åŠ è¾¹)**:\n",
    "1. ä»ç©ºå›¾å¼€å§‹\n",
    "2. è´ªå¿ƒåœ°æ·»åŠ è¾¹ï¼Œç›´åˆ° BIC ä¸å†å¢åŠ \n",
    "\n",
    "**Backward Phase (åˆ è¾¹)**:\n",
    "1. ä»å½“å‰å›¾å¼€å§‹\n",
    "2. è´ªå¿ƒåœ°åˆ é™¤è¾¹ï¼Œç›´åˆ° BIC ä¸å†å¢åŠ \n",
    "\n",
    "### ä¸ PC çš„å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | PC ç®—æ³• | GES ç®—æ³• |\n",
    "|------|---------|----------|\n",
    "| æ–¹æ³• | çº¦æŸä¼˜åŒ– | è¯„åˆ†ä¼˜åŒ– |\n",
    "| è¾“å…¥ | ç‹¬ç«‹æ€§æ£€éªŒ | è¯„åˆ†å‡½æ•° |\n",
    "| ä¼˜ç‚¹ | ç›´è§‚ã€å¿«é€Ÿ | å…¨å±€æœ€ä¼˜ |\n",
    "| ç¼ºç‚¹ | å±€éƒ¨å†³ç­– | è®¡ç®—æ˜‚è´µ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œ GES ç®—æ³•\n",
    "print(\"ğŸ” è¿è¡Œ GES ç®—æ³•...\")\n",
    "Record = ges(\n",
    "    data_array,\n",
    "    score_func='local_score_BIC',\n",
    "    maxP=None\n",
    ")\n",
    "\n",
    "ges_graph = Record['G']\n",
    "print(\"\\nâœ… GES ç®—æ³•å®Œæˆ\")\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = visualize_causal_graph(ges_graph, var_names, \"GES ç®—æ³•å­¦åˆ°çš„å› æœå›¾\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\næ¯”è¾ƒ PC å’Œ GES:\")\n",
    "print(\"PC: åŸºäºæ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ\")\n",
    "print(\"GES: åŸºäº BIC è¯„åˆ†ä¼˜åŒ–\")\n",
    "print(\"\\nä¸¤è€…å¯èƒ½äº§ç”Ÿä¸åŒçš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨æ ·æœ¬é‡è¾ƒå°æ—¶ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ’» TODO 2: æ¯”è¾ƒä¸åŒç®—æ³•\n\nè¿è¡Œå¤šä¸ªç®—æ³•ï¼Œæ¯”è¾ƒå®ƒä»¬çš„ç»“æœã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ğŸ’» ç»ƒä¹  2: æ¯”è¾ƒä¸åŒç®—æ³•\n# æç¤ºï¼š\n# 1. ä½¿ç”¨ä¸åŒçš„ alpha å‚æ•°è¿è¡Œ PC ç®—æ³•\n# 2. è¿è¡Œ GES ç®—æ³•\n# 3. æå–æ¯ä¸ªç®—æ³•çš„è¾¹é›†åˆ\n# 4. è®¡ç®— Jaccard ç›¸ä¼¼åº¦\n\ndef compare_causal_discovery_algorithms(data, var_names):\n    \"\"\"\n    æ¯”è¾ƒå¤šä¸ªå› æœå‘ç°ç®—æ³•\n    \n    è¿”å›:\n        å„ç®—æ³•çš„è¾¹é›†åˆ\n    \"\"\"\n    # TODO: å®ç°ç®—æ³•æ¯”è¾ƒ\n    # 1. å¯¹ä¸åŒ alpha è¿è¡Œ PC ç®—æ³•\n    # 2. è¿è¡Œ GES ç®—æ³•\n    # 3. æå–è¾¹é›†åˆ\n    pass\n\ndef extract_edges(graph, var_names):\n    \"\"\"ä»å›¾å¯¹è±¡æå–è¾¹\"\"\"\n    # TODO: å®ç°è¾¹æå–é€»è¾‘\n    # æ£€æŸ¥é‚»æ¥çŸ©é˜µï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å‘è¾¹\n    pass\n\n# æµ‹è¯•ä½ çš„å®ç°\n# results = compare_causal_discovery_algorithms(df, var_names)\n# print(results)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“– å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef compare_causal_discovery_algorithms(data, var_names):\n    \"\"\"æ¯”è¾ƒå¤šä¸ªå› æœå‘ç°ç®—æ³•\"\"\"\n    data_array = data.values\n    results = {}\n    \n    # è¿è¡Œ PC ç®—æ³•ï¼ˆä¸åŒè¶…å‚æ•°ï¼‰\n    for alpha in [0.01, 0.05, 0.1]:\n        cg = pc(data_array, alpha=alpha, indep_test='fisherz', stable=True)\n        edges = extract_edges(cg.G, var_names)\n        results[f'PC (Î±={alpha})'] = edges\n    \n    # è¿è¡Œ GES ç®—æ³•\n    Record = ges(data_array, score_func='local_score_BIC')\n    edges = extract_edges(Record['G'], var_names)\n    results['GES'] = edges\n    \n    return results\n\ndef extract_edges(graph, var_names):\n    \"\"\"ä»å›¾å¯¹è±¡æå–è¾¹\"\"\"\n    edges = []\n    adj = graph.graph\n    for i in range(len(var_names)):\n        for j in range(len(var_names)):\n            if adj[i, j] == -1 and adj[j, i] == 1:\n                edges.append(f\"{var_names[i]} â†’ {var_names[j]}\")\n    return set(edges)\n\n# è¿è¡Œæ¯”è¾ƒ\nresults = compare_causal_discovery_algorithms(df, var_names)\n\nprint(\"ç®—æ³•æ¯”è¾ƒç»“æœ:\\n\")\nfor algo, edges in results.items():\n    print(f\"{algo}:\")\n    for edge in sorted(edges):\n        print(f\"  {edge}\")\n    print()\n\n# è®¡ç®—ç®—æ³•é—´çš„ä¸€è‡´æ€§ï¼ˆJaccard ç›¸ä¼¼åº¦ï¼‰\nfrom itertools import combinations\n\nprint(\"ç®—æ³•ä¸€è‡´æ€§åˆ†æ:\")\nprint(\"\\nJaccard ç›¸ä¼¼åº¦çŸ©é˜µ:\")\nprint(\"-\" * 60)\n\nalgorithms = list(results.keys())\nfor algo1, algo2 in combinations(algorithms, 2):\n    edges1 = results[algo1]\n    edges2 = results[algo2]\n    \n    intersection = len(edges1 & edges2)\n    union = len(edges1 | edges2)\n    jaccard = intersection / union if union > 0 else 0\n    \n    print(f\"{algo1:<15} vs {algo2:<15}: {jaccard:.3f}\")\n```\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾…åŠ©å‡½æ•°ï¼šæå–å› æœå›¾çš„æœ‰å‘è¾¹\n",
    "# ï¼ˆä¾›åç»­ç¤ºèŒƒä»£ç ä½¿ç”¨ï¼‰\n",
    "def extract_edges(graph, var_names):\n",
    "    \"\"\"ä»å›¾å¯¹è±¡æå–è¾¹\"\"\"\n",
    "    edges = []\n",
    "    adj = graph.graph\n",
    "    for i in range(len(var_names)):\n",
    "        for j in range(len(var_names)):\n",
    "            if adj[i, j] == -1 and adj[j, i] == 1:\n",
    "                edges.append(f\"{var_names[i]} â†’ {var_names[j]}\")\n",
    "    return set(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: å‡½æ•°å› æœæ¨¡å‹\n",
    "\n",
    "## 4.1 LiNGAM (Linear Non-Gaussian Acyclic Model)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "åˆ©ç”¨**éé«˜æ–¯æ€§**è¯†åˆ«å› æœæ–¹å‘ã€‚\n",
    "\n",
    "**æ¨¡å‹å‡è®¾**:\n",
    "$$\n",
    "x_i = \\sum_{j \\in \\text{Pa}(i)} b_{ij} x_j + e_i\n",
    "$$\n",
    "\n",
    "å…¶ä¸­:\n",
    "- $e_i$ æ˜¯éé«˜æ–¯å™ªå£°\n",
    "- $e_i \\perp\\!\\!\\!\\perp e_j, \\forall i \\neq j$\n",
    "\n",
    "**å…³é”®æ´å¯Ÿ**:\n",
    "- å¦‚æœ $X \\rightarrow Y$: $Y = f(X) + e_Y$ï¼Œåˆ™ $X \\perp\\!\\!\\!\\perp e_Y$\n",
    "- å¦‚æœ $Y \\rightarrow X$: $X = g(Y) + e_X$ï¼Œåˆ™ $Y \\perp\\!\\!\\!\\perp e_X$\n",
    "- å¯¹äºéé«˜æ–¯æ•°æ®ï¼Œä¸¤è€…**ä¸èƒ½åŒæ—¶æˆç«‹**ï¼\n",
    "\n",
    "### ç®—æ³•\n",
    "\n",
    "**DirectLiNGAM**:\n",
    "1. æ‰¾åˆ°å¤–ç”Ÿå˜é‡ï¼ˆæ®‹å·®æœ€éé«˜æ–¯ï¼‰\n",
    "2. å›å½’æ¶ˆé™¤å…¶å½±å“\n",
    "3. é‡å¤ç›´åˆ°æ‰€æœ‰å˜é‡æ’åº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤º LiNGAM çš„æ€æƒ³\n",
    "def demonstrate_nongaussian_identifiability():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºéé«˜æ–¯æ€§å¦‚ä½•è¯†åˆ«å› æœæ–¹å‘\n",
    "    \"\"\"\n",
    "    n = 2000\n",
    "    \n",
    "    # Case 1: X -> Y (éé«˜æ–¯å™ªå£°)\n",
    "    X = np.random.laplace(0, 1, n)  # Laplace åˆ†å¸ƒï¼ˆéé«˜æ–¯ï¼‰\n",
    "    e_Y = np.random.laplace(0, 0.5, n)\n",
    "    Y = 2 * X + e_Y\n",
    "    \n",
    "    # æµ‹è¯•ä¸¤ä¸ªæ–¹å‘\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from scipy.stats import normaltest\n",
    "    \n",
    "    # X -> Y\n",
    "    reg_xy = LinearRegression().fit(X.reshape(-1, 1), Y)\n",
    "    residual_xy = Y - reg_xy.predict(X.reshape(-1, 1))\n",
    "    _, pval_xy = normaltest(residual_xy)\n",
    "    \n",
    "    # Y -> X\n",
    "    reg_yx = LinearRegression().fit(Y.reshape(-1, 1), X)\n",
    "    residual_yx = X - reg_yx.predict(Y.reshape(-1, 1))\n",
    "    _, pval_yx = normaltest(residual_yx)\n",
    "    \n",
    "    # ç‹¬ç«‹æ€§æ£€éªŒ\n",
    "    from scipy.stats import pearsonr\n",
    "    corr_x_res_xy, _ = pearsonr(X, residual_xy)\n",
    "    corr_y_res_yx, _ = pearsonr(Y, residual_yx)\n",
    "    \n",
    "    print(\"éé«˜æ–¯æ€§è¯†åˆ«å› æœæ–¹å‘:\\n\")\n",
    "    print(f\"å‡è®¾ X â†’ Y:\")\n",
    "    print(f\"  æ®‹å·®éé«˜æ–¯æ€§ p-value: {pval_xy:.4f}\")\n",
    "    print(f\"  X âŠ¥ residual: {abs(corr_x_res_xy):.4f}\")\n",
    "    print(f\"\\nå‡è®¾ Y â†’ X:\")\n",
    "    print(f\"  æ®‹å·®éé«˜æ–¯æ€§ p-value: {pval_yx:.4f}\")\n",
    "    print(f\"  Y âŠ¥ residual: {abs(corr_y_res_yx):.4f}\")\n",
    "    print(f\"\\nâœ… ç»“è®º: X â†’ Y æ›´å¯èƒ½ï¼ˆæ®‹å·®æ›´éé«˜æ–¯ä¸”æ›´ç‹¬ç«‹ï¼‰\")\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"X çš„åˆ†å¸ƒï¼ˆéé«˜æ–¯ï¼‰\",\n",
    "            \"Y çš„åˆ†å¸ƒ\",\n",
    "            \"å‡è®¾ X â†’ Y çš„æ®‹å·®\",\n",
    "            \"å‡è®¾ Y â†’ X çš„æ®‹å·®\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=X, name='X', marker_color='#2D9CDB', nbinsx=50), row=1, col=1)\n",
    "    fig.add_trace(go.Histogram(x=Y, name='Y', marker_color='#27AE60', nbinsx=50), row=1, col=2)\n",
    "    fig.add_trace(go.Histogram(x=residual_xy, name='Residual Xâ†’Y', marker_color='#9B59B6', nbinsx=50), row=2, col=1)\n",
    "    fig.add_trace(go.Histogram(x=residual_yx, name='Residual Yâ†’X', marker_color='#E74C3C', nbinsx=50), row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=600, title_text=\"LiNGAM åŸç†: éé«˜æ–¯æ€§è¯†åˆ«å› æœæ–¹å‘\", showlegend=False)\n",
    "    fig.show()\n",
    "\n",
    "demonstrate_nongaussian_identifiability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 åº”ç”¨ LiNGAM åˆ°ä¸šåŠ¡æ•°æ®\n",
    "\n",
    "æ³¨æ„ï¼šLiNGAM éœ€è¦**çº¿æ€§å…³ç³»**å’Œ**éé«˜æ–¯å™ªå£°**ã€‚æˆ‘ä»¬çš„æ•°æ®éƒ¨åˆ†æ»¡è¶³è¿™äº›å‡è®¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lingam\n",
    "except ImportError:\n",
    "    print(\"å®‰è£… lingam...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install lingam -q\n",
    "    import lingam\n",
    "\n",
    "# ç§»é™¤ç¦»æ•£å˜é‡ï¼ˆLiNGAM è¦æ±‚è¿ç»­ï¼‰\n",
    "df_continuous = df[['PurchaseFreq', 'BrowseTime', 'CouponUsage']].copy()\n",
    "\n",
    "# è¿è¡Œ DirectLiNGAM\n",
    "print(\"ğŸ” è¿è¡Œ DirectLiNGAM...\")\n",
    "model = lingam.DirectLiNGAM()\n",
    "model.fit(df_continuous)\n",
    "\n",
    "print(\"\\nå› æœé¡ºåº:\")\n",
    "for i, var_idx in enumerate(model.causal_order_):\n",
    "    print(f\"{i+1}. {df_continuous.columns[var_idx]}\")\n",
    "\n",
    "print(\"\\né‚»æ¥çŸ©é˜µ (i -> j):\")\n",
    "adj_matrix = pd.DataFrame(\n",
    "    model.adjacency_matrix_,\n",
    "    columns=df_continuous.columns,\n",
    "    index=df_continuous.columns\n",
    ")\n",
    "print(adj_matrix.round(3))\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=adj_matrix.values,\n",
    "    x=adj_matrix.columns,\n",
    "    y=adj_matrix.index,\n",
    "    colorscale='Blues',\n",
    "    text=adj_matrix.values,\n",
    "    texttemplate='%{text:.2f}',\n",
    "    textfont={\"size\": 12}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"LiNGAM å› æœå¼ºåº¦çŸ©é˜µ (è¡Œ â†’ åˆ—)\",\n",
    "    xaxis_title=\"To\",\n",
    "    yaxis_title=\"From\",\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è§£è¯»:\")\n",
    "print(\"éé›¶å…ƒç´  (i, j) è¡¨ç¤º i â†’ j çš„å› æœæ•ˆåº”å¼ºåº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: å®è·µæ³¨æ„äº‹é¡¹\n",
    "\n",
    "## 5.1 æ ·æœ¬é‡è¦æ±‚\n",
    "\n",
    "å› æœå‘ç°éœ€è¦**å¤§é‡æ•°æ®**ï¼Œå°¤å…¶æ˜¯å˜é‡å¤šæ—¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºæ ·æœ¬é‡çš„å½±å“\n",
    "def evaluate_sample_size_effect(true_graph_edges, sample_sizes, n_trials=5):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ ·æœ¬é‡å¯¹å› æœå‘ç°å‡†ç¡®æ€§çš„å½±å“\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        accuracies = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # ç”Ÿæˆæ•°æ®\n",
    "            data = generate_ecommerce_data(n)\n",
    "            data_array = data.values\n",
    "            \n",
    "            # è¿è¡Œ PC\n",
    "            cg = pc(data_array, alpha=0.05, indep_test='fisherz', stable=True)\n",
    "            learned_edges = extract_edges(cg.G, list(data.columns))\n",
    "            \n",
    "            # è®¡ç®—å‡†ç¡®ç‡\n",
    "            tp = len(learned_edges & true_graph_edges)\n",
    "            fp = len(learned_edges - true_graph_edges)\n",
    "            fn = len(true_graph_edges - learned_edges)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            accuracies.append(f1)\n",
    "        \n",
    "        results.append({\n",
    "            'Sample Size': n,\n",
    "            'F1 Score': np.mean(accuracies),\n",
    "            'Std': np.std(accuracies)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# çœŸå®å› æœè¾¹\n",
    "true_edges = {\n",
    "    'MemberLevel â†’ PurchaseFreq',\n",
    "    'MemberLevel â†’ CouponUsage',\n",
    "    'PurchaseFreq â†’ BrowseTime',\n",
    "    'Complaints â†’ CouponUsage',\n",
    "    'BrowseTime â†’ Churn',\n",
    "    'CouponUsage â†’ Churn',\n",
    "    'Complaints â†’ Churn'\n",
    "}\n",
    "\n",
    "print(\"ğŸ” è¯„ä¼°æ ·æœ¬é‡å½±å“...\")\n",
    "results_df = evaluate_sample_size_effect(\n",
    "    true_edges,\n",
    "    sample_sizes=[200, 500, 1000, 2000, 5000],\n",
    "    n_trials=3\n",
    ")\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df['Sample Size'],\n",
    "    y=results_df['F1 Score'],\n",
    "    mode='lines+markers',\n",
    "    name='F1 Score',\n",
    "    line=dict(color='#2D9CDB', width=3),\n",
    "    marker=dict(size=10),\n",
    "    error_y=dict(\n",
    "        type='data',\n",
    "        array=results_df['Std'],\n",
    "        visible=True\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"å› æœå‘ç°å‡†ç¡®ç‡ vs æ ·æœ¬é‡\",\n",
    "    xaxis_title=\"Sample Size\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ“Š å…³é”®å‘ç°:\")\n",
    "print(\"1. æ ·æœ¬é‡è¶Šå¤§ï¼Œå› æœå‘ç°è¶Šå‡†ç¡®\")\n",
    "print(\"2. è‡³å°‘éœ€è¦ 1000+ æ ·æœ¬æ‰èƒ½è·å¾—ç¨³å®šç»“æœ\")\n",
    "print(\"3. å°æ ·æœ¬ (<500) å¯èƒ½äº§ç”Ÿå¤§é‡é”™è¯¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 å‡è®¾éªŒè¯\n",
    "\n",
    "å› æœå‘ç°ç®—æ³•éƒ½æœ‰**å¼ºå‡è®¾**ï¼Œéœ€è¦éªŒè¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ’» ç»ƒä¹  3: è§£é‡Šå› æœå‘ç°ç»“æœ\n# æç¤ºï¼š\n# 1. ä»é‚»æ¥çŸ©é˜µä¸­æ‰¾åˆ°ç›®æ ‡å˜é‡çš„ç›´æ¥åŸå› \n# 2. ä½¿ç”¨ networkx æŸ¥æ‰¾é—´æ¥è·¯å¾„\n# 3. æ‰¾åˆ°æ ¹æœ¬åŸå› ï¼ˆæ— å…¥è¾¹çš„èŠ‚ç‚¹ï¼‰\n# 4. ç”Ÿæˆä¸šåŠ¡å»ºè®®\n\ndef interpret_causal_graph(graph, var_names, target='Churn'):\n    \"\"\"\n    è§£é‡Šå› æœå›¾ï¼Œç”Ÿæˆä¸šåŠ¡å»ºè®®\n    \n    å‚æ•°:\n        graph: å­¦åˆ°çš„å› æœå›¾\n        var_names: å˜é‡ååˆ—è¡¨\n        target: ç›®æ ‡å˜é‡\n    \n    è¿”å›:\n        ä¸šåŠ¡å»ºè®®å­—å…¸\n    \"\"\"\n    # TODO: å®ç°å› æœå›¾è§£é‡Š\n    # 1. æ‰¾åˆ° target çš„ç›´æ¥åŸå› ï¼ˆé‚»æ¥çŸ©é˜µä¸­æŒ‡å‘ target çš„è¾¹ï¼‰\n    # 2. ä½¿ç”¨ networkx æ‰¾åˆ°é—´æ¥åŸå› ï¼ˆæœ‰è·¯å¾„ä½†ä¸ç›´æ¥ç›¸è¿ï¼‰\n    # 3. æ‰¾åˆ°æ ¹æœ¬åŸå› ï¼ˆå¤–ç”Ÿå˜é‡ï¼Œæ— å…¥è¾¹ï¼‰\n    # 4. ç”Ÿæˆä¸šåŠ¡å»ºè®®\n    \n    pass\n\n# æµ‹è¯•ä½ çš„å®ç°\n# insights = interpret_causal_graph(graph, var_names, target='Churn')"
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“– å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef interpret_causal_graph(graph, var_names, target='Churn'):\n    \"\"\"è§£é‡Šå› æœå›¾ï¼Œç”Ÿæˆä¸šåŠ¡å»ºè®®\"\"\"\n    target_idx = var_names.index(target)\n    adj = graph.graph\n    \n    # æ‰¾åˆ° target çš„ç›´æ¥åŸå› \n    direct_causes = []\n    for i in range(len(var_names)):\n        if adj[i, target_idx] == -1 and adj[target_idx, i] == 1:\n            direct_causes.append(var_names[i])\n    \n    print(f\"ğŸ“Š {target} çš„å› æœåˆ†æ\\n\" + \"=\"*50)\n    print(f\"\\nç›´æ¥åŸå›  (å¯å¹²é¢„):\")\n    for cause in direct_causes:\n        print(f\"  âœ… {cause} â†’ {target}\")\n    \n    # æ‰¾åˆ°é—´æ¥åŸå› ï¼ˆé€šè¿‡ä¸­ä»‹ï¼‰\n    G = nx.DiGraph()\n    for i in range(len(var_names)):\n        for j in range(len(var_names)):\n            if adj[i, j] == -1 and adj[j, i] == 1:\n                G.add_edge(var_names[i], var_names[j])\n    \n    indirect_causes = []\n    for var in var_names:\n        if var != target and var not in direct_causes:\n            if nx.has_path(G, var, target):\n                indirect_causes.append(var)\n    \n    print(f\"\\né—´æ¥åŸå›  (éœ€è°¨æ…):\")\n    for cause in indirect_causes:\n        try:\n            path = nx.shortest_path(G, cause, target)\n            print(f\"  ğŸ”— {' â†’ '.join(path)}\")\n        except:\n            pass\n    \n    # æ‰¾åˆ°æ ¹æœ¬åŸå› ï¼ˆæ— å…¥è¾¹ï¼‰\n    root_causes = []\n    for i in range(len(var_names)):\n        has_parent = False\n        for j in range(len(var_names)):\n            if adj[j, i] == -1 and adj[i, j] == 1:\n                has_parent = True\n                break\n        if not has_parent and var_names[i] != target:\n            root_causes.append(var_names[i])\n    \n    print(f\"\\næ ¹æœ¬åŸå›  (å¤–ç”Ÿå˜é‡):\")\n    for cause in root_causes:\n        print(f\"  ğŸ¯ {cause} (æœ€ä¸Šæ¸¸)\")\n    \n    # ç”Ÿæˆå¹²é¢„å»ºè®®\n    print(f\"\\nğŸ’¡ ä¸šåŠ¡å»ºè®®:\")\n    print(\"1. ä¼˜å…ˆå¹²é¢„ç›´æ¥åŸå›  (è§æ•ˆå¿«):\")\n    for cause in direct_causes:\n        print(f\"   - æ”¹å–„ {cause}\")\n    \n    print(\"\\n2. é•¿æœŸä¼˜åŒ–æ ¹æœ¬åŸå›  (æ²»æœ¬):\")\n    for cause in root_causes:\n        print(f\"   - æå‡ {cause}\")\n    \n    return {\n        'direct_causes': direct_causes,\n        'indirect_causes': indirect_causes,\n        'root_causes': root_causes\n    }\n\n# ä½¿ç”¨ PC ç®—æ³•çš„ç»“æœ\ninsights = interpret_causal_graph(graph, var_names, target='Churn')\n```\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’» TODO 3: è§£é‡Šå› æœå‘ç°ç»“æœ\n",
    "\n",
    "å°†å­¦åˆ°çš„å› æœå›¾è½¬åŒ–ä¸ºä¸šåŠ¡æ´å¯Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å®Œæ•´å®ç°å› æœå›¾è§£é‡Š\ndef interpret_causal_graph(graph, var_names, target='Churn'):\n    \"\"\"\n    è§£é‡Šå› æœå›¾ï¼Œç”Ÿæˆä¸šåŠ¡å»ºè®®\n    \n    å‚æ•°:\n        graph: å­¦åˆ°çš„å› æœå›¾\n        var_names: å˜é‡ååˆ—è¡¨\n        target: ç›®æ ‡å˜é‡\n    \n    è¿”å›:\n        ä¸šåŠ¡å»ºè®®å­—å…¸\n    \"\"\"\n    target_idx = var_names.index(target)\n    adj = graph.graph\n    \n    # æ‰¾åˆ° target çš„ç›´æ¥åŸå› \n    direct_causes = []\n    for i in range(len(var_names)):\n        if adj[i, target_idx] == -1 and adj[target_idx, i] == 1:\n            direct_causes.append(var_names[i])\n    \n    print(f\"ğŸ“Š {target} çš„å› æœåˆ†æ\\n\" + \"=\"*50)\n    print(f\"\\nç›´æ¥åŸå›  (å¯å¹²é¢„):\")\n    for cause in direct_causes:\n        print(f\"  âœ… {cause} â†’ {target}\")\n    \n    # æ‰¾åˆ°é—´æ¥åŸå› ï¼ˆé€šè¿‡ä¸­ä»‹ï¼‰\n    # ä½¿ç”¨ networkx æŸ¥æ‰¾è·¯å¾„\n    G = nx.DiGraph()\n    for i in range(len(var_names)):\n        for j in range(len(var_names)):\n            if adj[i, j] == -1 and adj[j, i] == 1:\n                G.add_edge(var_names[i], var_names[j])\n    \n    indirect_causes = []\n    for i, var in enumerate(var_names):\n        if var != target and var not in direct_causes:\n            # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„åˆ° target\n            if nx.has_path(G, var, target):\n                indirect_causes.append(var)\n    \n    print(f\"\\né—´æ¥åŸå›  (éœ€è°¨æ…):\")\n    for cause in indirect_causes:\n        # æ‰¾ä¸€æ¡è·¯å¾„ç¤ºä¾‹\n        try:\n            path = nx.shortest_path(G, cause, target)\n            path_str = \" â†’ \".join(path)\n            print(f\"  ğŸ”— {path_str}\")\n        except:\n            pass\n    \n    # æ‰¾åˆ°æ ¹æœ¬åŸå› ï¼ˆæ— å…¥è¾¹ï¼‰\n    root_causes = []\n    for i in range(len(var_names)):\n        has_parent = False\n        for j in range(len(var_names)):\n            if adj[j, i] == -1 and adj[i, j] == 1:\n                has_parent = True\n                break\n        if not has_parent and var_names[i] != target:\n            root_causes.append(var_names[i])\n    \n    print(f\"\\næ ¹æœ¬åŸå›  (å¤–ç”Ÿå˜é‡):\")\n    for cause in root_causes:\n        print(f\"  ğŸ¯ {cause} (æœ€ä¸Šæ¸¸)\")\n    \n    # ç”Ÿæˆå¹²é¢„å»ºè®®\n    print(f\"\\nğŸ’¡ ä¸šåŠ¡å»ºè®®:\")\n    print(\"1. ä¼˜å…ˆå¹²é¢„ç›´æ¥åŸå›  (è§æ•ˆå¿«):\")\n    for cause in direct_causes:\n        print(f\"   - æ”¹å–„ {cause}\")\n    \n    print(\"\\n2. é•¿æœŸä¼˜åŒ–æ ¹æœ¬åŸå›  (æ²»æœ¬):\")\n    for cause in root_causes:\n        if cause != target:\n            print(f\"   - æå‡ {cause}\")\n    \n    print(\"\\n3. æ³¨æ„ä¸­ä»‹æ•ˆåº”:\")\n    if indirect_causes:\n        print(f\"   - å¹²é¢„ {indirect_causes[0]} ç­‰å˜é‡å¯èƒ½é€šè¿‡ä¸­ä»‹èµ·ä½œç”¨\")\n    else:\n        print(\"   - å½“å‰æœªå‘ç°æ˜æ˜¾çš„ä¸­ä»‹è·¯å¾„\")\n    \n    return {\n        'direct_causes': direct_causes,\n        'indirect_causes': indirect_causes,\n        'root_causes': root_causes\n    }\n\n# ä½¿ç”¨ PC ç®—æ³•çš„ç»“æœ\ninsights = interpret_causal_graph(graph, var_names, target='Churn')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 å› æœå‘ç°çš„å±€é™æ€§\n",
    "\n",
    "**å…³é”®é™åˆ¶**:\n",
    "\n",
    "1. **é©¬å°”å¯å¤«ç­‰ä»·ç±»**\n",
    "   - æŸäº›è¾¹çš„æ–¹å‘æ— æ³•å”¯ä¸€ç¡®å®š\n",
    "   - éœ€è¦é¢å¤–å‡è®¾æˆ–é¢†åŸŸçŸ¥è¯†\n",
    "\n",
    "2. **å‡è®¾æ•æ„Ÿæ€§**\n",
    "   - è¿åå‡è®¾ä¼šå¯¼è‡´é”™è¯¯ç»“æœ\n",
    "   - éš¾ä»¥éªŒè¯æ‰€æœ‰å‡è®¾\n",
    "\n",
    "3. **è®¡ç®—å¤æ‚åº¦**\n",
    "   - å˜é‡å¤šæ—¶æŒ‡æ•°çº§å¢é•¿\n",
    "   - éœ€è¦å¤§æ ·æœ¬é‡\n",
    "\n",
    "4. **éšå˜é‡**\n",
    "   - æœªè§‚æµ‹æ··æ·†ä¼šäº§ç”Ÿè™šå‡è¾¹\n",
    "   - FCI ç®—æ³•å¯éƒ¨åˆ†å¤„ç†\n",
    "\n",
    "5. **æ—¶é—´ä¿¡æ¯**\n",
    "   - æ¨ªæˆªé¢æ•°æ®éš¾ä»¥ç¡®å®šæ–¹å‘\n",
    "   - æ—¶é—´åºåˆ—æ•°æ®æ›´å¯é \n",
    "\n",
    "**æœ€ä½³å®è·µ**:\n",
    "\n",
    "```python\n",
    "# ç»„åˆå¤šç§æ–¹æ³•\n",
    "results = {\n",
    "    'PC': run_pc(data),\n",
    "    'GES': run_ges(data),\n",
    "    'LiNGAM': run_lingam(data),\n",
    "}\n",
    "\n",
    "# å–äº¤é›†ï¼ˆä¿å®ˆï¼‰\n",
    "consensus = set.intersection(*[set(r.edges) for r in results.values()])\n",
    "\n",
    "# ç»“åˆé¢†åŸŸçŸ¥è¯†\n",
    "validated = filter_with_domain_knowledge(consensus)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç»ƒä¹ ä¸æ€è€ƒé¢˜\n",
    "\n",
    "## ç»ƒä¹  1: æ•æ„Ÿæ€§åˆ†æ\n",
    "\n",
    "æ”¹å˜ PC ç®—æ³•çš„ $\\alpha$ å‚æ•°ï¼Œè§‚å¯Ÿç»“æœå¦‚ä½•å˜åŒ–ã€‚\n",
    "\n",
    "```python\n",
    "for alpha in [0.001, 0.01, 0.05, 0.1, 0.2]:\n",
    "    graph = run_pc(data, alpha=alpha)\n",
    "    print(f\"Î±={alpha}: {count_edges(graph)} edges\")\n",
    "```\n",
    "\n",
    "**æ€è€ƒ**: \n",
    "- $\\alpha$ è¶Šå¤§ï¼Œè¾¹è¶Šå¤šè¿˜æ˜¯è¶Šå°‘ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "- å¦‚ä½•é€‰æ‹©åˆé€‚çš„ $\\alpha$ï¼Ÿ\n",
    "\n",
    "## ç»ƒä¹  2: å¯¹æ¯”çœŸå®ç»“æ„\n",
    "\n",
    "æˆ‘ä»¬çŸ¥é“æ•°æ®ç”Ÿæˆçš„çœŸå®å› æœå›¾ã€‚è®¡ç®—å­¦åˆ°çš„å›¾ä¸çœŸå®å›¾çš„å·®å¼‚ã€‚\n",
    "\n",
    "```python\n",
    "true_edges = {...}  # çœŸå®è¾¹é›†\n",
    "learned_edges = {...}  # å­¦åˆ°çš„è¾¹é›†\n",
    "\n",
    "# è®¡ç®— Precision, Recall, F1\n",
    "```\n",
    "\n",
    "## ç»ƒä¹  3: å¤„ç†éšå˜é‡\n",
    "\n",
    "å°è¯•ä½¿ç”¨ FCI ç®—æ³•å¤„ç†å¯èƒ½å­˜åœ¨çš„éšå˜é‡ã€‚\n",
    "\n",
    "```python\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "\n",
    "graph_fci = fci(data, alpha=0.05)\n",
    "# æ¯”è¾ƒ FCI å’Œ PC çš„ç»“æœ\n",
    "```\n",
    "\n",
    "## æ€è€ƒé¢˜\n",
    "\n",
    "1. **å› æœå‘ç° vs å› æœæ¨æ–­**ï¼šä¸¤è€…çš„åŒºåˆ«å’Œè”ç³»æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ\n",
    "\n",
    "2. **æ—¶é—´åºåˆ—**ï¼šå¦‚æœæœ‰æ—¶é—´æˆ³ï¼Œå› æœå‘ç°ä¼šæ›´å®¹æ˜“å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "3. **å¹²é¢„å®éªŒ**ï¼šå¦‚æœå¯ä»¥åš A/B æµ‹è¯•ï¼Œè¿˜éœ€è¦å› æœå‘ç°å—ï¼Ÿ\n",
    "\n",
    "4. **ä¸šåŠ¡åº”ç”¨**ï¼šåœ¨ä½ çš„ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œå“ªäº›é—®é¢˜é€‚åˆç”¨å› æœå‘ç°ï¼Ÿå“ªäº›ä¸é€‚åˆï¼Ÿ\n",
    "\n",
    "5. **ä¼¦ç†é—®é¢˜**ï¼šè‡ªåŠ¨å­¦ä¹ å› æœå…³ç³»å¯èƒ½å¸¦æ¥ä»€ä¹ˆé£é™©ï¼Ÿå¦‚ä½•è§„é¿ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# æ€»ç»“\n",
    "\n",
    "## å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **å› æœå‘ç°çš„ç›®æ ‡**\n",
    "   - ä»æ•°æ®å­¦ä¹ å› æœç»“æ„\n",
    "   - åŒºåˆ†ç›¸å…³å’Œå› æœ\n",
    "\n",
    "2. **ä¸»è¦æ–¹æ³•**\n",
    "   - åŸºäºçº¦æŸ: PC, FCI (æ¡ä»¶ç‹¬ç«‹æ€§)\n",
    "   - åŸºäºå¾—åˆ†: GES (BIC ä¼˜åŒ–)\n",
    "   - å‡½æ•°å› æœæ¨¡å‹: LiNGAM (éé«˜æ–¯æ€§)\n",
    "\n",
    "3. **å®è·µå»ºè®®**\n",
    "   - éœ€è¦å¤§æ ·æœ¬ (1000+)\n",
    "   - éªŒè¯å‡è®¾\n",
    "   - ç»“åˆé¢†åŸŸçŸ¥è¯†\n",
    "   - ä½¿ç”¨å¤šç§æ–¹æ³•äº¤å‰éªŒè¯\n",
    "\n",
    "4. **å±€é™æ€§**\n",
    "   - é©¬å°”å¯å¤«ç­‰ä»·\n",
    "   - éšå˜é‡é—®é¢˜\n",
    "   - å¼ºå‡è®¾ä¾èµ–\n",
    "\n",
    "## è¿›ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "- **ä¹¦ç±**: \n",
    "  - *Causation, Prediction, and Search* (Spirtes et al.)\n",
    "  - *Elements of Causal Inference* (Peters et al.)\n",
    "\n",
    "- **å·¥å…·**: \n",
    "  - causal-learn (Python)\n",
    "  - pcalg (R)\n",
    "  - Tetrad (GUI)\n",
    "\n",
    "- **å‰æ²¿**:\n",
    "  - æ·±åº¦å­¦ä¹  + å› æœå‘ç°\n",
    "  - éçº¿æ€§å› æœå‘ç°\n",
    "  - å› æœè¡¨å¾å­¦ä¹ \n",
    "\n",
    "## ä¸‹ä¸€æ­¥\n",
    "\n",
    "å­¦ä¹ **å› æœæ¨æ–­ä¸æœºå™¨å­¦ä¹ çš„ç»“åˆ**ï¼š\n",
    "- Causal ML\n",
    "- Meta-learners\n",
    "- Double ML\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œä½ å®Œæˆå› æœå‘ç°çš„å­¦ä¹ ï¼ğŸ‰**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å› æœç»“æ„çš„æ–¹æ³•ã€‚ç»“åˆå‰é¢å­¦åˆ°çš„å› æœæ¨æ–­æŠ€æœ¯ï¼Œä½ ç°åœ¨å¯ä»¥ï¼š\n",
    "1. å‘ç°å› æœå…³ç³» (Causal Discovery)\n",
    "2. ä¼°è®¡å› æœæ•ˆåº” (Causal Inference)\n",
    "3. é¢„æµ‹å¹²é¢„ç»“æœ (Causal Prediction)\n",
    "\n",
    "ç»§ç»­æ¢ç´¢å› æœç§‘å­¦çš„å¥‡å¦™ä¸–ç•Œå§ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}