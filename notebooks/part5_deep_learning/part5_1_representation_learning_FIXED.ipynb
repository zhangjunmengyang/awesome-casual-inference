{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.1: è¡¨ç¤ºå­¦ä¹ ä¸ IPM ç†è®º - æ·±åº¦å› æœæ¨æ–­çš„æ•°å­¦åŸºç¡€\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£ä¸ºä»€ä¹ˆéœ€è¦è¡¨ç¤ºå­¦ä¹  (Representation Learning)\n",
    "2. æŒæ¡ IPM (Integral Probability Metrics) çš„ç†è®ºåŸºç¡€\n",
    "3. æ·±å…¥ç†è§£ MMD å’Œ Wasserstein è·ç¦»\n",
    "4. å®ç°ä»é›¶å¼€å§‹çš„è¡¨ç¤ºå­¦ä¹ å’Œå¹³è¡¡æ£€éªŒ\n",
    "5. ä¸º TARNet/DragonNet æ‰“ä¸‹åšå®åŸºç¡€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: æ•°æ®ç”Ÿæˆ (å®Œæ•´ç­”æ¡ˆ)\n",
    "\n",
    "### ç»ƒä¹  1.1: ç”Ÿæˆéçº¿æ€§æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_data(\n",
    "    n: int = 1000,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆéçº¿æ€§æ•°æ® - å®Œæ•´ç­”æ¡ˆ\n",
    "    \n",
    "    DGP:\n",
    "    - X1, X2 ~ N(0, 1)\n",
    "    - Phi1 = sin(X1), Phi2 = X1 * X2\n",
    "    - T ~ Bernoulli(sigmoid(Phi1 + 0.5*Phi2))\n",
    "    - Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç­”æ¡ˆ: ç”ŸæˆåŸå§‹ç‰¹å¾\n",
    "    X1 = np.random.randn(n)\n",
    "    X2 = np.random.randn(n)\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®¡ç®—æœ‰ç”¨ç‰¹å¾\n",
    "    Phi1 = np.sin(X1)\n",
    "    Phi2 = X1 * X2\n",
    "    \n",
    "    # ç­”æ¡ˆ: ç”Ÿæˆå¤„ç†\n",
    "    logit = Phi1 + 0.5 * Phi2\n",
    "    propensity = 1 / (1 + np.exp(-logit))\n",
    "    T = np.random.binomial(1, propensity, n)\n",
    "    \n",
    "    # ç­”æ¡ˆ: ç”Ÿæˆç»“æœ\n",
    "    noise = np.random.randn(n) * 0.5\n",
    "    Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise\n",
    "    \n",
    "    X = np.column_stack([X1, X2])\n",
    "    \n",
    "    return X, T, Y\n",
    "\n",
    "# æµ‹è¯•\n",
    "X, T, Y = generate_nonlinear_data()\n",
    "print(f\"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ\")\n",
    "print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}, T={T.shape}, Y={Y.shape}\")\n",
    "print(f\"å¤„ç†æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "print(f\"å¹³å‡ç»“æœ: {Y.mean():.4f}\")\n",
    "print(f\"\\nçœŸå® ATE = 2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹  1.2: æœ´ç´ çº¿æ€§ä¼°è®¡ (å®Œæ•´ç­”æ¡ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_linear_estimation(X, T, Y):\n",
    "    \"\"\"\n",
    "    çº¿æ€§å›å½’ä¼°è®¡ ATE - å®Œæ•´ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    # ç­”æ¡ˆ: æ„é€ ç‰¹å¾çŸ©é˜µ\n",
    "    features = np.column_stack([T, X])\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®­ç»ƒçº¿æ€§å›å½’\n",
    "    model = LinearRegression()\n",
    "    model.fit(features, Y)\n",
    "    \n",
    "    # ç­”æ¡ˆ: è¿”å› T çš„ç³»æ•°\n",
    "    ate_estimate = model.coef_[0]\n",
    "    \n",
    "    return ate_estimate\n",
    "\n",
    "# æµ‹è¯•\n",
    "ate_linear = naive_linear_estimation(X, T, Y)\n",
    "print(f\"âœ… çº¿æ€§ä¼°è®¡ ATE: {ate_linear:.4f}\")\n",
    "print(f\"çœŸå® ATE: 2.0\")\n",
    "print(f\"è¯¯å·®: {abs(ate_linear - 2.0):.4f}\")\n",
    "print(f\"\\nğŸ’¡ ç”±äºå¿½ç•¥äº†éçº¿æ€§å…³ç³» (sin(X1), X1*X2)ï¼Œçº¿æ€§ä¼°è®¡å­˜åœ¨åå·®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ (å®Œæ•´ç­”æ¡ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRepresentation(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€å•è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ - å®Œæ•´ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, repr_dim: int = 10, hidden_dim: int = 20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç­”æ¡ˆ: å®šä¹‰ç½‘ç»œå±‚\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, repr_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def train_representation(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    repr_dim: int = 10,\n",
    "    n_epochs: int = 100\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    è®­ç»ƒè¡¨ç¤ºå­¦ä¹ ç½‘ç»œ - å®Œæ•´ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    # è½¬æ¢ä¸º Tensor\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    Y_tensor = torch.FloatTensor(Y).unsqueeze(1)\n",
    "    \n",
    "    # æ¨¡å‹\n",
    "    repr_model = SimpleRepresentation(input_dim=X.shape[1], repr_dim=repr_dim)\n",
    "    \n",
    "    # ç­”æ¡ˆ: å®šä¹‰é¢„æµ‹å¤´\n",
    "    prediction_head = nn.Linear(repr_dim, 1)\n",
    "    \n",
    "    # ç­”æ¡ˆ: å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±\n",
    "    params = list(repr_model.parameters()) + list(prediction_head.parameters())\n",
    "    optimizer = optim.Adam(params, lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        phi = repr_model(X_tensor)\n",
    "        y_pred = prediction_head(phi)\n",
    "        loss = criterion(y_pred, Y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return repr_model\n",
    "\n",
    "# æµ‹è¯•\n",
    "repr_model = train_representation(X, T, Y, repr_dim=10, n_epochs=100)\n",
    "print(\"âœ… è¡¨ç¤ºå­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: å¯è§†åŒ–ä¸å¹³è¡¡æ£€æŸ¥ (å®Œæ•´ç­”æ¡ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_representation(\n",
    "    repr_model: nn.Module,\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–è¡¨ç¤ºç©ºé—´ - å®Œæ•´ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    repr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        \n",
    "        # ç­”æ¡ˆ: æå–è¡¨ç¤º\n",
    "        phi = repr_model(X_tensor)\n",
    "        phi = phi.numpy()\n",
    "    \n",
    "    # ç­”æ¡ˆ: PCA é™ç»´åˆ° 2D\n",
    "    if phi.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        phi = pca.fit_transform(phi)\n",
    "    \n",
    "    return phi, T\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "phi_2d, _ = visualize_representation(repr_model, X, T)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[T==0, 0], X[T==0, 1], alpha=0.5, label='Control', s=10)\n",
    "plt.scatter(X[T==1, 0], X[T==1, 1], alpha=0.5, label='Treated', s=10)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('åŸå§‹ç‰¹å¾ç©ºé—´')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(phi_2d[T==0, 0], phi_2d[T==0, 1], alpha=0.5, label='Control', s=10)\n",
    "plt.scatter(phi_2d[T==1, 0], phi_2d[T==1, 1], alpha=0.5, label='Treated', s=10)\n",
    "plt.xlabel('Phi1 (PCA)')\n",
    "plt.ylabel('Phi2 (PCA)')\n",
    "plt.title('å­¦ä¹ åˆ°çš„è¡¨ç¤ºç©ºé—´')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… å¯è§†åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_representation_balance(\n",
    "    phi: np.ndarray,\n",
    "    T: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    è¡¨ç¤ºå¹³è¡¡æ€§æ£€æŸ¥ - å®Œæ•´ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    phi_treated = phi[T == 1]\n",
    "    phi_control = phi[T == 0]\n",
    "    \n",
    "    mean_t = phi_treated.mean(axis=0)\n",
    "    mean_c = phi_control.mean(axis=0)\n",
    "    std_all = phi.std(axis=0) + 1e-8\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®¡ç®— SMD\n",
    "    smd = np.abs(mean_t - mean_c) / std_all\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®¡ç®— MMD (ç®€åŒ–ç‰ˆ)\n",
    "    mmd = np.sum((mean_t - mean_c)**2)\n",
    "    \n",
    "    return {\n",
    "        'smd_mean': np.mean(smd),\n",
    "        'smd_max': np.max(smd),\n",
    "        'mmd': mmd\n",
    "    }\n",
    "\n",
    "# æµ‹è¯•\n",
    "repr_model.eval()\n",
    "with torch.no_grad():\n",
    "    phi = repr_model(torch.FloatTensor(X)).numpy()\n",
    "\n",
    "balance = check_representation_balance(phi, T)\n",
    "print(\"âœ… å¹³è¡¡æ€§æ£€æŸ¥ç»“æœ:\")\n",
    "print(f\"å¹³å‡ SMD: {balance['smd_mean']:.4f}\")\n",
    "print(f\"æœ€å¤§ SMD: {balance['smd_max']:.4f}\")\n",
    "print(f\"MMD: {balance['mmd']:.4f}\")\n",
    "print(f\"\\nğŸ’¡ SMD < 0.1 è¡¨ç¤ºè‰¯å¥½å¹³è¡¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: IPM ç†è®ºæ·±å…¥ - MMD å®ç° (å®Œæ•´ç­”æ¡ˆ)\n",
    "\n",
    "### æ•°å­¦æ¨å¯¼: MMD æ ¸æŠ€å·§\n",
    "\n",
    "**å®šä¹‰**: å¯¹äºä¸¤ä¸ªåˆ†å¸ƒ $P$ å’Œ $Q$ï¼ŒMMD å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\\text{MMD}^2(P, Q) = \\left\\| \\mathbb{E}_{x \\sim P}[\\phi(x)] - \\mathbb{E}_{y \\sim Q}[\\phi(y)] \\right\\|^2$$\n",
    "\n",
    "**å±•å¼€**:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{MMD}^2(P, Q) &= \\left\\langle \\mathbb{E}_P[\\phi(x)], \\mathbb{E}_P[\\phi(x)] \\right\\rangle \\\\\n",
    "&\\quad - 2 \\left\\langle \\mathbb{E}_P[\\phi(x)], \\mathbb{E}_Q[\\phi(y)] \\right\\rangle \\\\\n",
    "&\\quad + \\left\\langle \\mathbb{E}_Q[\\phi(y)], \\mathbb{E}_Q[\\phi(y)] \\right\\rangle\n",
    "\\end{align}$$\n",
    "\n",
    "**æ ¸æŠ€å·§**: ä»¤ $k(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$ï¼Œåˆ™ï¼š\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{MMD}^2(P, Q) &= \\mathbb{E}_{x, x' \\sim P}[k(x, x')] \\\\\n",
    "&\\quad - 2\\mathbb{E}_{x \\sim P, y \\sim Q}[k(x, y)] \\\\\n",
    "&\\quad + \\mathbb{E}_{y, y' \\sim Q}[k(y, y')]\n",
    "\\end{align}$$\n",
    "\n",
    "**æ— åä¼°è®¡**: ç»™å®šæ ·æœ¬ $\\{x_i\\}_{i=1}^n \\sim P$ å’Œ $\\{y_j\\}_{j=1}^m \\sim Q$ï¼š\n",
    "\n",
    "$$\\widehat{\\text{MMD}}^2 = \\frac{1}{n(n-1)}\\sum_{i \\neq i'} k(x_i, x_{i'}) - \\frac{2}{nm}\\sum_{i,j} k(x_i, y_j) + \\frac{1}{m(m-1)}\\sum_{j \\neq j'} k(y_j, y_{j'})$$\n",
    "\n",
    "æ³¨æ„å»æ‰å¯¹è§’çº¿é¡¹ä»¥è·å¾—æ— åä¼°è®¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    RBF æ ¸å‡½æ•° - å®Œæ•´ç­”æ¡ˆ\n",
    "    \n",
    "    k(x, y) = exp(-gamma * ||x - y||^2)\n",
    "    \"\"\"\n",
    "    # ç­”æ¡ˆ: è®¡ç®—æ¬§æ°è·ç¦»å¹³æ–¹\n",
    "    XX = np.sum(X**2, axis=1).reshape(-1, 1)  # (n, 1)\n",
    "    YY = np.sum(Y**2, axis=1).reshape(1, -1)  # (1, m)\n",
    "    XY = X @ Y.T  # (n, m)\n",
    "    \n",
    "    dist_sq = XX + YY - 2 * XY\n",
    "    \n",
    "    # ç­”æ¡ˆ: åº”ç”¨é«˜æ–¯æ ¸\n",
    "    K = np.exp(-gamma * dist_sq)\n",
    "    \n",
    "    return K\n",
    "\n",
    "\n",
    "def compute_mmd(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    kernel: str = 'rbf',\n",
    "    gamma: float = 1.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®— MMD - å®Œæ•´ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®¡ç®—æ ¸çŸ©é˜µ\n",
    "    K_XX = rbf_kernel(X, X, gamma)\n",
    "    K_XY = rbf_kernel(X, Y, gamma)\n",
    "    K_YY = rbf_kernel(Y, Y, gamma)\n",
    "    \n",
    "    # ç­”æ¡ˆ: è®¡ç®— MMD^2 (æ— åä¼°è®¡)\n",
    "    term1 = (K_XX.sum() - np.trace(K_XX)) / (n * (n - 1))\n",
    "    term2 = K_XY.sum() / (n * m)\n",
    "    term3 = (K_YY.sum() - np.trace(K_YY)) / (m * (m - 1))\n",
    "    \n",
    "    mmd_sq = term1 - 2 * term2 + term3\n",
    "    \n",
    "    return np.sqrt(max(mmd_sq, 0))\n",
    "\n",
    "# æµ‹è¯•\n",
    "repr_model.eval()\n",
    "with torch.no_grad():\n",
    "    phi = repr_model(torch.FloatTensor(X)).numpy()\n",
    "\n",
    "phi_treated = phi[T == 1]\n",
    "phi_control = phi[T == 0]\n",
    "\n",
    "mmd_original = compute_mmd(X[T == 1], X[T == 0], gamma=0.5)\n",
    "mmd_repr = compute_mmd(phi_treated, phi_control, gamma=0.5)\n",
    "\n",
    "print(\"âœ… MMD è®¡ç®—å®Œæˆ\")\n",
    "print(\"=\" * 50)\n",
    "print(\"MMD å¯¹æ¯”åˆ†æ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"åŸå§‹ç‰¹å¾ç©ºé—´ MMD: {mmd_original:.6f}\")\n",
    "print(f\"è¡¨ç¤ºç©ºé—´ MMD:     {mmd_repr:.6f}\")\n",
    "print(f\"æ”¹è¿›æ¯”ä¾‹: {(1 - mmd_repr/mmd_original)*100:.2f}%\")\n",
    "print(\"\\nğŸ’¡ MMD è¶Šå°ï¼Œä¸¤ç»„åˆ†å¸ƒè¶Šæ¥è¿‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Wasserstein è·ç¦»å®ç°\n",
    "\n",
    "### æ•°å­¦æ¨å¯¼: Wasserstein è·ç¦»\n",
    "\n",
    "**Kantorovich-Rubinstein å¯¹å¶**:\n",
    "\n",
    "$$W_1(P, Q) = \\sup_{\\|f\\|_L \\leq 1} \\left| \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{y \\sim Q}[f(y)] \\right|$$\n",
    "\n",
    "å…¶ä¸­ $\\|f\\|_L \\leq 1$ è¡¨ç¤º $f$ æ˜¯ 1-Lipschitz å‡½æ•°ã€‚\n",
    "\n",
    "**ä¸€ç»´æƒ…å†µçš„ç®€åŒ–**: å¯¹äºä¸€ç»´åˆ†å¸ƒï¼ŒWasserstein è·ç¦»æœ‰é—­å¼è§£ï¼š\n",
    "\n",
    "$$W_1(P, Q) = \\int_0^1 |F_P^{-1}(u) - F_Q^{-1}(u)| du$$\n",
    "\n",
    "å…¶ä¸­ $F^{-1}$ æ˜¯åˆ†ä½å‡½æ•°ã€‚\n",
    "\n",
    "**Sliced Wasserstein Distance**: å¯¹äºé«˜ç»´æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºæŠ•å½±ï¼š\n",
    "\n",
    "$$\\text{SWD}(P, Q) = \\int_{\\mathbb{S}^{d-1}} W_1(P_\\theta, Q_\\theta) d\\theta$$\n",
    "\n",
    "å…¶ä¸­ $P_\\theta$ æ˜¯ $P$ åœ¨æ–¹å‘ $\\theta$ ä¸Šçš„æŠ•å½±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_distance(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    n_projections: int = 50\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Sliced Wasserstein Distance - å®Œæ•´å®ç°\n",
    "    \"\"\"\n",
    "    dim = X.shape[1]\n",
    "    distances = []\n",
    "    \n",
    "    for _ in range(n_projections):\n",
    "        # éšæœºæ–¹å‘\n",
    "        theta = np.random.randn(dim)\n",
    "        theta = theta / np.linalg.norm(theta)\n",
    "        \n",
    "        # æŠ•å½±\n",
    "        X_proj = X @ theta\n",
    "        Y_proj = Y @ theta\n",
    "        \n",
    "        # ä¸€ç»´ Wasserstein è·ç¦»\n",
    "        dist = wasserstein_distance(X_proj, Y_proj)\n",
    "        distances.append(dist)\n",
    "    \n",
    "    return np.mean(distances)\n",
    "\n",
    "# æµ‹è¯•\n",
    "swd_original = sliced_wasserstein_distance(X[T == 1], X[T == 0], n_projections=50)\n",
    "swd_repr = sliced_wasserstein_distance(phi_treated, phi_control, n_projections=50)\n",
    "\n",
    "print(\"âœ… Sliced Wasserstein Distance è®¡ç®—å®Œæˆ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"åŸå§‹ç‰¹å¾ç©ºé—´ SWD: {swd_original:.6f}\")\n",
    "print(f\"è¡¨ç¤ºç©ºé—´ SWD:     {swd_repr:.6f}\")\n",
    "print(f\"æ”¹è¿›æ¯”ä¾‹: {(1 - swd_repr/swd_original)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ç†è®ºè¯¯å·®ç•Œ\n",
    "\n",
    "### IPM åœ¨å› æœæ¨æ–­ä¸­çš„ç†è®ºä¿è¯\n",
    "\n",
    "**å®šç† (Shalit et al., 2017)**:\n",
    "\n",
    "å‡è®¾ $\\Phi: \\mathcal{X} \\to \\mathcal{R}$ æ˜¯è¡¨ç¤ºå‡½æ•°ï¼Œ$h_0, h_1: \\mathcal{R} \\to \\mathbb{R}$ æ˜¯ç»“æœé¢„æµ‹å‡½æ•°ã€‚å®šä¹‰ï¼š\n",
    "\n",
    "$$\\epsilon_{\\text{h}} = \\mathbb{E}_{(x,t,y) \\sim P} [(y - h_t(\\Phi(x)))^2]$$\n",
    "\n",
    "ä¸ºé¢„æµ‹è¯¯å·®ã€‚åˆ™ ATE ä¼°è®¡çš„è¯¯å·®ä¸Šç•Œä¸ºï¼š\n",
    "\n",
    "$$\\epsilon_{\\text{ATE}} \\leq \\sqrt{\\epsilon_{\\text{h}}} + \\lambda \\cdot \\text{IPM}_{\\mathcal{F}}(P_{\\Phi}^{t=0}, P_{\\Phi}^{t=1})$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\lambda$ æ˜¯å‡è®¾å‡½æ•°çš„ Lipschitz å¸¸æ•°\n",
    "- $P_{\\Phi}^{t}$ æ˜¯è¡¨ç¤ºç©ºé—´ä¸­å¤„ç†ç»„ $t$ çš„åˆ†å¸ƒ\n",
    "\n",
    "**å«ä¹‰**:\n",
    "1. å‡å°‘é¢„æµ‹è¯¯å·® $\\epsilon_h$ â†’ æé«˜å‡†ç¡®æ€§\n",
    "2. å‡å°‘ IPM â†’ å¹³è¡¡å¤„ç†ç»„å’Œå¯¹ç…§ç»„\n",
    "3. ä¸¤è€…éœ€è¦æƒè¡¡ï¼š$\\alpha$ å‚æ•°\n",
    "\n",
    "**æœ€ä¼˜åŒ–ç›®æ ‡**:\n",
    "\n",
    "$$\\min_{\\Phi, h_0, h_1} \\epsilon_h + \\alpha \\cdot \\text{IPM}(P_{\\Phi}^{t=0}, P_{\\Phi}^{t=1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: é¢è¯•é¢˜ä¸ç¼–ç¨‹ç»ƒä¹ \n",
    "\n",
    "### ç†è®ºé¢è¯•é¢˜\n",
    "\n",
    "#### 1. ä¸ºä»€ä¹ˆæ·±åº¦å› æœæ¨¡å‹éœ€è¦è¡¨ç¤ºå¹³è¡¡ï¼Ÿ\n",
    "\n",
    "**å‚è€ƒç­”æ¡ˆ**:\n",
    "- **æ ¹æœ¬åŸå› **: å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„åå˜é‡åˆ†å¸ƒä¸åŒå¯¼è‡´æ··æ·†åå·®\n",
    "- **è¡¨ç¤ºå­¦ä¹ çš„ä½œç”¨**: å­¦ä¹ ä¸€ä¸ªè¡¨ç¤º $\\Phi(X)$ï¼Œä½¿å¾—åœ¨è¡¨ç¤ºç©ºé—´ä¸­ä¸¤ç»„åˆ†å¸ƒæ¥è¿‘\n",
    "- **ç†è®ºä¿è¯**: æ ¹æ®è¯¯å·®ç•Œï¼Œ$\\text{IPM}(P_{\\Phi}^{t=0}, P_{\\Phi}^{t=1})$ è¶Šå°ï¼ŒATE ä¼°è®¡è¶Šå‡†\n",
    "- **å®è·µæ„ä¹‰**: å‡å°‘é€‰æ‹©åå·®ï¼Œæé«˜åäº‹å®é¢„æµ‹çš„æ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "#### 2. MMD å’Œ Wasserstein è·ç¦»æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ\n",
    "\n",
    "**å‚è€ƒç­”æ¡ˆ**:\n",
    "\n",
    "| ç»´åº¦ | MMD | Wasserstein |\n",
    "|------|-----|-------------|\n",
    "| **å‡½æ•°ç±»** | RKHS (ä¾èµ–æ ¸å‡½æ•°) | 1-Lipschitz å‡½æ•° |\n",
    "| **å‡ ä½•æ„ä¹‰** | é«˜ç»´ç©ºé—´ä¸­å¿ƒç‚¹è·ç¦» | æœ€ä¼˜ä¼ è¾“è·ç¦» |\n",
    "| **è®¡ç®—å¤æ‚åº¦** | $O(n^2)$ | $O(n^3)$ æˆ– $O(n^2)$ (Sliced) |\n",
    "| **å‚æ•°æ•æ„Ÿæ€§** | å¯¹æ ¸å‚æ•° $\\gamma$ æ•æ„Ÿ | å¯¹æŠ•å½±æ•°æ•æ„Ÿ (Sliced) |\n",
    "| **é€‚ç”¨åœºæ™¯** | ä¸­ä½ç»´ï¼Œå¿«é€ŸåŸå‹ | é«˜ç»´ï¼Œå›¾åƒæ•°æ® |\n",
    "\n",
    "**æ ¸å¿ƒåŒºåˆ«**: MMD æ¯”è¾ƒåˆ†å¸ƒçš„"å‡å€¼"ï¼ŒWasserstein è€ƒè™‘"å‡ ä½•ç»“æ„"ã€‚\n",
    "\n",
    "#### 3. åœ¨æŸå¤±å‡½æ•°ä¸­å¦‚ä½•é€‰æ‹©å¹³è¡¡æ­£åˆ™åŒ–æƒé‡ $\\alpha$ï¼Ÿ\n",
    "\n",
    "**å‚è€ƒç­”æ¡ˆ**:\n",
    "\n",
    "**ç†è®ºæŒ‡å¯¼**:\n",
    "$$\\alpha \\approx \\frac{\\sqrt{\\epsilon_h}}{\\lambda \\cdot \\text{IPM}}$$\n",
    "\n",
    "**å®è·µç­–ç•¥**:\n",
    "1. **ç½‘æ ¼æœç´¢**: $\\alpha \\in \\{0.01, 0.1, 1.0, 10, 100\\}$\n",
    "2. **éªŒè¯é›†é€‰æ‹©**: ä½¿ç”¨ PEHE (å¦‚æœæœ‰çœŸå® ITE)\n",
    "3. **å¯å‘å¼è§„åˆ™**:\n",
    "   - æ··æ·†ä¸¥é‡ â†’ å¤§ $\\alpha$ (å¦‚ 10)\n",
    "   - RCT æ•°æ® â†’ å° $\\alpha$ (å¦‚ 0.1)\n",
    "4. **åŠ¨æ€è°ƒæ•´**: è®­ç»ƒåˆæœŸå¤§ $\\alpha$ï¼ŒåæœŸå‡å°\n",
    "\n",
    "**$\\alpha$ çš„å½±å“**:\n",
    "- $\\alpha$ å¤ªå° â†’ æ¬ å¹³è¡¡ï¼Œæ··æ·†åå·®å¤§\n",
    "- $\\alpha$ å¤ªå¤§ â†’ è¿‡å¹³è¡¡ï¼Œé¢„æµ‹èƒ½åŠ›å·®\n",
    "\n",
    "#### 4. IPM è¯¯å·®ç•Œä¸­çš„ Lipschitz å¸¸æ•° $\\lambda$ ä»£è¡¨ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "**å‚è€ƒç­”æ¡ˆ**:\n",
    "- **å®šä¹‰**: $|h(\\phi_1) - h(\\phi_2)| \\leq \\lambda \\|\\phi_1 - \\phi_2\\|$\n",
    "- **å«ä¹‰**: å‡è®¾å‡½æ•°çš„"æ•æ„Ÿåº¦"â€”â€”è¡¨ç¤ºå˜åŒ–å¤šå°‘ä¼šå¯¼è‡´é¢„æµ‹å˜åŒ–å¤šå°‘\n",
    "- **å½±å“**: \n",
    "  - $\\lambda$ å¤§ â†’ IPM çš„å½±å“å¤§ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„å¹³è¡¡\n",
    "  - $\\lambda$ å° â†’ è¡¨ç¤ºå¹³è¡¡ä¸é‚£ä¹ˆé‡è¦\n",
    "- **å®è·µ**: é€šå¸¸ä¸æ˜¾å¼è®¡ç®—ï¼Œé€šè¿‡ $\\alpha$ éšå¼è°ƒæ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¼–ç¨‹é¢è¯•é¢˜\n",
    "\n",
    "#### é¢˜ç›® 1: ä»é›¶å®ç° TARNet çš„å…±äº«è¡¨ç¤ºå±‚\n",
    "\n",
    "**è¦æ±‚**:\n",
    "1. å®ç°å…±äº«è¡¨ç¤ºç½‘ç»œ\n",
    "2. å®ç°åŒå¤´è¾“å‡º (Y0, Y1)\n",
    "3. ä½¿ç”¨ Factual Loss è®­ç»ƒ\n",
    "4. æ·»åŠ  IPM å¹³è¡¡æ­£åˆ™åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TARNetSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    ç¼–ç¨‹é¢˜: å®ç°ç®€åŒ–ç‰ˆ TARNet\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, repr_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: å®ç°å…±äº«è¡¨ç¤ºå±‚\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, repr_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # TODO: å®ç°åŒå¤´\n",
    "        self.head_0 = nn.Sequential(\n",
    "            nn.Linear(repr_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "        \n",
    "        self.head_1 = nn.Sequential(\n",
    "            nn.Linear(repr_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "        phi = self.shared(x)\n",
    "        y0 = self.head_0(phi).squeeze()\n",
    "        y1 = self.head_1(phi).squeeze()\n",
    "        \n",
    "        if t is not None:\n",
    "            # Factual prediction\n",
    "            y = torch.where(t == 1, y1, y0)\n",
    "            return y, y0, y1, phi\n",
    "        else:\n",
    "            return y0, y1, phi\n",
    "    \n",
    "    def predict_ite(self, x):\n",
    "        y0, y1, _ = self.forward(x)\n",
    "        return y1 - y0\n",
    "\n",
    "\n",
    "def train_tarnet_with_balance(\n",
    "    model: TARNetSimple,\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    alpha: float = 1.0,\n",
    "    n_epochs: int = 100,\n",
    "    lr: float = 1e-3\n",
    "):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒ TARNet å¹¶åŠ å…¥ IPM å¹³è¡¡æ­£åˆ™åŒ–\n",
    "    \"\"\"\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    T_tensor = torch.FloatTensor(T)\n",
    "    Y_tensor = torch.FloatTensor(Y)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_pred, y0, y1, phi = model(X_tensor, T_tensor)\n",
    "        \n",
    "        # Factual Loss\n",
    "        factual_loss = nn.MSELoss()(y_pred, Y_tensor)\n",
    "        \n",
    "        # IPM Balance Loss\n",
    "        phi_np = phi.detach().numpy()\n",
    "        phi_t = phi_np[T == 1]\n",
    "        phi_c = phi_np[T == 0]\n",
    "        \n",
    "        if len(phi_t) > 0 and len(phi_c) > 0:\n",
    "            balance_loss = compute_mmd(phi_t, phi_c, gamma=0.5)\n",
    "        else:\n",
    "            balance_loss = 0.0\n",
    "        \n",
    "        # Total Loss\n",
    "        loss = factual_loss + alpha * balance_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Factual={factual_loss:.4f}, Balance={balance_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"\\n=== ç¼–ç¨‹é¢˜æµ‹è¯•: TARNet with Balance ===\")\n",
    "tarnet = TARNetSimple(input_dim=X.shape[1])\n",
    "tarnet = train_tarnet_with_balance(tarnet, X, T, Y, alpha=1.0, n_epochs=100)\n",
    "\n",
    "# è¯„ä¼°\n",
    "tarnet.eval()\n",
    "with torch.no_grad():\n",
    "    ite_pred = tarnet.predict_ite(torch.FloatTensor(X)).numpy()\n",
    "\n",
    "ate_pred = ite_pred.mean()\n",
    "print(f\"\\nâœ… é¢„æµ‹ ATE: {ate_pred:.4f}\")\n",
    "print(f\"çœŸå® ATE: 2.0\")\n",
    "print(f\"è¯¯å·®: {abs(ate_pred - 2.0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é¢˜ç›® 2: å®ç° IPM çš„æ¢¯åº¦åå‘ä¼ æ’­\n",
    "\n",
    "**è¦æ±‚**: å°† MMD é›†æˆåˆ° PyTorch è‡ªåŠ¨å¾®åˆ†ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_loss_pytorch(\n",
    "    phi_t: torch.Tensor,\n",
    "    phi_c: torch.Tensor,\n",
    "    gamma: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    PyTorch ç‰ˆæœ¬çš„ MMD Loss (å¯å¾®åˆ†)\n",
    "    \n",
    "    ç¼–ç¨‹é¢˜: å®ç°å¯ä»¥åå‘ä¼ æ’­çš„ MMD\n",
    "    \"\"\"\n",
    "    def rbf_kernel_torch(X, Y, gamma):\n",
    "        XX = torch.sum(X**2, dim=1, keepdim=True)\n",
    "        YY = torch.sum(Y**2, dim=1, keepdim=True)\n",
    "        XY = X @ Y.T\n",
    "        \n",
    "        dist_sq = XX + YY.T - 2 * XY\n",
    "        return torch.exp(-gamma * dist_sq)\n",
    "    \n",
    "    n = phi_t.shape[0]\n",
    "    m = phi_c.shape[0]\n",
    "    \n",
    "    K_TT = rbf_kernel_torch(phi_t, phi_t, gamma)\n",
    "    K_TC = rbf_kernel_torch(phi_t, phi_c, gamma)\n",
    "    K_CC = rbf_kernel_torch(phi_c, phi_c, gamma)\n",
    "    \n",
    "    # æ— åä¼°è®¡\n",
    "    term1 = (K_TT.sum() - torch.trace(K_TT)) / (n * (n - 1))\n",
    "    term2 = K_TC.sum() / (n * m)\n",
    "    term3 = (K_CC.sum() - torch.trace(K_CC)) / (m * (m - 1))\n",
    "    \n",
    "    mmd_sq = term1 - 2 * term2 + term3\n",
    "    \n",
    "    return mmd_sq  # è¿”å›å¹³æ–¹ï¼Œé¿å… sqrt çš„æ¢¯åº¦é—®é¢˜\n",
    "\n",
    "# æµ‹è¯•æ¢¯åº¦\n",
    "print(\"\\n=== ç¼–ç¨‹é¢˜æµ‹è¯•: MMD æ¢¯åº¦ ===\")\n",
    "phi_t_tensor = torch.FloatTensor(phi_treated).requires_grad_(True)\n",
    "phi_c_tensor = torch.FloatTensor(phi_control).requires_grad_(True)\n",
    "\n",
    "mmd = mmd_loss_pytorch(phi_t_tensor, phi_c_tensor, gamma=0.5)\n",
    "mmd.backward()\n",
    "\n",
    "print(f\"âœ… MMD å€¼: {mmd.item():.6f}\")\n",
    "print(f\"âœ… æ¢¯åº¦å·²è®¡ç®—: phi_t.grad.shape = {phi_t_tensor.grad.shape}\")\n",
    "print(f\"âœ… æ¢¯åº¦èŒƒæ•°: {phi_t_tensor.grad.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒæ”¶è·\n",
    "\n",
    "1. **è¡¨ç¤ºå­¦ä¹ **: è‡ªåŠ¨å­¦ä¹ æœ‰ç”¨ç‰¹å¾ï¼Œæ•è·éçº¿æ€§å…³ç³»\n",
    "2. **IPM ç†è®º**: æä¾›ç†è®ºä¿è¯ï¼ŒæŒ‡å¯¼æŸå¤±å‡½æ•°è®¾è®¡\n",
    "3. **MMD vs Wasserstein**: ä¸¤ç§ä¸»æµ IPMï¼Œå„æœ‰ä¼˜åŠ£\n",
    "4. **è¯¯å·®ç•Œ**: é¢„æµ‹å‡†ç¡®æ€§ + åˆ†å¸ƒå¹³è¡¡ = å¥½çš„å› æœä¼°è®¡\n",
    "5. **å®ç°ç»†èŠ‚**: ä»ç†è®ºå…¬å¼åˆ° PyTorch ä»£ç \n",
    "\n",
    "### ä¸‹ä¸€æ­¥: TARNet & DragonNet\n",
    "\n",
    "åœ¨ä¸‹ä¸€ä¸ª notebook ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ :\n",
    "- TARNet çš„å®Œæ•´æ¶æ„\n",
    "- DragonNet çš„å€¾å‘å¾—åˆ†æ­£åˆ™åŒ–\n",
    "- Targeted Regularization çš„ç†è®ºä¸å®ç°\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆè¡¨ç¤ºå­¦ä¹ ä¸ IPM ç†è®ºçš„æ·±å…¥å­¦ä¹ ï¼** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
