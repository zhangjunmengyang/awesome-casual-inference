{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive 04: å†·å¯åŠ¨åœºæ™¯ä¸‹çš„ Uplift å»ºæ¨¡\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ notebook åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. **ç†è§£** å†·å¯åŠ¨åœºæ™¯ä¸‹ Uplift å»ºæ¨¡çš„æŒ‘æˆ˜\n",
    "2. **æŒæ¡** Thompson Sampling åœ¨ Uplift ä¸­çš„åº”ç”¨\n",
    "3. **å®ç°** å¸¦æœ‰å…ˆéªŒæ›´æ–°çš„åœ¨çº¿ Uplift ç­–ç•¥\n",
    "4. **è®¾è®¡** ä»æ¢ç´¢åˆ°åˆ©ç”¨çš„å¹³æ»‘è¿‡æ¸¡æœºåˆ¶\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– èƒŒæ™¯æ•…äº‹\n",
    "\n",
    "> **åœºæ™¯ï¼šæ–°ç”¨æˆ·é¦–å•ä¼˜æƒ ç­–ç•¥**\n",
    ">\n",
    "> ä½ æ˜¯ç”µå•†å¹³å°çš„ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ–°ç”¨æˆ·çš„é¦–å•ä¼˜æƒ ç­–ç•¥ã€‚é—®é¢˜æ¥äº†ï¼š\n",
    ">\n",
    "> - æ–°ç”¨æˆ·æ²¡æœ‰å†å²è´­ä¹°æ•°æ®\n",
    "> - ä¼ ç»Ÿ Uplift æ¨¡å‹éœ€è¦ç”¨æˆ·ç‰¹å¾æ¥é¢„æµ‹å¤„ç†æ•ˆåº”\n",
    "> - å¯¹æ–°ç”¨æˆ·ï¼Œæˆ‘ä»¬åªæœ‰æ³¨å†Œæ—¶çš„åŸºç¡€ä¿¡æ¯ï¼ˆå¹´é¾„ã€åŸå¸‚ã€æ¸ é“ï¼‰\n",
    ">\n",
    "> äº§å“ç»ç†é—®ï¼š**\"å¯¹äºä¸€ä¸ªåˆšæ³¨å†Œçš„ç”¨æˆ·ï¼Œæˆ‘ä»¬åº”è¯¥å‘ä»€ä¹ˆä¼˜æƒ åˆ¸ï¼Ÿ\"**\n",
    ">\n",
    "> **æœ¬ notebook å°†è§£å†³è¿™ä¸ªå†·å¯åŠ¨ Uplift é—®é¢˜ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: å†·å¯åŠ¨é—®é¢˜åˆ†æ\n",
    "\n",
    "### 1.1 ä¸ºä»€ä¹ˆå†·å¯åŠ¨æ˜¯ Uplift çš„éš¾é¢˜ï¼Ÿ\n",
    "\n",
    "| ä¼ ç»Ÿ Uplift | å†·å¯åŠ¨åœºæ™¯ |\n",
    "|------------|----------|\n",
    "| ä¸°å¯Œçš„ç”¨æˆ·å†å²ç‰¹å¾ | åªæœ‰æ³¨å†Œæ—¶çš„åŸºç¡€ä¿¡æ¯ |\n",
    "| å¯ä»¥è®­ç»ƒç²¾ç¡®çš„ CATE æ¨¡å‹ | æ¨¡å‹é¢„æµ‹ä¸å¯é  |\n",
    "| ç›´æ¥ç”¨æ¨¡å‹é¢„æµ‹é€‰æ‹©å¤„ç† | éœ€è¦æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ |\n",
    "\n",
    "### 1.2 æ ¸å¿ƒæŒ‘æˆ˜\n",
    "\n",
    "1. **ä¿¡æ¯ä¸è¶³**ï¼šæ–°ç”¨æˆ·ç¼ºä¹è¡Œä¸ºæ•°æ®ï¼Œæ— æ³•å‡†ç¡®é¢„æµ‹å…¶å¯¹å¤„ç†çš„å“åº”\n",
    "2. **æ¢ç´¢-åˆ©ç”¨æƒè¡¡**ï¼šæ˜¯ç”¨å½“å‰æœ€ä½³ç­–ç•¥ï¼ˆåˆ©ç”¨ï¼‰è¿˜æ˜¯å°è¯•æ–°ç­–ç•¥æ”¶é›†æ•°æ®ï¼ˆæ¢ç´¢ï¼‰ï¼Ÿ\n",
    "3. **å†·å¯åŠ¨é€Ÿåº¦**ï¼šéœ€è¦å¿«é€Ÿç§¯ç´¯è¶³å¤Ÿæ•°æ®æ¥åšå‡ºå¯é å†³ç­–\n",
    "\n",
    "### 1.3 è§£å†³æ€è·¯\n",
    "\n",
    "```\n",
    "ä¼ ç»Ÿ Uplift:  ç”¨æˆ·ç‰¹å¾ â†’ CATE æ¨¡å‹ â†’ é€‰æ‹©å¤„ç†\n",
    "\n",
    "å†·å¯åŠ¨ Uplift: \n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ 1. è®¾ç½®åˆç†å…ˆéªŒ (åŸºäºç¾¤ä½“ç»Ÿè®¡)          â”‚\n",
    "  â”‚ 2. Thompson Sampling é€‰æ‹©å¤„ç†          â”‚\n",
    "  â”‚ 3. è§‚å¯Ÿç»“æœï¼Œæ›´æ–°åéªŒ                   â”‚\n",
    "  â”‚ 4. éšç€æ•°æ®ç§¯ç´¯ï¼Œé€æ¸æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥     â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import expit  # sigmoid\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Thompson Sampling åŸç†\n",
    "\n",
    "### 2.1 è´å¶æ–¯æ–¹æ³•å›é¡¾\n",
    "\n",
    "å¯¹äºäºŒå…ƒç»“æœï¼ˆè½¬åŒ–/ä¸è½¬åŒ–ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ Beta-Bernoulli æ¨¡å‹ï¼š\n",
    "\n",
    "- **å…ˆéªŒ**: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n",
    "- **ä¼¼ç„¶**: $Y | \\theta \\sim \\text{Bernoulli}(\\theta)$\n",
    "- **åéªŒ**: $\\theta | Y \\sim \\text{Beta}(\\alpha + \\sum Y, \\beta + n - \\sum Y)$\n",
    "\n",
    "### 2.2 Thompson Sampling ç®—æ³•\n",
    "\n",
    "```\n",
    "å¯¹äºæ¯ä¸ªç”¨æˆ·:\n",
    "  1. å¯¹æ¯ç§å¤„ç† kï¼Œä»åéªŒ Beta(Î±_k, Î²_k) é‡‡æ · Î¸Ì‚_k\n",
    "  2. é€‰æ‹© Î¸Ì‚ æœ€å¤§çš„å¤„ç†\n",
    "  3. è§‚å¯Ÿç»“æœ Y\n",
    "  4. æ›´æ–°åéªŒï¼šÎ±_k += Y, Î²_k += (1-Y)\n",
    "```\n",
    "\n",
    "**å…³é”®æ´è§**ï¼šThompson Sampling è‡ªç„¶åœ°å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼\n",
    "- ä¸ç¡®å®šæ€§é«˜çš„å¤„ç† â†’ é‡‡æ ·æ–¹å·®å¤§ â†’ æœ‰æœºä¼šè¢«é€‰ä¸­ï¼ˆæ¢ç´¢ï¼‰\n",
    "- æ•ˆæœå¥½çš„å¤„ç† â†’ åéªŒå‡å€¼é«˜ â†’ æ›´å¯èƒ½è¢«é€‰ä¸­ï¼ˆåˆ©ç”¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– Thompson Sampling çš„æ¢ç´¢-åˆ©ç”¨æƒè¡¡\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸åŒé˜¶æ®µçš„åéªŒåˆ†å¸ƒ\n",
    "stages = [\n",
    "    {'name': 'åˆå§‹é˜¶æ®µ (n=0)', 'params': [(1, 1), (1, 1), (1, 1)]},\n",
    "    {'name': 'æ¢ç´¢é˜¶æ®µ (n=20)', 'params': [(5, 17), (8, 14), (6, 16)]},\n",
    "    {'name': 'æ”¶æ•›é˜¶æ®µ (n=100)', 'params': [(25, 77), (45, 57), (30, 72)]},\n",
    "]\n",
    "\n",
    "treatment_names = ['æ— ä¼˜æƒ ', '5æŠ˜åˆ¸', 'æ»¡å‡åˆ¸']\n",
    "colors = ['#95a5a6', '#e74c3c', '#3498db']\n",
    "\n",
    "for col, stage in enumerate(stages):\n",
    "    ax = axes[0, col]\n",
    "    x = np.linspace(0, 1, 200)\n",
    "    \n",
    "    for i, (a, b) in enumerate(stage['params']):\n",
    "        y = stats.beta.pdf(x, a, b)\n",
    "        ax.plot(x, y, label=f\"{treatment_names[i]} (Î±={a}, Î²={b})\", \n",
    "               color=colors[i], linewidth=2)\n",
    "        ax.fill_between(x, y, alpha=0.2, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('è½¬åŒ–ç‡ Î¸')\n",
    "    ax.set_ylabel('å¯†åº¦')\n",
    "    ax.set_title(stage['name'])\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# æ¨¡æ‹Ÿé‡‡æ ·è¿‡ç¨‹\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "\n",
    "for col, stage in enumerate(stages):\n",
    "    ax = axes[1, col]\n",
    "    \n",
    "    samples = []\n",
    "    for a, b in stage['params']:\n",
    "        samples.append(np.random.beta(a, b, n_samples))\n",
    "    \n",
    "    # æ¯æ¬¡é‡‡æ ·é€‰æ‹©çš„å¤„ç†\n",
    "    choices = np.argmax(samples, axis=0)\n",
    "    choice_counts = np.bincount(choices, minlength=3)\n",
    "    \n",
    "    ax.bar(treatment_names, choice_counts / n_samples, color=colors)\n",
    "    ax.set_ylabel('é€‰æ‹©æ¦‚ç‡')\n",
    "    ax.set_title(f'å¤„ç†é€‰æ‹©åˆ†å¸ƒ ({stage[\"name\"]})')\n",
    "    \n",
    "    # æ ‡æ³¨é€‰æ‹©æ¦‚ç‡\n",
    "    for i, count in enumerate(choice_counts):\n",
    "        ax.text(i, count/n_samples + 0.02, f'{count/n_samples:.1%}', \n",
    "               ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè§‚å¯Ÿï¼š\")\n",
    "print(\"  åˆå§‹é˜¶æ®µï¼šä¸‰ç§å¤„ç†è¢«é€‰æ‹©çš„æ¦‚ç‡æ¥è¿‘å‡ç­‰ï¼ˆæ¢ç´¢ï¼‰\")\n",
    "print(\"  æ¢ç´¢é˜¶æ®µï¼šå¼€å§‹å‘æ•ˆæœå¥½çš„å¤„ç†å€¾æ–œ\")\n",
    "print(\"  æ”¶æ•›é˜¶æ®µï¼šé›†ä¸­åœ¨æœ€ä½³å¤„ç†ï¼ˆåˆ©ç”¨ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: åŸºç¡€ Thompson Sampling Uplift\n",
    "\n",
    "### ğŸ§ª TODO ç»ƒä¹  1: å®ç°åŸºç¡€ Thompson Sampling\n",
    "\n",
    "è¯·å®ç°ä¸€ä¸ªåŸºç¡€çš„ Thompson Sampling ç­–ç•¥ï¼Œç”¨äºé€‰æ‹©æœ€ä½³å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BasicThompsonSampling:\n    \"\"\"\n    åŸºç¡€ Thompson Sampling\n    ä½¿ç”¨ Beta-Bernoulli æ¨¡å‹\n    \"\"\"\n    def __init__(self, n_treatments, prior_alpha=1, prior_beta=1):\n        \"\"\"\n        Args:\n            n_treatments: å¤„ç†æ•°é‡\n            prior_alpha: Beta å…ˆéªŒçš„ Î± å‚æ•°\n            prior_beta: Beta å…ˆéªŒçš„ Î² å‚æ•°\n        \"\"\"\n        self.n_treatments = n_treatments\n        # åˆå§‹åŒ–æ¯ç§å¤„ç†çš„ Î± å’Œ Î²\n        self.alphas = [prior_alpha] * n_treatments\n        self.betas = [prior_beta] * n_treatments\n        self.n_pulls = [0] * n_treatments  # è®°å½•æ¯ç§å¤„ç†è¢«é€‰æ‹©çš„æ¬¡æ•°\n    \n    def select_treatment(self):\n        \"\"\"\n        ä½¿ç”¨ Thompson Sampling é€‰æ‹©å¤„ç†\n        \n        Returns:\n            selected: é€‰æ‹©çš„å¤„ç†ç´¢å¼•\n        \"\"\"\n        # å®ç° Thompson Sampling\n        # 1. å¯¹æ¯ç§å¤„ç†ï¼Œä» Beta(Î±, Î²) é‡‡æ ·\n        samples = [np.random.beta(self.alphas[i], self.betas[i]) \n                  for i in range(self.n_treatments)]\n        \n        # 2. é€‰æ‹©é‡‡æ ·å€¼æœ€å¤§çš„å¤„ç†\n        selected = np.argmax(samples)\n        \n        return selected\n    \n    def update(self, treatment, outcome):\n        \"\"\"\n        è§‚å¯Ÿç»“æœåæ›´æ–°åéªŒ\n        \n        Args:\n            treatment: ä½¿ç”¨çš„å¤„ç†\n            outcome: ç»“æœ (0 æˆ– 1)\n        \"\"\"\n        # æ›´æ–°åéªŒå‚æ•°\n        # Î± += outcome, Î² += (1 - outcome)\n        self.alphas[treatment] += outcome\n        self.betas[treatment] += (1 - outcome)\n        self.n_pulls[treatment] += 1\n    \n    def get_posterior_stats(self):\n        \"\"\"è·å–åéªŒç»Ÿè®¡é‡\"\"\"\n        stats = []\n        for i in range(self.n_treatments):\n            mean = self.alphas[i] / (self.alphas[i] + self.betas[i])\n            std = np.sqrt(self.alphas[i] * self.betas[i] / \n                         ((self.alphas[i] + self.betas[i])**2 * \n                          (self.alphas[i] + self.betas[i] + 1)))\n            stats.append({\n                'mean': mean, \n                'std': std, \n                'alpha': self.alphas[i], \n                'beta': self.betas[i],\n                'n_pulls': self.n_pulls[i]\n            })\n        return stats\n\nprint(\"âœ… BasicThompsonSampling ç±»å®šä¹‰å®Œæˆï¼ˆå·²å®Œæˆ TODO éƒ¨åˆ†ï¼‰\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ å‚è€ƒç­”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å‚è€ƒç­”æ¡ˆï¼šåŸºç¡€ Thompson Sampling å®Œæ•´å®ç°\n",
    "# ============================================================\n",
    "\n",
    "class BasicThompsonSampling:\n",
    "    \"\"\"\n",
    "    åŸºç¡€ Thompson Sampling - å®Œæ•´å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, n_treatments, prior_alpha=1, prior_beta=1):\n",
    "        self.n_treatments = n_treatments\n",
    "        self.alphas = [prior_alpha] * n_treatments\n",
    "        self.betas = [prior_beta] * n_treatments\n",
    "        self.n_pulls = [0] * n_treatments  # è®°å½•æ¯ç§å¤„ç†è¢«é€‰æ‹©çš„æ¬¡æ•°\n",
    "    \n",
    "    def select_treatment(self):\n",
    "        \"\"\"ä½¿ç”¨ Thompson Sampling é€‰æ‹©å¤„ç†\"\"\"\n",
    "        # ä»æ¯ç§å¤„ç†çš„åéªŒé‡‡æ ·\n",
    "        samples = [np.random.beta(self.alphas[i], self.betas[i]) \n",
    "                  for i in range(self.n_treatments)]\n",
    "        # é€‰æ‹©é‡‡æ ·å€¼æœ€å¤§çš„\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, treatment, outcome):\n",
    "        \"\"\"æ›´æ–°åéªŒ\"\"\"\n",
    "        self.alphas[treatment] += outcome\n",
    "        self.betas[treatment] += (1 - outcome)\n",
    "        self.n_pulls[treatment] += 1\n",
    "    \n",
    "    def get_posterior_stats(self):\n",
    "        \"\"\"è·å–åéªŒç»Ÿè®¡é‡\"\"\"\n",
    "        stats = []\n",
    "        for i in range(self.n_treatments):\n",
    "            mean = self.alphas[i] / (self.alphas[i] + self.betas[i])\n",
    "            std = np.sqrt(self.alphas[i] * self.betas[i] / \n",
    "                         ((self.alphas[i] + self.betas[i])**2 * \n",
    "                          (self.alphas[i] + self.betas[i] + 1)))\n",
    "            stats.append({\n",
    "                'mean': mean, \n",
    "                'std': std, \n",
    "                'alpha': self.alphas[i], \n",
    "                'beta': self.betas[i],\n",
    "                'n_pulls': self.n_pulls[i]\n",
    "            })\n",
    "        return stats\n",
    "\n",
    "# æµ‹è¯•\n",
    "ts = BasicThompsonSampling(n_treatments=3)\n",
    "print(\"âœ… BasicThompsonSampling åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"   åˆå§‹åéªŒ: {ts.get_posterior_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: æ¨¡æ‹Ÿå†·å¯åŠ¨åœºæ™¯\n",
    "\n",
    "### 4.1 åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColdStartEnvironment:\n",
    "    \"\"\"\n",
    "    å†·å¯åŠ¨æ¨¡æ‹Ÿç¯å¢ƒ\n",
    "    \n",
    "    æ¨¡æ‹Ÿæ–°ç”¨æˆ·é¦–å•åœºæ™¯ï¼š\n",
    "    - ç”¨æˆ·åªæœ‰åŸºç¡€ç‰¹å¾ï¼ˆç”¨æˆ·ç¾¤ç»„ï¼‰\n",
    "    - ä¸åŒç¾¤ç»„å¯¹ä¸åŒå¤„ç†çš„å“åº”ä¸åŒ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_groups=3, n_treatments=3, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.n_groups = n_groups\n",
    "        self.n_treatments = n_treatments\n",
    "        \n",
    "        # çœŸå®çš„è½¬åŒ–ç‡çŸ©é˜µ (group x treatment)\n",
    "        # æ¯ä¸ªç¾¤ç»„å¯¹æ¯ç§å¤„ç†çš„çœŸå®å“åº”ç‡\n",
    "        self.true_conversion_rates = np.array([\n",
    "            # æ— ä¼˜æƒ   5æŠ˜åˆ¸  æ»¡å‡åˆ¸\n",
    "            [0.05,   0.15,   0.10],  # ç¾¤ç»„0: ä»·æ ¼æ•æ„Ÿå‹ï¼Œå¯¹æŠ˜æ‰£å“åº”å¤§\n",
    "            [0.10,   0.12,   0.18],  # ç¾¤ç»„1: å‡‘å•å‹ï¼Œå¯¹æ»¡å‡å“åº”å¤§\n",
    "            [0.15,   0.16,   0.14],  # ç¾¤ç»„2: å“è´¨å‹ï¼Œå¯¹ä¼˜æƒ ä¸å¤ªæ•æ„Ÿ\n",
    "        ])\n",
    "        \n",
    "        # çœŸå®æœ€ä¼˜å¤„ç†\n",
    "        self.optimal_treatments = np.argmax(self.true_conversion_rates, axis=1)\n",
    "        \n",
    "        # ç¾¤ç»„åˆ†å¸ƒ\n",
    "        self.group_probs = [0.4, 0.35, 0.25]  # ç¾¤ç»„0æœ€å¤š\n",
    "    \n",
    "    def generate_user(self):\n",
    "        \"\"\"ç”Ÿæˆä¸€ä¸ªæ–°ç”¨æˆ·\"\"\"\n",
    "        group = np.random.choice(self.n_groups, p=self.group_probs)\n",
    "        return {'group': group}\n",
    "    \n",
    "    def get_outcome(self, user, treatment):\n",
    "        \"\"\"è·å–ç”¨æˆ·åœ¨æŸå¤„ç†ä¸‹çš„ç»“æœ\"\"\"\n",
    "        group = user['group']\n",
    "        prob = self.true_conversion_rates[group, treatment]\n",
    "        return np.random.binomial(1, prob)\n",
    "    \n",
    "    def get_optimal_treatment(self, user):\n",
    "        \"\"\"è·å–ç”¨æˆ·çš„çœŸå®æœ€ä¼˜å¤„ç†\"\"\"\n",
    "        return self.optimal_treatments[user['group']]\n",
    "    \n",
    "    def get_oracle_value(self):\n",
    "        \"\"\"è®¡ç®— Oracle ç­–ç•¥çš„æœŸæœ›è½¬åŒ–ç‡\"\"\"\n",
    "        value = 0\n",
    "        for g, prob in enumerate(self.group_probs):\n",
    "            optimal_t = self.optimal_treatments[g]\n",
    "            value += prob * self.true_conversion_rates[g, optimal_t]\n",
    "        return value\n",
    "\n",
    "# åˆ›å»ºç¯å¢ƒ\n",
    "env = ColdStartEnvironment()\n",
    "\n",
    "print(\"ç¯å¢ƒè®¾ç½®ï¼š\")\n",
    "print(f\"\\nçœŸå®è½¬åŒ–ç‡çŸ©é˜µ (è¡Œ=ç¾¤ç»„, åˆ—=å¤„ç†):\")\n",
    "treatment_names = ['æ— ä¼˜æƒ ', '5æŠ˜åˆ¸', 'æ»¡å‡åˆ¸']\n",
    "df_rates = pd.DataFrame(env.true_conversion_rates, \n",
    "                        columns=treatment_names,\n",
    "                        index=[f'ç¾¤ç»„{i}' for i in range(3)])\n",
    "print(df_rates.to_string())\n",
    "\n",
    "print(f\"\\næ¯ä¸ªç¾¤ç»„çš„æœ€ä¼˜å¤„ç†:\")\n",
    "for g in range(3):\n",
    "    print(f\"  ç¾¤ç»„{g}: {treatment_names[env.optimal_treatments[g]]}\")\n",
    "\n",
    "print(f\"\\nOracle ç­–ç•¥æœŸæœ›è½¬åŒ–ç‡: {env.get_oracle_value():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æ¯”è¾ƒä¸åŒç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(env, policy, n_users=1000, seed=42):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿç­–ç•¥è¿è¡Œ\n",
    "    \n",
    "    Args:\n",
    "        env: ç¯å¢ƒ\n",
    "        policy: ç­–ç•¥å¯¹è±¡ï¼ˆéœ€è¦æœ‰ select_treatment å’Œ update æ–¹æ³•ï¼‰\n",
    "        n_users: ç”¨æˆ·æ•°\n",
    "        \n",
    "    Returns:\n",
    "        history: è¿è¡Œå†å²\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    history = {\n",
    "        'cumulative_conversions': [],\n",
    "        'cumulative_regret': [],\n",
    "        'treatments': [],\n",
    "        'outcomes': [],\n",
    "        'optimal_treatments': []\n",
    "    }\n",
    "    \n",
    "    total_conversions = 0\n",
    "    total_regret = 0\n",
    "    \n",
    "    for t in range(n_users):\n",
    "        # ç”Ÿæˆç”¨æˆ·\n",
    "        user = env.generate_user()\n",
    "        \n",
    "        # é€‰æ‹©å¤„ç†\n",
    "        if hasattr(policy, 'select_treatment_for_user'):\n",
    "            treatment = policy.select_treatment_for_user(user)\n",
    "        else:\n",
    "            treatment = policy.select_treatment()\n",
    "        \n",
    "        # è·å–ç»“æœ\n",
    "        outcome = env.get_outcome(user, treatment)\n",
    "        \n",
    "        # æ›´æ–°ç­–ç•¥\n",
    "        if hasattr(policy, 'update_for_user'):\n",
    "            policy.update_for_user(user, treatment, outcome)\n",
    "        else:\n",
    "            policy.update(treatment, outcome)\n",
    "        \n",
    "        # è®¡ç®—é—æ†¾\n",
    "        optimal_t = env.get_optimal_treatment(user)\n",
    "        optimal_rate = env.true_conversion_rates[user['group'], optimal_t]\n",
    "        actual_rate = env.true_conversion_rates[user['group'], treatment]\n",
    "        regret = optimal_rate - actual_rate\n",
    "        \n",
    "        # è®°å½•\n",
    "        total_conversions += outcome\n",
    "        total_regret += regret\n",
    "        \n",
    "        history['cumulative_conversions'].append(total_conversions)\n",
    "        history['cumulative_regret'].append(total_regret)\n",
    "        history['treatments'].append(treatment)\n",
    "        history['outcomes'].append(outcome)\n",
    "        history['optimal_treatments'].append(optimal_t)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# å®šä¹‰ä¸åŒç­–ç•¥\n",
    "\n",
    "class RandomPolicy:\n",
    "    \"\"\"éšæœºç­–ç•¥\"\"\"\n",
    "    def __init__(self, n_treatments):\n",
    "        self.n_treatments = n_treatments\n",
    "    \n",
    "    def select_treatment(self):\n",
    "        return np.random.randint(self.n_treatments)\n",
    "    \n",
    "    def update(self, treatment, outcome):\n",
    "        pass\n",
    "\n",
    "class GreedyPolicy:\n",
    "    \"\"\"è´ªå©ªç­–ç•¥ï¼šæ€»æ˜¯é€‰æ‹©å½“å‰è§‚æµ‹è½¬åŒ–ç‡æœ€é«˜çš„å¤„ç†\"\"\"\n",
    "    def __init__(self, n_treatments):\n",
    "        self.n_treatments = n_treatments\n",
    "        self.successes = [0] * n_treatments\n",
    "        self.trials = [0] * n_treatments\n",
    "    \n",
    "    def select_treatment(self):\n",
    "        # ç¡®ä¿æ¯ç§å¤„ç†è‡³å°‘å°è¯•ä¸€æ¬¡\n",
    "        for i in range(self.n_treatments):\n",
    "            if self.trials[i] == 0:\n",
    "                return i\n",
    "        \n",
    "        # é€‰æ‹©è§‚æµ‹è½¬åŒ–ç‡æœ€é«˜çš„\n",
    "        rates = [self.successes[i] / self.trials[i] for i in range(self.n_treatments)]\n",
    "        return np.argmax(rates)\n",
    "    \n",
    "    def update(self, treatment, outcome):\n",
    "        self.successes[treatment] += outcome\n",
    "        self.trials[treatment] += 1\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    \"\"\"Îµ-Greedy ç­–ç•¥\"\"\"\n",
    "    def __init__(self, n_treatments, epsilon=0.1):\n",
    "        self.n_treatments = n_treatments\n",
    "        self.epsilon = epsilon\n",
    "        self.successes = [0] * n_treatments\n",
    "        self.trials = [0] * n_treatments\n",
    "    \n",
    "    def select_treatment(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_treatments)\n",
    "        \n",
    "        # ç¡®ä¿æ¯ç§å¤„ç†è‡³å°‘å°è¯•ä¸€æ¬¡\n",
    "        for i in range(self.n_treatments):\n",
    "            if self.trials[i] == 0:\n",
    "                return i\n",
    "        \n",
    "        rates = [self.successes[i] / self.trials[i] for i in range(self.n_treatments)]\n",
    "        return np.argmax(rates)\n",
    "    \n",
    "    def update(self, treatment, outcome):\n",
    "        self.successes[treatment] += outcome\n",
    "        self.trials[treatment] += 1\n",
    "\n",
    "print(\"âœ… ç­–ç•¥å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "n_users = 2000\n",
    "\n",
    "policies = {\n",
    "    'éšæœºç­–ç•¥': RandomPolicy(n_treatments=3),\n",
    "    'è´ªå©ªç­–ç•¥': GreedyPolicy(n_treatments=3),\n",
    "    'Îµ-Greedy (Îµ=0.1)': EpsilonGreedy(n_treatments=3, epsilon=0.1),\n",
    "    'Thompson Sampling': BasicThompsonSampling(n_treatments=3),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, policy in policies.items():\n",
    "    print(f\"è¿è¡Œ {name}...\")\n",
    "    results[name] = simulate_policy(env, policy, n_users=n_users)\n",
    "\n",
    "print(\"\\nâœ… æ¨¡æ‹Ÿå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ¯”è¾ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {'éšæœºç­–ç•¥': '#95a5a6', 'è´ªå©ªç­–ç•¥': '#e74c3c', \n",
    "          'Îµ-Greedy (Îµ=0.1)': '#f39c12', 'Thompson Sampling': '#2ecc71'}\n",
    "\n",
    "# ç´¯è®¡è½¬åŒ–\n",
    "ax1 = axes[0]\n",
    "for name, history in results.items():\n",
    "    ax1.plot(history['cumulative_conversions'], label=name, color=colors[name], linewidth=2)\n",
    "ax1.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax1.set_ylabel('ç´¯è®¡è½¬åŒ–')\n",
    "ax1.set_title('ç´¯è®¡è½¬åŒ–æ•°å¯¹æ¯”')\n",
    "ax1.legend()\n",
    "\n",
    "# ç´¯è®¡é—æ†¾\n",
    "ax2 = axes[1]\n",
    "for name, history in results.items():\n",
    "    ax2.plot(history['cumulative_regret'], label=name, color=colors[name], linewidth=2)\n",
    "ax2.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax2.set_ylabel('ç´¯è®¡é—æ†¾')\n",
    "ax2.set_title('ç´¯è®¡é—æ†¾å¯¹æ¯”')\n",
    "ax2.legend()\n",
    "\n",
    "# æœ€ç»ˆç»Ÿè®¡\n",
    "ax3 = axes[2]\n",
    "names = list(results.keys())\n",
    "final_conversions = [results[n]['cumulative_conversions'][-1] for n in names]\n",
    "bars = ax3.bar(range(len(names)), final_conversions, color=[colors[n] for n in names])\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax3.set_ylabel('æ€»è½¬åŒ–æ•°')\n",
    "ax3.set_title(f'{n_users} ç”¨æˆ·åçš„æ€»è½¬åŒ–')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, conv in zip(bars, final_conversions):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "            f'{conv}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ±‡æ€»è¡¨æ ¼\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ç­–ç•¥å¯¹æ¯”æ±‡æ€»\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'ç­–ç•¥':<20} {'æ€»è½¬åŒ–':<10} {'è½¬åŒ–ç‡':<12} {'ç´¯è®¡é—æ†¾':<12}\")\n",
    "print(\"-\" * 54)\n",
    "for name in names:\n",
    "    conv = results[name]['cumulative_conversions'][-1]\n",
    "    rate = conv / n_users\n",
    "    regret = results[name]['cumulative_regret'][-1]\n",
    "    print(f\"{name:<20} {conv:<10} {rate:<12.2%} {regret:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ Thompson Sampling\n",
    "\n",
    "åŸºç¡€ Thompson Sampling å¿½ç•¥äº†ç”¨æˆ·ç‰¹å¾ã€‚ç°åœ¨æˆ‘ä»¬æ‰©å±•åˆ°**ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ç‰ˆæœ¬ï¼Œä¸ºä¸åŒç”¨æˆ·ç¾¤ä½“åˆ†åˆ«ç»´æŠ¤åéªŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualThompsonSampling:\n",
    "    \"\"\"\n",
    "    ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ Thompson Sampling\n",
    "    \n",
    "    ä¸ºæ¯ä¸ªç”¨æˆ·ç¾¤ç»„åˆ†åˆ«ç»´æŠ¤åéªŒåˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_treatments, n_groups, prior_alpha=1, prior_beta=1):\n",
    "        self.n_treatments = n_treatments\n",
    "        self.n_groups = n_groups\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ª (ç¾¤ç»„, å¤„ç†) ç»„åˆç»´æŠ¤åéªŒå‚æ•°\n",
    "        self.alphas = np.ones((n_groups, n_treatments)) * prior_alpha\n",
    "        self.betas = np.ones((n_groups, n_treatments)) * prior_beta\n",
    "        self.n_pulls = np.zeros((n_groups, n_treatments))\n",
    "    \n",
    "    def select_treatment_for_user(self, user):\n",
    "        \"\"\"æ ¹æ®ç”¨æˆ·ç¾¤ç»„é€‰æ‹©å¤„ç†\"\"\"\n",
    "        group = user['group']\n",
    "        \n",
    "        # ä»è¯¥ç¾¤ç»„çš„åéªŒé‡‡æ ·\n",
    "        samples = [np.random.beta(self.alphas[group, t], self.betas[group, t]) \n",
    "                  for t in range(self.n_treatments)]\n",
    "        \n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update_for_user(self, user, treatment, outcome):\n",
    "        \"\"\"æ›´æ–°ç‰¹å®šç¾¤ç»„çš„åéªŒ\"\"\"\n",
    "        group = user['group']\n",
    "        self.alphas[group, treatment] += outcome\n",
    "        self.betas[group, treatment] += (1 - outcome)\n",
    "        self.n_pulls[group, treatment] += 1\n",
    "    \n",
    "    def get_posterior_stats(self):\n",
    "        \"\"\"è·å–æ‰€æœ‰ç¾¤ç»„çš„åéªŒç»Ÿè®¡é‡\"\"\"\n",
    "        stats = {}\n",
    "        for g in range(self.n_groups):\n",
    "            stats[f'ç¾¤ç»„{g}'] = []\n",
    "            for t in range(self.n_treatments):\n",
    "                mean = self.alphas[g, t] / (self.alphas[g, t] + self.betas[g, t])\n",
    "                stats[f'ç¾¤ç»„{g}'].append({\n",
    "                    'mean': mean,\n",
    "                    'alpha': self.alphas[g, t],\n",
    "                    'beta': self.betas[g, t],\n",
    "                    'n_pulls': self.n_pulls[g, t]\n",
    "                })\n",
    "        return stats\n",
    "\n",
    "# è¿è¡Œ Contextual Thompson Sampling\n",
    "contextual_ts = ContextualThompsonSampling(n_treatments=3, n_groups=3)\n",
    "results['Contextual TS'] = simulate_policy(env, contextual_ts, n_users=n_users)\n",
    "\n",
    "print(\"\\nContextual Thompson Sampling åéªŒç»Ÿè®¡ï¼š\")\n",
    "stats = contextual_ts.get_posterior_stats()\n",
    "for group, group_stats in stats.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for t, t_stats in enumerate(group_stats):\n",
    "        true_rate = env.true_conversion_rates[int(group[-1]), t]\n",
    "        print(f\"  {treatment_names[t]}: ä¼°è®¡={t_stats['mean']:.3f}, \"\n",
    "              f\"çœŸå®={true_rate:.3f}, n={t_stats['n_pulls']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”åŸºç¡€ TS å’Œ Contextual TS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ç´¯è®¡é—æ†¾å¯¹æ¯”\n",
    "ax1 = axes[0]\n",
    "ax1.plot(results['Thompson Sampling']['cumulative_regret'], \n",
    "         label='åŸºç¡€ TS', linewidth=2, color='#3498db')\n",
    "ax1.plot(results['Contextual TS']['cumulative_regret'], \n",
    "         label='Contextual TS', linewidth=2, color='#2ecc71')\n",
    "ax1.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax1.set_ylabel('ç´¯è®¡é—æ†¾')\n",
    "ax1.set_title('ç´¯è®¡é—æ†¾å¯¹æ¯”')\n",
    "ax1.legend()\n",
    "\n",
    "# æœ€ä¼˜å¤„ç†é€‰æ‹©å‡†ç¡®ç‡\n",
    "ax2 = axes[1]\n",
    "\n",
    "# è®¡ç®—æ»šåŠ¨å‡†ç¡®ç‡\n",
    "window = 100\n",
    "for name in ['Thompson Sampling', 'Contextual TS']:\n",
    "    history = results[name]\n",
    "    correct = [1 if t == o else 0 \n",
    "               for t, o in zip(history['treatments'], history['optimal_treatments'])]\n",
    "    rolling_acc = pd.Series(correct).rolling(window=window).mean()\n",
    "    ax2.plot(rolling_acc, label=name, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax2.set_ylabel('æœ€ä¼˜å¤„ç†é€‰æ‹©å‡†ç¡®ç‡ (æ»šåŠ¨å¹³å‡)')\n",
    "ax2.set_title(f'æœ€ä¼˜å¤„ç†é€‰æ‹©å‡†ç¡®ç‡ (çª—å£={window})')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æœ€ç»ˆå¯¹æ¯”\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"åŸºç¡€ TS vs Contextual TS å¯¹æ¯”\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name in ['Thompson Sampling', 'Contextual TS']:\n",
    "    history = results[name]\n",
    "    conv = history['cumulative_conversions'][-1]\n",
    "    regret = history['cumulative_regret'][-1]\n",
    "    acc = np.mean([1 if t == o else 0 \n",
    "                   for t, o in zip(history['treatments'], history['optimal_treatments'])])\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  æ€»è½¬åŒ–: {conv}\")\n",
    "    print(f\"  ç´¯è®¡é—æ†¾: {regret:.2f}\")\n",
    "    print(f\"  æœ€ä¼˜é€‰æ‹©å‡†ç¡®ç‡: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: å¸¦ä¿¡æ¯å…ˆéªŒçš„ Thompson Sampling\n",
    "\n",
    "åœ¨å®é™…ä¸šåŠ¡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰å†å²æ•°æ®å¯ä»¥è®¾ç½®æ›´å¥½çš„å…ˆéªŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformedThompsonSampling:\n",
    "    \"\"\"\n",
    "    å¸¦ä¿¡æ¯å…ˆéªŒçš„ Thompson Sampling\n",
    "    \n",
    "    ä½¿ç”¨å†å²æ•°æ®è®¾ç½®å…ˆéªŒ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_treatments, n_groups, historical_data=None, \n",
    "                 prior_strength=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            historical_data: dict, å†å²æ•°æ® {(group, treatment): (successes, trials)}\n",
    "            prior_strength: å…ˆéªŒå¼ºåº¦ï¼ˆç›¸å½“äºå¤šå°‘è™šæ‹Ÿæ ·æœ¬ï¼‰\n",
    "        \"\"\"\n",
    "        self.n_treatments = n_treatments\n",
    "        self.n_groups = n_groups\n",
    "        \n",
    "        # åˆå§‹åŒ–å…ˆéªŒ\n",
    "        self.alphas = np.ones((n_groups, n_treatments))\n",
    "        self.betas = np.ones((n_groups, n_treatments))\n",
    "        \n",
    "        if historical_data is not None:\n",
    "            for (g, t), (successes, trials) in historical_data.items():\n",
    "                if trials > 0:\n",
    "                    rate = successes / trials\n",
    "                    # å°†å†å²æ•°æ®è½¬åŒ–ä¸ºç­‰æ•ˆçš„å…ˆéªŒæ ·æœ¬\n",
    "                    self.alphas[g, t] = rate * prior_strength + 1\n",
    "                    self.betas[g, t] = (1 - rate) * prior_strength + 1\n",
    "        \n",
    "        self.n_pulls = np.zeros((n_groups, n_treatments))\n",
    "    \n",
    "    def select_treatment_for_user(self, user):\n",
    "        group = user['group']\n",
    "        samples = [np.random.beta(self.alphas[group, t], self.betas[group, t]) \n",
    "                  for t in range(self.n_treatments)]\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update_for_user(self, user, treatment, outcome):\n",
    "        group = user['group']\n",
    "        self.alphas[group, treatment] += outcome\n",
    "        self.betas[group, treatment] += (1 - outcome)\n",
    "        self.n_pulls[group, treatment] += 1\n",
    "\n",
    "# æ¨¡æ‹Ÿå†å²æ•°æ®ï¼ˆå‡è®¾ä¹‹å‰åšè¿‡å°è§„æ¨¡æµ‹è¯•ï¼‰\n",
    "historical_data = {\n",
    "    # (ç¾¤ç»„, å¤„ç†): (æˆåŠŸæ¬¡æ•°, æ€»æ¬¡æ•°)\n",
    "    (0, 0): (5, 100),   # ç¾¤ç»„0 æ— ä¼˜æƒ \n",
    "    (0, 1): (14, 100),  # ç¾¤ç»„0 5æŠ˜åˆ¸\n",
    "    (0, 2): (9, 100),   # ç¾¤ç»„0 æ»¡å‡åˆ¸\n",
    "    (1, 0): (10, 100),\n",
    "    (1, 1): (11, 100),\n",
    "    (1, 2): (17, 100),\n",
    "    (2, 0): (15, 100),\n",
    "    (2, 1): (16, 100),\n",
    "    (2, 2): (13, 100),\n",
    "}\n",
    "\n",
    "print(\"å†å²æ•°æ®è®¾ç½®çš„å…ˆéªŒï¼š\")\n",
    "print(\"(åŸºäºä¹‹å‰æ¯ç»„ 100 æ ·æœ¬çš„æµ‹è¯•)\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "for g in range(3):\n",
    "    print(f\"\\nç¾¤ç»„{g}:\")\n",
    "    for t in range(3):\n",
    "        successes, trials = historical_data[(g, t)]\n",
    "        rate = successes / trials\n",
    "        true_rate = env.true_conversion_rates[g, t]\n",
    "        print(f\"  {treatment_names[t]}: å†å²ç‡={rate:.2f}, çœŸå®ç‡={true_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”æ— å…ˆéªŒ vs æœ‰å…ˆéªŒ\n",
    "np.random.seed(42)\n",
    "\n",
    "# æ— ä¿¡æ¯å…ˆéªŒ\n",
    "contextual_ts_flat = ContextualThompsonSampling(n_treatments=3, n_groups=3)\n",
    "results_flat = simulate_policy(env, contextual_ts_flat, n_users=n_users, seed=42)\n",
    "\n",
    "# æœ‰ä¿¡æ¯å…ˆéªŒ\n",
    "informed_ts = InformedThompsonSampling(n_treatments=3, n_groups=3, \n",
    "                                        historical_data=historical_data,\n",
    "                                        prior_strength=50)\n",
    "results_informed = simulate_policy(env, informed_ts, n_users=n_users, seed=42)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(results_flat['cumulative_regret'], label='æ— ä¿¡æ¯å…ˆéªŒ', linewidth=2)\n",
    "ax1.plot(results_informed['cumulative_regret'], label='æœ‰ä¿¡æ¯å…ˆéªŒ', linewidth=2)\n",
    "ax1.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax1.set_ylabel('ç´¯è®¡é—æ†¾')\n",
    "ax1.set_title('ç´¯è®¡é—æ†¾å¯¹æ¯”')\n",
    "ax1.legend()\n",
    "\n",
    "# å‰ 500 ç”¨æˆ·æ”¾å¤§\n",
    "ax2 = axes[1]\n",
    "ax2.plot(results_flat['cumulative_regret'][:500], label='æ— ä¿¡æ¯å…ˆéªŒ', linewidth=2)\n",
    "ax2.plot(results_informed['cumulative_regret'][:500], label='æœ‰ä¿¡æ¯å…ˆéªŒ', linewidth=2)\n",
    "ax2.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax2.set_ylabel('ç´¯è®¡é—æ†¾')\n",
    "ax2.set_title('å‰ 500 ç”¨æˆ·çš„ç´¯è®¡é—æ†¾ï¼ˆå†·å¯åŠ¨é˜¶æ®µï¼‰')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nå…³é”®å‘ç°ï¼š\")\n",
    "print(f\"  æ— ä¿¡æ¯å…ˆéªŒ - æœ€ç»ˆé—æ†¾: {results_flat['cumulative_regret'][-1]:.2f}\")\n",
    "print(f\"  æœ‰ä¿¡æ¯å…ˆéªŒ - æœ€ç»ˆé—æ†¾: {results_informed['cumulative_regret'][-1]:.2f}\")\n",
    "print(f\"  âœ… æœ‰ä¿¡æ¯å…ˆéªŒæ˜¾è‘—é™ä½äº†å†·å¯åŠ¨é˜¶æ®µçš„é—æ†¾ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: å®é™…éƒ¨ç½²è€ƒè™‘\n",
    "\n",
    "### 7.1 åˆ†é˜¶æ®µç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StagedUpliftPolicy:\n",
    "    \"\"\"\n",
    "    åˆ†é˜¶æ®µ Uplift ç­–ç•¥\n",
    "    \n",
    "    é˜¶æ®µ 1 (æ¢ç´¢æœŸ): Thompson Sampling\n",
    "    é˜¶æ®µ 2 (åˆ©ç”¨æœŸ): å›ºå®šæœ€ä¼˜ç­–ç•¥\n",
    "    \"\"\"\n",
    "    def __init__(self, n_treatments, n_groups, exploration_users=500, \n",
    "                 min_samples_per_arm=50):\n",
    "        self.n_treatments = n_treatments\n",
    "        self.n_groups = n_groups\n",
    "        self.exploration_users = exploration_users\n",
    "        self.min_samples_per_arm = min_samples_per_arm\n",
    "        \n",
    "        self.ts = ContextualThompsonSampling(n_treatments, n_groups)\n",
    "        self.n_users = 0\n",
    "        self.best_treatments = None\n",
    "        self.stage = 'exploration'\n",
    "    \n",
    "    def _determine_best_treatments(self):\n",
    "        \"\"\"ç¡®å®šæ¯ä¸ªç¾¤ç»„çš„æœ€ä¼˜å¤„ç†\"\"\"\n",
    "        best = []\n",
    "        for g in range(self.n_groups):\n",
    "            # æ£€æŸ¥æ ·æœ¬é‡æ˜¯å¦è¶³å¤Ÿ\n",
    "            if np.min(self.ts.n_pulls[g]) < self.min_samples_per_arm:\n",
    "                best.append(None)  # æ ·æœ¬ä¸è¶³ï¼Œç»§ç»­æ¢ç´¢\n",
    "            else:\n",
    "                # é€‰æ‹©åéªŒå‡å€¼æœ€é«˜çš„å¤„ç†\n",
    "                means = self.ts.alphas[g] / (self.ts.alphas[g] + self.ts.betas[g])\n",
    "                best.append(np.argmax(means))\n",
    "        return best\n",
    "    \n",
    "    def select_treatment_for_user(self, user):\n",
    "        group = user['group']\n",
    "        \n",
    "        if self.stage == 'exploration':\n",
    "            return self.ts.select_treatment_for_user(user)\n",
    "        else:\n",
    "            if self.best_treatments[group] is not None:\n",
    "                return self.best_treatments[group]\n",
    "            else:\n",
    "                # è¯¥ç¾¤ç»„ä»éœ€æ¢ç´¢\n",
    "                return self.ts.select_treatment_for_user(user)\n",
    "    \n",
    "    def update_for_user(self, user, treatment, outcome):\n",
    "        self.ts.update_for_user(user, treatment, outcome)\n",
    "        self.n_users += 1\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦è¿›å…¥åˆ©ç”¨æœŸ\n",
    "        if self.n_users >= self.exploration_users and self.stage == 'exploration':\n",
    "            self.best_treatments = self._determine_best_treatments()\n",
    "            if all(t is not None for t in self.best_treatments):\n",
    "                self.stage = 'exploitation'\n",
    "                print(f\"\\n[ç”¨æˆ· {self.n_users}] è¿›å…¥åˆ©ç”¨æœŸ\")\n",
    "                print(f\"  ç¡®å®šçš„æœ€ä¼˜ç­–ç•¥: {[treatment_names[t] for t in self.best_treatments]}\")\n",
    "\n",
    "# è¿è¡Œåˆ†é˜¶æ®µç­–ç•¥\n",
    "staged_policy = StagedUpliftPolicy(n_treatments=3, n_groups=3, \n",
    "                                    exploration_users=500)\n",
    "results['åˆ†é˜¶æ®µç­–ç•¥'] = simulate_policy(env, staged_policy, n_users=n_users, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆå¯¹æ¯”\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "policies_to_compare = ['éšæœºç­–ç•¥', 'Thompson Sampling', 'Contextual TS', 'åˆ†é˜¶æ®µç­–ç•¥']\n",
    "colors_compare = ['#95a5a6', '#3498db', '#2ecc71', '#9b59b6']\n",
    "\n",
    "for name, color in zip(policies_to_compare, colors_compare):\n",
    "    ax.plot(results[name]['cumulative_regret'], label=name, linewidth=2, color=color)\n",
    "\n",
    "ax.axvline(x=500, color='gray', linestyle='--', alpha=0.5, label='æ¢ç´¢æœŸç»“æŸ')\n",
    "ax.set_xlabel('ç”¨æˆ·æ•°')\n",
    "ax.set_ylabel('ç´¯è®¡é—æ†¾')\n",
    "ax.set_title('å„ç­–ç•¥ç´¯è®¡é—æ†¾å¯¹æ¯”')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æœ€ç»ˆè¡¨æ ¼\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æœ€ç»ˆç»“æœæ±‡æ€»\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'ç­–ç•¥':<20} {'æ€»è½¬åŒ–':<10} {'è½¬åŒ–ç‡':<12} {'ç´¯è®¡é—æ†¾':<12} {'vs éšæœºæå‡':<12}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "random_conv = results['éšæœºç­–ç•¥']['cumulative_conversions'][-1]\n",
    "\n",
    "for name in policies_to_compare:\n",
    "    history = results[name]\n",
    "    conv = history['cumulative_conversions'][-1]\n",
    "    rate = conv / n_users\n",
    "    regret = history['cumulative_regret'][-1]\n",
    "    improvement = (conv - random_conv) / random_conv\n",
    "    print(f\"{name:<20} {conv:<10} {rate:<12.2%} {regret:<12.2f} {improvement:<12.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ æ€è€ƒé¢˜\n",
    "\n",
    "1. **éå¹³ç¨³ç¯å¢ƒ**ï¼šå¦‚æœç”¨æˆ·åå¥½éšæ—¶é—´å˜åŒ–ï¼Œå¦‚ä½•ä¿®æ”¹ Thompson Samplingï¼Ÿ\n",
    "   - æç¤ºï¼šè€ƒè™‘è¡°å‡å†å²æ•°æ®çš„æƒé‡\n",
    "\n",
    "2. **è¿ç»­å¤„ç†**ï¼šå¦‚æœå¤„ç†æ˜¯è¿ç»­çš„ï¼ˆå¦‚ä¼˜æƒ é‡‘é¢ï¼‰ï¼Œå¦‚ä½•æ‰©å±•ï¼Ÿ\n",
    "   - æç¤ºï¼šè€ƒè™‘ Linear UCB æˆ– Gaussian Process\n",
    "\n",
    "3. **å»¶è¿Ÿåé¦ˆ**ï¼šå¦‚æœè½¬åŒ–æœ‰å»¶è¿Ÿï¼ˆå¦‚ 7 å¤©åæ‰çŸ¥é“æ˜¯å¦å¤è´­ï¼‰ï¼Œå¦‚ä½•å¤„ç†ï¼Ÿ\n",
    "   - æç¤ºï¼šè€ƒè™‘ importance sampling æˆ– time-to-event å»ºæ¨¡\n",
    "\n",
    "4. **å¤šç›®æ ‡**ï¼šå¦‚æœæ—¢è¦è€ƒè™‘è½¬åŒ–ç‡åˆè¦è€ƒè™‘ LTVï¼Œå¦‚ä½•å¹³è¡¡ï¼Ÿ\n",
    "   - æç¤ºï¼šè€ƒè™‘å¤åˆæŒ‡æ ‡æˆ– Pareto ä¼˜åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒæ–¹æ³•\n",
    "\n",
    "| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
    "|------|---------|------|------|\n",
    "| åŸºç¡€ TS | å…¨å±€æœ€ä¼˜å¤„ç† | ç®€å• | å¿½ç•¥å¼‚è´¨æ€§ |\n",
    "| Contextual TS | ç¾¤ç»„å·®å¼‚ | æ•æ‰å¼‚è´¨æ€§ | éœ€è¦æ›´å¤šæ ·æœ¬ |\n",
    "| Informed TS | æœ‰å†å²æ•°æ® | å¿«é€Ÿæ”¶æ•› | ä¾èµ–å…ˆéªŒè´¨é‡ |\n",
    "| åˆ†é˜¶æ®µç­–ç•¥ | ç”Ÿäº§ç¯å¢ƒ | å¹³è¡¡æ¢ç´¢/åˆ©ç”¨ | éœ€è¦è°ƒå‚ |\n",
    "\n",
    "### å®æ–½å»ºè®®\n",
    "\n",
    "1. **å†·å¯åŠ¨é˜¶æ®µ**ï¼šä½¿ç”¨ Thompson Sampling + ä¿¡æ¯å…ˆéªŒ\n",
    "2. **æ•°æ®ç§¯ç´¯å**ï¼šåˆ‡æ¢åˆ° Contextual ç‰ˆæœ¬\n",
    "3. **è¶³å¤Ÿæ•°æ®å**ï¼šå¯ä»¥è®­ç»ƒä¼ ç»Ÿ Uplift æ¨¡å‹\n",
    "\n",
    "### é¢è¯•è¦ç‚¹\n",
    "\n",
    "- èƒ½è§£é‡Š Thompson Sampling çš„æ¢ç´¢-åˆ©ç”¨æƒè¡¡\n",
    "- ç†è§£ Beta-Bernoulli å…±è½­\n",
    "- èƒ½è®¾è®¡åˆ†é˜¶æ®µçš„ç­–ç•¥\n",
    "- çŸ¥é“å¦‚ä½•åˆ©ç”¨å…ˆéªŒä¿¡æ¯åŠ é€Ÿæ”¶æ•›"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}