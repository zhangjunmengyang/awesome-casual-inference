{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‰ Chapter 4 ç»ƒä¹  3: DragonNet - å› æœæ¨æ–­çš„ã€Œä¸‰å¤´é¾™ã€\n",
    "\n",
    "## ä» TARNet åˆ° DragonNet: ä¸€ä¸ªå·§å¦™çš„å‡çº§\n",
    "\n",
    "ä¸Šä¸€èŠ‚æˆ‘ä»¬å­¦ä¹ äº† TARNetï¼Œå®ƒå°±åƒä¸€ä¸ªåŒè¯­ç¿»è¯‘å®˜ã€‚ä½†å¦‚æœè¿™ä¸ªç¿»è¯‘å®˜åŒæ—¶è¿˜èƒ½**åˆ¤æ–­å®¢æˆ·æ¥è‡ªå“ªä¸ªå›½å®¶**ï¼ˆå€¾å‘å¾—åˆ†ï¼‰ï¼Œç¿»è¯‘æ•ˆæœä¼šä¸ä¼šæ›´å¥½å‘¢ï¼Ÿ\n",
    "\n",
    "è¿™å°±æ˜¯ **DragonNet** çš„æ ¸å¿ƒæ€æƒ³ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£å€¾å‘å¾—åˆ†å¤´çš„ä½œç”¨\n",
    "2. å®ç° DragonNet æ¶æ„ï¼ˆä¸‰å¤´é¾™ï¼ï¼‰\n",
    "3. æŒæ¡ DragonNet çš„å¤åˆæŸå¤±å‡½æ•°\n",
    "4. ç†è§£ Targeted Regularization çš„é­”æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ DragonNet çš„ç›´è§‰: ä¸‰å¤´é¾™çš„æ•…äº‹\n",
    "\n",
    "### é—®é¢˜å›é¡¾\n",
    "\n",
    "åœ¨å› æœæ¨æ–­ä¸­ï¼Œ**æ··æ·†**æ˜¯æœ€å¤§çš„æ•Œäººã€‚æƒ³è±¡ä½ åœ¨ç ”ç©¶ã€Œå¹¿å‘Šæ˜¯å¦å¢åŠ ç”¨æˆ·è´­ä¹°ã€ï¼š\n",
    "\n",
    "- ç»å¸¸è´­ç‰©çš„ç”¨æˆ·**æ›´å®¹æ˜“çœ‹åˆ°å¹¿å‘Š**ï¼ˆç®—æ³•æ¨èï¼‰\n",
    "- ç»å¸¸è´­ç‰©çš„ç”¨æˆ·**æœ¬æ¥å°±ä¼šä¹°æ›´å¤š**\n",
    "\n",
    "è¿™å°±æ˜¯**é€‰æ‹©åå·®**ï¼šå¤„ç†åˆ†é…ä¸æ˜¯éšæœºçš„ï¼Œè€Œæ˜¯ä¾èµ–äºæ··æ·†å› å­ã€‚\n",
    "\n",
    "### TARNet çš„å±€é™\n",
    "\n",
    "TARNet åªå­¦ä¹ ã€Œé¢„æµ‹ç»“æœã€ï¼Œä½†ä¸æ˜¾å¼åœ°ç†è§£ã€Œè°ä¼šè¢«å¤„ç†ã€ã€‚è¿™å°±åƒä¸€ä¸ªç¿»è¯‘å®˜è™½ç„¶èƒ½ç¿»è¯‘ï¼Œä½†ä¸äº†è§£å®¢æˆ·çš„èƒŒæ™¯ã€‚\n",
    "\n",
    "### DragonNet çš„è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "DragonNet æ·»åŠ äº†**ç¬¬ä¸‰ä¸ªå¤´**â€”â€”å€¾å‘å¾—åˆ†å¤´ï¼\n",
    "\n",
    "```\n",
    "                       ğŸ§  å…±äº«è¡¨ç¤ºå±‚\n",
    "                           |                    \n",
    "       +------------------+-------------------+\n",
    "       |                  |                   |\n",
    "    ğŸ¯ Y(0)å¤´         ğŸ¯ Y(1)å¤´          ğŸ“Š å€¾å‘å¾—åˆ†å¤´\n",
    "       |                  |                   |\n",
    "  æ§åˆ¶ç»„ç»“æœé¢„æµ‹    å¤„ç†ç»„ç»“æœé¢„æµ‹      \"è°ä¼šè¢«å¤„ç†?\"\n",
    "```\n",
    "\n",
    "ä¸ºä»€ä¹ˆå«ã€Œé¾™ç½‘ã€ï¼ˆDragonNetï¼‰ï¼Ÿå› ä¸ºè¿™ä¸‰ä¸ªå¤´å°±åƒä¼ è¯´ä¸­çš„**ä¸‰å¤´é¾™**ï¼\n",
    "\n",
    "### å…³é”®æ´å¯Ÿ\n",
    "\n",
    "æ·»åŠ å€¾å‘å¾—åˆ†å¤´æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ\n",
    "\n",
    "1. **æ­£åˆ™åŒ–ä½œç”¨**: å¼ºè¿«è¡¨ç¤ºå±‚åŒæ—¶å­¦ä¼šã€Œé¢„æµ‹ç»“æœã€å’Œã€Œé¢„æµ‹å¤„ç†ã€\n",
    "2. **å¹³è¡¡è¡¨ç¤º**: è®©è¡¨ç¤ºå±‚æ•è·ä¸**å¤„ç†åˆ†é…ç›¸å…³çš„ä¿¡æ¯**ï¼Œè¿™æ­£æ˜¯æ··æ·†å› å­ï¼\n",
    "3. **Targeted Regularization**: ç”¨å€¾å‘å¾—åˆ†æ¥è°ƒæ•´æŸå¤±å‡½æ•°ï¼Œå‡å°‘åå·®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "### DragonNet æ¶æ„\n",
    "\n",
    "$$\\Phi(X) = f_{\\text{repr}}(X) \\quad \\text{(å…±äº«è¡¨ç¤º)}$$\n",
    "\n",
    "$$\\hat{Y}(0) = h_0(\\Phi(X)), \\quad \\hat{Y}(1) = h_1(\\Phi(X)) \\quad \\text{(ç»“æœå¤´)}$$\n",
    "\n",
    "$$\\hat{e}(X) = h_e(\\Phi(X)) \\quad \\text{(å€¾å‘å¾—åˆ†å¤´)}$$\n",
    "\n",
    "### å¤åˆæŸå¤±å‡½æ•°\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DragonNet}} = \\mathcal{L}_{\\text{factual}} + \\alpha \\cdot \\mathcal{L}_{\\text{propensity}} + \\beta \\cdot \\mathcal{L}_{\\text{targeted}}$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "1. **Factual Loss** (å’Œ TARNet ä¸€æ ·):\n",
    "$$\\mathcal{L}_{\\text{factual}} = \\frac{1}{N}\\sum_{i=1}^{N} (Y_i - [T_i \\cdot \\hat{Y}_i(1) + (1-T_i) \\cdot \\hat{Y}_i(0)])^2$$\n",
    "\n",
    "2. **Propensity Loss** (äºŒåˆ†ç±»äº¤å‰ç†µ):\n",
    "$$\\mathcal{L}_{\\text{propensity}} = -\\frac{1}{N}\\sum_{i=1}^{N} [T_i \\log \\hat{e}_i + (1-T_i) \\log (1-\\hat{e}_i)]$$\n",
    "\n",
    "3. **Targeted Regularization** (DragonNet çš„åˆ›æ–°):\n",
    "$$\\mathcal{L}_{\\text{targeted}} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(Y_i - \\hat{Y}_i - \\epsilon \\cdot h_i\\right)^2$$\n",
    "\n",
    "å…¶ä¸­ $h_i = \\frac{T_i}{\\hat{e}(X_i)} - \\frac{1-T_i}{1-\\hat{e}(X_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”® Targeted Regularization: é­”æ³•å…¬å¼çš„ç›´è§‰\n",
    "\n",
    "### é‚£ä¸ªç¥ç§˜çš„ h æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "$$h = \\frac{T}{e(X)} - \\frac{1-T}{1-e(X)}$$\n",
    "\n",
    "è¿™ä¸ªå…¬å¼æ¥è‡ªäº**åŠå‚æ•°æ•ˆç‡ç†è®º**å’Œ **TMLEï¼ˆTargeted Maximum Likelihood Estimationï¼‰**ã€‚\n",
    "\n",
    "### ä¸€ä¸ªç›´è§‰è§£é‡Š\n",
    "\n",
    "æƒ³è±¡ä½ åœ¨åšæ°‘æ„è°ƒæŸ¥ï¼Œä½†æœ‰äº›äººç¾¤æ›´éš¾æ¥è§¦åˆ°ï¼ˆæ¯”å¦‚å¹´è½»äººä¸çˆ±æ¥ç”µè¯ï¼‰ï¼š\n",
    "\n",
    "- **e(X)** = æŸäººè¢«è°ƒæŸ¥åˆ°çš„æ¦‚ç‡ï¼ˆå€¾å‘å¾—åˆ†ï¼‰\n",
    "- **T/e(X)**: å¦‚æœè¢«è°ƒæŸ¥åˆ°ï¼ˆT=1ï¼‰ï¼Œç”¨ 1/e(X) åŠ æƒï¼Œç›¸å½“äºã€Œå°‘è§çš„äººæ›´é‡è¦ã€\n",
    "- **-(1-T)/(1-e(X))**: å¦‚æœæ²¡è¢«è°ƒæŸ¥åˆ°ï¼ˆT=0ï¼‰ï¼Œç”¨åå‘æƒé‡\n",
    "\n",
    "è¿™ä¸ª **h** æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**åŒé‡é²æ£’æ€§**çš„è°ƒæ•´é¡¹ï¼\n",
    "\n",
    "### epsilon çš„ä½œç”¨\n",
    "\n",
    "**epsilon (Îµ)** æ˜¯ä¸€ä¸ª**å¯å­¦ä¹ çš„æ ‡é‡å‚æ•°**ï¼š\n",
    "\n",
    "- å®ƒè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ ã€Œéœ€è¦å¤šå°‘è°ƒæ•´ã€\n",
    "- å¦‚æœæ¨¡å‹å·²ç»å¾ˆå‡†ï¼Œepsilon ä¼šè¶‹è¿‘äº 0\n",
    "- å¦‚æœæœ‰åå·®ï¼Œepsilon ä¼šå­¦åˆ°ä¸€ä¸ªéé›¶å€¼æ¥ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒè®¾ç½®\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ç»˜å›¾è®¾ç½®\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"ç¯å¢ƒé…ç½®å®Œæˆ! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.1: ç”Ÿæˆå¼ºæ··æ·†æ•°æ®\n",
    "\n",
    "é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ª**å¼ºæ··æ·†**çš„æ•°æ®é›†ã€‚DragonNet åœ¨è¿™ç§åœºæ™¯ä¸‹ç‰¹åˆ«æœ‰ç”¨ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confounded_data(\n",
    "    n: int = 1000,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæœ‰å¼ºæ··æ·†çš„æ•°æ®\n",
    "    \n",
    "    åœºæ™¯: å¹¿å‘ŠæŠ•æ”¾æ•ˆæœè¯„ä¼°\n",
    "    - X: ç”¨æˆ·ç‰¹å¾ï¼ˆè´­ç‰©é¢‘ç‡ã€å¹³å°æ´»è·ƒåº¦ã€æ¶ˆè´¹èƒ½åŠ›ç­‰ï¼‰\n",
    "    - T: æ˜¯å¦çœ‹åˆ°å¹¿å‘Šï¼ˆä¸æ˜¯éšæœºåˆ†é…ï¼é«˜ä»·å€¼ç”¨æˆ·æ›´å®¹æ˜“çœ‹åˆ°ï¼‰\n",
    "    - Y: æ˜¯å¦è´­ä¹°\n",
    "    \n",
    "    å¼ºæ··æ·†: ç”¨æˆ·ç‰¹å¾åŒæ—¶å½±å“ã€Œæ˜¯å¦çœ‹å¹¿å‘Šã€å’Œã€Œæ˜¯å¦è´­ä¹°ã€\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç”¨æˆ·ç‰¹å¾\n",
    "    X = np.random.randn(n, 5)\n",
    "    # X[:, 0]: è´­ç‰©é¢‘ç‡\n",
    "    # X[:, 1]: å¹³å°æ´»è·ƒåº¦\n",
    "    # X[:, 2]: æ¶ˆè´¹èƒ½åŠ›\n",
    "    # X[:, 3]: æµè§ˆå†å²\n",
    "    # X[:, 4]: ç‚¹å‡»ç‡\n",
    "    \n",
    "    # å¼ºæ··æ·†çš„å¤„ç†åˆ†é…ï¼ˆé«˜ä»·å€¼ç”¨æˆ·æ›´å®¹æ˜“çœ‹åˆ°å¹¿å‘Šï¼‰\n",
    "    # çœŸå®å€¾å‘å¾—åˆ†\n",
    "    propensity = 1 / (1 + np.exp(-(\n",
    "        1.0 * X[:, 0] +   # è´­ç‰©é¢‘ç‡é«˜ -> æ›´å¯èƒ½çœ‹å¹¿å‘Š\n",
    "        0.8 * X[:, 1] +   # æ´»è·ƒåº¦é«˜ -> æ›´å¯èƒ½çœ‹å¹¿å‘Š\n",
    "        0.6 * X[:, 2]     # æ¶ˆè´¹èƒ½åŠ›é«˜ -> æ›´å¯èƒ½çœ‹å¹¿å‘Š\n",
    "    )))\n",
    "    \n",
    "    # TODO: æ ¹æ®å€¾å‘å¾—åˆ†ç”Ÿæˆå¤„ç†åˆ†é… T\n",
    "    # æç¤º: ä½¿ç”¨ np.random.binomial(1, propensity)\n",
    "    T = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: ç”Ÿæˆæ½œåœ¨ç»“æœ\n",
    "    # Y(0): ä¸çœ‹å¹¿å‘Šçš„è´­ä¹°æ¦‚ç‡/é‡‘é¢\n",
    "    # Y(1): çœ‹å¹¿å‘Šçš„è´­ä¹°æ¦‚ç‡/é‡‘é¢\n",
    "    # æç¤º: ç»“æœä¹Ÿåº”è¯¥ä¾èµ–äº Xï¼ˆè¿™æ˜¯æ··æ·†ï¼ï¼‰\n",
    "    #       Y0 = 2.0 + X[:, 0] + 0.5*X[:, 1] + noise\n",
    "    #       Y1 = Y0 + å¤„ç†æ•ˆåº”ï¼ˆå¯ä»¥æ˜¯å¼‚è´¨çš„ï¼‰\n",
    "    \n",
    "    Y0 = None  # ä½ çš„ä»£ç \n",
    "    Y1 = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # è§‚æµ‹ç»“æœ\n",
    "    Y = np.where(T == 1, Y1, Y0)\n",
    "    \n",
    "    return X, T, Y, Y0, Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ•°æ®ç”Ÿæˆ\n",
    "X, T, Y, Y0, Y1 = generate_confounded_data(n=2000)\n",
    "\n",
    "if T is not None:\n",
    "    print(\"æ•°æ®ç”ŸæˆæˆåŠŸ! ğŸ‰\")\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}\")\n",
    "    print(f\"å¤„ç†ç»„æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "    print(f\"\\nçœŸå®å› æœæ•ˆåº”:\")\n",
    "    print(f\"  çœŸå® ATE: {np.mean(Y1 - Y0):.4f}\")\n",
    "    print(f\"  çœŸå® ATT: {np.mean((Y1 - Y0)[T == 1]):.4f}\")\n",
    "    print(f\"\\næœ´ç´ ä¼°è®¡ï¼ˆæœ‰åï¼ï¼‰:\")\n",
    "    naive_ate = Y[T == 1].mean() - Y[T == 0].mean()\n",
    "    print(f\"  æœ´ç´  ATE: {naive_ate:.4f}\")\n",
    "    print(f\"  åå·®: {naive_ate - np.mean(Y1 - Y0):.4f}\")\n",
    "else:\n",
    "    print(\"[TODO] è¯·å®Œæˆ generate_confounded_data å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.2: DragonNet æ¶æ„\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬å®ç°ä¸‰å¤´é¾™ï¼\n",
    "\n",
    "```\n",
    "è¾“å…¥ X (5ç»´)\n",
    "    |\n",
    "    v\n",
    "[å…±äº«è¡¨ç¤ºå±‚]\n",
    "Linear(5 -> 50) + ELU\n",
    "Linear(50 -> 25) + ELU\n",
    "    |\n",
    "    v\n",
    "Î¦(X) (25ç»´è¡¨ç¤º)\n",
    "    |\n",
    "+---+---+---+\n",
    "|   |   |   |\n",
    "v   v   v   v\n",
    "å¤´0  å¤´1  å€¾å‘å¾—åˆ†å¤´\n",
    "|   |       |\n",
    "v   v       v\n",
    "Y0  Y1    e(X)âˆˆ[0,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDragonNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–ç‰ˆ DragonNet - ä¸‰å¤´é¾™ç½‘ç»œ\n",
    "    \n",
    "    ä¸ TARNet çš„åŒºåˆ«:\n",
    "    1. å¤šäº†ä¸€ä¸ªå€¾å‘å¾—åˆ†å¤´\n",
    "    2. æœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„ epsilon å‚æ•°\n",
    "    3. ä½¿ç”¨ ELU æ¿€æ´»å‡½æ•°ï¼ˆåŸè®ºæ–‡æ¨èï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 50,\n",
    "        repr_dim: int = 25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: å®šä¹‰å…±äº«è¡¨ç¤ºå±‚\n",
    "        # Input -> Hidden (ELU) -> Representation (ELU)\n",
    "        # æ³¨æ„: DragonNet ä½¿ç”¨ ELU æ¿€æ´»å‡½æ•°\n",
    "        self.representation = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç \n",
    "        )\n",
    "        \n",
    "        # TODO: å®šä¹‰æ§åˆ¶ç»„è¾“å‡ºå¤´ (Y0)\n",
    "        # Representation -> Hidden (ELU) -> 1\n",
    "        self.head0 = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç \n",
    "        )\n",
    "        \n",
    "        # TODO: å®šä¹‰å¤„ç†ç»„è¾“å‡ºå¤´ (Y1)\n",
    "        self.head1 = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç \n",
    "        )\n",
    "        \n",
    "        # TODO: å®šä¹‰å€¾å‘å¾—åˆ†å¤´ (è¿™æ˜¯å…³é”®ï¼)\n",
    "        # Representation -> Hidden (ELU) -> 1 (Sigmoid)\n",
    "        # æ³¨æ„: æœ€åä½¿ç”¨ Sigmoid å°†è¾“å‡ºé™åˆ¶åœ¨ [0, 1]\n",
    "        self.propensity_head = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç \n",
    "        )\n",
    "        \n",
    "        # TODO: å®šä¹‰ epsilon å‚æ•° (å¯å­¦ä¹ çš„æ ‡é‡)\n",
    "        # æç¤º: ä½¿ç”¨ nn.Parameter(torch.tensor(0.0))\n",
    "        self.epsilon = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Returns:\n",
    "            (y0_pred, y1_pred, propensity, epsilon, representation)\n",
    "        \"\"\"\n",
    "        # TODO: å®Œæˆå‰å‘ä¼ æ’­\n",
    "        phi = None      # å…±äº«è¡¨ç¤º\n",
    "        y0 = None       # æ§åˆ¶ç»„é¢„æµ‹\n",
    "        y1 = None       # å¤„ç†ç»„é¢„æµ‹\n",
    "        propensity = None  # å€¾å‘å¾—åˆ†\n",
    "        \n",
    "        return y0, y1, propensity, self.epsilon, phi\n",
    "    \n",
    "    def predict_ite(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"é¢„æµ‹ä¸ªä½“å¤„ç†æ•ˆåº”\"\"\"\n",
    "        y0, y1, _, _, _ = self.forward(x)\n",
    "        return y1 - y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• DragonNet æ¶æ„\n",
    "if X is not None:\n",
    "    model = SimpleDragonNet(input_dim=X.shape[1])\n",
    "    X_sample = torch.FloatTensor(X[:5])\n",
    "    \n",
    "    try:\n",
    "        y0, y1, prop, eps, phi = model(X_sample)\n",
    "        if y0 is not None:\n",
    "            print(\"DragonNet æ¶æ„æµ‹è¯•é€šè¿‡! ğŸ‰\")\n",
    "            print(f\"\\nè¾“å‡ºå½¢çŠ¶:\")\n",
    "            print(f\"  Y0 é¢„æµ‹: {y0.shape}\")\n",
    "            print(f\"  Y1 é¢„æµ‹: {y1.shape}\")\n",
    "            print(f\"  å€¾å‘å¾—åˆ†: {prop.shape}\")\n",
    "            print(f\"  è¡¨ç¤ºå‘é‡: {phi.shape}\")\n",
    "            print(f\"\\nå…³é”®å‚æ•°:\")\n",
    "            print(f\"  Epsilon: {eps}\")\n",
    "            print(f\"  å€¾å‘å¾—åˆ†èŒƒå›´: [{prop.min():.4f}, {prop.max():.4f}]\")\n",
    "        else:\n",
    "            print(\"[TODO] è¯·å®Œæˆ SimpleDragonNet.forward å‡½æ•°\")\n",
    "    except Exception as e:\n",
    "        print(f\"[TODO] è¯·å®Œæˆ SimpleDragonNet å®šä¹‰: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.3: DragonNet å¤åˆæŸå¤±å‡½æ•°\n",
    "\n",
    "è¿™æ˜¯ DragonNet çš„ç²¾é«“ï¼æˆ‘ä»¬éœ€è¦å®ç°ä¸‰ä¸ªæŸå¤±é¡¹ï¼š\n",
    "\n",
    "1. **Factual Loss**: é¢„æµ‹è§‚æµ‹ç»“æœï¼ˆå’Œ TARNet ä¸€æ ·ï¼‰\n",
    "2. **Propensity Loss**: é¢„æµ‹å¤„ç†åˆ†é…\n",
    "3. **Targeted Regularization**: é­”æ³•è°ƒæ•´é¡¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dragonnet_loss(\n",
    "    y_true: torch.Tensor,\n",
    "    t_true: torch.Tensor,\n",
    "    y0_pred: torch.Tensor,\n",
    "    y1_pred: torch.Tensor,\n",
    "    propensity: torch.Tensor,\n",
    "    epsilon: torch.Tensor,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1.0\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    DragonNet å¤åˆæŸå¤±å‡½æ•°\n",
    "    \n",
    "    L = L_factual + alpha * L_propensity + beta * L_targeted\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha: å€¾å‘å¾—åˆ†æŸå¤±æƒé‡\n",
    "    beta: targeted regularization æƒé‡\n",
    "    \"\"\"\n",
    "    \n",
    "    # ç¡®ä¿å½¢çŠ¶æ­£ç¡®\n",
    "    y0_pred = y0_pred.squeeze()\n",
    "    y1_pred = y1_pred.squeeze()\n",
    "    propensity = propensity.squeeze()\n",
    "    \n",
    "    # TODO 1: Factual Loss\n",
    "    # æ ¹æ® T é€‰æ‹©å¯¹åº”çš„é¢„æµ‹å€¼ï¼Œè®¡ç®— MSE\n",
    "    # y_pred = T * y1_pred + (1-T) * y0_pred\n",
    "    y_pred = None  # ä½ çš„ä»£ç \n",
    "    factual_loss = None  # ä½ çš„ä»£ç  (ä½¿ç”¨ MSE)\n",
    "    \n",
    "    # TODO 2: Propensity Loss\n",
    "    # äºŒåˆ†ç±»äº¤å‰ç†µ: BCE(propensity, T)\n",
    "    # æç¤º: ä½¿ç”¨ F.binary_cross_entropy æˆ–æ‰‹åŠ¨è®¡ç®—\n",
    "    # æ·»åŠ å°å¸¸æ•° 1e-8 é¿å… log(0)\n",
    "    propensity_loss = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO 3: Targeted Regularization (DragonNet çš„åˆ›æ–°!)\n",
    "    # h = T / (e(X) + eps) - (1-T) / (1-e(X) + eps)\n",
    "    # L_targeted = mean((Y - Y_pred - epsilon * h)^2)\n",
    "    # \n",
    "    # æç¤º:\n",
    "    # - e(X) æ˜¯ propensity\n",
    "    # - epsilon æ˜¯å¯å­¦ä¹ çš„å‚æ•°\n",
    "    # - æ·»åŠ å°å¸¸æ•° 1e-8 é¿å…é™¤é›¶\n",
    "    \n",
    "    h = None  # ä½ çš„ä»£ç \n",
    "    targeted_reg = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: æ€»æŸå¤±\n",
    "    total_loss = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    return {\n",
    "        'total': total_loss,\n",
    "        'factual': factual_loss,\n",
    "        'propensity': propensity_loss,\n",
    "        'targeted': targeted_reg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æŸå¤±å‡½æ•°\n",
    "try:\n",
    "    y_true = torch.FloatTensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    t_true = torch.FloatTensor([1.0, 0.0, 1.0, 0.0, 1.0])\n",
    "    y0_pred = torch.FloatTensor([1.5, 2.0, 2.5, 4.5, 4.0])\n",
    "    y1_pred = torch.FloatTensor([2.0, 2.5, 3.0, 5.0, 5.5])\n",
    "    propensity = torch.FloatTensor([0.7, 0.3, 0.6, 0.4, 0.8])\n",
    "    epsilon = torch.tensor(0.1)\n",
    "    \n",
    "    losses = dragonnet_loss(\n",
    "        y_true, t_true, y0_pred, y1_pred,\n",
    "        propensity, epsilon, alpha=1.0, beta=1.0\n",
    "    )\n",
    "    \n",
    "    if losses['total'] is not None:\n",
    "        print(\"æŸå¤±å‡½æ•°æµ‹è¯•é€šè¿‡! ğŸ‰\")\n",
    "        print(f\"\\nå„æŸå¤±é¡¹:\")\n",
    "        print(f\"  Factual Loss: {losses['factual'].item():.4f}\")\n",
    "        print(f\"  Propensity Loss: {losses['propensity'].item():.4f}\")\n",
    "        print(f\"  Targeted Reg: {losses['targeted'].item():.4f}\")\n",
    "        print(f\"  Total Loss: {losses['total'].item():.4f}\")\n",
    "    else:\n",
    "        print(\"[TODO] è¯·å®Œæˆ dragonnet_loss å‡½æ•°\")\n",
    "except Exception as e:\n",
    "    print(f\"[TODO] è¯·å®Œæˆ dragonnet_loss å‡½æ•°: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.4: è®­ç»ƒ DragonNet\n",
    "\n",
    "è®©æˆ‘ä»¬æŠŠä¸‰å¤´é¾™è®­ç»ƒèµ·æ¥ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dragonnet(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    n_epochs: int = 200,\n",
    "    batch_size: int = 64,\n",
    "    learning_rate: float = 1e-3,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1.0,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[SimpleDragonNet, dict]:\n",
    "    \"\"\"\n",
    "    è®­ç»ƒ DragonNet\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha: å€¾å‘å¾—åˆ†æŸå¤±æƒé‡\n",
    "    beta: targeted regularization æƒé‡\n",
    "    \n",
    "    Returns:\n",
    "        (trained_model, training_history)\n",
    "    \"\"\"\n",
    "    \n",
    "    # æ•°æ®å‡†å¤‡\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    T_tensor = torch.FloatTensor(T)\n",
    "    Y_tensor = torch.FloatTensor(Y)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, T_tensor, Y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    model = SimpleDragonNet(input_dim=X.shape[1])\n",
    "    \n",
    "    # TODO: å®šä¹‰ä¼˜åŒ–å™¨ (Adam, å­¦ä¹ ç‡=learning_rate)\n",
    "    optimizer = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # è®­ç»ƒå†å²\n",
    "    history = {\n",
    "        'total_loss': [],\n",
    "        'factual_loss': [],\n",
    "        'propensity_loss': [],\n",
    "        'targeted_loss': [],\n",
    "        'epsilon': []\n",
    "    }\n",
    "    \n",
    "    # è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_losses = {k: 0 for k in history.keys() if k != 'epsilon'}\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch_x, batch_t, batch_y in dataloader:\n",
    "            # TODO: å®Œæˆè®­ç»ƒæ­¥éª¤\n",
    "            # 1. æ¸…é›¶æ¢¯åº¦\n",
    "            # 2. å‰å‘ä¼ æ’­\n",
    "            # 3. è®¡ç®— DragonNet æŸå¤±\n",
    "            # 4. åå‘ä¼ æ’­\n",
    "            # 5. æ›´æ–°å‚æ•°\n",
    "            # 6. è®°å½•å„é¡¹æŸå¤±\n",
    "            \n",
    "            # ä½ çš„ä»£ç \n",
    "            pass\n",
    "        \n",
    "        # è®°å½•å¹³å‡æŸå¤±\n",
    "        for k in epoch_losses.keys():\n",
    "            history[k.replace('_loss', '_loss')].append(\n",
    "                epoch_losses[k] / n_batches if n_batches > 0 else 0\n",
    "            )\n",
    "        \n",
    "        # è®°å½• epsilon\n",
    "        if model.epsilon is not None:\n",
    "            history['epsilon'].append(model.epsilon.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(f\"  Total: {history['total_loss'][-1]:.4f}\")\n",
    "            print(f\"  Factual: {history['factual_loss'][-1]:.4f}\")\n",
    "            print(f\"  Propensity: {history['propensity_loss'][-1]:.4f}\")\n",
    "            if model.epsilon is not None:\n",
    "                print(f\"  Epsilon: {model.epsilon.item():.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒ DragonNet\n",
    "if X is not None:\n",
    "    print(\"å¼€å§‹è®­ç»ƒ DragonNet...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        model, history = train_dragonnet(\n",
    "            X, T, Y,\n",
    "            n_epochs=200,\n",
    "            batch_size=64,\n",
    "            alpha=1.0,\n",
    "            beta=1.0\n",
    "        )\n",
    "        \n",
    "        if model is not None and len(history['total_loss']) > 0:\n",
    "            print(\"\\nè®­ç»ƒå®Œæˆ! ğŸ‰\")\n",
    "            print(f\"æœ€ç»ˆæ€»æŸå¤±: {history['total_loss'][-1]:.4f}\")\n",
    "        else:\n",
    "            print(\"[TODO] è¯·å®Œæˆ train_dragonnet å‡½æ•°\")\n",
    "    except Exception as e:\n",
    "        print(f\"[TODO] è®­ç»ƒå‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "if 'history' in dir() and history is not None and len(history['total_loss']) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # æ€»æŸå¤±\n",
    "    axes[0, 0].plot(history['total_loss'], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Total Loss')\n",
    "    axes[0, 0].set_title('Total Loss æ”¶æ•›æ›²çº¿')\n",
    "    \n",
    "    # ä¸‰ä¸ªæŸå¤±é¡¹\n",
    "    axes[0, 1].plot(history['factual_loss'], 'g-', label='Factual', linewidth=2)\n",
    "    axes[0, 1].plot(history['propensity_loss'], 'r-', label='Propensity', linewidth=2)\n",
    "    if 'targeted_loss' in history and len(history['targeted_loss']) > 0:\n",
    "        axes[0, 1].plot(history['targeted_loss'], 'purple', label='Targeted', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('å„æŸå¤±é¡¹æ”¶æ•›æ›²çº¿')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Epsilon å˜åŒ–\n",
    "    if len(history['epsilon']) > 0:\n",
    "        axes[1, 0].plot(history['epsilon'], 'orange', linewidth=2)\n",
    "        axes[1, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Epsilon')\n",
    "        axes[1, 0].set_title('Epsilon å‚æ•°å˜åŒ– (å¯å­¦ä¹ çš„è°ƒæ•´é¡¹)')\n",
    "    \n",
    "    # æŸå¤±æ¯”ä¾‹\n",
    "    if len(history['factual_loss']) > 0:\n",
    "        total = np.array(history['total_loss'])\n",
    "        factual = np.array(history['factual_loss'])\n",
    "        propensity = np.array(history['propensity_loss'])\n",
    "        \n",
    "        axes[1, 1].stackplot(\n",
    "            range(len(total)),\n",
    "            factual, propensity,\n",
    "            labels=['Factual', 'Propensity'],\n",
    "            colors=['#2ecc71', '#e74c3c'],\n",
    "            alpha=0.7\n",
    "        )\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].set_title('æŸå¤±ç»„æˆå †å å›¾')\n",
    "        axes[1, 1].legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.5: è¯„ä¼°å€¾å‘å¾—åˆ†ä¼°è®¡è´¨é‡\n",
    "\n",
    "DragonNet çš„å€¾å‘å¾—åˆ†å¤´å­¦å¾—å¥½ä¸å¥½ï¼Ÿè®©æˆ‘ä»¬æ¥è¯„ä¼°ä¸€ä¸‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_propensity_score(\n",
    "    model: SimpleDragonNet,\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°å€¾å‘å¾—åˆ†ä¼°è®¡è´¨é‡\n",
    "    \n",
    "    è¯„ä¼°æŒ‡æ ‡:\n",
    "    1. AUC: é¢„æµ‹å¤„ç†åˆ†é…çš„èƒ½åŠ›\n",
    "    2. æ ¡å‡†æ›²çº¿: é¢„æµ‹æ¦‚ç‡ vs å®é™…å¤„ç†ç‡\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        _, _, propensity_pred, _, _ = model(X_tensor)\n",
    "        propensity_pred = propensity_pred.numpy().squeeze()\n",
    "    \n",
    "    # TODO: è®¡ç®— AUC\n",
    "    # æç¤º: ä½¿ç”¨ roc_auc_score(T, propensity_pred)\n",
    "    auc = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: è®¡ç®—æ ¡å‡†æŒ‡æ ‡\n",
    "    # å°†æ ·æœ¬æŒ‰é¢„æµ‹å€¾å‘å¾—åˆ†åˆ†æˆ 5 ç»„\n",
    "    # å¯¹æ¯”æ¯ç»„çš„å¹³å‡é¢„æµ‹å€¾å‘å¾—åˆ†å’Œå®é™…å¤„ç†ç‡\n",
    "    n_bins = 5\n",
    "    calibration = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        # ä½ çš„ä»£ç \n",
    "        # æ‰¾å‡ºå€¾å‘å¾—åˆ†åœ¨ç¬¬ i ä¸ªåˆ†ä½åŒºé—´çš„æ ·æœ¬\n",
    "        # è®¡ç®—è¯¥åŒºé—´çš„å¹³å‡é¢„æµ‹å€¾å‘å¾—åˆ†å’Œå®é™…å¤„ç†ç‡\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'calibration': calibration,\n",
    "        'propensity_pred': propensity_pred,\n",
    "        'propensity_mean': propensity_pred.mean(),\n",
    "        'propensity_std': propensity_pred.std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯„ä¼°å€¾å‘å¾—åˆ†\n",
    "if 'model' in dir() and model is not None:\n",
    "    prop_metrics = evaluate_propensity_score(model, X, T)\n",
    "    \n",
    "    if prop_metrics['auc'] is not None:\n",
    "        print(\"å€¾å‘å¾—åˆ†è¯„ä¼°ç»“æœ:\")\n",
    "        print(f\"  AUC: {prop_metrics['auc']:.4f}\")\n",
    "        print(f\"  å‡å€¼: {prop_metrics['propensity_mean']:.4f}\")\n",
    "        print(f\"  æ ‡å‡†å·®: {prop_metrics['propensity_std']:.4f}\")\n",
    "        print(f\"  å®é™…å¤„ç†ç‡: {T.mean():.4f}\")\n",
    "        \n",
    "        # å¯è§†åŒ–å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # åˆ†ç»„åˆ†å¸ƒ\n",
    "        axes[0].hist(\n",
    "            prop_metrics['propensity_pred'][T == 0],\n",
    "            bins=30, alpha=0.6, label='æ§åˆ¶ç»„', color='#3498db'\n",
    "        )\n",
    "        axes[0].hist(\n",
    "            prop_metrics['propensity_pred'][T == 1],\n",
    "            bins=30, alpha=0.6, label='å¤„ç†ç»„', color='#e74c3c'\n",
    "        )\n",
    "        axes[0].set_xlabel('é¢„æµ‹å€¾å‘å¾—åˆ†')\n",
    "        axes[0].set_ylabel('é¢‘æ•°')\n",
    "        axes[0].set_title('å€¾å‘å¾—åˆ†åˆ†å¸ƒ (æŒ‰å¤„ç†åˆ†ç»„)')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # æ ¡å‡†æ›²çº¿\n",
    "        if len(prop_metrics['calibration']) > 0:\n",
    "            pred_props = [c['pred_mean'] for c in prop_metrics['calibration']]\n",
    "            actual_rates = [c['actual_rate'] for c in prop_metrics['calibration']]\n",
    "            \n",
    "            axes[1].scatter(pred_props, actual_rates, s=100, c='#2ecc71', edgecolors='black')\n",
    "            axes[1].plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡å‡†')\n",
    "            axes[1].set_xlabel('é¢„æµ‹å€¾å‘å¾—åˆ†å‡å€¼')\n",
    "            axes[1].set_ylabel('å®é™…å¤„ç†ç‡')\n",
    "            axes[1].set_title('å€¾å‘å¾—åˆ†æ ¡å‡†æ›²çº¿')\n",
    "            axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[TODO] è¯·å®Œæˆ evaluate_propensity_score å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.6: è¯„ä¼°å› æœæ•ˆåº”ä¼°è®¡\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ DragonNet çš„å› æœæ•ˆåº”ä¼°è®¡æœ‰å¤šå‡†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_causal_effect(\n",
    "    model: SimpleDragonNet,\n",
    "    X: np.ndarray,\n",
    "    Y0: np.ndarray,\n",
    "    Y1: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°å› æœæ•ˆåº”ä¼°è®¡è´¨é‡\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y0_pred, y1_pred, _, _, _ = model(X_tensor)\n",
    "        y0_pred = y0_pred.numpy().squeeze()\n",
    "        y1_pred = y1_pred.numpy().squeeze()\n",
    "    \n",
    "    # çœŸå® ITE\n",
    "    ite_true = Y1 - Y0\n",
    "    ite_pred = y1_pred - y0_pred\n",
    "    \n",
    "    # PEHE (Precision in Estimation of Heterogeneous Effect)\n",
    "    pehe = np.sqrt(np.mean((ite_true - ite_pred) ** 2))\n",
    "    \n",
    "    # ATE è¯¯å·®\n",
    "    ate_true = np.mean(ite_true)\n",
    "    ate_pred = np.mean(ite_pred)\n",
    "    ate_error = np.abs(ate_true - ate_pred)\n",
    "    \n",
    "    # ITE ç›¸å…³æ€§\n",
    "    ite_corr = np.corrcoef(ite_true.flatten(), ite_pred.flatten())[0, 1]\n",
    "    \n",
    "    return {\n",
    "        'pehe': pehe,\n",
    "        'ate_true': ate_true,\n",
    "        'ate_pred': ate_pred,\n",
    "        'ate_error': ate_error,\n",
    "        'ite_corr': ite_corr,\n",
    "        'ite_true': ite_true,\n",
    "        'ite_pred': ite_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯„ä¼°å› æœæ•ˆåº”\n",
    "if 'model' in dir() and model is not None and Y0 is not None:\n",
    "    causal_metrics = evaluate_causal_effect(model, X, Y0, Y1)\n",
    "    \n",
    "    print(\"å› æœæ•ˆåº”ä¼°è®¡ç»“æœ:\")\n",
    "    print(f\"  PEHE: {causal_metrics['pehe']:.4f}\")\n",
    "    print(f\"  çœŸå® ATE: {causal_metrics['ate_true']:.4f}\")\n",
    "    print(f\"  é¢„æµ‹ ATE: {causal_metrics['ate_pred']:.4f}\")\n",
    "    print(f\"  ATE è¯¯å·®: {causal_metrics['ate_error']:.4f}\")\n",
    "    print(f\"  ITE ç›¸å…³æ€§: {causal_metrics['ite_corr']:.4f}\")\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ITE æ•£ç‚¹å›¾\n",
    "    axes[0].scatter(\n",
    "        causal_metrics['ite_true'],\n",
    "        causal_metrics['ite_pred'],\n",
    "        alpha=0.3, c='#3498db'\n",
    "    )\n",
    "    min_val = min(causal_metrics['ite_true'].min(), causal_metrics['ite_pred'].min())\n",
    "    max_val = max(causal_metrics['ite_true'].max(), causal_metrics['ite_pred'].max())\n",
    "    axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', label='å®Œç¾é¢„æµ‹')\n",
    "    axes[0].set_xlabel('çœŸå® ITE')\n",
    "    axes[0].set_ylabel('é¢„æµ‹ ITE')\n",
    "    axes[0].set_title(f'ITE é¢„æµ‹ vs çœŸå® (ç›¸å…³æ€§: {causal_metrics[\"ite_corr\"]:.3f})')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # ITE è¯¯å·®åˆ†å¸ƒ\n",
    "    ite_error = causal_metrics['ite_pred'] - causal_metrics['ite_true']\n",
    "    axes[1].hist(ite_error, bins=50, alpha=0.7, color='#9b59b6', edgecolor='black')\n",
    "    axes[1].axvline(x=0, color='red', linestyle='--', label='é›¶è¯¯å·®')\n",
    "    axes[1].axvline(x=ite_error.mean(), color='orange', linestyle='-', label=f'å‡å€¼: {ite_error.mean():.3f}')\n",
    "    axes[1].set_xlabel('ITE é¢„æµ‹è¯¯å·®')\n",
    "    axes[1].set_ylabel('é¢‘æ•°')\n",
    "    axes[1].set_title('ITE é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  3.7: TARNet vs DragonNet å¯¹æ¯”å®éªŒ\n",
    "\n",
    "DragonNet æ¯” TARNet å¥½åœ¨å“ªé‡Œï¼Ÿè®©æˆ‘ä»¬æ¥åšä¸ªå¯¹æ¯”å®éªŒï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tarnet_dragonnet(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    Y0: np.ndarray,\n",
    "    Y1: np.ndarray,\n",
    "    n_epochs: int = 200\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    å¯¹æ¯” TARNet å’Œ DragonNet çš„æ€§èƒ½\n",
    "    \n",
    "    TARNet = DragonNet with alpha=0, beta=0 (åªç”¨ Factual Loss)\n",
    "    DragonNet = DragonNet with alpha=1, beta=1 (å®Œæ•´æŸå¤±)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: è®­ç»ƒ TARNet (alpha=0, beta=0)\n",
    "    # æç¤º: ä½¿ç”¨ train_dragonnet ä½†è®¾ç½® alpha=0, beta=0\n",
    "    print(\"è®­ç»ƒ TARNet (åªç”¨ Factual Loss)...\")\n",
    "    tarnet, _ = None, None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: è®­ç»ƒ DragonNet (alpha=1.0, beta=1.0)\n",
    "    print(\"\\nè®­ç»ƒ DragonNet (å®Œæ•´æŸå¤±)...\")\n",
    "    dragonnet, _ = None, None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # è¯„ä¼°å‡½æ•°\n",
    "    def evaluate_model(model):\n",
    "        if model is None:\n",
    "            return None\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            y0_pred, y1_pred, _, _, _ = model(X_tensor)\n",
    "            y0_pred = y0_pred.numpy().squeeze()\n",
    "            y1_pred = y1_pred.numpy().squeeze()\n",
    "        \n",
    "        ite_true = Y1 - Y0\n",
    "        ite_pred = y1_pred - y0_pred\n",
    "        pehe = np.sqrt(np.mean((ite_true - ite_pred) ** 2))\n",
    "        \n",
    "        ate_true = np.mean(Y1 - Y0)\n",
    "        ate_pred = np.mean(y1_pred - y0_pred)\n",
    "        ate_error = np.abs(ate_true - ate_pred)\n",
    "        \n",
    "        return {'pehe': pehe, 'ate_error': ate_error}\n",
    "    \n",
    "    tarnet_metrics = evaluate_model(tarnet)\n",
    "    dragonnet_metrics = evaluate_model(dragonnet)\n",
    "    \n",
    "    return {\n",
    "        'tarnet': tarnet_metrics,\n",
    "        'dragonnet': dragonnet_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”å®éªŒ\n",
    "if X is not None and Y0 is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TARNet vs DragonNet å¯¹æ¯”å®éªŒ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        comparison = compare_tarnet_dragonnet(X, T, Y, Y0, Y1, n_epochs=200)\n",
    "        \n",
    "        if comparison['tarnet'] is not None and comparison['dragonnet'] is not None:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"å¯¹æ¯”ç»“æœ:\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"\\n{'æ¨¡å‹':<15} {'PEHE':<15} {'ATE è¯¯å·®':<15}\")\n",
    "            print(\"-\"*45)\n",
    "            print(f\"{'TARNet':<15} {comparison['tarnet']['pehe']:<15.4f} {comparison['tarnet']['ate_error']:<15.4f}\")\n",
    "            print(f\"{'DragonNet':<15} {comparison['dragonnet']['pehe']:<15.4f} {comparison['dragonnet']['ate_error']:<15.4f}\")\n",
    "            \n",
    "            # è®¡ç®—æ”¹è¿›\n",
    "            pehe_improve = (comparison['tarnet']['pehe'] - comparison['dragonnet']['pehe']) / comparison['tarnet']['pehe'] * 100\n",
    "            ate_improve = (comparison['tarnet']['ate_error'] - comparison['dragonnet']['ate_error']) / comparison['tarnet']['ate_error'] * 100\n",
    "            \n",
    "            print(f\"\\næ”¹è¿›:\")\n",
    "            print(f\"  PEHE: {pehe_improve:+.1f}%\")\n",
    "            print(f\"  ATE è¯¯å·®: {ate_improve:+.1f}%\")\n",
    "            \n",
    "            # å¯è§†åŒ–å¯¹æ¯”\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            x = np.arange(2)\n",
    "            width = 0.35\n",
    "            \n",
    "            tarnet_vals = [comparison['tarnet']['pehe'], comparison['tarnet']['ate_error']]\n",
    "            dragonnet_vals = [comparison['dragonnet']['pehe'], comparison['dragonnet']['ate_error']]\n",
    "            \n",
    "            bars1 = ax.bar(x - width/2, tarnet_vals, width, label='TARNet', color='#3498db')\n",
    "            bars2 = ax.bar(x + width/2, dragonnet_vals, width, label='DragonNet', color='#e74c3c')\n",
    "            \n",
    "            ax.set_ylabel('è¯¯å·®å€¼')\n",
    "            ax.set_title('TARNet vs DragonNet æ€§èƒ½å¯¹æ¯”')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(['PEHE', 'ATE Error'])\n",
    "            ax.legend()\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for bar, val in zip(bars1, tarnet_vals):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                       f'{val:.3f}', ha='center', va='bottom')\n",
    "            for bar, val in zip(bars2, dragonnet_vals):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                       f'{val:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"[TODO] è¯·å®Œæˆ compare_tarnet_dragonnet å‡½æ•°\")\n",
    "    except Exception as e:\n",
    "        print(f\"[TODO] å¯¹æ¯”å®éªŒå‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” æ€è€ƒé¢˜\n",
    "\n",
    "åœ¨ä»£ç å•å…ƒæ ¼ä¸­å†™ä¸‹ä½ çš„ç­”æ¡ˆï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 1: å€¾å‘å¾—åˆ†å¤´åœ¨ DragonNet ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "# ä¸ºä»€ä¹ˆä¸å•ç‹¬è®­ç»ƒä¸€ä¸ªå€¾å‘å¾—åˆ†æ¨¡å‹ï¼Ÿ\n",
    "\n",
    "answer_1 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 2: ä¸ºä»€ä¹ˆ Targeted Regularization ä¸­éœ€è¦ h = T/e(X) - (1-T)/(1-e(X))?\n",
    "# è¿™ä¸ªå…¬å¼çš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "answer_2 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 3: epsilon å‚æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒæ˜¯å¯å­¦ä¹ çš„ï¼Ÿ\n",
    "\n",
    "answer_3 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 4: ä»€ä¹ˆæ—¶å€™ DragonNet ä¼šæ¯” TARNet æ•ˆæœæ›´å¥½ï¼Ÿ\n",
    "\n",
    "answer_4 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 5: å¦‚æœå€¾å‘å¾—åˆ†å¤´çš„é¢„æµ‹å¾ˆå·®ï¼Œä¼šå¯¹æ•´ä¸ªæ¨¡å‹æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
    "\n",
    "answer_5 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 6: DragonNet å’Œä¼ ç»Ÿçš„å€¾å‘å¾—åˆ†æ–¹æ³•ï¼ˆå¦‚ IPWï¼‰æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "\n",
    "answer_6 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ 7: åœ¨å®è·µä¸­ï¼Œå¦‚ä½•é€‰æ‹© alpha å’Œ beta è¿™ä¸¤ä¸ªè¶…å‚æ•°ï¼Ÿ\n",
    "\n",
    "answer_7 = \"\"\"\n",
    "ä½ çš„ç­”æ¡ˆ:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ€»ç»“\n",
    "\n",
    "### DragonNet æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "| ç»„ä»¶ | ä½œç”¨ | å…³é”®ç‚¹ |\n",
    "|------|------|--------|\n",
    "| å…±äº«è¡¨ç¤ºå±‚ | å­¦ä¹ å¹³è¡¡è¡¨ç¤º | åŒæ—¶ç”¨äºé¢„æµ‹ç»“æœå’Œå€¾å‘å¾—åˆ† |\n",
    "| Y(0)å¤´ | é¢„æµ‹æ§åˆ¶ç»„ç»“æœ | ä¸ TARNet ç›¸åŒ |\n",
    "| Y(1)å¤´ | é¢„æµ‹å¤„ç†ç»„ç»“æœ | ä¸ TARNet ç›¸åŒ |\n",
    "| å€¾å‘å¾—åˆ†å¤´ | é¢„æµ‹å¤„ç†æ¦‚ç‡ | å¸®åŠ©å­¦ä¹ æ··æ·†å› å­ |\n",
    "| epsilon | è°ƒæ•´å‚æ•° | å¯å­¦ä¹ ï¼Œè‡ªåŠ¨é€‚åº”åå·® |\n",
    "\n",
    "### æŸå¤±å‡½æ•°å¯¹æ¯”\n",
    "\n",
    "| æ¨¡å‹ | æŸå¤±å‡½æ•° |\n",
    "|------|----------|\n",
    "| TARNet | $\\mathcal{L}_{\\text{factual}}$ |\n",
    "| DragonNet | $\\mathcal{L}_{\\text{factual}} + \\alpha \\mathcal{L}_{\\text{propensity}} + \\beta \\mathcal{L}_{\\text{targeted}}$ |\n",
    "\n",
    "### ä»€ä¹ˆæ—¶å€™ç”¨ DragonNet?\n",
    "\n",
    "âœ… **é€‚åˆåœºæ™¯**:\n",
    "- å­˜åœ¨å¼ºæ··æ·†\n",
    "- å¤„ç†åˆ†é…é«˜åº¦ä¸å¹³è¡¡\n",
    "- éœ€è¦æ›´ç¨³å¥çš„ä¼°è®¡\n",
    "\n",
    "âŒ **ä¸é€‚åˆåœºæ™¯**:\n",
    "- éšæœºå¯¹ç…§å®éªŒï¼ˆæ— æ··æ·†ï¼‰\n",
    "- æ•°æ®é‡éå¸¸å°ï¼ˆå¤æ‚æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ è¿›é˜¶æŒ‘æˆ˜\n",
    "\n",
    "å¦‚æœä½ å·²ç»å®Œæˆäº†åŸºç¡€ç»ƒä¹ ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æŒ‘æˆ˜ï¼š\n",
    "\n",
    "1. **å®ç° CEVAE** (Causal Effect Variational Autoencoder)\n",
    "2. **æ·»åŠ æ­£åˆ™åŒ–**: å°è¯•æ·»åŠ  L2 æ­£åˆ™åŒ–æˆ– Dropout\n",
    "3. **å¤šå¤„ç†ç»„æ‰©å±•**: å°† DragonNet æ‰©å±•åˆ°å¤šä¸ªå¤„ç†ç»„\n",
    "4. **ä¸ç¡®å®šæ€§ä¼°è®¡**: æ·»åŠ  Bayesian æ–¹æ³•ä¼°è®¡ CATE çš„ä¸ç¡®å®šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ æ­å–œå®Œæˆ DragonNet ç»ƒä¹ !\")\n",
    "print(\"\\nä½ å·²ç»å­¦ä¼šäº†:\")\n",
    "print(\"  âœ“ DragonNet çš„ä¸‰å¤´æ¶æ„\")\n",
    "print(\"  âœ“ å€¾å‘å¾—åˆ†å¤´çš„ä½œç”¨\")\n",
    "print(\"  âœ“ Targeted Regularization çš„åŸç†\")\n",
    "print(\"  âœ“ TARNet vs DragonNet çš„å¯¹æ¯”\")\n",
    "print(\"\\nä¸‹ä¸€æ­¥: Chapter 5 å¼‚è´¨æ•ˆåº”åˆ†æ - å› æœæ£®æ—!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
