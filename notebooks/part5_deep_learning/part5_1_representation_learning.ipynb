{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Exercise 1: è¡¨ç¤ºå­¦ä¹ åŸºç¡€ - ä¸ºæ·±åº¦å› æœæ¨¡å‹é“ºè·¯\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ è¡¨ç¤º (Representation Learning)\n",
    "2. æŒæ¡ç®€å•çš„ç¥ç»ç½‘ç»œç‰¹å¾æå–\n",
    "3. ç†è§£å¤„ç†ç»„å’Œå¯¹ç…§ç»„è¡¨ç¤ºçš„å·®å¼‚\n",
    "4. ä¸ºæ·±åº¦å› æœæ¨¡å‹æ‰“ä¸‹åŸºç¡€\n",
    "\n",
    "---\n",
    "\n",
    "## ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°æ·±åº¦å› æœæ¨¡å‹\n",
    "\n",
    "### ä¼ ç»Ÿæ–¹æ³•çš„å±€é™\n",
    "\n",
    "æˆ‘ä»¬ä¹‹å‰å­¦ä¹ çš„ S-Learnerã€T-Learner ç­‰æ–¹æ³•éƒ½ä¾èµ–äº **æ‰‹å·¥ç‰¹å¾**ã€‚ä½†ç°å®ä¸–ç•Œä¸­ï¼š\n",
    "\n",
    "- ç‰¹å¾å¯èƒ½æ˜¯éçº¿æ€§çš„\n",
    "- çœŸæ­£æœ‰ç”¨çš„ä¿¡æ¯å¯èƒ½éšè—åœ¨ç‰¹å¾çš„å¤æ‚ç»„åˆä¸­\n",
    "- é«˜ç»´æ•°æ® (å¦‚å›¾åƒã€æ–‡æœ¬) æ— æ³•ç›´æ¥ä½¿ç”¨\n",
    "\n",
    "### è¡¨ç¤ºå­¦ä¹ çš„é­”åŠ›\n",
    "\n",
    "**è¡¨ç¤ºå­¦ä¹  (Representation Learning)** è®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æœ€æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºï¼š\n",
    "\n",
    "$$X \\xrightarrow{\\text{ç¥ç»ç½‘ç»œ}} \\Phi(X) \\xrightarrow{\\text{å› æœæ¨¡å‹}} Y(0), Y(1)$$\n",
    "\n",
    "è¿™ä¸ª $\\Phi(X)$ å°±æ˜¯å­¦ä¹ åˆ°çš„ \"è¡¨ç¤º\"ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæ´»åŒ–ç±»æ¯”ï¼šç¿»è¯‘å™¨\n",
    "\n",
    "æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªåªæ‡‚ä¸­æ–‡çš„äººï¼Œè¦åˆ¤æ–­ä¸€ç¯‡è‹±æ–‡è®ºæ–‡çš„è´¨é‡ã€‚\n",
    "\n",
    "**ä¼ ç»Ÿæ–¹æ³•**ï¼š\n",
    "- æ‰‹å·¥æå–ç‰¹å¾ï¼šå•è¯æ•°ã€å¥å­é•¿åº¦ã€æ ‡ç‚¹ç¬¦å·æ•°...\n",
    "- è¿™äº›ç‰¹å¾å¯èƒ½å’Œè®ºæ–‡è´¨é‡å…³ç³»ä¸å¤§\n",
    "\n",
    "**è¡¨ç¤ºå­¦ä¹ æ–¹æ³•**ï¼š\n",
    "- å…ˆç”¨ç¥ç»ç½‘ç»œæŠŠè‹±æ–‡ \"ç¿»è¯‘\" æˆä¸€ç§é€šç”¨è¡¨ç¤º\n",
    "- è¿™ä¸ªè¡¨ç¤ºæ•è·äº†è®ºæ–‡çš„è¯­ä¹‰ä¿¡æ¯\n",
    "- ç„¶åç”¨è¿™ä¸ªè¡¨ç¤ºæ¥é¢„æµ‹è®ºæ–‡è´¨é‡\n",
    "\n",
    "ç¥ç»ç½‘ç»œå°±åƒä¸€ä¸ª **æ™ºèƒ½ç¿»è¯‘å™¨**ï¼ŒæŠŠåŸå§‹æ•°æ®ç¿»è¯‘æˆå¯¹ä»»åŠ¡æœ€æœ‰ç”¨çš„å½¢å¼ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: éçº¿æ€§æ•°æ®çš„æŒ‘æˆ˜\n",
    "\n",
    "### ä¸ºä»€ä¹ˆçº¿æ€§æ¨¡å‹ä¸å¤Ÿç”¨?\n",
    "\n",
    "å‡è®¾çœŸå®çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹æ˜¯éçº¿æ€§çš„ï¼š\n",
    "\n",
    "- åŸå§‹ç‰¹å¾: $X_1, X_2 \\sim N(0, 1)$\n",
    "- çœŸæ­£æœ‰ç”¨çš„ç‰¹å¾: $\\Phi_1 = \\sin(X_1)$, $\\Phi_2 = X_1 \\times X_2$\n",
    "- å¤„ç†åˆ†é…: $P(T=1|X) = \\sigma(\\Phi_1 + \\Phi_2)$\n",
    "- ç»“æœ: $Y = 1 + 2T + \\Phi_1 + 0.5\\Phi_2 + \\epsilon$\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬åªç”¨ $X_1, X_2$ å»ºæ¨¡ï¼Œä¼šé”™è¿‡å…³é”®çš„éçº¿æ€§å…³ç³»ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  1.1: ç”Ÿæˆéçº¿æ€§æ•°æ®\n\ndef generate_nonlinear_data(\n    n: int = 1000,\n    seed: int = 42\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    ç”Ÿæˆéçº¿æ€§æ•°æ®: åŸå§‹ç‰¹å¾æ— æ³•ç›´æ¥æ•è·å› æœå…³ç³»\n    \n    DGP (Data Generating Process):\n    - X1, X2 ~ N(0, 1)\n    - çœŸå®æœ‰ç”¨ç‰¹å¾: Phi1 = sin(X1), Phi2 = X1 * X2\n    - T ~ Bernoulli(sigmoid(Phi1 + Phi2))\n    - Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise\n    \"\"\"\n    np.random.seed(seed)\n    \n    # ç”ŸæˆåŸå§‹ç‰¹å¾ X1, X2\n    X1 = np.random.randn(n)\n    X2 = np.random.randn(n)\n    \n    # è®¡ç®—æœ‰ç”¨ç‰¹å¾\n    Phi1 = np.sin(X1)\n    Phi2 = X1 * X2\n    \n    # ç”Ÿæˆå¤„ç† T (é€šè¿‡ sigmoid å‡½æ•°)\n    logit = Phi1 + 0.5 * Phi2\n    propensity = 1 / (1 + np.exp(-logit))\n    T = np.random.binomial(1, propensity, n)\n    \n    # ç”Ÿæˆç»“æœ Y\n    noise = np.random.randn(n) * 0.5\n    Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise\n    \n    X = np.column_stack([X1, X2])\n    \n    return X, T, Y\n\n# æµ‹è¯•\nX, T, Y = generate_nonlinear_data()\nprint(f\"æ•°æ®å½¢çŠ¶: X={X.shape}, T={T.shape}, Y={Y.shape}\")\nprint(f\"å¤„ç†æ¯”ä¾‹: {T.mean():.2%}\")\nprint(f\"å¹³å‡ç»“æœ: {Y.mean():.4f}\")\nprint(f\"\\nçœŸå® ATE = 2.0 (è¿™æ˜¯æˆ‘ä»¬è®¾å®šçš„)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  1.1 ç»­: æœ´ç´ çº¿æ€§ä¼°è®¡\n\ndef naive_linear_estimation(X, T, Y):\n    \"\"\"\n    ä½¿ç”¨çº¿æ€§å›å½’ä¼°è®¡ ATE (ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾)\n    \n    æç¤º: Y = beta0 + beta1*T + beta2*X1 + beta3*X2 + epsilon\n          beta1 å°±æ˜¯ ATE ä¼°è®¡\n    \"\"\"\n    # æ„é€ ç‰¹å¾çŸ©é˜µ [T, X]\n    features = np.column_stack([T, X])\n    \n    # è®­ç»ƒçº¿æ€§å›å½’\n    model = LinearRegression()\n    model.fit(features, Y)\n    \n    # è¿”å› T çš„ç³»æ•°ä½œä¸º ATE ä¼°è®¡\n    ate_estimate = model.coef_[0]  # T æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾\n    \n    return ate_estimate\n\n# æµ‹è¯•\nate_linear = naive_linear_estimation(X, T, Y)\nprint(f\"çº¿æ€§å›å½’ä¼°è®¡çš„ ATE: {ate_linear:.4f}\")\nprint(f\"çœŸå® ATE: 2.0\")\nprint(f\"è¯¯å·®: {abs(ate_linear - 2.0):.4f}\")\nprint(f\"\\nç”±äºå¿½ç•¥äº†éçº¿æ€§å…³ç³»ï¼Œçº¿æ€§ä¼°è®¡å¯èƒ½æœ‰åå·®!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: ç®€å•çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n",
    "\n",
    "### ç¥ç»ç½‘ç»œæ¶æ„\n",
    "\n",
    "```\n",
    "åŸå§‹ç‰¹å¾ X (2ç»´)\n",
    "     |\n",
    "     â†“\n",
    "[Hidden Layer] (20 neurons, ReLU)\n",
    "     |\n",
    "     â†“\n",
    "è¡¨ç¤º Î¦(X) (10ç»´)\n",
    "```\n",
    "\n",
    "è¿™ä¸ªç½‘ç»œå°† 2 ç»´çš„åŸå§‹ç‰¹å¾è½¬æ¢æˆ 10 ç»´çš„ \"è¡¨ç¤º\"ã€‚å¸Œæœ›è¿™ä¸ªè¡¨ç¤ºèƒ½è‡ªåŠ¨æ•è·éçº¿æ€§å…³ç³»ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  1.2: ç®€å•çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n\nclass SimpleRepresentation(nn.Module):\n    \"\"\"\n    ç®€å•çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n    \n    X -> [Hidden Layer] -> Phi(X) (å­¦ä¹ åˆ°çš„è¡¨ç¤º)\n    \"\"\"\n    \n    def __init__(self, input_dim: int, repr_dim: int = 10, hidden_dim: int = 20):\n        super().__init__()\n        \n        # å®šä¹‰ç½‘ç»œå±‚\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, repr_dim),\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n        return self.network(x)\n\n\ndef train_representation(\n    X: np.ndarray,\n    T: np.ndarray,\n    Y: np.ndarray,\n    repr_dim: int = 10,\n    n_epochs: int = 100\n) -> nn.Module:\n    \"\"\"\n    è®­ç»ƒè¡¨ç¤ºå­¦ä¹ ç½‘ç»œ\n    \n    ç›®æ ‡: å­¦ä¹ èƒ½é¢„æµ‹ Y çš„è¡¨ç¤º Phi(X)\n    \n    è®­ç»ƒç­–ç•¥:\n    1. Phi(X) -> [Linear] -> Y_pred\n    2. æœ€å°åŒ– MSE(Y, Y_pred)\n    3. å­¦åˆ°çš„ Phi(X) å¯ç”¨äºåç»­å› æœæ¨æ–­\n    \"\"\"\n    \n    # è½¬æ¢ä¸º Tensor\n    X_tensor = torch.FloatTensor(X)\n    Y_tensor = torch.FloatTensor(Y).unsqueeze(1)\n    \n    # æ¨¡å‹\n    repr_model = SimpleRepresentation(input_dim=X.shape[1], repr_dim=repr_dim)\n    \n    # å®šä¹‰é¢„æµ‹å¤´ (Phi(X) -> Y)\n    prediction_head = nn.Linear(repr_dim, 1)\n    \n    # å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n    params = list(repr_model.parameters()) + list(prediction_head.parameters())\n    optimizer = optim.Adam(params, lr=0.01)\n    criterion = nn.MSELoss()\n    \n    # è®­ç»ƒå¾ªç¯\n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        phi = repr_model(X_tensor)\n        y_pred = prediction_head(phi)\n        loss = criterion(y_pred, Y_tensor)\n        loss.backward()\n        optimizer.step()\n    \n    return repr_model\n\n# æµ‹è¯•\nrepr_model = train_representation(X, T, Y, repr_dim=5, n_epochs=50)\nprint(\"è¡¨ç¤ºå­¦ä¹ æ¨¡å‹åˆ›å»ºå®Œæˆ\")\nprint(f\"æ¨¡å‹ç»“æ„:\")\nprint(repr_model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: å¯è§†åŒ–è¡¨ç¤ºç©ºé—´\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¦å¯è§†åŒ–?\n",
    "\n",
    "é€šè¿‡å¯è§†åŒ–å­¦åˆ°çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ï¼š\n",
    "\n",
    "1. **è¡¨ç¤ºæ˜¯å¦æœ‰æ„ä¹‰?** ç›¸ä¼¼çš„æ ·æœ¬åº”è¯¥åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ¥è¿‘\n",
    "2. **å¤„ç†ç»„å’Œæ§åˆ¶ç»„æ˜¯å¦å¯åŒºåˆ†?** å¦‚æœå®Œå…¨åˆ†ç¦»ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜\n",
    "3. **è¡¨ç¤ºæ˜¯å¦æ•è·äº†é‡è¦ä¿¡æ¯?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.3: å¯è§†åŒ–è¡¨ç¤ºç©ºé—´\n",
    "\n",
    "def visualize_representation(\n",
    "    repr_model: nn.Module,\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å­¦åˆ°çš„è¡¨ç¤ºç©ºé—´\n",
    "    \n",
    "    TODO:\n",
    "    1. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æå–è¡¨ç¤º Phi(X)\n",
    "    2. å¦‚æœç»´åº¦ > 2, ä½¿ç”¨ PCA é™ç»´åˆ° 2D\n",
    "    3. è¿”å› 2D è¡¨ç¤ºç”¨äºç»˜å›¾\n",
    "    \n",
    "    Returns:\n",
    "        (phi_2d, T) - 2D è¡¨ç¤ºå’Œå¤„ç†æ ‡ç­¾\n",
    "    \"\"\"\n",
    "    \n",
    "    repr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        \n",
    "        # TODO: æå–è¡¨ç¤º\n",
    "        phi = None  # ä½ çš„ä»£ç : repr_model(X_tensor)\n",
    "        \n",
    "        if phi is None:\n",
    "            return None, T\n",
    "        \n",
    "        phi = phi.numpy()\n",
    "    \n",
    "    # TODO: å¦‚æœç»´åº¦ > 2, ä½¿ç”¨ PCA é™ç»´\n",
    "    if phi.shape[1] > 2:\n",
    "        # ä½ çš„ä»£ç :\n",
    "        # pca = PCA(n_components=2)\n",
    "        # phi = pca.fit_transform(phi)\n",
    "        pass\n",
    "    \n",
    "    return phi, T\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "if X is not None and repr_model is not None:\n",
    "    phi_2d, _ = visualize_representation(repr_model, X, T)\n",
    "    \n",
    "    if phi_2d is not None:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # åŸå§‹ç‰¹å¾ç©ºé—´\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(X[T==0, 0], X[T==0, 1], alpha=0.5, label='Control', s=10)\n",
    "        plt.scatter(X[T==1, 0], X[T==1, 1], alpha=0.5, label='Treated', s=10)\n",
    "        plt.xlabel('X1')\n",
    "        plt.ylabel('X2')\n",
    "        plt.title('Original Feature Space')\n",
    "        plt.legend()\n",
    "        \n",
    "        # è¡¨ç¤ºç©ºé—´\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if phi_2d.shape[1] >= 2:\n",
    "            plt.scatter(phi_2d[T==0, 0], phi_2d[T==0, 1], alpha=0.5, label='Control', s=10)\n",
    "            plt.scatter(phi_2d[T==1, 0], phi_2d[T==1, 1], alpha=0.5, label='Treated', s=10)\n",
    "            plt.xlabel('Phi1')\n",
    "            plt.ylabel('Phi2')\n",
    "        plt.title('Learned Representation Space')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ visualize_representation å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: è¡¨ç¤ºå¹³è¡¡æ€§æ£€æŸ¥\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯è¡¨ç¤ºå¹³è¡¡?\n",
    "\n",
    "åœ¨å› æœæ¨æ–­ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å¤„ç†ç»„å’Œæ§åˆ¶ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­çš„åˆ†å¸ƒå°½å¯èƒ½ç›¸ä¼¼ã€‚\n",
    "\n",
    "å¦‚æœä¸¤ç»„åˆ†å¸ƒå·®å¼‚å¤ªå¤§ï¼Œè¯´æ˜å­˜åœ¨ **åå˜é‡ä¸å¹³è¡¡**ï¼Œå¯èƒ½å¯¼è‡´ä¼°è®¡åå·®ã€‚\n",
    "\n",
    "### å¹³è¡¡æ€§æŒ‡æ ‡\n",
    "\n",
    "1. **SMD (æ ‡å‡†åŒ–å‡å€¼å·®)**: $\\frac{|\\bar{\\Phi}_T - \\bar{\\Phi}_C|}{\\sigma_{\\Phi}}$\n",
    "   - SMD < 0.1 é€šå¸¸è¢«è®¤ä¸ºæ˜¯è‰¯å¥½å¹³è¡¡\n",
    "\n",
    "2. **MMD (æœ€å¤§å‡å€¼å·®å¼‚)**: $||\\bar{\\Phi}_T - \\bar{\\Phi}_C||^2$\n",
    "   - è¡¡é‡ä¸¤ç»„åˆ†å¸ƒçš„æ•´ä½“å·®å¼‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç»ƒä¹  1.4: è¡¨ç¤ºå¹³è¡¡æ€§æ£€æŸ¥\n\ndef check_representation_balance(\n    phi: np.ndarray,\n    T: np.ndarray\n) -> dict:\n    \"\"\"\n    æ£€æŸ¥è¡¨ç¤ºåœ¨å¤„ç†ç»„å’Œå¯¹ç…§ç»„ä¹‹é—´çš„å¹³è¡¡æ€§\n    \n    å¹³è¡¡æ€§æŒ‡æ ‡:\n    1. æ ‡å‡†åŒ–å‡å€¼å·® (SMD): |mean(Phi_T) - mean(Phi_C)| / std(Phi)\n    2. MMD (Maximum Mean Discrepancy): ||mean(Phi_T) - mean(Phi_C)||^2\n    \n    TODO: è®¡ç®—å¹³è¡¡æ€§æŒ‡æ ‡\n    \n    Returns:\n        dict with balance metrics\n    \"\"\"\n    \n    # åˆ†ç¦»å¤„ç†ç»„å’Œå¯¹ç…§ç»„\n    phi_treated = phi[T == 1]\n    phi_control = phi[T == 0]\n    \n    # TODO: è®¡ç®— SMD (å¯¹æ¯ä¸ªç»´åº¦)\n    # SMD_j = |mean(Phi_T[:, j]) - mean(Phi_C[:, j])| / std(Phi[:, j])\n    mean_t = phi_treated.mean(axis=0)\n    mean_c = phi_control.mean(axis=0)\n    std_all = phi.std(axis=0) + 1e-8  # é¿å…é™¤é›¶\n    \n    smd = None  # ä½ çš„ä»£ç : np.abs(mean_t - mean_c) / std_all\n    \n    # TODO: è®¡ç®— MMD (ç®€åŒ–ç‰ˆ: æ¬§æ°è·ç¦»)\n    # MMD = ||mean(Phi_T) - mean(Phi_C)||^2\n    mmd = None  # ä½ çš„ä»£ç : np.sum((mean_t - mean_c)**2)\n    \n    return {\n        'smd_mean': np.mean(smd) if smd is not None else None,\n        'smd_max': np.max(smd) if smd is not None else None,\n        'mmd': mmd\n    }\n\n# æµ‹è¯•\nif X is not None and repr_model is not None:\n    repr_model.eval()\n    with torch.no_grad():\n        phi = repr_model(torch.FloatTensor(X)).numpy()\n    \n    balance = check_representation_balance(phi, T)\n    if balance['mmd'] is not None:\n        print(f\"å¹³å‡ SMD: {balance['smd_mean']:.4f}\")\n        print(f\"æœ€å¤§ SMD: {balance['smd_max']:.4f}\")\n        print(f\"MMD: {balance['mmd']:.4f}\")\n        print(f\"\\næç¤º: SMD < 0.1 è¡¨ç¤ºè‰¯å¥½å¹³è¡¡\")\n    else:\n        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ check_representation_balance å‡½æ•°\")"
  },
  {
   "cell_type": "code",
   "source": "# ç»ƒä¹  6.4: å¯è§†åŒ– IPM ä¸ gamma å‚æ•°çš„å…³ç³»\n\ndef visualize_ipm_sensitivity(\n    X: np.ndarray,\n    T: np.ndarray\n):\n    \"\"\"\n    å¯è§†åŒ– MMD å¯¹æ ¸å‚æ•° gamma çš„æ•æ„Ÿæ€§\n    \n    TODO: æ¢ç´¢ä¸åŒ gamma å€¼å¯¹ MMD çš„å½±å“\n    \"\"\"\n    \n    X_treated = X[T == 1]\n    X_control = X[T == 0]\n    \n    # æµ‹è¯•ä¸åŒçš„ gamma å€¼\n    gammas = np.logspace(-2, 2, 20)  # 0.01 åˆ° 100\n    mmds = []\n    \n    for gamma in gammas:\n        mmd = compute_mmd(X_treated, X_control, gamma=gamma)\n        mmds.append(mmd)\n    \n    # ç»˜å›¾\n    plt.figure(figsize=(12, 4))\n    \n    # å­å›¾ 1: MMD vs gamma\n    plt.subplot(1, 2, 1)\n    plt.plot(gammas, mmds, 'o-', linewidth=2, markersize=4)\n    plt.xscale('log')\n    plt.xlabel('Gamma (æ ¸å‚æ•°)', fontsize=12)\n    plt.ylabel('MMD', fontsize=12)\n    plt.title('MMD å¯¹æ ¸å‚æ•°çš„æ•æ„Ÿæ€§', fontsize=14)\n    plt.grid(True, alpha=0.3)\n    \n    # æ ‡æ³¨æœ€ä¼˜ gamma\n    if len(mmds) > 0 and max(mmds) > 0:\n        opt_idx = np.argmax(mmds)\n        plt.axvline(gammas[opt_idx], color='red', linestyle='--', alpha=0.5, \n                   label=f'æœ€å¤§ MMD at Î³={gammas[opt_idx]:.3f}')\n        plt.legend()\n    \n    # å­å›¾ 2: ä¸åŒ IPM å¯¹æ¯”\n    plt.subplot(1, 2, 2)\n    \n    # è®¡ç®—ä¸åŒ IPM\n    if repr_model is not None:\n        repr_model.eval()\n        with torch.no_grad():\n            phi = repr_model(torch.FloatTensor(X)).numpy()\n        \n        phi_t = phi[T == 1]\n        phi_c = phi[T == 0]\n        \n        ipm_values = {\n            'MMD\\n(Î³=0.1)': compute_mmd(phi_t, phi_c, gamma=0.1),\n            'MMD\\n(Î³=0.5)': compute_mmd(phi_t, phi_c, gamma=0.5),\n            'MMD\\n(Î³=2.0)': compute_mmd(phi_t, phi_c, gamma=2.0),\n            'Sliced\\nWasserstein': sliced_wasserstein_distance(phi_t, phi_c, n_projections=50),\n        }\n        \n        colors = ['#2D9CDB', '#27AE60', '#F2994A', '#EB5757']\n        bars = plt.bar(range(len(ipm_values)), list(ipm_values.values()), color=colors, alpha=0.7)\n        plt.xticks(range(len(ipm_values)), list(ipm_values.keys()), fontsize=10)\n        plt.ylabel('è·ç¦»å€¼', fontsize=12)\n        plt.title('ä¸åŒ IPM å¯¹æ¯”', fontsize=14)\n        plt.grid(True, alpha=0.3, axis='y')\n        \n        # æ·»åŠ æ•°å€¼æ ‡ç­¾\n        for i, (bar, val) in enumerate(zip(bars, ipm_values.values())):\n            plt.text(i, val + 0.001, f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nå…³é”®å‘ç°:\")\n    print(\"1. MMD å¯¹ gamma å‚æ•°æ•æ„Ÿï¼Œéœ€è¦ä»”ç»†é€‰æ‹©\")\n    print(\"2. gamma å¤ªå°: æ ¸å‡½æ•°å¤ª\"å®½\"ï¼ŒåŒºåˆ†åº¦å·®\")\n    print(\"3. gamma å¤ªå¤§: æ ¸å‡½æ•°å¤ª\"çª„\"ï¼Œè¿‡äºæ•æ„Ÿ\")\n    print(\"4. Sliced Wasserstein ä¸éœ€è¦è°ƒå‚ï¼Œæ›´ç¨³å¥\")\n\n# è¿è¡Œå¯è§†åŒ–\nif X is not None and repr_model is not None:\n    visualize_ipm_sensitivity(X, T)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 6.4: IPM ä¸å…¶ä»–æ–¹æ³•çš„è”ç³»\n\n### ä¸ Domain Adaptationï¼ˆåŸŸé€‚åº”ï¼‰çš„å…³ç³»\n\n**åŸŸé€‚åº”é—®é¢˜**ï¼š\n- æºåŸŸ (Source Domain)ï¼šæœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®\n- ç›®æ ‡åŸŸ (Target Domain)ï¼šæ— æ ‡ç­¾çš„æµ‹è¯•æ•°æ®\n- ç›®æ ‡ï¼šå­¦ä¹ åœ¨ç›®æ ‡åŸŸä¸Šè¡¨ç°å¥½çš„æ¨¡å‹\n\n**ä¸å› æœæ¨æ–­çš„ç›¸ä¼¼æ€§**ï¼š\n\n| åŸŸé€‚åº” | å› æœæ¨æ–­ |\n|--------|---------|\n| æºåŸŸ | å¯¹ç…§ç»„ |\n| ç›®æ ‡åŸŸ | å¤„ç†ç»„ |\n| æ ‡ç­¾ | ç»“æœ $Y$ |\n| åŸŸè¿ç§» | å¤„ç†æ•ˆåº” |\n\n**æ ¸å¿ƒæ€æƒ³éƒ½æ˜¯**ï¼šå­¦ä¹ ä¸€ä¸ªè¡¨ç¤ºï¼Œä½¿å¾—ä¸åŒ\"åŸŸ\"ï¼ˆæˆ–ç»„ï¼‰çš„åˆ†å¸ƒæ¥è¿‘ï¼\n\n### Domain Adversarial Neural Network (DANN)\n\nDANN ä½¿ç”¨ **å¯¹æŠ—è®­ç»ƒ** æ¥å­¦ä¹ åŸŸä¸å˜è¡¨ç¤ºï¼š\n\n```\nX -> [Shared Encoder Î¦] -> Representation\n                           /            \\\n                          /              \\\n                    [Label Predictor]  [Domain Classifier]\n                         (maximize)     (minimize)\n```\n\n- **Label Predictor**ï¼šé¢„æµ‹æ ‡ç­¾ï¼ˆæœ€å¤§åŒ–å‡†ç¡®ç‡ï¼‰\n- **Domain Classifier**ï¼šé¢„æµ‹æ ·æœ¬æ¥è‡ªå“ªä¸ªåŸŸï¼ˆæœ€å°åŒ–å‡†ç¡®ç‡ï¼‰\n\n**ä¸å› æœæ¨æ–­çš„è”ç³»**ï¼š\n- Domain Classifier ç›¸å½“äºåœ¨æœ€å°åŒ– IPM\n- å½“ Domain Classifier æ— æ³•åŒºåˆ†ä¸¤åŸŸæ—¶ï¼ŒIPM æ¥è¿‘ 0\n\n### ä¸ Fair Representationï¼ˆå…¬å¹³è¡¨ç¤ºï¼‰çš„å…³ç³»\n\n**å…¬å¹³æœºå™¨å­¦ä¹ é—®é¢˜**ï¼š\n- æ•æ„Ÿå±æ€§ $S$ (å¦‚æ€§åˆ«ã€ç§æ—)\n- ç›®æ ‡ï¼šå­¦ä¹ ä¸ä¾èµ– $S$ çš„é¢„æµ‹æ¨¡å‹\n\n**ä¸å› æœæ¨æ–­çš„ç›¸ä¼¼æ€§**ï¼š\n\n| å…¬å¹³è¡¨ç¤º | å› æœæ¨æ–­ |\n|---------|---------|\n| æ•æ„Ÿå±æ€§ $S$ | å¤„ç† $T$ |\n| ä¸åŒç¾¤ä½“ | å¤„ç†ç»„/å¯¹ç…§ç»„ |\n| å…¬å¹³æ€§çº¦æŸ | å¹³è¡¡æ€§çº¦æŸ |\n\n**æ ¸å¿ƒæ€æƒ³éƒ½æ˜¯**ï¼šå­¦ä¹ çš„è¡¨ç¤ºåº”è¯¥å¯¹ $S$ï¼ˆæˆ– $T$ï¼‰\"ç›²\"ï¼\n\n### Demographic Parity via Representation\n\nä¸€ç§å¸¸è§çš„å…¬å¹³æ€§å®šä¹‰æ˜¯ **Demographic Parity**ï¼š\n\n$$P(\\hat{Y} = 1 | S = 0) = P(\\hat{Y} = 1 | S = 1)$$\n\n**é€šè¿‡è¡¨ç¤ºå®ç°**ï¼š\n\n$$P(\\Phi(X) | S = 0) = P(\\Phi(X) | S = 1)$$\n\nè¿™æ­£æ˜¯æœ€å°åŒ– IPMï¼\n\n### ç»Ÿä¸€æ¡†æ¶ï¼šåˆ†å¸ƒåŒ¹é…\n\næ‰€æœ‰è¿™äº›æ–¹æ³•çš„æ ¸å¿ƒéƒ½æ˜¯ **åˆ†å¸ƒåŒ¹é… (Distribution Matching)**ï¼š\n\n$$\\min_{\\Phi} \\underbrace{\\mathcal{L}_{\\text{task}}(\\Phi)}_{\\text{ä»»åŠ¡æŸå¤±}} + \\underbrace{\\alpha \\cdot \\text{IPM}(P_1, P_2)}_{\\text{åˆ†å¸ƒåŒ¹é…}}$$\n\n- **åŸŸé€‚åº”**ï¼š$P_1$ = æºåŸŸï¼Œ$P_2$ = ç›®æ ‡åŸŸ\n- **å› æœæ¨æ–­**ï¼š$P_1$ = å¤„ç†ç»„ï¼Œ$P_2$ = å¯¹ç…§ç»„\n- **å…¬å¹³è¡¨ç¤º**ï¼š$P_1$ = ç¾¤ä½“ 1ï¼Œ$P_2$ = ç¾¤ä½“ 2\n\n### æ–‡çŒ®è¿æ¥\n\n1. **TARNet** (Shalit et al., 2017)\n   - ç¬¬ä¸€ä¸ªåœ¨å› æœæ¨æ–­ä¸­ä½¿ç”¨è¡¨ç¤ºå¹³è¡¡çš„æ·±åº¦æ¨¡å‹\n   - ä½¿ç”¨ IPM æ­£åˆ™åŒ–\n\n2. **DANN** (Ganin et al., 2015)\n   - ä½¿ç”¨å¯¹æŠ—è®­ç»ƒè¿›è¡ŒåŸŸé€‚åº”\n   - ç†è®ºä¸Šç­‰ä»·äºæœ€å°åŒ– IPM\n\n3. **Fair Representation** (Zemel et al., 2013)\n   - å­¦ä¹ å¯¹æ•æ„Ÿå±æ€§ä¸å˜çš„è¡¨ç¤º\n   - ä½¿ç”¨åˆ†å¸ƒè·ç¦»ä½œä¸ºå…¬å¹³æ€§åº¦é‡\n\n4. **Wasserstein GAN** (Arjovsky et al., 2017)\n   - ä½¿ç”¨ Wasserstein è·ç¦»è®­ç»ƒ GAN\n   - æ›´ç¨³å®šçš„å¯¹æŠ—è®­ç»ƒ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç»ƒä¹  6.3: å¯¹æ¯”ä¸åŒ IPM çš„æ•ˆæœ\n\ndef compare_ipm_methods(\n    X: np.ndarray,\n    T: np.ndarray,\n    Y: np.ndarray,\n    repr_dim: int = 10,\n    n_epochs: int = 100\n) -> dict:\n    \"\"\"\n    å¯¹æ¯”ä½¿ç”¨ä¸åŒ IPM çš„è¡¨ç¤ºå­¦ä¹ æ•ˆæœ\n    \n    TODO: å®ç° IPM å¯¹æ¯”å®éªŒ\n    \n    æ¯”è¾ƒä¸‰ç§æ–¹æ³•:\n    1. æ— å¹³è¡¡æ­£åˆ™åŒ– (alpha = 0)\n    2. MMD å¹³è¡¡æ­£åˆ™åŒ–\n    3. Sliced Wasserstein å¹³è¡¡æ­£åˆ™åŒ–\n    \n    Returns:\n        dict with comparison results\n    \"\"\"\n    \n    from copy import deepcopy\n    \n    results = {}\n    \n    # æ–¹æ³• 1: æ— å¹³è¡¡æ­£åˆ™åŒ–\n    print(\"è®­ç»ƒæ— å¹³è¡¡æ­£åˆ™åŒ–æ¨¡å‹...\")\n    model1 = train_representation(X, T, Y, repr_dim=repr_dim, n_epochs=n_epochs)\n    \n    model1.eval()\n    with torch.no_grad():\n        phi1 = model1(torch.FloatTensor(X)).numpy()\n    \n    phi1_t = phi1[T == 1]\n    phi1_c = phi1[T == 0]\n    \n    results['no_balance'] = {\n        'mmd': compute_mmd(phi1_t, phi1_c, gamma=0.5),\n        'swd': sliced_wasserstein_distance(phi1_t, phi1_c, n_projections=50),\n        'smd_max': np.max(np.abs(phi1_t.mean(0) - phi1_c.mean(0)) / (phi1.std(0) + 1e-8))\n    }\n    \n    # TODO: æ–¹æ³• 2 å’Œ 3 ç•™ä½œæ‰©å±•\n    # åœ¨ä¸‹ä¸€ä¸ª notebook (TARNet) ä¸­ï¼Œæˆ‘ä»¬ä¼šå®ç°å®Œæ•´çš„å¹³è¡¡æ­£åˆ™åŒ–\n    \n    print(\"\\nå½“å‰ç»“æœ (æ— å¹³è¡¡æ­£åˆ™åŒ–):\")\n    print(f\"MMD: {results['no_balance']['mmd']:.6f}\")\n    print(f\"SWD: {results['no_balance']['swd']:.6f}\")\n    print(f\"Max SMD: {results['no_balance']['smd_max']:.4f}\")\n    \n    return results\n\n# è¿è¡Œå¯¹æ¯”å®éªŒ\nif X is not None:\n    comparison_results = compare_ipm_methods(X, T, Y, repr_dim=10, n_epochs=50)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"æç¤º: åœ¨ä¸‹ä¸€ä¸ªç»ƒä¹  (TARNet) ä¸­ï¼Œæˆ‘ä»¬ä¼šå­¦ä¹ å¦‚ä½•:\")\n    print(\"  1. åœ¨è®­ç»ƒä¸­åŠ å…¥ IPM æ­£åˆ™åŒ–\")\n    print(\"  2. è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ alpha\")\n    print(\"  3. å¯¹æ¯”ä¸åŒ IPM çš„æ•ˆæœ\")\n    print(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## æ€è€ƒé¢˜\n\n### Part 1-5: åŸºç¡€æ¦‚å¿µ\n\n**1. ä¸ºä»€ä¹ˆçº¿æ€§æ¨¡å‹åœ¨éçº¿æ€§æ•°æ®ä¸Šè¡¨ç°ä¸å¥½?**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**2. è¡¨ç¤ºå­¦ä¹ å¦‚ä½•å¸®åŠ©å› æœæ¨æ–­?**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**3. ä»€ä¹ˆæ˜¯ \"è¡¨ç¤ºå¹³è¡¡\" (Representation Balance)? ä¸ºä»€ä¹ˆé‡è¦?**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**4. åœ¨æ·±åº¦å› æœæ¨¡å‹ä¸­ï¼Œå…±äº«è¡¨ç¤ºå±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆ?**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**5. å¦‚æœå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¡¨ç¤ºåˆ†å¸ƒå®Œå…¨ä¸é‡å ï¼Œä¼šæœ‰ä»€ä¹ˆé—®é¢˜?**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n### Part 6: IPM ç†è®º\n\n**6. ä»€ä¹ˆæ˜¯ IPM (Integral Probability Metrics)ï¼Ÿå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**7. MMD å’Œ Wasserstein è·ç¦»æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå„è‡ªé€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**8. ä¸ºä»€ä¹ˆæœ€å°åŒ– IPM èƒ½å¸®åŠ©å› æœæ¨æ–­ï¼Ÿè¯·ä»ç†è®ºè¯¯å·®ç•Œçš„è§’åº¦è§£é‡Šã€‚**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**9. åœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œå¹³è¡¡æ­£åˆ™åŒ–çš„æƒé‡ Î± åº”è¯¥å¦‚ä½•é€‰æ‹©ï¼ŸÎ± å¤ªå¤§æˆ–å¤ªå°ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿ**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**10. è¡¨ç¤ºå­¦ä¹ åœ¨å› æœæ¨æ–­ä¸­çš„åº”ç”¨ä¸åŸŸé€‚åº”ã€å…¬å¹³è¡¨ç¤ºæœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**11. å¦‚æœä½ è¦è®¾è®¡ä¸€ä¸ªæ–°çš„å› æœæ¨æ–­æ·±åº¦æ¨¡å‹ï¼Œä½ ä¼šé€‰æ‹© MMD è¿˜æ˜¯ Wasserstein ä½œä¸ºå¹³è¡¡æŸå¤±ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ**\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n\n**12. åœ¨ IPM è¯¯å·®ç•Œä¸­ï¼ŒLipschitz å¸¸æ•° Î» ä»£è¡¨ä»€ä¹ˆï¼Ÿå®ƒå¦‚ä½•å½±å“ Î± çš„é€‰æ‹©ï¼Ÿ**\n\n**ä½ çš„ç­”æ¡ˆ:**\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "---\n\n## æ€»ç»“\n\n### æ ¸å¿ƒæ¦‚å¿µå›é¡¾\n\n| æ¦‚å¿µ | è¦ç‚¹ |\n|------|------|\n| **è¡¨ç¤ºå­¦ä¹ ** | è®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤º |\n| **éçº¿æ€§å…³ç³»** | åŸå§‹ç‰¹å¾çš„å¤æ‚ç»„åˆå¯èƒ½æ‰æ˜¯çœŸæ­£æœ‰ç”¨çš„ |\n| **è¡¨ç¤ºå¹³è¡¡** | å¤„ç†ç»„å’Œæ§åˆ¶ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­åº”è¯¥åˆ†å¸ƒç›¸ä¼¼ |\n| **SMD** | è¡¡é‡å•ä¸ªç»´åº¦çš„ä¸å¹³è¡¡ç¨‹åº¦ (< 0.1 ä¸ºè‰¯å¥½) |\n| **MMD** | ä½¿ç”¨æ ¸å‡½æ•°åº¦é‡åˆ†å¸ƒå·®å¼‚ï¼Œè®¡ç®—æ•ˆç‡é«˜ |\n| **Wasserstein** | æœ€ä¼˜ä¼ è¾“è·ç¦»ï¼Œè€ƒè™‘å‡ ä½•ç»“æ„ |\n| **IPM** | åˆ†å¸ƒè·ç¦»çš„ç»Ÿä¸€æ¡†æ¶ï¼ŒMMD å’Œ Wasserstein éƒ½æ˜¯ IPM |\n| **è¯¯å·®ç•Œ** | $\\epsilon_{\\text{ATE}} \\leq \\epsilon_{\\text{pred}} + \\lambda \\cdot \\text{IPM}$ |\n| **æ­£åˆ™åŒ–æƒé‡** | å¹³è¡¡é¢„æµ‹è¯¯å·®å’Œåˆ†å¸ƒåŒ¹é…ï¼Œéœ€è¦è°ƒå‚ |\n\n### IPM å®¶æ—å¯¹æ¯”\n\n| IPM ç±»å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n|---------|------|------|---------|\n| **MMD (RBF æ ¸)** | è®¡ç®—å¿«ï¼Œæ˜“å®ç° | å¯¹æ ¸å‚æ•°æ•æ„Ÿ | ä¸­ä½ç»´æ•°æ®ï¼Œå¿«é€ŸåŸå‹ |\n| **Wasserstein** | å‡ ä½•æ„ä¹‰å¼º | è®¡ç®—å¤æ‚ | é«˜ç»´æ•°æ®ï¼Œå›¾åƒ |\n| **Sliced Wasserstein** | è®¡ç®—å¿«ï¼Œè¿‘ä¼¼å¥½ | éœ€è¦å¤šæ¬¡æŠ•å½± | é«˜ç»´æ•°æ®çš„å¿«é€Ÿè¿‘ä¼¼ |\n\n### ç†è®ºè¿æ¥\n\n```\n        åˆ†å¸ƒåŒ¹é… (Distribution Matching)\n              /        |         \\\n             /         |          \\\n      åŸŸé€‚åº”        å› æœæ¨æ–­      å…¬å¹³è¡¨ç¤º\n   (Domain Adapt)  (Causal Inf)  (Fair Rep)\n        |             |             |\n    æºåŸŸ vs ç›®æ ‡åŸŸ  å¤„ç†ç»„ vs å¯¹ç…§ç»„  ç¾¤ä½“ A vs ç¾¤ä½“ B\n```\n\næ‰€æœ‰è¿™äº›é—®é¢˜éƒ½å¯ä»¥ç”¨ **è¡¨ç¤ºå­¦ä¹  + IPM** æ¥è§£å†³ï¼\n\n### å…³é”®å…¬å¼\n\n**IPM å®šä¹‰**ï¼š\n$$\\text{IPM}_{\\mathcal{F}}(P, Q) = \\sup_{f \\in \\mathcal{F}} \\left| \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\right|$$\n\n**MMD æ ¸æŠ€å·§**ï¼š\n$$\\text{MMD}^2(P, Q) = \\mathbb{E}_{x, x'}[k(x, x')] - 2\\mathbb{E}_{x, y}[k(x, y)] + \\mathbb{E}_{y, y'}[k(y, y')]$$\n\n**å› æœæ¨æ–­æŸå¤±**ï¼š\n$$\\mathcal{L} = \\mathcal{L}_{\\text{prediction}} + \\alpha \\cdot \\text{IPM}(P_T, P_C)$$\n\n**è¯¯å·®ç•Œ**ï¼š\n$$\\epsilon_{\\text{ATE}} \\leq \\epsilon_{\\text{prediction}} + \\lambda \\cdot \\text{IPM}(P_T, P_C)$$\n\n### ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦?\n\n1. **ç†è®ºåŸºç¡€**ï¼šIPM ä¸ºæ·±åº¦å› æœæ¨¡å‹æä¾›äº†ç†è®ºä¿è¯\n2. **å®è·µæŒ‡å¯¼**ï¼šæŒ‡å¯¼æˆ‘ä»¬å¦‚ä½•è®¾è®¡æŸå¤±å‡½æ•°å’Œé€‰æ‹©è¶…å‚æ•°\n3. **æ–¹æ³•ç»Ÿä¸€**ï¼šè¿æ¥äº†å› æœæ¨æ–­ã€åŸŸé€‚åº”ã€å…¬å¹³è¡¨ç¤ºç­‰å¤šä¸ªé¢†åŸŸ\n4. **æœªæ¥æ–¹å‘**ï¼šä¸º TARNetã€DragonNet ç­‰é«˜çº§æ¨¡å‹é“ºå¹³é“è·¯\n\n### ä¸‹ä¸€æ­¥\n\nåœ¨ä¸‹ä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  **TARNet** â€”â€” ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå› æœæ¨æ–­è®¾è®¡çš„ç¥ç»ç½‘ç»œï¼\n\nTARNet å°†ä½¿ç”¨ï¼š\n- âœ… å…±äº«è¡¨ç¤ºå±‚ï¼ˆæœ¬èŠ‚å†…å®¹ï¼‰\n- âœ… IPM å¹³è¡¡æ­£åˆ™åŒ–ï¼ˆæœ¬èŠ‚å†…å®¹ï¼‰\n- â• åˆ†ç¦»çš„å¤„ç†æ•ˆåº”ä¼°è®¡å¤´\n- â• ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥\n\n---\n\n**æ­å–œä½ å®Œæˆäº†è¡¨ç¤ºå­¦ä¹ ä¸ IPM ç†è®ºçš„æ·±å…¥å­¦ä¹ ï¼**",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 6.2: Wasserstein è·ç¦»\n\n### ä»€ä¹ˆæ˜¯ Wasserstein è·ç¦»ï¼Ÿ\n\nWasserstein è·ç¦»ï¼ˆä¹Ÿå« Earth Mover's Distanceï¼‰æœ‰ä¸€ä¸ªç›´è§‚çš„ç‰©ç†è§£é‡Šï¼š\n\n**æƒ³è±¡ä¸¤å †æ²™å­**ï¼š\n- å¤„ç†ç»„åˆ†å¸ƒ $P$ æ˜¯ä¸€å †æ²™å­\n- å¯¹ç…§ç»„åˆ†å¸ƒ $Q$ æ˜¯å¦ä¸€å †æ²™å­\n- Wasserstein è·ç¦»æ˜¯æŠŠç¬¬ä¸€å †æ²™å­ \"æ¬è¿\" æˆç¬¬äºŒå †çš„æœ€å°å·¥ä½œé‡\n\n### æ•°å­¦å®šä¹‰\n\n1-Wasserstein è·ç¦»å®šä¹‰ä¸ºï¼š\n\n$$W_1(P, Q) = \\inf_{\\gamma \\in \\Gamma(P, Q)} \\mathbb{E}_{(x, y) \\sim \\gamma}[\\|x - y\\|]$$\n\nå…¶ä¸­ $\\Gamma(P, Q)$ æ˜¯æ‰€æœ‰è¾¹é™…åˆ†å¸ƒä¸º $P$ å’Œ $Q$ çš„è”åˆåˆ†å¸ƒï¼ˆå³æ‰€æœ‰å¯èƒ½çš„ \"æ¬è¿æ–¹æ¡ˆ\"ï¼‰ã€‚\n\n**Kantorovich-Rubinstein å¯¹å¶å½¢å¼**ï¼ˆè¿™æ˜¯ IPM å½¢å¼ï¼‰ï¼š\n\n$$W_1(P, Q) = \\sup_{f: \\text{Lip}(f) \\leq 1} \\left| \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{y \\sim Q}[f(y)] \\right|$$\n\nå…¶ä¸­ $\\text{Lip}(f) \\leq 1$ è¡¨ç¤º $f$ æ˜¯ 1-Lipschitz å‡½æ•°ï¼ˆå³ $|f(x) - f(y)| \\leq \\|x - y\\|$ï¼‰ã€‚\n\n### Wasserstein vs MMD\n\n| ç‰¹æ€§ | MMD | Wasserstein |\n|------|-----|-------------|\n| **å‡½æ•°ç±»** | RKHSï¼ˆä¾èµ–æ ¸å‡½æ•°ï¼‰ | 1-Lipschitz å‡½æ•° |\n| **å‡ ä½•æ„ä¹‰** | é«˜ç»´ç©ºé—´å‡å€¼å·® | æœ€ä¼˜ä¼ è¾“è·ç¦» |\n| **è®¡ç®—å¤æ‚åº¦** | $O(n^2)$ | $O(n^3)$ (ç²¾ç¡®), $O(n^2)$ (è¿‘ä¼¼) |\n| **ä¼˜ç‚¹** | æ˜“äºè®¡ç®—ï¼Œæ ¸å‡½æ•°çµæ´» | è€ƒè™‘å‡ ä½•ç»“æ„ï¼Œé€‚åˆå›¾åƒ |\n| **ç¼ºç‚¹** | å¯¹æ ¸å‚æ•°æ•æ„Ÿ | è®¡ç®—é‡å¤§ |\n\n### ä»€ä¹ˆæ—¶å€™ç”¨ Wassersteinï¼Ÿ\n\n- **é«˜ç»´æ•°æ®**ï¼ˆå¦‚å›¾åƒï¼‰ï¼šWasserstein æ›´å¥½åœ°æ•è·å‡ ä½•ç»“æ„\n- **GAN è®­ç»ƒ**ï¼šWasserstein GAN (WGAN) æ›´ç¨³å®š\n- **è¿ç§»å­¦ä¹ **ï¼šåŸŸé€‚åº”é—®é¢˜\n\n### ä»€ä¹ˆæ—¶å€™ç”¨ MMDï¼Ÿ\n\n- **ä¸­ä½ç»´æ•°æ®**ï¼šè®¡ç®—æ•ˆç‡é«˜\n- **å› æœæ¨æ–­**ï¼šè¡¨ç¤ºå¹³è¡¡æ£€éªŒ\n- **å¿«é€ŸåŸå‹**ï¼šå®ç°ç®€å•",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç»ƒä¹  6.1: å®ç° MMD æŸå¤±\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n    \"\"\"\n    è®¡ç®— RBF (é«˜æ–¯) æ ¸çŸ©é˜µ\n    \n    k(x, y) = exp(-gamma * ||x - y||^2)\n    \n    TODO: å®ç° RBF æ ¸\n    \n    Args:\n        X: (n, d) çŸ©é˜µ\n        Y: (m, d) çŸ©é˜µ\n        gamma: æ ¸å‚æ•°\n    \n    Returns:\n        K: (n, m) æ ¸çŸ©é˜µï¼ŒK[i, j] = k(X[i], Y[j])\n    \"\"\"\n    \n    # TODO: è®¡ç®—æ¬§æ°è·ç¦»çš„å¹³æ–¹\n    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2<x, y>\n    \n    XX = None  # ä½ çš„ä»£ç : np.sum(X**2, axis=1).reshape(-1, 1)  # (n, 1)\n    YY = None  # ä½ çš„ä»£ç : np.sum(Y**2, axis=1).reshape(1, -1)  # (1, m)\n    XY = None  # ä½ çš„ä»£ç : X @ Y.T  # (n, m)\n    \n    if XX is None:\n        return np.zeros((X.shape[0], Y.shape[0]))\n    \n    dist_sq = XX + YY - 2 * XY\n    \n    # TODO: åº”ç”¨é«˜æ–¯æ ¸\n    K = None  # ä½ çš„ä»£ç : np.exp(-gamma * dist_sq)\n    \n    return K if K is not None else np.zeros((X.shape[0], Y.shape[0]))\n\n\ndef compute_mmd(\n    X: np.ndarray,\n    Y: np.ndarray,\n    kernel: str = 'rbf',\n    gamma: float = 1.0\n) -> float:\n    \"\"\"\n    è®¡ç®— Maximum Mean Discrepancy (MMD)\n    \n    MMD^2 = E[k(x, x')] - 2*E[k(x, y)] + E[k(y, y')]\n    \n    TODO: å®ç° MMD è®¡ç®—\n    \n    Args:\n        X: å¤„ç†ç»„æ ·æœ¬ (n, d)\n        Y: å¯¹ç…§ç»„æ ·æœ¬ (m, d)\n        kernel: æ ¸å‡½æ•°ç±»å‹\n        gamma: æ ¸å‚æ•°\n    \n    Returns:\n        mmd: MMD å€¼\n    \"\"\"\n    \n    n = X.shape[0]\n    m = Y.shape[0]\n    \n    # TODO: è®¡ç®—ä¸‰ä¸ªæ ¸çŸ©é˜µ\n    K_XX = None  # ä½ çš„ä»£ç : rbf_kernel(X, X, gamma)\n    K_XY = None  # ä½ çš„ä»£ç : rbf_kernel(X, Y, gamma)\n    K_YY = None  # ä½ çš„ä»£ç : rbf_kernel(Y, Y, gamma)\n    \n    if K_XX is None:\n        return 0.0\n    \n    # TODO: è®¡ç®— MMD^2\n    # MMD^2 = (sum(K_XX) - n) / (n*(n-1))  # å»æ‰å¯¹è§’çº¿\n    #       - 2 * sum(K_XY) / (n*m)\n    #       + (sum(K_YY) - m) / (m*(m-1))\n    \n    term1 = None  # ä½ çš„ä»£ç : (K_XX.sum() - np.trace(K_XX)) / (n * (n - 1))\n    term2 = None  # ä½ çš„ä»£ç : K_XY.sum() / (n * m)\n    term3 = None  # ä½ çš„ä»£ç : (K_YY.sum() - np.trace(K_YY)) / (m * (m - 1))\n    \n    if term1 is None:\n        return 0.0\n    \n    mmd_sq = term1 - 2 * term2 + term3\n    \n    # è¿”å› MMD (å¼€å¹³æ–¹)\n    return np.sqrt(max(mmd_sq, 0))  # max ç¡®ä¿éè´Ÿ\n\n\n# æµ‹è¯• MMD\nif X is not None:\n    # æå–å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¡¨ç¤º\n    if repr_model is not None:\n        repr_model.eval()\n        with torch.no_grad():\n            phi = repr_model(torch.FloatTensor(X)).numpy()\n        \n        phi_treated = phi[T == 1]\n        phi_control = phi[T == 0]\n        \n        # è®¡ç®—åŸå§‹ç‰¹å¾ç©ºé—´çš„ MMD\n        mmd_original = compute_mmd(X[T == 1], X[T == 0], gamma=0.5)\n        \n        # è®¡ç®—è¡¨ç¤ºç©ºé—´çš„ MMD\n        mmd_repr = compute_mmd(phi_treated, phi_control, gamma=0.5)\n        \n        if mmd_original > 0:\n            print(\"=\" * 50)\n            print(\"MMD å¯¹æ¯”åˆ†æ\")\n            print(\"=\" * 50)\n            print(f\"åŸå§‹ç‰¹å¾ç©ºé—´ MMD: {mmd_original:.6f}\")\n            print(f\"è¡¨ç¤ºç©ºé—´ MMD:     {mmd_repr:.6f}\")\n            print(f\"æ”¹è¿›æ¯”ä¾‹: {(1 - mmd_repr/mmd_original)*100:.2f}%\")\n            print(\"\\næç¤º: MMD è¶Šå°ï¼Œä¸¤ç»„åˆ†å¸ƒè¶Šæ¥è¿‘\")\n        else:\n            print(\"[æœªå®Œæˆ] è¯·å®Œæˆ compute_mmd å‡½æ•°\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 6.1: MMD (Maximum Mean Discrepancy) æ·±å…¥\n\n### ä»€ä¹ˆæ˜¯ MMDï¼Ÿ\n\nMMD ä½¿ç”¨ **æ ¸å‡½æ•°** æ¥åº¦é‡åˆ†å¸ƒå·®å¼‚ã€‚æ ¸å¿ƒæ€æƒ³ï¼š\n\n1. å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ (é€šè¿‡æ ¸å‡½æ•° $k$)\n2. è®¡ç®—ä¸¤ç»„åœ¨é«˜ç»´ç©ºé—´ä¸­çš„ \"å¹³å‡ä½ç½®\" çš„è·ç¦»\n3. è¿™ä¸ªè·ç¦»å°±æ˜¯ MMD\n\n### æ•°å­¦æ¨å¯¼\n\nå¯¹äºä¸¤ä¸ªåˆ†å¸ƒ $P$ (å¤„ç†ç»„) å’Œ $Q$ (å¯¹ç…§ç»„)ï¼š\n\n$$\\text{MMD}^2(P, Q) = \\left\\| \\mathbb{E}_{x \\sim P}[\\phi(x)] - \\mathbb{E}_{y \\sim Q}[\\phi(y)] \\right\\|^2$$\n\nå…¶ä¸­ $\\phi$ æ˜¯æ ¸å‡½æ•° $k$ å¯¹åº”çš„ç‰¹å¾æ˜ å°„ï¼ˆé€šå¸¸æ˜¯æ— é™ç»´çš„ï¼‰ã€‚\n\n**æ ¸æŠ€å·§ (Kernel Trick)**ï¼šæˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼è®¡ç®— $\\phi$ï¼Œå¯ä»¥ç›´æ¥ç”¨æ ¸å‡½æ•°ï¼š\n\n$$\\text{MMD}^2(P, Q) = \\mathbb{E}_{x, x'}[k(x, x')] - 2\\mathbb{E}_{x, y}[k(x, y)] + \\mathbb{E}_{y, y'}[k(y, y')]$$\n\nå…¶ä¸­ï¼š\n- $x, x' \\sim P$ (å¤„ç†ç»„æ ·æœ¬)\n- $y, y' \\sim Q$ (å¯¹ç…§ç»„æ ·æœ¬)\n\n**é€šä¿—ç†è§£**ï¼š\n- ç¬¬ä¸€é¡¹ï¼šå¤„ç†ç»„å†…éƒ¨çš„ç›¸ä¼¼åº¦\n- ç¬¬äºŒé¡¹ï¼šå¤„ç†ç»„å’Œå¯¹ç…§ç»„ä¹‹é—´çš„ç›¸ä¼¼åº¦ (ä¹˜ä»¥ -2)\n- ç¬¬ä¸‰é¡¹ï¼šå¯¹ç…§ç»„å†…éƒ¨çš„ç›¸ä¼¼åº¦\n- MMD è¶Šå°ï¼Œä¸¤ç»„è¶Šç›¸ä¼¼\n\n### å¸¸ç”¨æ ¸å‡½æ•°\n\n1. **é«˜æ–¯æ ¸ (RBF)**ï¼š$k(x, y) = \\exp(-\\gamma \\|x - y\\|^2)$\n   - æœ€å¸¸ç”¨ï¼Œå‚æ•° $\\gamma$ æ§åˆ¶ \"æ•æ„Ÿåº¦\"\n   \n2. **çº¿æ€§æ ¸**ï¼š$k(x, y) = x^T y$\n   - ç®€å•ï¼Œç›¸å½“äºåªæ¯”è¾ƒå‡å€¼\n\n3. **å¤šé¡¹å¼æ ¸**ï¼š$k(x, y) = (x^T y + c)^d$\n   - å¯ä»¥æ•è·éçº¿æ€§å…³ç³»",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### IPM å®¶æ—æˆå‘˜\n\nä¸åŒçš„å‡½æ•°ç±» $\\mathcal{F}$ å¯¹åº”ä¸åŒçš„ IPMï¼š\n\n| IPM ç±»å‹ | å‡½æ•°ç±» $\\mathcal{F}$ | ç‰¹ç‚¹ | åº”ç”¨åœºæ™¯ |\n|---------|---------------------|------|----------|\n| **MMD** | RKHS (å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´) | æ˜“äºè®¡ç®—ï¼Œå¯ç”¨æ ¸å‡½æ•° | è¡¨ç¤ºå¹³è¡¡ã€åˆ†å¸ƒåŒ¹é… |\n| **Wasserstein** | 1-Lipschitz å‡½æ•° | è€ƒè™‘\"å‡ ä½•è·ç¦»\" | GANã€è¿ç§»å­¦ä¹  |\n| **Total Variation** | æ‰€æœ‰æœ‰ç•Œå‡½æ•° | ç†è®ºå¼ºä½†éš¾è®¡ç®— | ç†è®ºåˆ†æ |\n\næˆ‘ä»¬é‡ç‚¹å…³æ³¨ **MMD** å’Œ **Wasserstein**ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ·±åº¦å› æœæ¨¡å‹ä¸­åº”ç”¨æœ€å¹¿ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 6: IPM æ·±å…¥ç†è§£ - ä¸ºä»€ä¹ˆéœ€è¦ã€Œè·ç¦»åº¦é‡ã€ï¼Ÿ\n\n### ä»ç›´è§‰åˆ°ç†è®º\n\nåœ¨ Part 4 ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† MMD (Maximum Mean Discrepancy) æ¥è¡¡é‡å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„åˆ†å¸ƒå·®å¼‚ã€‚ä½†æ˜¯ï¼š\n\n**ä¸ºä»€ä¹ˆè¦è¡¡é‡åˆ†å¸ƒå·®å¼‚ï¼Ÿ**\n- å› æœæ¨æ–­çš„æ ¸å¿ƒï¼šå¤„ç†ç»„å’Œå¯¹ç…§ç»„åº”è¯¥ \"å¯æ¯”\"\n- å¦‚æœä¸¤ç»„åˆ†å¸ƒå·®å¼‚å¤ªå¤§ï¼Œè¯´æ˜å­˜åœ¨ä¸¥é‡çš„åå˜é‡ä¸å¹³è¡¡\n- è¡¨ç¤ºå­¦ä¹ çš„ç›®æ ‡ï¼šå­¦ä¹ ä¸€ä¸ªè¡¨ç¤ºï¼Œä½¿å¾—ä¸¤ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­ \"æ¥è¿‘\"\n\n**ä»€ä¹ˆæ˜¯ IPM (Integral Probability Metrics)ï¼Ÿ**\n- IPM æ˜¯ä¸€ç±» **åˆ†å¸ƒè·ç¦»çš„åº¦é‡æ–¹æ³•**\n- MMDã€Wasserstein è·ç¦»éƒ½å±äº IPM å®¶æ—\n- å®ƒä»¬å¯ä»¥ç²¾ç¡®é‡åŒ–ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚\n\n### IPM çš„æ•°å­¦å®šä¹‰\n\nå¯¹äºä¸¤ä¸ªåˆ†å¸ƒ $P$ å’Œ $Q$ï¼ŒIPM å®šä¹‰ä¸ºï¼š\n\n$$\\text{IPM}_{\\mathcal{F}}(P, Q) = \\sup_{f \\in \\mathcal{F}} \\left| \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\right|$$\n\nå…¶ä¸­ $\\mathcal{F}$ æ˜¯ä¸€ä¸ªå‡½æ•°ç±»ï¼ˆä¾‹å¦‚æ‰€æœ‰æœ‰ç•Œå‡½æ•°ã€Lipschitz å‡½æ•°ç­‰ï¼‰ã€‚\n\n**é€šä¿—ç†è§£**ï¼š\n- æƒ³è±¡æœ‰å¾ˆå¤š \"æ¢æµ‹å™¨\" (å‡½æ•° $f$)ï¼Œæ¯ä¸ªæ¢æµ‹å™¨ç»™æ•°æ®æ‰“åˆ†\n- å¯¹äºæ¯ä¸ªæ¢æµ‹å™¨ï¼Œè®¡ç®—ä¸¤ç»„çš„å¹³å‡åˆ†æ•°å·®\n- IPM æ˜¯æ‰€æœ‰æ¢æµ‹å™¨ä¸­ï¼Œå·®å¼‚æœ€å¤§çš„é‚£ä¸ª",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: å¯¹æ¯”å®éªŒ - çº¿æ€§ vs è¡¨ç¤ºå­¦ä¹ \n",
    "\n",
    "è®©æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹ï¼šä½¿ç”¨è¡¨ç¤ºå­¦ä¹ æ˜¯å¦èƒ½æ”¹å–„ ATE ä¼°è®¡ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1.5: å¯¹æ¯”çº¿æ€§æ–¹æ³•å’Œè¡¨ç¤ºå­¦ä¹ \n",
    "\n",
    "def compare_linear_vs_learned(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    true_ate: float = 2.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”çº¿æ€§æ–¹æ³•å’Œè¡¨ç¤ºå­¦ä¹ æ–¹æ³•\n",
    "    \n",
    "    TODO:\n",
    "    1. ä½¿ç”¨çº¿æ€§å›å½’ (åŸå§‹ç‰¹å¾) ä¼°è®¡ ATE\n",
    "    2. è®­ç»ƒè¡¨ç¤ºå­¦ä¹ æ¨¡å‹\n",
    "    3. ä½¿ç”¨å­¦åˆ°çš„è¡¨ç¤ºä¼°è®¡ ATE\n",
    "    4. æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„è¯¯å·®\n",
    "    \n",
    "    Returns:\n",
    "        dict with comparison results\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. çº¿æ€§ä¼°è®¡\n",
    "    linear_ate = naive_linear_estimation(X, T, Y)\n",
    "    \n",
    "    # 2. è¡¨ç¤ºå­¦ä¹ \n",
    "    repr_model = train_representation(X, T, Y, repr_dim=10, n_epochs=200)\n",
    "    \n",
    "    # 3. æå–è¡¨ç¤ºå¹¶ä¼°è®¡ ATE\n",
    "    repr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        phi = repr_model(torch.FloatTensor(X)).numpy()\n",
    "    \n",
    "    # ç”¨ Phi æ›¿ä»£ X è¿›è¡Œçº¿æ€§å›å½’\n",
    "    features_phi = np.column_stack([T, phi])\n",
    "    model = LinearRegression()\n",
    "    model.fit(features_phi, Y)\n",
    "    learned_ate = model.coef_[0]  # T çš„ç³»æ•°\n",
    "    \n",
    "    return {\n",
    "        'linear_ate': linear_ate,\n",
    "        'linear_error': abs(linear_ate - true_ate) if linear_ate else None,\n",
    "        'learned_ate': learned_ate,\n",
    "        'learned_error': abs(learned_ate - true_ate) if learned_ate else None,\n",
    "        'true_ate': true_ate\n",
    "    }\n",
    "\n",
    "# è¿è¡Œå¯¹æ¯”\n",
    "if X is not None:\n",
    "    results = compare_linear_vs_learned(X, T, Y, true_ate=2.0)\n",
    "    if results['linear_ate'] is not None:\n",
    "        print(\"=\" * 40)\n",
    "        print(\"çº¿æ€§æ–¹æ³• vs è¡¨ç¤ºå­¦ä¹  å¯¹æ¯”\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"çœŸå® ATE: {results['true_ate']:.4f}\")\n",
    "        print(f\"\\nçº¿æ€§ä¼°è®¡: {results['linear_ate']:.4f} (è¯¯å·®: {results['linear_error']:.4f})\")\n",
    "        if results['learned_ate'] is not None:\n",
    "            print(f\"è¡¨ç¤ºå­¦ä¹ ä¼°è®¡: {results['learned_ate']:.4f} (è¯¯å·®: {results['learned_error']:.4f})\")\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆæ‰€æœ‰å‰åºç»ƒä¹ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒé¢˜\n",
    "\n",
    "### 1. ä¸ºä»€ä¹ˆçº¿æ€§æ¨¡å‹åœ¨éçº¿æ€§æ•°æ®ä¸Šè¡¨ç°ä¸å¥½?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 2. è¡¨ç¤ºå­¦ä¹ å¦‚ä½•å¸®åŠ©å› æœæ¨æ–­?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 3. ä»€ä¹ˆæ˜¯ \"è¡¨ç¤ºå¹³è¡¡\" (Representation Balance)? ä¸ºä»€ä¹ˆé‡è¦?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 4. åœ¨æ·±åº¦å› æœæ¨¡å‹ä¸­ï¼Œå…±äº«è¡¨ç¤ºå±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆ?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 5. å¦‚æœå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¡¨ç¤ºåˆ†å¸ƒå®Œå…¨ä¸é‡å ï¼Œä¼šæœ‰ä»€ä¹ˆé—®é¢˜?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "| æ¦‚å¿µ | è¦ç‚¹ |\n",
    "|------|------|\n",
    "| **è¡¨ç¤ºå­¦ä¹ ** | è®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤º |\n",
    "| **éçº¿æ€§å…³ç³»** | åŸå§‹ç‰¹å¾çš„å¤æ‚ç»„åˆå¯èƒ½æ‰æ˜¯çœŸæ­£æœ‰ç”¨çš„ |\n",
    "| **è¡¨ç¤ºå¹³è¡¡** | å¤„ç†ç»„å’Œæ§åˆ¶ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­åº”è¯¥åˆ†å¸ƒç›¸ä¼¼ |\n",
    "| **SMD** | è¡¡é‡å•ä¸ªç»´åº¦çš„ä¸å¹³è¡¡ç¨‹åº¦ |\n",
    "| **MMD** | è¡¡é‡æ•´ä½“åˆ†å¸ƒå·®å¼‚ |\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦?\n",
    "\n",
    "è¡¨ç¤ºå­¦ä¹ æ˜¯æ·±åº¦å› æœæ¨¡å‹çš„åŸºç¡€ã€‚åœ¨ä¸‹ä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  **TARNet** â€”â€” ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå› æœæ¨æ–­è®¾è®¡çš„ç¥ç»ç½‘ç»œï¼\n",
    "\n",
    "---\n",
    "\n",
    "*æ­å–œä½ å®Œæˆäº†è¡¨ç¤ºå­¦ä¹ åŸºç¡€çš„å­¦ä¹ ï¼* ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}