{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¤šé‡å¤„ç†æ•ˆåº” - å½“é€‰æ‹©ä¸æ­¢ä¸¤ä¸ªæ—¶\n",
    "\n",
    "---\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç»ƒä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. ç†è§£å¤šé‡å¤„ç† (Multiple Treatments) ä¸äºŒå…ƒå¤„ç†çš„åŒºåˆ«\n",
    "2. æŒæ¡å¤šé‡å¤„ç†çš„å› æœè¯†åˆ«æ¡ä»¶\n",
    "3. å®ç°å¹¿ä¹‰å€¾å‘å¾—åˆ† (GPS) å’Œé€†æ¦‚ç‡åŠ æƒ (IPW) \n",
    "4. åº”ç”¨ Meta-Learners äºå¤šé‡å¤„ç†åœºæ™¯\n",
    "5. æ¯”è¾ƒä¸åŒå¤„ç†ä¹‹é—´çš„æ•ˆåº”å·®å¼‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## çœŸå®åœºæ™¯ï¼šå¤šæ¡£ä½ä¼˜æƒ ç­–ç•¥\n",
    "\n",
    "### åœºæ™¯æè¿°\n",
    "\n",
    "ä½ æ˜¯æŸå¤–å–å¹³å°çš„å¢é•¿è´Ÿè´£äººï¼Œéœ€è¦è®¾è®¡æ–°ç”¨æˆ·é¦–å•ä¼˜æƒ ç­–ç•¥ã€‚\n",
    "\n",
    "**å¯é€‰æ–¹æ¡ˆ**ï¼š\n",
    "- T=0ï¼šæ— ä¼˜æƒ ï¼ˆå¯¹ç…§ç»„ï¼‰\n",
    "- T=1ï¼šæ»¡ 20 å‡ 5\n",
    "- T=2ï¼šæ»¡ 30 å‡ 10\n",
    "- T=3ï¼šæ»¡ 50 å‡ 20\n",
    "\n",
    "**é—®é¢˜**ï¼š\n",
    "> ä¸åŒç”¨æˆ·åº”è¯¥åˆ†é…åˆ°å“ªä¸ªä¼˜æƒ æ¡£ä½ï¼Ÿæ¯ä¸ªæ¡£ä½çš„å¢é‡æ•ˆåº”æ˜¯å¤šå°‘ï¼Ÿ\n",
    "\n",
    "### ä¸äºŒå…ƒå¤„ç†çš„åŒºåˆ«\n",
    "\n",
    "| ç»´åº¦ | äºŒå…ƒå¤„ç† | å¤šé‡å¤„ç† |\n",
    "|------|---------|----------|\n",
    "| **å¤„ç†å˜é‡** | T âˆˆ {0, 1} | T âˆˆ {0, 1, 2, ..., K} |\n",
    "| **æ½œåœ¨ç»“æœ** | Y(0), Y(1) | Y(0), Y(1), ..., Y(K) |\n",
    "| **å¤„ç†æ•ˆåº”** | ATE = E[Y(1) - Y(0)] | K ä¸ªå¯¹æ¯” |\n",
    "| **å€¾å‘å¾—åˆ†** | e(X) = P(T=1|X) | å¤šé¡¹ Logit æˆ– K ä¸ªäºŒå…ƒ |\n",
    "\n",
    "### ç”ŸåŠ¨æ¯”å–»ï¼šç‚¹èœ\n",
    "\n",
    "- **äºŒå…ƒå¤„ç†**ï¼šã€Œè¦ä¸è¦åŠ è¾£ï¼Ÿã€\n",
    "- **å¤šé‡å¤„ç†**ï¼šã€Œå¾®è¾£ã€ä¸­è¾£ã€ç‰¹è¾£ã€è¿˜æ˜¯å˜æ€è¾£ï¼Ÿã€\n",
    "\n",
    "ä¸åŒäººå¯¹ä¸åŒè¾£åº¦çš„ã€Œæ‰¿å—èƒ½åŠ›ã€ä¸åŒï¼Œæˆ‘ä»¬è¦æ‰¾åˆ°æ¯ä¸ªäººçš„ã€Œæœ€ä½³è¾£åº¦ã€ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### å¤šé‡å¤„ç†çš„æ½œåœ¨ç»“æœæ¡†æ¶\n",
    "\n",
    "å¯¹äº K+1 ä¸ªå¤„ç†æ°´å¹³ $T \\in \\{0, 1, ..., K\\}$ï¼š\n",
    "\n",
    "- **æ½œåœ¨ç»“æœ**: $\\{Y(0), Y(1), ..., Y(K)\\}$\n",
    "- **è§‚æµ‹ç»“æœ**: $Y = \\sum_{k=0}^{K} \\mathbb{1}(T=k) \\cdot Y(k)$\n",
    "\n",
    "### å› æœæ•ˆåº”çš„å®šä¹‰\n",
    "\n",
    "**ä¸å¯¹ç…§ç»„ç›¸æ¯”çš„æ•ˆåº”**ï¼š\n",
    "$$\\tau_k = E[Y(k) - Y(0)]$$\n",
    "\n",
    "**ä¸¤ä¸ªå¤„ç†ä¹‹é—´çš„æ•ˆåº”å·®**ï¼š\n",
    "$$\\tau_{k,j} = E[Y(k) - Y(j)]$$\n",
    "\n",
    "### è¯†åˆ«å‡è®¾\n",
    "\n",
    "1. **æ— æ··æ·† (No Confounding)**ï¼š\n",
    "   $$Y(k) \\perp T | X, \\quad \\forall k$$\n",
    "\n",
    "2. **é‡å  (Overlap)**ï¼š\n",
    "   $$0 < P(T=k | X) < 1, \\quad \\forall k, X$$\n",
    "\n",
    "3. **SUTVA**ï¼š\n",
    "   - æ— å¹²æ‰°ï¼šä¸€ä¸ªäººçš„å¤„ç†ä¸å½±å“å¦ä¸€ä¸ªäººçš„ç»“æœ\n",
    "   - å¤„ç†ä¸€è‡´æ€§ï¼šåŒä¸€å¤„ç†å¯¹åŒä¸€äººæ•ˆæœç›¸åŒ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¹¿ä¹‰å€¾å‘å¾—åˆ† (GPS)\n",
    "\n",
    "### å®šä¹‰\n",
    "\n",
    "å¯¹äºå¤šé‡å¤„ç†ï¼Œå€¾å‘å¾—åˆ†å˜æˆ**å‘é‡**ï¼š\n",
    "\n",
    "$$\\mathbf{e}(X) = [P(T=0|X), P(T=1|X), ..., P(T=K|X)]$$\n",
    "\n",
    "### ä¼°è®¡æ–¹æ³•\n",
    "\n",
    "1. **å¤šé¡¹ Logistic å›å½’ (Multinomial Logit)**\n",
    "\n",
    "$$P(T=k|X) = \\frac{\\exp(\\beta_k^T X)}{\\sum_{j=0}^{K} \\exp(\\beta_j^T X)}$$\n",
    "\n",
    "2. **K ä¸ªäºŒå…ƒåˆ†ç±»å™¨ (One-vs-Rest)**\n",
    "\n",
    "$$P(T=k|X) \\approx \\frac{\\hat{p}_k(X)}{\\sum_{j=0}^{K} \\hat{p}_j(X)}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# é¢œè‰²æ–¹æ¡ˆ\n",
    "COLORS = {\n",
    "    'primary': '#2D9CDB',\n",
    "    'success': '#27AE60',\n",
    "    'danger': '#EB5757',\n",
    "    'warning': '#F2994A',\n",
    "    'info': '#9B51E0',\n",
    "}\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: æ•°æ®ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_treatment_data(\n",
    "    n_samples: int = 5000,\n",
    "    n_treatments: int = 4,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå¤šé‡å¤„ç†æ•°æ®\n",
    "    \n",
    "    åœºæ™¯ï¼šå¤–å–å¹³å°ä¼˜æƒ ç­–ç•¥\n",
    "    - X: ç”¨æˆ·ç‰¹å¾\n",
    "    - T: ä¼˜æƒ æ¡£ä½ (0=æ— , 1=æ»¡20å‡5, 2=æ»¡30å‡10, 3=æ»¡50å‡20)\n",
    "    - Y: é¦–å•æ¶ˆè´¹é‡‘é¢\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç”¨æˆ·ç‰¹å¾\n",
    "    X = np.random.randn(n_samples, 5)\n",
    "    # X[:, 0]: å†å²æ¶ˆè´¹èƒ½åŠ›\n",
    "    # X[:, 1]: ä»·æ ¼æ•æ„Ÿåº¦\n",
    "    # X[:, 2]: ä½¿ç”¨é¢‘ç‡\n",
    "    # X[:, 3]: è®¾å¤‡ç±»å‹ (é«˜ç«¯ vs ä½ç«¯)\n",
    "    # X[:, 4]: è·ç¦»é¤å…è¿œè¿‘\n",
    "    \n",
    "    # ééšæœºåˆ†é…ï¼šåŸºäºç”¨æˆ·ç‰¹å¾\n",
    "    # é«˜æ¶ˆè´¹èƒ½åŠ›ç”¨æˆ·æ›´å¯èƒ½æ”¶åˆ°å¤§é¢ä¼˜æƒ ï¼ˆä¸šåŠ¡ç­–ç•¥ï¼‰\n",
    "    logits = np.zeros((n_samples, n_treatments))\n",
    "    logits[:, 0] = 0  # åŸºå‡†\n",
    "    logits[:, 1] = 0.5 + 0.3 * X[:, 0] + 0.2 * X[:, 1]\n",
    "    logits[:, 2] = 0.3 + 0.5 * X[:, 0] + 0.3 * X[:, 1]\n",
    "    logits[:, 3] = 0.1 + 0.8 * X[:, 0] + 0.4 * X[:, 1]\n",
    "    \n",
    "    # Softmax\n",
    "    exp_logits = np.exp(logits)\n",
    "    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # æŠ½æ ·å¤„ç†\n",
    "    T = np.array([np.random.choice(n_treatments, p=p) for p in probs])\n",
    "    \n",
    "    # çœŸå®æ½œåœ¨ç»“æœ\n",
    "    noise = np.random.randn(n_samples) * 5\n",
    "    \n",
    "    # Y(0): æ— ä¼˜æƒ \n",
    "    Y0 = 30 + 10 * X[:, 0] + 5 * X[:, 2] + noise\n",
    "    \n",
    "    # Y(1): æ»¡20å‡5ï¼Œæ•ˆåº”å–å†³äºä»·æ ¼æ•æ„Ÿåº¦\n",
    "    tau1 = 8 + 3 * X[:, 1]  # ä»·æ ¼æ•æ„Ÿçš„äººå“åº”æ›´å¤§\n",
    "    Y1 = Y0 + tau1\n",
    "    \n",
    "    # Y(2): æ»¡30å‡10\n",
    "    tau2 = 15 + 5 * X[:, 1] - 2 * X[:, 0]  # ä½æ¶ˆè´¹èƒ½åŠ›äººç¾¤æ•ˆåº”æ›´å¤§\n",
    "    Y2 = Y0 + tau2\n",
    "    \n",
    "    # Y(3): æ»¡50å‡20\n",
    "    tau3 = 25 + 8 * X[:, 1] - 5 * X[:, 0]  # è¾¹é™…æ•ˆåº”æ›´å¤§\n",
    "    Y3 = Y0 + tau3\n",
    "    \n",
    "    # çœŸå®æ½œåœ¨ç»“æœçŸ©é˜µ\n",
    "    Y_potential = np.column_stack([Y0, Y1, Y2, Y3])\n",
    "    \n",
    "    # è§‚æµ‹ç»“æœ\n",
    "    Y = np.array([Y_potential[i, T[i]] for i in range(n_samples)])\n",
    "    \n",
    "    # çœŸå® ATE\n",
    "    true_ate = {\n",
    "        'tau_1': np.mean(tau1),\n",
    "        'tau_2': np.mean(tau2),\n",
    "        'tau_3': np.mean(tau3),\n",
    "    }\n",
    "    \n",
    "    return X, T, Y, Y_potential, true_ate, probs\n",
    "\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "X, T, Y, Y_potential, true_ate, true_probs = generate_multi_treatment_data()\n",
    "\n",
    "print(\"æ•°æ®ç”Ÿæˆå®Œæˆï¼\")\n",
    "print(f\"æ ·æœ¬æ•°: {len(X)}\")\n",
    "print(f\"\\nå„å¤„ç†ç»„åˆ†å¸ƒ:\")\n",
    "for k in range(4):\n",
    "    print(f\"  T={k}: {(T==k).sum()} ({(T==k).mean():.1%})\")\n",
    "\n",
    "print(f\"\\nçœŸå® ATE (vs å¯¹ç…§ç»„):\")\n",
    "for k, v in true_ate.items():\n",
    "    print(f\"  {k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: å¹¿ä¹‰å€¾å‘å¾—åˆ†ä¼°è®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedPropensityScore:\n",
    "    \"\"\"\n",
    "    å¹¿ä¹‰å€¾å‘å¾—åˆ†ä¼°è®¡\n",
    "    \n",
    "    ä½¿ç”¨å¤šé¡¹ Logistic å›å½’ä¼°è®¡ P(T=k|X)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method: str = 'multinomial'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'multinomial' æˆ– 'ovr' (one-vs-rest)\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.n_treatments = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, T: np.ndarray):\n",
    "        \"\"\"\n",
    "        æ‹Ÿåˆ GPS æ¨¡å‹\n",
    "        \"\"\"\n",
    "        self.n_treatments = len(np.unique(T))\n",
    "        \n",
    "        if self.method == 'multinomial':\n",
    "            self.model = LogisticRegression(\n",
    "                multi_class='multinomial',\n",
    "                solver='lbfgs',\n",
    "                max_iter=1000\n",
    "            )\n",
    "        else:\n",
    "            self.model = LogisticRegression(\n",
    "                multi_class='ovr',\n",
    "                solver='lbfgs',\n",
    "                max_iter=1000\n",
    "            )\n",
    "        \n",
    "        self.model.fit(X, T)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        é¢„æµ‹å„å¤„ç†çš„æ¦‚ç‡\n",
    "        \n",
    "        Returns:\n",
    "            probs: å½¢çŠ¶ (n_samples, n_treatments)\n",
    "        \"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def get_propensity(self, X: np.ndarray, T: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è·å–æ¯ä¸ªæ ·æœ¬å®é™…æ¥å—çš„å¤„ç†çš„å€¾å‘å¾—åˆ†\n",
    "        \n",
    "        Returns:\n",
    "            ps: å½¢çŠ¶ (n_samples,)\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        ps = np.array([probs[i, T[i]] for i in range(len(T))])\n",
    "        return ps\n",
    "\n",
    "\n",
    "# ä¼°è®¡ GPS\n",
    "gps_model = GeneralizedPropensityScore(method='multinomial')\n",
    "gps_model.fit(X, T)\n",
    "\n",
    "# é¢„æµ‹æ¦‚ç‡\n",
    "pred_probs = gps_model.predict_proba(X)\n",
    "\n",
    "print(\"GPS ä¼°è®¡å®Œæˆï¼\")\n",
    "print(f\"\\né¢„æµ‹æ¦‚ç‡åˆ†å¸ƒï¼ˆå‰ 5 ä¸ªæ ·æœ¬ï¼‰:\")\n",
    "print(pd.DataFrame(pred_probs[:5], columns=[f'P(T={k})' for k in range(4)]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ GPS ä¼°è®¡è´¨é‡\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('çœŸå® vs é¢„æµ‹æ¦‚ç‡', 'å€¾å‘å¾—åˆ†åˆ†å¸ƒ'))\n",
    "\n",
    "# çœŸå® vs é¢„æµ‹\n",
    "for k in range(4):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=true_probs[:100, k],\n",
    "        y=pred_probs[:100, k],\n",
    "        mode='markers',\n",
    "        name=f'T={k}',\n",
    "        marker=dict(size=5, opacity=0.5)\n",
    "    ), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    line=dict(dash='dash', color='gray'),\n",
    "    name='ç†æƒ³çº¿',\n",
    "    showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "ps = gps_model.get_propensity(X, T)\n",
    "for k in range(4):\n",
    "    mask = T == k\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=ps[mask],\n",
    "        name=f'T={k}',\n",
    "        opacity=0.5,\n",
    "        nbinsx=30\n",
    "    ), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='çœŸå®æ¦‚ç‡', row=1, col=1)\n",
    "fig.update_yaxes(title_text='é¢„æµ‹æ¦‚ç‡', row=1, col=1)\n",
    "fig.update_xaxes(title_text='å€¾å‘å¾—åˆ†', row=1, col=2)\n",
    "fig.update_yaxes(title_text='é¢‘æ•°', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, template='plotly_white',\n",
    "                  title_text='å¹¿ä¹‰å€¾å‘å¾—åˆ†ä¼°è®¡è´¨é‡')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: å¤šé‡å¤„ç† IPW ä¼°è®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTreatmentIPW:\n",
    "    \"\"\"\n",
    "    å¤šé‡å¤„ç†é€†æ¦‚ç‡åŠ æƒä¼°è®¡\n",
    "    \n",
    "    ä¼°è®¡ E[Y(k)] = E[Y * I(T=k) / P(T=k|X)]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trim_quantile: float = 0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trim_quantile: æƒé‡è£å‰ªåˆ†ä½æ•°\n",
    "        \"\"\"\n",
    "        self.trim_quantile = trim_quantile\n",
    "        self.gps = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray):\n",
    "        \"\"\"\n",
    "        æ‹Ÿåˆ IPW ä¼°è®¡å™¨\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.Y = Y\n",
    "        self.n_treatments = len(np.unique(T))\n",
    "        \n",
    "        # ä¼°è®¡ GPS\n",
    "        self.gps = GeneralizedPropensityScore()\n",
    "        self.gps.fit(X, T)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def estimate_ate(self, reference: int = 0) -> Dict:\n",
    "        \"\"\"\n",
    "        ä¼°è®¡ç›¸å¯¹äºå‚ç…§ç»„çš„ ATE\n",
    "        \n",
    "        Args:\n",
    "            reference: å‚ç…§å¤„ç†æ°´å¹³\n",
    "        \n",
    "        Returns:\n",
    "            å„å¤„ç†æ°´å¹³çš„ ATE ä¼°è®¡\n",
    "        \"\"\"\n",
    "        probs = self.gps.predict_proba(self.X)\n",
    "        \n",
    "        results = {}\n",
    "        mu_estimates = {}\n",
    "        \n",
    "        # ä¼°è®¡æ¯ä¸ªå¤„ç†æ°´å¹³çš„å‡å€¼\n",
    "        for k in range(self.n_treatments):\n",
    "            mask = self.T == k\n",
    "            \n",
    "            # è·å–æƒé‡\n",
    "            weights = 1 / probs[mask, k]\n",
    "            \n",
    "            # è£å‰ªæç«¯æƒé‡\n",
    "            lower = np.quantile(weights, self.trim_quantile)\n",
    "            upper = np.quantile(weights, 1 - self.trim_quantile)\n",
    "            weights = np.clip(weights, lower, upper)\n",
    "            \n",
    "            # å½’ä¸€åŒ–æƒé‡\n",
    "            weights = weights / weights.sum() * mask.sum()\n",
    "            \n",
    "            # åŠ æƒå‡å€¼\n",
    "            mu_estimates[k] = np.average(self.Y[mask], weights=weights)\n",
    "        \n",
    "        # è®¡ç®— ATE\n",
    "        for k in range(self.n_treatments):\n",
    "            if k != reference:\n",
    "                results[f'tau_{k}'] = mu_estimates[k] - mu_estimates[reference]\n",
    "        \n",
    "        results['mu_estimates'] = mu_estimates\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# IPW ä¼°è®¡\n",
    "ipw_estimator = MultiTreatmentIPW(trim_quantile=0.01)\n",
    "ipw_estimator.fit(X, T, Y)\n",
    "ipw_results = ipw_estimator.estimate_ate(reference=0)\n",
    "\n",
    "print(\"IPW ä¼°è®¡ç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nå¤„ç†æ°´å¹³å‡å€¼ä¼°è®¡:\")\n",
    "for k, mu in ipw_results['mu_estimates'].items():\n",
    "    print(f\"  E[Y({k})]: {mu:.2f}\")\n",
    "\n",
    "print(f\"\\nATE ä¼°è®¡ (vs å¯¹ç…§ç»„):\")\n",
    "print(f\"{'å¤„ç†':<10} {'IPWä¼°è®¡':<15} {'çœŸå®å€¼':<15} {'è¯¯å·®':<10}\")\n",
    "print(\"-\"*50)\n",
    "for k in [1, 2, 3]:\n",
    "    ipw_est = ipw_results[f'tau_{k}']\n",
    "    true_val = true_ate[f'tau_{k}']\n",
    "    error = ipw_est - true_val\n",
    "    print(f\"tau_{k:<7} {ipw_est:<15.2f} {true_val:<15.2f} {error:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: å¤šé‡å¤„ç† Meta-Learners\n",
    "\n",
    "### T-Learner çš„å¤šå¤„ç†æ‰©å±•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTreatmentTLearner:\n",
    "    \"\"\"\n",
    "    å¤šé‡å¤„ç† T-Learner\n",
    "    \n",
    "    ä¸ºæ¯ä¸ªå¤„ç†æ°´å¹³è®­ç»ƒç‹¬ç«‹çš„ç»“æœæ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model=None):\n",
    "        if base_model is None:\n",
    "            base_model = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n",
    "        self.base_model = base_model\n",
    "        self.models = {}\n",
    "    \n",
    "    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray):\n",
    "        \"\"\"\n",
    "        ä¸ºæ¯ä¸ªå¤„ç†æ°´å¹³è®­ç»ƒæ¨¡å‹\n",
    "        \"\"\"\n",
    "        self.n_treatments = len(np.unique(T))\n",
    "        \n",
    "        for k in range(self.n_treatments):\n",
    "            mask = T == k\n",
    "            \n",
    "            # å…‹éš†åŸºæ¨¡å‹\n",
    "            from sklearn.base import clone\n",
    "            model = clone(self.base_model)\n",
    "            \n",
    "            # è®­ç»ƒ\n",
    "            model.fit(X[mask], Y[mask])\n",
    "            self.models[k] = model\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_potential_outcomes(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        é¢„æµ‹æ‰€æœ‰æ½œåœ¨ç»“æœ\n",
    "        \n",
    "        Returns:\n",
    "            Y_pred: å½¢çŠ¶ (n_samples, n_treatments)\n",
    "        \"\"\"\n",
    "        Y_pred = np.zeros((len(X), self.n_treatments))\n",
    "        \n",
    "        for k in range(self.n_treatments):\n",
    "            Y_pred[:, k] = self.models[k].predict(X)\n",
    "        \n",
    "        return Y_pred\n",
    "    \n",
    "    def estimate_cate(self, X: np.ndarray, reference: int = 0) -> Dict:\n",
    "        \"\"\"\n",
    "        ä¼°è®¡ CATE\n",
    "        \"\"\"\n",
    "        Y_pred = self.predict_potential_outcomes(X)\n",
    "        \n",
    "        cate = {}\n",
    "        for k in range(self.n_treatments):\n",
    "            if k != reference:\n",
    "                cate[f'tau_{k}'] = Y_pred[:, k] - Y_pred[:, reference]\n",
    "        \n",
    "        return cate\n",
    "    \n",
    "    def estimate_ate(self, X: np.ndarray, reference: int = 0) -> Dict:\n",
    "        \"\"\"\n",
    "        ä¼°è®¡ ATE\n",
    "        \"\"\"\n",
    "        cate = self.estimate_cate(X, reference)\n",
    "        \n",
    "        ate = {}\n",
    "        for key, values in cate.items():\n",
    "            ate[key] = np.mean(values)\n",
    "        \n",
    "        return ate\n",
    "\n",
    "\n",
    "# T-Learner ä¼°è®¡\n",
    "t_learner = MultiTreatmentTLearner()\n",
    "t_learner.fit(X, T, Y)\n",
    "t_learner_ate = t_learner.estimate_ate(X, reference=0)\n",
    "\n",
    "print(\"T-Learner ä¼°è®¡ç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nATE ä¼°è®¡ (vs å¯¹ç…§ç»„):\")\n",
    "print(f\"{'å¤„ç†':<10} {'T-Learnerä¼°è®¡':<15} {'çœŸå®å€¼':<15} {'è¯¯å·®':<10}\")\n",
    "print(\"-\"*50)\n",
    "for k in [1, 2, 3]:\n",
    "    t_est = t_learner_ate[f'tau_{k}']\n",
    "    true_val = true_ate[f'tau_{k}']\n",
    "    error = t_est - true_val\n",
    "    print(f\"tau_{k:<7} {t_est:<15.2f} {true_val:<15.2f} {error:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŒé‡ç¨³å¥ä¼°è®¡ (AIPW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTreatmentAIPW:\n",
    "    \"\"\"\n",
    "    å¤šé‡å¤„ç†çš„å¢å¼ºé€†æ¦‚ç‡åŠ æƒ (AIPW)\n",
    "    \n",
    "    ç»“åˆ IPW å’Œç»“æœå›å½’ï¼ŒåŒé‡ç¨³å¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trim_quantile: float = 0.01):\n",
    "        self.trim_quantile = trim_quantile\n",
    "    \n",
    "    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.Y = Y\n",
    "        self.n_treatments = len(np.unique(T))\n",
    "        \n",
    "        # ä¼°è®¡ GPS\n",
    "        self.gps = GeneralizedPropensityScore()\n",
    "        self.gps.fit(X, T)\n",
    "        \n",
    "        # ä¼°è®¡ç»“æœæ¨¡å‹\n",
    "        self.outcome_models = {}\n",
    "        for k in range(self.n_treatments):\n",
    "            mask = T == k\n",
    "            model = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n",
    "            model.fit(X[mask], Y[mask])\n",
    "            self.outcome_models[k] = model\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def estimate_ate(self, reference: int = 0) -> Dict:\n",
    "        \"\"\"\n",
    "        AIPW ä¼°è®¡\n",
    "        \n",
    "        å…¬å¼:\n",
    "        E[Y(k)] = (1/n) * Î£ [Î¼_k(X) + I(T=k)/e_k(X) * (Y - Î¼_k(X))]\n",
    "        \"\"\"\n",
    "        n = len(self.X)\n",
    "        probs = self.gps.predict_proba(self.X)\n",
    "        \n",
    "        mu_estimates = {}\n",
    "        \n",
    "        for k in range(self.n_treatments):\n",
    "            # ç»“æœæ¨¡å‹é¢„æµ‹\n",
    "            mu_k = self.outcome_models[k].predict(self.X)\n",
    "            \n",
    "            # å€¾å‘å¾—åˆ†\n",
    "            e_k = probs[:, k]\n",
    "            e_k = np.clip(e_k, 0.01, 0.99)  # è£å‰ª\n",
    "            \n",
    "            # æŒ‡ç¤ºå‡½æ•°\n",
    "            indicator = (self.T == k).astype(float)\n",
    "            \n",
    "            # AIPW ä¼°è®¡\n",
    "            aipw_terms = mu_k + indicator / e_k * (self.Y - mu_k)\n",
    "            mu_estimates[k] = np.mean(aipw_terms)\n",
    "        \n",
    "        # è®¡ç®— ATE\n",
    "        results = {}\n",
    "        for k in range(self.n_treatments):\n",
    "            if k != reference:\n",
    "                results[f'tau_{k}'] = mu_estimates[k] - mu_estimates[reference]\n",
    "        \n",
    "        results['mu_estimates'] = mu_estimates\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# AIPW ä¼°è®¡\n",
    "aipw_estimator = MultiTreatmentAIPW()\n",
    "aipw_estimator.fit(X, T, Y)\n",
    "aipw_results = aipw_estimator.estimate_ate(reference=0)\n",
    "\n",
    "print(\"AIPW ä¼°è®¡ç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nATE ä¼°è®¡ (vs å¯¹ç…§ç»„):\")\n",
    "print(f\"{'å¤„ç†':<10} {'AIPWä¼°è®¡':<15} {'çœŸå®å€¼':<15} {'è¯¯å·®':<10}\")\n",
    "print(\"-\"*50)\n",
    "for k in [1, 2, 3]:\n",
    "    aipw_est = aipw_results[f'tau_{k}']\n",
    "    true_val = true_ate[f'tau_{k}']\n",
    "    error = aipw_est - true_val\n",
    "    print(f\"tau_{k:<7} {aipw_est:<15.2f} {true_val:<15.2f} {error:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: æ–¹æ³•å¯¹æ¯”ä¸å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‡æ€»æ‰€æœ‰æ–¹æ³•çš„ä¼°è®¡ç»“æœ\n",
    "comparison = pd.DataFrame({\n",
    "    'çœŸå®å€¼': [true_ate['tau_1'], true_ate['tau_2'], true_ate['tau_3']],\n",
    "    'IPW': [ipw_results['tau_1'], ipw_results['tau_2'], ipw_results['tau_3']],\n",
    "    'T-Learner': [t_learner_ate['tau_1'], t_learner_ate['tau_2'], t_learner_ate['tau_3']],\n",
    "    'AIPW': [aipw_results['tau_1'], aipw_results['tau_2'], aipw_results['tau_3']],\n",
    "}, index=['æ»¡20å‡5', 'æ»¡30å‡10', 'æ»¡50å‡20'])\n",
    "\n",
    "print(\"æ–¹æ³•å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.round(2).to_string())\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = go.Figure()\n",
    "\n",
    "treatments = ['æ»¡20å‡5', 'æ»¡30å‡10', 'æ»¡50å‡20']\n",
    "methods = ['çœŸå®å€¼', 'IPW', 'T-Learner', 'AIPW']\n",
    "colors = [COLORS['success'], COLORS['primary'], COLORS['warning'], COLORS['danger']]\n",
    "\n",
    "for method, color in zip(methods, colors):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=method,\n",
    "        x=treatments,\n",
    "        y=comparison[method].values,\n",
    "        marker_color=color\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='å¤šé‡å¤„ç† ATE ä¼°è®¡å¯¹æ¯”',\n",
    "    xaxis_title='ä¼˜æƒ æ¡£ä½',\n",
    "    yaxis_title='ATE (ç›¸å¯¹äºæ— ä¼˜æƒ )',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=450\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: å¼‚è´¨æ•ˆåº”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¸åŒç”¨æˆ·ç¾¤ä½“çš„ CATE\n",
    "cate_all = t_learner.estimate_cate(X, reference=0)\n",
    "\n",
    "# æŒ‰ä»·æ ¼æ•æ„Ÿåº¦åˆ†ç»„\n",
    "sensitivity_groups = pd.cut(X[:, 1], bins=3, labels=['ä½æ•æ„Ÿ', 'ä¸­æ•æ„Ÿ', 'é«˜æ•æ„Ÿ'])\n",
    "\n",
    "cate_by_group = pd.DataFrame({\n",
    "    'ä»·æ ¼æ•æ„Ÿåº¦': sensitivity_groups,\n",
    "    'CATE_1': cate_all['tau_1'],\n",
    "    'CATE_2': cate_all['tau_2'],\n",
    "    'CATE_3': cate_all['tau_3'],\n",
    "})\n",
    "\n",
    "group_means = cate_by_group.groupby('ä»·æ ¼æ•æ„Ÿåº¦').mean()\n",
    "\n",
    "print(\"ä¸åŒä»·æ ¼æ•æ„Ÿåº¦ç”¨æˆ·çš„ CATE\")\n",
    "print(\"=\"*60)\n",
    "print(group_means.round(2).to_string())\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = go.Figure()\n",
    "\n",
    "for col in ['CATE_1', 'CATE_2', 'CATE_3']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=col.replace('CATE_', 'æ–¹æ¡ˆ'),\n",
    "        x=group_means.index.tolist(),\n",
    "        y=group_means[col].values\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ä¸åŒä»·æ ¼æ•æ„Ÿåº¦ç”¨æˆ·çš„ CATE',\n",
    "    xaxis_title='ä»·æ ¼æ•æ„Ÿåº¦',\n",
    "    yaxis_title='CATE',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š è§‚å¯Ÿ:\n",
    "1. é«˜ä»·æ ¼æ•æ„Ÿåº¦ç”¨æˆ·å¯¹å„æ¡£ä½ä¼˜æƒ çš„å“åº”éƒ½æ›´å¤§\n",
    "2. å¤§é¢ä¼˜æƒ ï¼ˆæ»¡50å‡20ï¼‰å¯¹ä½æ•æ„Ÿåº¦ç”¨æˆ·æ•ˆæœæœ‰é™\n",
    "3. è¿™æç¤ºæˆ‘ä»¬åº”è¯¥ç»™é«˜æ•æ„Ÿåº¦ç”¨æˆ·å‘å¤§é¢ä¼˜æƒ ï¼Œç»™ä½æ•æ„Ÿåº¦ç”¨æˆ·å‘å°é¢ä¼˜æƒ \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒé¢˜\n",
    "\n",
    "### åŸºç¡€ç†è§£\n",
    "\n",
    "1. **å¤šé‡å¤„ç†å’Œè¿ç»­å¤„ç†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ** ä»€ä¹ˆæ—¶å€™ç”¨å“ªç§æ–¹æ³•ï¼Ÿ\n",
    "\n",
    "2. **ä¸ºä»€ä¹ˆå¤šé‡å¤„ç†éœ€è¦æ›´å¼ºçš„é‡å å‡è®¾ï¼Ÿ** å¦‚æœæŸä¸ªå¤„ç†æ°´å¹³çš„æ ·æœ¬å¾ˆå°‘æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "### æ·±å…¥åˆ†æ\n",
    "\n",
    "3. **å¦‚ä½•å¤„ç†ã€Œå¤„ç†ä¹‹é—´å­˜åœ¨å±‚çº§å…³ç³»ã€çš„æƒ…å†µï¼Ÿ** ä¾‹å¦‚ï¼Œæ»¡50å‡20 åŒ…å«äº† æ»¡30å‡10 çš„ä¼˜æƒ ã€‚\n",
    "\n",
    "4. **AIPW ä¸ºä»€ä¹ˆæ˜¯ã€ŒåŒé‡ç¨³å¥ã€çš„ï¼Ÿ** å¦‚æœ GPS æˆ–ç»“æœæ¨¡å‹æœ‰ä¸€ä¸ªæ˜¯é”™è¯¯çš„ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "### å®æˆ˜åº”ç”¨\n",
    "\n",
    "5. **å¦‚ä½•ä¸ºæ¯ä¸ªç”¨æˆ·é€‰æ‹©æœ€ä¼˜çš„å¤„ç†æ°´å¹³ï¼Ÿ** éœ€è¦è€ƒè™‘å“ªäº›å› ç´ ï¼ˆæˆæœ¬ã€é£é™©ç­‰ï¼‰ï¼Ÿ\n",
    "\n",
    "6. **å¦‚æœæœ‰é¢„ç®—çº¦æŸï¼Œå¦‚ä½•åœ¨ç”¨æˆ·ç¾¤ä½“ä¸­åˆ†é…ä¸åŒæ¡£ä½çš„ä¼˜æƒ ï¼Ÿ**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "| æ¦‚å¿µ | å®šä¹‰ | é‡è¦æ€§ |\n",
    "|------|------|--------|\n",
    "| **å¤šé‡å¤„ç†** | T âˆˆ {0, 1, ..., K} | æ›´çœŸå®çš„ä¸šåŠ¡åœºæ™¯ |\n",
    "| **å¹¿ä¹‰å€¾å‘å¾—åˆ†** | P(T=k\\|X) å‘é‡ | è¯†åˆ«çš„åŸºç¡€ |\n",
    "| **å¤šå¤„ç† IPW** | åŠ æƒä¼°è®¡ E[Y(k)] | ç»å…¸æ–¹æ³• |\n",
    "| **å¤šå¤„ç† AIPW** | ç»“åˆ IPW å’Œ OR | åŒé‡ç¨³å¥ |\n",
    "| **æœ€ä¼˜å¤„ç†é€‰æ‹©** | argmax_k CATE_k | ä¸ªæ€§åŒ–ç­–ç•¥ |\n",
    "\n",
    "### æ–¹æ³•å¯¹æ¯”\n",
    "\n",
    "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| IPW | ç®€å• | æ–¹å·®å¤§ | å¤§æ ·æœ¬ |\n",
    "| T-Learner | çµæ´» | æ— æ³•å¤„ç†ç¨€ç– | å„ç»„æ ·æœ¬å……è¶³ |\n",
    "| AIPW | åŒé‡ç¨³å¥ | è®¡ç®—å¤æ‚ | é€šç”¨ |\n",
    "\n",
    "---\n",
    "\n",
    "*æ­å–œä½ å®Œæˆäº†å¤šé‡å¤„ç†æ•ˆåº”çš„å­¦ä¹ ï¼* ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
