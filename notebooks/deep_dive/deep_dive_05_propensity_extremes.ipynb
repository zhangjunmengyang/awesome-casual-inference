{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive 05: æç«¯å€¾å‘å¾—åˆ†çš„è¯Šæ–­ä¸å¤„ç†\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ notebook åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. **è¯Šæ–­** æç«¯å€¾å‘å¾—åˆ†é—®é¢˜ï¼ˆ0.99 æˆ– 0.01ï¼‰\n",
    "2. **ç†è§£** æç«¯æƒé‡å¯¹å› æœä¼°è®¡çš„å½±å“\n",
    "3. **æŒæ¡** å¤šç§å¤„ç†æ–¹æ³•ï¼šTrimmingã€Overlap Weightsã€CRUMP bounds\n",
    "4. **é€‰æ‹©** é€‚åˆå…·ä½“åœºæ™¯çš„æœ€ä½³æ–¹æ¡ˆ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– èƒŒæ™¯æ•…äº‹\n",
    "\n",
    "> **åœºæ™¯ï¼šIPW ä¼°è®¡çš„å¥‡æ€ªç»“æœ**\n",
    ">\n",
    "> ä½ åœ¨è¯„ä¼°ä¸€ä¸ªç”¨æˆ·æ¿€åŠ±ç­–ç•¥çš„æ•ˆæœï¼Œä½¿ç”¨ IPW ä¼°è®¡ ATEã€‚ç»“æœå‘ç°ï¼š\n",
    "> - æ ‡å‡†è¯¯å¼‚å¸¸å¤§\n",
    "> - ä¼°è®¡å€¼ä¸ç¨³å®šï¼ˆæ¯æ¬¡è¿è¡Œå·®åˆ«å¾ˆå¤§ï¼‰\n",
    "> - ç½®ä¿¡åŒºé—´å®½å¾—ç¦»è°±\n",
    ">\n",
    "> æ£€æŸ¥å€¾å‘å¾—åˆ†åˆ†å¸ƒåï¼Œä½ å‘ç°é—®é¢˜äº†ï¼š\n",
    "> - æœ‰äº›ç”¨æˆ·çš„å€¾å‘å¾—åˆ† e(x) = 0.99ï¼ˆå‡ ä¹å¿…ç„¶è¢«å¤„ç†ï¼‰\n",
    "> - æœ‰äº›ç”¨æˆ·çš„å€¾å‘å¾—åˆ† e(x) = 0.01ï¼ˆå‡ ä¹ä¸å¯èƒ½è¢«å¤„ç†ï¼‰\n",
    ">\n",
    "> **è¿™äº›æç«¯å€¼å¯¼è‡´ IPW æƒé‡çˆ†ç‚¸ï¼æœ¬ notebook å°†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: é—®é¢˜è¯Šæ–­\n",
    "\n",
    "### 1.1 ä¸ºä»€ä¹ˆæç«¯å€¾å‘å¾—åˆ†æ˜¯é—®é¢˜ï¼Ÿ\n",
    "\n",
    "å›é¡¾ IPW ä¼°è®¡é‡ï¼š\n",
    "\n",
    "$$\\hat{\\tau}_{IPW} = \\frac{1}{n} \\sum_{i} \\left[ \\frac{T_i Y_i}{e(X_i)} - \\frac{(1-T_i) Y_i}{1-e(X_i)} \\right]$$\n",
    "\n",
    "**é—®é¢˜**ï¼š\n",
    "- å½“ $e(X_i) \\approx 0$ï¼Œæƒé‡ $1/e(X_i) \\to \\infty$\n",
    "- å½“ $e(X_i) \\approx 1$ï¼Œæƒé‡ $1/(1-e(X_i)) \\to \\infty$\n",
    "\n",
    "**åæœ**ï¼š\n",
    "1. **æ–¹å·®çˆ†ç‚¸**ï¼šå°‘æ•°æ ·æœ¬ä¸»å¯¼ä¼°è®¡\n",
    "2. **ä¼°è®¡ä¸ç¨³å®š**ï¼šå¯¹æç«¯æ ·æœ¬é«˜åº¦æ•æ„Ÿ\n",
    "3. **ç½®ä¿¡åŒºé—´å¤±æ•ˆ**ï¼šå®é™…è¦†ç›–ç‡è¿œä½äºåä¹‰æ°´å¹³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extreme_propensity_data(n=2000, seed=42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå¸¦æœ‰æç«¯å€¾å‘å¾—åˆ†çš„æ•°æ®\n",
    "    \n",
    "    åœºæ™¯ï¼šé«˜ä»·å€¼ç”¨æˆ·å‡ ä¹å¿…ç„¶è¢«å¤„ç†ï¼ˆe(x) â‰ˆ 1ï¼‰\n",
    "         ä½ä»·å€¼ç”¨æˆ·å‡ ä¹ä¸è¢«å¤„ç†ï¼ˆe(x) â‰ˆ 0ï¼‰\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # åå˜é‡\n",
    "    X = np.random.randn(n, 3)\n",
    "    X[:, 0] = X[:, 0] * 2  # ç¬¬ä¸€ä¸ªç‰¹å¾å½±å“æ›´å¤§\n",
    "    \n",
    "    # çœŸå®å€¾å‘å¾—åˆ†ï¼ˆæ•…æ„åˆ¶é€ æç«¯å€¼ï¼‰\n",
    "    logit = 3 * X[:, 0] + 1.5 * X[:, 1]  # å¼ºä¾èµ–äºåå˜é‡\n",
    "    true_propensity = 1 / (1 + np.exp(-logit))\n",
    "    \n",
    "    # å¤„ç†åˆ†é…\n",
    "    T = np.random.binomial(1, true_propensity)\n",
    "    \n",
    "    # çœŸå®å¤„ç†æ•ˆåº”\n",
    "    true_ate = 5.0\n",
    "    true_cate = 5 + 2 * X[:, 0]  # å¼‚è´¨æ€§æ•ˆåº”\n",
    "    \n",
    "    # ç»“æœ\n",
    "    Y0 = 10 + 2 * X[:, 0] + X[:, 1] + np.random.randn(n) * 2\n",
    "    Y1 = Y0 + true_cate\n",
    "    Y = T * Y1 + (1 - T) * Y0\n",
    "    \n",
    "    return {\n",
    "        'X': X,\n",
    "        'T': T,\n",
    "        'Y': Y,\n",
    "        'true_propensity': true_propensity,\n",
    "        'true_ate': true_ate,\n",
    "        'true_cate': true_cate,\n",
    "        'Y0': Y0,\n",
    "        'Y1': Y1\n",
    "    }\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "data = generate_extreme_propensity_data(n=3000)\n",
    "\n",
    "print(\"æ•°æ®æ¦‚å†µï¼š\")\n",
    "print(f\"  æ ·æœ¬æ•°: {len(data['Y'])}\")\n",
    "print(f\"  å¤„ç†ç»„: {data['T'].sum()} ({data['T'].mean():.1%})\")\n",
    "print(f\"  çœŸå® ATE: {data['true_ate']}\")\n",
    "print(f\"\\nå€¾å‘å¾—åˆ†åˆ†å¸ƒ:\")\n",
    "print(f\"  æœ€å°å€¼: {data['true_propensity'].min():.4f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {data['true_propensity'].max():.4f}\")\n",
    "print(f\"  < 0.05: {(data['true_propensity'] < 0.05).sum()} ({(data['true_propensity'] < 0.05).mean():.1%})\")\n",
    "print(f\"  > 0.95: {(data['true_propensity'] > 0.95).sum()} ({(data['true_propensity'] > 0.95).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å€¾å‘å¾—åˆ†åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# å€¾å‘å¾—åˆ†ç›´æ–¹å›¾\n",
    "ax1 = axes[0]\n",
    "ax1.hist(data['true_propensity'], bins=50, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0.1, color='red', linestyle='--', label='å±é™©åŒºåŸŸè¾¹ç•Œ (0.1)')\n",
    "ax1.axvline(x=0.9, color='red', linestyle='--')\n",
    "ax1.fill_betweenx([0, 500], 0, 0.1, alpha=0.3, color='red')\n",
    "ax1.fill_betweenx([0, 500], 0.9, 1, alpha=0.3, color='red')\n",
    "ax1.set_xlabel('å€¾å‘å¾—åˆ† e(X)')\n",
    "ax1.set_ylabel('é¢‘æ•°')\n",
    "ax1.set_title('å€¾å‘å¾—åˆ†åˆ†å¸ƒ\\n(çº¢è‰²åŒºåŸŸä¸ºæç«¯å€¼)')\n",
    "ax1.legend()\n",
    "\n",
    "# æŒ‰å¤„ç†ç»„åˆ†å¸ƒ\n",
    "ax2 = axes[1]\n",
    "ax2.hist(data['true_propensity'][data['T']==0], bins=30, alpha=0.6, \n",
    "         label='æ§åˆ¶ç»„', color='#3498db')\n",
    "ax2.hist(data['true_propensity'][data['T']==1], bins=30, alpha=0.6, \n",
    "         label='å¤„ç†ç»„', color='#e74c3c')\n",
    "ax2.set_xlabel('å€¾å‘å¾—åˆ† e(X)')\n",
    "ax2.set_ylabel('é¢‘æ•°')\n",
    "ax2.set_title('æŒ‰å¤„ç†ç»„çš„å€¾å‘å¾—åˆ†åˆ†å¸ƒ')\n",
    "ax2.legend()\n",
    "\n",
    "# IPW æƒé‡åˆ†å¸ƒ\n",
    "ax3 = axes[2]\n",
    "weights_treated = 1 / data['true_propensity'][data['T']==1]\n",
    "weights_control = 1 / (1 - data['true_propensity'][data['T']==0])\n",
    "\n",
    "ax3.hist(weights_treated, bins=50, alpha=0.6, label='å¤„ç†ç»„æƒé‡ 1/e(X)', color='#e74c3c')\n",
    "ax3.hist(weights_control, bins=50, alpha=0.6, label='æ§åˆ¶ç»„æƒé‡ 1/(1-e(X))', color='#3498db')\n",
    "ax3.set_xlabel('IPW æƒé‡')\n",
    "ax3.set_ylabel('é¢‘æ•°')\n",
    "ax3.set_title('IPW æƒé‡åˆ†å¸ƒ\\n(æ³¨æ„æç«¯å€¼!)')\n",
    "ax3.set_xlim(0, 50)  # é™åˆ¶æ˜¾ç¤ºèŒƒå›´\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\næƒé‡ç»Ÿè®¡:\")\n",
    "print(f\"  å¤„ç†ç»„æƒé‡æœ€å¤§å€¼: {weights_treated.max():.1f}\")\n",
    "print(f\"  æ§åˆ¶ç»„æƒé‡æœ€å¤§å€¼: {weights_control.max():.1f}\")\n",
    "print(f\"  âš ï¸ å•ä¸ªæ ·æœ¬çš„æƒé‡å¯èƒ½è¶…è¿‡ 100 å€å¹³å‡æƒé‡!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æœ‰æ•ˆæ ·æœ¬é‡ (ESS)\n",
    "\n",
    "æœ‰æ•ˆæ ·æœ¬é‡è¡¡é‡åŠ æƒåæ ·æœ¬çš„\"æœ‰æ•ˆ\"æ•°é‡ï¼š\n",
    "\n",
    "$$\\text{ESS} = \\frac{(\\sum_i w_i)^2}{\\sum_i w_i^2}$$\n",
    "\n",
    "å½“æƒé‡æç«¯ä¸å¹³è¡¡æ—¶ï¼ŒESS ä¼šè¿œå°äºå®é™…æ ·æœ¬é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ess(weights):\n",
    "    \"\"\"è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\"\"\"\n",
    "    return (np.sum(weights) ** 2) / np.sum(weights ** 2)\n",
    "\n",
    "def diagnose_propensity_scores(e, T):\n",
    "    \"\"\"\n",
    "    å€¾å‘å¾—åˆ†è¯Šæ–­æŠ¥å‘Š\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"å€¾å‘å¾—åˆ†è¯Šæ–­æŠ¥å‘Š\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡\n",
    "    print(f\"\\n1. åŸºæœ¬ç»Ÿè®¡\")\n",
    "    print(f\"   æ ·æœ¬é‡: {len(e)}\")\n",
    "    print(f\"   å¤„ç†ç»„: {T.sum()} ({T.mean():.1%})\")\n",
    "    print(f\"   å€¾å‘å¾—åˆ†èŒƒå›´: [{e.min():.4f}, {e.max():.4f}]\")\n",
    "    \n",
    "    # æç«¯å€¼æ£€æµ‹\n",
    "    print(f\"\\n2. æç«¯å€¼æ£€æµ‹\")\n",
    "    thresholds = [0.01, 0.05, 0.1]\n",
    "    for th in thresholds:\n",
    "        n_low = (e < th).sum()\n",
    "        n_high = (e > 1-th).sum()\n",
    "        print(f\"   e < {th:.2f} æˆ– e > {1-th:.2f}: {n_low + n_high} ({(n_low + n_high)/len(e):.1%})\")\n",
    "    \n",
    "    # æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    print(f\"\\n3. æœ‰æ•ˆæ ·æœ¬é‡ (ESS)\")\n",
    "    \n",
    "    # å¤„ç†ç»„\n",
    "    weights_t = 1 / e[T==1]\n",
    "    ess_t = calculate_ess(weights_t)\n",
    "    print(f\"   å¤„ç†ç»„: ESS = {ess_t:.1f} / {T.sum()} ({ess_t/T.sum():.1%})\")\n",
    "    \n",
    "    # æ§åˆ¶ç»„\n",
    "    weights_c = 1 / (1 - e[T==0])\n",
    "    ess_c = calculate_ess(weights_c)\n",
    "    print(f\"   æ§åˆ¶ç»„: ESS = {ess_c:.1f} / {(1-T).sum()} ({ess_c/(1-T).sum():.1%})\")\n",
    "    \n",
    "    # æ¨è\n",
    "    print(f\"\\n4. è¯Šæ–­ç»“è®º\")\n",
    "    if ess_t/T.sum() < 0.5 or ess_c/(1-T).sum() < 0.5:\n",
    "        print(\"   âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ ·æœ¬é‡æŸå¤±ä¸¥é‡ (>50%)ï¼Œå»ºè®®å¤„ç†æç«¯æƒé‡\")\n",
    "    elif ess_t/T.sum() < 0.8 or ess_c/(1-T).sum() < 0.8:\n",
    "        print(\"   âš¡ æ³¨æ„: æœ‰æ•ˆæ ·æœ¬é‡æœ‰ä¸€å®šæŸå¤± (20-50%)ï¼Œå¯è€ƒè™‘å¤„ç†æç«¯æƒé‡\")\n",
    "    else:\n",
    "        print(\"   âœ… æœ‰æ•ˆæ ·æœ¬é‡æŸå¤±è¾ƒå° (<20%)ï¼Œå¯ç›´æ¥ä½¿ç”¨ IPW\")\n",
    "\n",
    "# è¿è¡Œè¯Šæ–­\n",
    "diagnose_propensity_scores(data['true_propensity'], data['T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: å¤„ç†æ–¹æ³•\n",
    "\n",
    "### 2.1 æ–¹æ³•æ€»è§ˆ\n",
    "\n",
    "| æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
    "|------|---------|------|------|\n",
    "| Trimming | åˆ é™¤æç«¯æ ·æœ¬ | ç®€å•ç›´æ¥ | ä¸¢å¤±æ ·æœ¬ï¼Œå¯èƒ½æœ‰å |\n",
    "| Truncated IPW | æˆªæ–­æç«¯æƒé‡ | å‡å°‘æ–¹å·® | å¼•å…¥åå·® |\n",
    "| Overlap Weights | ä¼˜åŒ–æƒé‡è®¾è®¡ | ç†è®ºæœ€ä¼˜ | ä¼°è®¡ ATO é ATE |\n",
    "| Stabilized Weights | ç¨³å®šåŒ–æƒé‡ | å‡å°‘æ–¹å·® | ä»å¯èƒ½æœ‰æç«¯å€¼ |\n",
    "| CRUMP Bounds | æ•°æ®é©±åŠ¨é˜ˆå€¼ | æœ‰ç†è®ºæ”¯æ’‘ | å¯èƒ½è¿‡äºä¿å®ˆ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–¹æ³• 1: Trimmingï¼ˆä¿®å‰ªï¼‰\n",
    "\n",
    "æœ€ç®€å•çš„æ–¹æ³•ï¼šåˆ é™¤å€¾å‘å¾—åˆ†åœ¨æç«¯åŒºåŸŸçš„æ ·æœ¬ã€‚\n",
    "\n",
    "$$\\text{ä¿ç•™æ ·æœ¬: } \\alpha < e(X_i) < 1 - \\alpha$$\n",
    "\n",
    "å¸¸è§é˜ˆå€¼ï¼š$\\alpha = 0.1$ æˆ– $\\alpha = 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_with_trimming(Y, T, e, trim_threshold=0.1):\n",
    "    \"\"\"\n",
    "    å¸¦ Trimming çš„ IPW ä¼°è®¡\n",
    "    \n",
    "    Args:\n",
    "        Y: ç»“æœ\n",
    "        T: å¤„ç†\n",
    "        e: å€¾å‘å¾—åˆ†\n",
    "        trim_threshold: ä¿®å‰ªé˜ˆå€¼\n",
    "        \n",
    "    Returns:\n",
    "        ate: ATE ä¼°è®¡\n",
    "        se: æ ‡å‡†è¯¯\n",
    "        n_trimmed: è¢«ä¿®å‰ªçš„æ ·æœ¬æ•°\n",
    "    \"\"\"\n",
    "    # ä¿®å‰ª\n",
    "    keep_mask = (e > trim_threshold) & (e < 1 - trim_threshold)\n",
    "    n_trimmed = (~keep_mask).sum()\n",
    "    \n",
    "    Y_trim = Y[keep_mask]\n",
    "    T_trim = T[keep_mask]\n",
    "    e_trim = e[keep_mask]\n",
    "    \n",
    "    # IPW ä¼°è®¡\n",
    "    weights_t = T_trim / e_trim\n",
    "    weights_c = (1 - T_trim) / (1 - e_trim)\n",
    "    \n",
    "    # å½’ä¸€åŒ–æƒé‡\n",
    "    weights_t = weights_t / weights_t.sum() * len(weights_t)\n",
    "    weights_c = weights_c / weights_c.sum() * len(weights_c)\n",
    "    \n",
    "    mu1 = np.sum(weights_t * Y_trim * T_trim) / np.sum(weights_t * T_trim)\n",
    "    mu0 = np.sum(weights_c * Y_trim * (1-T_trim)) / np.sum(weights_c * (1-T_trim))\n",
    "    ate = mu1 - mu0\n",
    "    \n",
    "    # æ ‡å‡†è¯¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\n",
    "    n = len(Y_trim)\n",
    "    var_t = np.var(Y_trim[T_trim==1], ddof=1) / (T_trim==1).sum()\n",
    "    var_c = np.var(Y_trim[T_trim==0], ddof=1) / (T_trim==0).sum()\n",
    "    se = np.sqrt(var_t + var_c)\n",
    "    \n",
    "    return ate, se, n_trimmed\n",
    "\n",
    "# æµ‹è¯•ä¸åŒé˜ˆå€¼\n",
    "thresholds = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "results_trimming = []\n",
    "\n",
    "for th in thresholds:\n",
    "    ate, se, n_trimmed = ipw_with_trimming(data['Y'], data['T'], data['true_propensity'], th)\n",
    "    results_trimming.append({\n",
    "        'threshold': th,\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'n_trimmed': n_trimmed,\n",
    "        'pct_trimmed': n_trimmed / len(data['Y'])\n",
    "    })\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "print(\"\\nTrimming æ–¹æ³•ç»“æœ (çœŸå® ATE = 5.0):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'é˜ˆå€¼':<10} {'ATE ä¼°è®¡':<12} {'æ ‡å‡†è¯¯':<12} {'ä¿®å‰ªæ ·æœ¬æ•°':<15} {'ä¿®å‰ªæ¯”ä¾‹':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results_trimming:\n",
    "    print(f\"{r['threshold']:<10} {r['ate']:<12.3f} {r['se']:<12.3f} {r['n_trimmed']:<15} {r['pct_trimmed']:<10.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 æ–¹æ³• 2: Overlap Weights\n",
    "\n",
    "Overlap Weights (Li et al., 2018) ä½¿ç”¨æœ€ä¼˜æƒé‡è®¾è®¡ï¼Œé¿å…æç«¯å€¼ã€‚\n",
    "\n",
    "**æƒé‡å®šä¹‰**ï¼š\n",
    "$$w(X) = e(X)(1 - e(X))$$\n",
    "\n",
    "**ä¼°è®¡ç›®æ ‡**ï¼šATO (Average Treatment Effect on the Overlap population)\n",
    "$$\\tau_{ATO} = E[Y(1) - Y(0) | \\text{æœ‰é‡å }]$$\n",
    "\n",
    "**ä¼˜ç‚¹**ï¼š\n",
    "- è‡ªåŠ¨ä¸‹è°ƒæç«¯æ ·æœ¬çš„æƒé‡\n",
    "- æœ€å°åŒ–ä¼°è®¡æ–¹å·®\n",
    "- ä¸éœ€è¦äººå·¥é€‰æ‹©é˜ˆå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_with_overlap_weights(Y, T, e):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Overlap Weights çš„ IPW ä¼°è®¡\n",
    "    \n",
    "    ä¼°è®¡ ATO: Average Treatment Effect on the Overlap population\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    \n",
    "    # Overlap æƒé‡: w(X) = e(X) * (1 - e(X))\n",
    "    overlap_weights = e * (1 - e)\n",
    "    \n",
    "    # å¤„ç†ç»„æƒé‡: (1 - e(X)) å¯¹äºè¢«å¤„ç†çš„ä¸ªä½“\n",
    "    # æ§åˆ¶ç»„æƒé‡: e(X) å¯¹äºæœªè¢«å¤„ç†çš„ä¸ªä½“\n",
    "    weights_t = (1 - e) * T  # å¤„ç†ç»„\n",
    "    weights_c = e * (1 - T)  # æ§åˆ¶ç»„\n",
    "    \n",
    "    # ä¼°è®¡ ATO\n",
    "    mu1 = np.sum(weights_t * Y) / np.sum(weights_t)\n",
    "    mu0 = np.sum(weights_c * Y) / np.sum(weights_c)\n",
    "    ato = mu1 - mu0\n",
    "    \n",
    "    # æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    ess_t = calculate_ess(weights_t[T==1])\n",
    "    ess_c = calculate_ess(weights_c[T==0])\n",
    "    \n",
    "    # æ ‡å‡†è¯¯\n",
    "    residuals_t = (Y - mu1) * weights_t\n",
    "    residuals_c = (Y - mu0) * weights_c\n",
    "    var_t = np.sum(residuals_t**2) / (np.sum(weights_t)**2)\n",
    "    var_c = np.sum(residuals_c**2) / (np.sum(weights_c)**2)\n",
    "    se = np.sqrt(var_t + var_c)\n",
    "    \n",
    "    return {\n",
    "        'ato': ato,\n",
    "        'se': se,\n",
    "        'ess_treated': ess_t,\n",
    "        'ess_control': ess_c\n",
    "    }\n",
    "\n",
    "# è®¡ç®— Overlap Weights ç»“æœ\n",
    "result_overlap = ipw_with_overlap_weights(data['Y'], data['T'], data['true_propensity'])\n",
    "\n",
    "print(\"\\nOverlap Weights ç»“æœ:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  ATO ä¼°è®¡: {result_overlap['ato']:.3f}\")\n",
    "print(f\"  æ ‡å‡†è¯¯: {result_overlap['se']:.3f}\")\n",
    "print(f\"  95% CI: [{result_overlap['ato']-1.96*result_overlap['se']:.3f}, \"\n",
    "      f\"{result_overlap['ato']+1.96*result_overlap['se']:.3f}]\")\n",
    "print(f\"  å¤„ç†ç»„ ESS: {result_overlap['ess_treated']:.1f}\")\n",
    "print(f\"  æ§åˆ¶ç»„ ESS: {result_overlap['ess_control']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 æ–¹æ³• 3: Stabilized Weights\n",
    "\n",
    "Stabilized Weights å°†æƒé‡ä¹˜ä»¥è¾¹é™…å¤„ç†æ¦‚ç‡ï¼Œå‡å°‘æ–¹å·®ã€‚\n",
    "\n",
    "$$w^{stab} = \\frac{P(T)}{e(X)} \\cdot T + \\frac{1 - P(T)}{1 - e(X)} \\cdot (1-T)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_with_stabilized_weights(Y, T, e):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Stabilized Weights çš„ IPW ä¼°è®¡\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    \n",
    "    # è¾¹é™…å¤„ç†æ¦‚ç‡\n",
    "    p_treat = T.mean()\n",
    "    \n",
    "    # Stabilized weights\n",
    "    weights = np.where(T == 1, \n",
    "                       p_treat / e,\n",
    "                       (1 - p_treat) / (1 - e))\n",
    "    \n",
    "    # åŠ æƒå‡å€¼\n",
    "    mu1 = np.sum(weights * Y * T) / np.sum(weights * T)\n",
    "    mu0 = np.sum(weights * Y * (1-T)) / np.sum(weights * (1-T))\n",
    "    ate = mu1 - mu0\n",
    "    \n",
    "    # æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    ess_t = calculate_ess(weights[T==1])\n",
    "    ess_c = calculate_ess(weights[T==0])\n",
    "    \n",
    "    # æ ‡å‡†è¯¯\n",
    "    n_t = (T==1).sum()\n",
    "    n_c = (T==0).sum()\n",
    "    var_t = np.sum((weights * (Y - mu1))**2 * T) / (n_t * (n_t - 1))\n",
    "    var_c = np.sum((weights * (Y - mu0))**2 * (1-T)) / (n_c * (n_c - 1))\n",
    "    se = np.sqrt(var_t + var_c)\n",
    "    \n",
    "    return {\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'ess_treated': ess_t,\n",
    "        'ess_control': ess_c,\n",
    "        'max_weight': weights.max()\n",
    "    }\n",
    "\n",
    "# è®¡ç®— Stabilized Weights ç»“æœ\n",
    "result_stabilized = ipw_with_stabilized_weights(data['Y'], data['T'], data['true_propensity'])\n",
    "\n",
    "print(\"\\nStabilized Weights ç»“æœ:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  ATE ä¼°è®¡: {result_stabilized['ate']:.3f}\")\n",
    "print(f\"  æ ‡å‡†è¯¯: {result_stabilized['se']:.3f}\")\n",
    "print(f\"  å¤„ç†ç»„ ESS: {result_stabilized['ess_treated']:.1f}\")\n",
    "print(f\"  æ§åˆ¶ç»„ ESS: {result_stabilized['ess_control']:.1f}\")\n",
    "print(f\"  æœ€å¤§æƒé‡: {result_stabilized['max_weight']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 æ–¹æ³• 4: CRUMP Bounds\n",
    "\n",
    "CRUMP et al. (2009) æå‡ºæ•°æ®é©±åŠ¨çš„é˜ˆå€¼é€‰æ‹©æ–¹æ³•ã€‚\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šé€‰æ‹©é˜ˆå€¼ä½¿ä¼°è®¡æ–¹å·®æœ€å°åŒ–ã€‚\n",
    "\n",
    "**è¿‘ä¼¼æœ€ä¼˜é˜ˆå€¼**ï¼š\n",
    "$$\\alpha^* \\approx \\frac{1}{2} - \\sqrt{\\frac{1}{4} - \\frac{1}{\\sqrt{n}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crump_threshold(n):\n",
    "    \"\"\"\n",
    "    è®¡ç®— CRUMP æ¨èçš„é˜ˆå€¼\n",
    "    \"\"\"\n",
    "    # è¿‘ä¼¼å…¬å¼\n",
    "    inner = 0.25 - 1 / np.sqrt(n)\n",
    "    if inner < 0:\n",
    "        return 0.1  # é»˜è®¤å€¼\n",
    "    return 0.5 - np.sqrt(inner)\n",
    "\n",
    "def ipw_with_crump(Y, T, e):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ CRUMP bounds çš„ IPW ä¼°è®¡\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    threshold = crump_threshold(n)\n",
    "    \n",
    "    return ipw_with_trimming(Y, T, e, trim_threshold=threshold), threshold\n",
    "\n",
    "# è®¡ç®— CRUMP ç»“æœ\n",
    "(ate_crump, se_crump, n_trimmed_crump), threshold_crump = ipw_with_crump(\n",
    "    data['Y'], data['T'], data['true_propensity']\n",
    ")\n",
    "\n",
    "print(f\"\\nCRUMP Bounds ç»“æœ:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  æ¨èé˜ˆå€¼: {threshold_crump:.4f}\")\n",
    "print(f\"  ATE ä¼°è®¡: {ate_crump:.3f}\")\n",
    "print(f\"  æ ‡å‡†è¯¯: {se_crump:.3f}\")\n",
    "print(f\"  ä¿®å‰ªæ ·æœ¬: {n_trimmed_crump} ({n_trimmed_crump/len(data['Y']):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: æ–¹æ³•æ¯”è¾ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡å‡† IPWï¼ˆæ— å¤„ç†ï¼‰\n",
    "def standard_ipw(Y, T, e):\n",
    "    weights_t = T / e\n",
    "    weights_c = (1 - T) / (1 - e)\n",
    "    \n",
    "    mu1 = np.sum(weights_t * Y) / np.sum(weights_t)\n",
    "    mu0 = np.sum(weights_c * Y) / np.sum(weights_c)\n",
    "    \n",
    "    ate = mu1 - mu0\n",
    "    \n",
    "    # ç®€åŒ–æ ‡å‡†è¯¯\n",
    "    se = np.sqrt(np.var(Y[T==1])/T.sum() + np.var(Y[T==0])/(1-T).sum())\n",
    "    \n",
    "    return ate, se\n",
    "\n",
    "ate_standard, se_standard = standard_ipw(data['Y'], data['T'], data['true_propensity'])\n",
    "\n",
    "# æ±‡æ€»æ‰€æœ‰æ–¹æ³•\n",
    "methods_comparison = [\n",
    "    {'æ–¹æ³•': 'æ ‡å‡† IPW', 'ATE/ATO': ate_standard, 'æ ‡å‡†è¯¯': se_standard, 'ç›®æ ‡': 'ATE'},\n",
    "    {'æ–¹æ³•': 'Trimming (Î±=0.1)', 'ATE/ATO': results_trimming[2]['ate'], \n",
    "     'æ ‡å‡†è¯¯': results_trimming[2]['se'], 'ç›®æ ‡': 'ATE (trimmed)'},\n",
    "    {'æ–¹æ³•': 'Overlap Weights', 'ATE/ATO': result_overlap['ato'], \n",
    "     'æ ‡å‡†è¯¯': result_overlap['se'], 'ç›®æ ‡': 'ATO'},\n",
    "    {'æ–¹æ³•': 'Stabilized Weights', 'ATE/ATO': result_stabilized['ate'], \n",
    "     'æ ‡å‡†è¯¯': result_stabilized['se'], 'ç›®æ ‡': 'ATE'},\n",
    "    {'æ–¹æ³•': 'CRUMP Bounds', 'ATE/ATO': ate_crump, \n",
    "     'æ ‡å‡†è¯¯': se_crump, 'ç›®æ ‡': 'ATE (optimal trim)'},\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"æ–¹æ³•æ¯”è¾ƒæ±‡æ€» (çœŸå® ATE = 5.0)\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'ä¼°è®¡å€¼':<12} {'æ ‡å‡†è¯¯':<12} {'ä¼°è®¡ç›®æ ‡':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for m in methods_comparison:\n",
    "    print(f\"{m['æ–¹æ³•']:<25} {m['ATE/ATO']:<12.3f} {m['æ ‡å‡†è¯¯']:<12.3f} {m['ç›®æ ‡']:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ¯”è¾ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ä¼°è®¡å€¼å’Œç½®ä¿¡åŒºé—´\n",
    "ax1 = axes[0]\n",
    "methods = [m['æ–¹æ³•'] for m in methods_comparison]\n",
    "estimates = [m['ATE/ATO'] for m in methods_comparison]\n",
    "ses = [m['æ ‡å‡†è¯¯'] for m in methods_comparison]\n",
    "\n",
    "y_pos = np.arange(len(methods))\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "ax1.barh(y_pos, estimates, xerr=[1.96*s for s in ses], \n",
    "         color=colors, capsize=5, alpha=0.7)\n",
    "ax1.axvline(x=5.0, color='red', linestyle='--', linewidth=2, label='çœŸå® ATE = 5.0')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(methods)\n",
    "ax1.set_xlabel('ä¼°è®¡å€¼ Â± 95% CI')\n",
    "ax1.set_title('ä¸åŒæ–¹æ³•çš„ä¼°è®¡å€¼å’Œç½®ä¿¡åŒºé—´')\n",
    "ax1.legend()\n",
    "\n",
    "# æ ‡å‡†è¯¯æ¯”è¾ƒ\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(range(len(methods)), ses, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(methods)))\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.set_ylabel('æ ‡å‡†è¯¯')\n",
    "ax2.set_title('ä¸åŒæ–¹æ³•çš„æ ‡å‡†è¯¯')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, se in zip(bars, ses):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "            f'{se:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: æ¨¡æ‹Ÿç ”ç©¶ - å“ªç§æ–¹æ³•æœ€å¥½ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_study(n_simulations=200, n_samples=1000):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿç ”ç©¶ï¼šæ¯”è¾ƒä¸åŒæ–¹æ³•çš„åå·®å’Œè¦†ç›–ç‡\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'Standard IPW': {'estimates': [], 'covered': []},\n",
    "        'Trimming (0.1)': {'estimates': [], 'covered': []},\n",
    "        'Overlap Weights': {'estimates': [], 'covered': []},\n",
    "        'Stabilized Weights': {'estimates': [], 'covered': []},\n",
    "    }\n",
    "    \n",
    "    true_ate = 5.0\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        # ç”Ÿæˆæ•°æ®\n",
    "        data = generate_extreme_propensity_data(n=n_samples, seed=sim)\n",
    "        Y, T, e = data['Y'], data['T'], data['true_propensity']\n",
    "        \n",
    "        # Standard IPW\n",
    "        ate, se = standard_ipw(Y, T, e)\n",
    "        results['Standard IPW']['estimates'].append(ate)\n",
    "        results['Standard IPW']['covered'].append(\n",
    "            (true_ate >= ate - 1.96*se) and (true_ate <= ate + 1.96*se)\n",
    "        )\n",
    "        \n",
    "        # Trimming\n",
    "        ate, se, _ = ipw_with_trimming(Y, T, e, trim_threshold=0.1)\n",
    "        results['Trimming (0.1)']['estimates'].append(ate)\n",
    "        results['Trimming (0.1)']['covered'].append(\n",
    "            (true_ate >= ate - 1.96*se) and (true_ate <= ate + 1.96*se)\n",
    "        )\n",
    "        \n",
    "        # Overlap Weights (æ³¨æ„ï¼šä¼°è®¡ ATOï¼Œä¸æ˜¯ ATE)\n",
    "        result = ipw_with_overlap_weights(Y, T, e)\n",
    "        results['Overlap Weights']['estimates'].append(result['ato'])\n",
    "        results['Overlap Weights']['covered'].append(\n",
    "            (true_ate >= result['ato'] - 1.96*result['se']) and \n",
    "            (true_ate <= result['ato'] + 1.96*result['se'])\n",
    "        )\n",
    "        \n",
    "        # Stabilized Weights\n",
    "        result = ipw_with_stabilized_weights(Y, T, e)\n",
    "        results['Stabilized Weights']['estimates'].append(result['ate'])\n",
    "        results['Stabilized Weights']['covered'].append(\n",
    "            (true_ate >= result['ate'] - 1.96*result['se']) and \n",
    "            (true_ate <= result['ate'] + 1.96*result['se'])\n",
    "        )\n",
    "    \n",
    "    # æ±‡æ€»\n",
    "    summary = []\n",
    "    for method, res in results.items():\n",
    "        estimates = np.array(res['estimates'])\n",
    "        summary.append({\n",
    "            'method': method,\n",
    "            'mean': np.mean(estimates),\n",
    "            'bias': np.mean(estimates) - true_ate,\n",
    "            'std': np.std(estimates),\n",
    "            'rmse': np.sqrt(np.mean((estimates - true_ate)**2)),\n",
    "            'coverage': np.mean(res['covered'])\n",
    "        })\n",
    "    \n",
    "    return summary, results\n",
    "\n",
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "print(\"è¿è¡Œæ¨¡æ‹Ÿç ”ç©¶ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...\")\n",
    "summary, sim_results = simulation_study(n_simulations=200, n_samples=1000)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"æ¨¡æ‹Ÿç ”ç©¶ç»“æœ (200 æ¬¡æ¨¡æ‹Ÿ, çœŸå® ATE = 5.0)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'å‡å€¼':<10} {'åå·®':<10} {'æ ‡å‡†å·®':<10} {'RMSE':<10} {'è¦†ç›–ç‡':<10}\")\n",
    "print(\"-\" * 75)\n",
    "for s in summary:\n",
    "    print(f\"{s['method']:<25} {s['mean']:<10.3f} {s['bias']:<10.3f} {s['std']:<10.3f} \"\n",
    "          f\"{s['rmse']:<10.3f} {s['coverage']:<10.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ¨¡æ‹Ÿç»“æœ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "methods = [s['method'] for s in summary]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "# ä¼°è®¡åˆ†å¸ƒ\n",
    "ax1 = axes[0]\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "    estimates = sim_results[method]['estimates']\n",
    "    ax1.hist(estimates, bins=20, alpha=0.5, label=method, color=color)\n",
    "ax1.axvline(x=5.0, color='black', linestyle='--', linewidth=2, label='çœŸå® ATE')\n",
    "ax1.set_xlabel('ATE ä¼°è®¡')\n",
    "ax1.set_ylabel('é¢‘æ•°')\n",
    "ax1.set_title('ä¼°è®¡å€¼åˆ†å¸ƒ')\n",
    "ax1.legend(fontsize=8)\n",
    "\n",
    "# RMSE æ¯”è¾ƒ\n",
    "ax2 = axes[1]\n",
    "rmses = [s['rmse'] for s in summary]\n",
    "bars = ax2.bar(range(len(methods)), rmses, color=colors)\n",
    "ax2.set_xticks(range(len(methods)))\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('RMSE æ¯”è¾ƒ')\n",
    "\n",
    "# è¦†ç›–ç‡æ¯”è¾ƒ\n",
    "ax3 = axes[2]\n",
    "coverages = [s['coverage'] for s in summary]\n",
    "bars = ax3.bar(range(len(methods)), coverages, color=colors)\n",
    "ax3.axhline(y=0.95, color='red', linestyle='--', label='åä¹‰æ°´å¹³ 95%')\n",
    "ax3.set_xticks(range(len(methods)))\n",
    "ax3.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax3.set_ylabel('95% CI è¦†ç›–ç‡')\n",
    "ax3.set_title('è¦†ç›–ç‡æ¯”è¾ƒ')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: æœ€ä½³å®è·µæµç¨‹\n",
    "\n",
    "### ğŸ§ª TODO ç»ƒä¹ : å®ç°å®Œæ•´çš„è¯Šæ–­å’Œå¤„ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>### ğŸ§ª TODO ç»ƒä¹ : å®ç°å®Œæ•´çš„è¯Šæ–­å’Œå¤„ç†æµç¨‹\n\nè¯·å®ç°ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å€¾å‘å¾—åˆ†å¤„ç†æµç¨‹ï¼Œèƒ½å¤Ÿï¼š\n1. ä¼°è®¡å€¾å‘å¾—åˆ†\n2. è¯Šæ–­æç«¯å€¼é—®é¢˜\n3. è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤„ç†æ–¹æ³•\n4. è¾“å‡ºä¼°è®¡ç»“æœ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def propensity_score_pipeline(Y, T, X, method='auto'):\n    \"\"\"\n    å®Œæ•´çš„å€¾å‘å¾—åˆ†å¤„ç†æµç¨‹\n\n    Args:\n        Y: ç»“æœ\n        T: å¤„ç†\n        X: åå˜é‡\n        method: å¤„ç†æ–¹æ³• ('auto', 'trimming', 'overlap', 'stabilized')\n\n    Returns:\n        result: ä¼°è®¡ç»“æœå’Œè¯Šæ–­ä¿¡æ¯\n    \"\"\"\n    # Step 1 - ä¼°è®¡å€¾å‘å¾—åˆ†\n    # ä½¿ç”¨ LogisticRegression æ‹Ÿåˆ\n    ps_model = LogisticRegression(max_iter=1000)\n    ps_model.fit(X, T)\n    e = ps_model.predict_proba(X)[:, 1]\n\n    # Step 2 - è¯Šæ–­\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Step 1: å€¾å‘å¾—åˆ†ä¼°è®¡å®Œæˆ\")\n    print(\"=\" * 60)\n    diagnose_propensity_scores(e, T)\n\n    # Step 3 - è‡ªåŠ¨é€‰æ‹©æ–¹æ³•\n    # æ ¹æ® ESS æŸå¤±ç¨‹åº¦é€‰æ‹©æ–¹æ³•\n    # - ESS æŸå¤± > 70%: overlap\n    # - ESS æŸå¤± 40-70%: trimming\n    # - ESS æŸå¤± < 40%: stabilized\n\n    if method == 'auto':\n        # è®¡ç®— ESS æŸå¤±\n        weights_t = 1 / e[T==1]\n        weights_c = 1 / (1 - e[T==0])\n        ess_t = calculate_ess(weights_t)\n        ess_c = calculate_ess(weights_c)\n        \n        ess_loss_t = 1 - ess_t / (T==1).sum()\n        ess_loss_c = 1 - ess_c / (T==0).sum()\n        ess_loss = max(ess_loss_t, ess_loss_c)\n        \n        if ess_loss > 0.7:\n            method = 'overlap'\n        elif ess_loss > 0.4:\n            method = 'trimming'\n        else:\n            method = 'stabilized'\n\n    print(f\"\\né€‰æ‹©çš„æ–¹æ³•: {method}\")\n\n    # Step 4 - æ‰§è¡Œä¼°è®¡\n    # æ ¹æ® method è°ƒç”¨ç›¸åº”å‡½æ•°\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"Step 2: ä½¿ç”¨ {method} æ–¹æ³•ä¼°è®¡\")\n    print(\"=\" * 60)\n    \n    if method == 'trimming':\n        ate, se, n_trimmed = ipw_with_trimming(Y, T, e, trim_threshold=0.1)\n        result = {\n            'method': 'Trimming (Î±=0.1)',\n            'estimate': ate,\n            'se': se,\n            'ci_lower': ate - 1.96 * se,\n            'ci_upper': ate + 1.96 * se,\n            'n_trimmed': n_trimmed\n        }\n        print(f\"  ATE ä¼°è®¡: {ate:.3f}\")\n        print(f\"  æ ‡å‡†è¯¯: {se:.3f}\")\n        print(f\"  95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\")\n        print(f\"  ä¿®å‰ªæ ·æœ¬: {n_trimmed} ({n_trimmed/len(Y):.1%})\")\n        \n    elif method == 'overlap':\n        result_dict = ipw_with_overlap_weights(Y, T, e)\n        result = {\n            'method': 'Overlap Weights',\n            'estimate': result_dict['ato'],\n            'se': result_dict['se'],\n            'ci_lower': result_dict['ato'] - 1.96 * result_dict['se'],\n            'ci_upper': result_dict['ato'] + 1.96 * result_dict['se'],\n            'ess_treated': result_dict['ess_treated'],\n            'ess_control': result_dict['ess_control']\n        }\n        print(f\"  ATO ä¼°è®¡: {result['estimate']:.3f}\")\n        print(f\"  æ ‡å‡†è¯¯: {result['se']:.3f}\")\n        print(f\"  95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\")\n        print(f\"  å¤„ç†ç»„ ESS: {result['ess_treated']:.1f}\")\n        print(f\"  æ§åˆ¶ç»„ ESS: {result['ess_control']:.1f}\")\n        \n    elif method == 'stabilized':\n        result_dict = ipw_with_stabilized_weights(Y, T, e)\n        result = {\n            'method': 'Stabilized Weights',\n            'estimate': result_dict['ate'],\n            'se': result_dict['se'],\n            'ci_lower': result_dict['ate'] - 1.96 * result_dict['se'],\n            'ci_upper': result_dict['ate'] + 1.96 * result_dict['se'],\n            'ess_treated': result_dict['ess_treated'],\n            'ess_control': result_dict['ess_control']\n        }\n        print(f\"  ATE ä¼°è®¡: {result['estimate']:.3f}\")\n        print(f\"  æ ‡å‡†è¯¯: {result['se']:.3f}\")\n        print(f\"  95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\")\n        print(f\"  å¤„ç†ç»„ ESS: {result['ess_treated']:.1f}\")\n        print(f\"  æ§åˆ¶ç»„ ESS: {result['ess_control']:.1f}\")\n    else:\n        result = None\n\n    return result, e\n\nprint(\"âœ… propensity_score_pipeline å‡½æ•°å®šä¹‰å®Œæˆï¼ˆå·²å®Œæˆ TODO éƒ¨åˆ†ï¼‰\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}