{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ ç¬¬ä¸€ç«  ç»ƒä¹  3: æ··æ·†åå·® (Confounding Bias)\n",
    "\n",
    "---\n",
    "\n",
    "## è®©åå·®ã€Œç°å½¢ã€\n",
    "\n",
    "åœ¨å‰ä¸¤ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“äº†ï¼š\n",
    "- æ½œåœ¨ç»“æœæ¡†æ¶å‘Šè¯‰æˆ‘ä»¬ã€Œä»€ä¹ˆæ˜¯å› æœæ•ˆåº”ã€\n",
    "- å› æœå›¾å‘Šè¯‰æˆ‘ä»¬ã€Œä¸ºä»€ä¹ˆä¼šæœ‰åå·®ã€\n",
    "\n",
    "ä½†ä¸€ä¸ªå…³é”®é—®é¢˜è¿˜æ²¡å›ç­”ï¼š**åå·®åˆ°åº•æœ‰å¤šå¤§ï¼Ÿèƒ½é‡åŒ–å—ï¼Ÿ**\n",
    "\n",
    "### ä¸€ä¸ªè®©ä½ å´©æºƒçš„åœºæ™¯ ğŸ˜±\n",
    "\n",
    "ä½ æ˜¯ä¸€ååŒ»å­¦ç ”ç©¶è€…ï¼Œå‘ç°äº†ä¸€ä¸ªæƒŠäººçš„ç»“è®ºï¼š\n",
    "\n",
    "> \"å–çº¢é…’çš„äººæ¯”ä¸å–é…’çš„äººå¿ƒè„ç—…å‘ç—…ç‡ä½ 30%ï¼\"\n",
    "\n",
    "æ–°é—»æ ‡é¢˜å·²ç»æƒ³å¥½äº†ï¼š*ã€Šç§‘å­¦è¯æ˜ï¼šçº¢é…’æ˜¯å¿ƒè„çš„ä¿æŠ¤ç¥ã€‹*\n",
    "\n",
    "ä½†ç­‰ç­‰...ä½ çš„åŒäº‹é—®äº†å‡ ä¸ªé—®é¢˜ï¼š\n",
    "\n",
    "- å–çº¢é…’çš„äººæ˜¯ä¸æ˜¯æ”¶å…¥æ›´é«˜ï¼Ÿï¼ˆèƒ½ä¹°å¾—èµ·çº¢é…’ï¼‰\n",
    "- æ”¶å…¥é«˜çš„äººæ˜¯ä¸æ˜¯åŒ»ç–—æ¡ä»¶æ›´å¥½ï¼Ÿ\n",
    "- é‚£è¿™ 30% çš„å·®å¼‚æœ‰å¤šå°‘æ˜¯çº¢é…’çš„åŠŸåŠ³ï¼Œå¤šå°‘æ˜¯æ”¶å…¥çš„åŠŸåŠ³ï¼Ÿ\n",
    "\n",
    "è¿™å°±æ˜¯**æ··æ·†åå·®**çš„å¯æ€•ä¹‹å¤„â€”â€”å®ƒä¼šè®©æ— æ•ˆçš„æ²»ç–—çœ‹èµ·æ¥æœ‰æ•ˆï¼Œä¹Ÿä¼šè®©æœ‰æ•ˆçš„æ²»ç–—çœ‹èµ·æ¥æ— æ•ˆï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£æ··æ·†åå·®çš„æ•°å­¦å…¬å¼\n",
    "2. å­¦ä¼šé‡åŒ–åå·®çš„å¤§å°\n",
    "3. æ·±å…¥ç†è§£ Simpson's Paradox\n",
    "4. æŒæ¡æ•æ„Ÿæ€§åˆ†ææ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§® Part 1: æ··æ·†åå·®å…¬å¼ (Omitted Variable Bias)\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "å½“æˆ‘ä»¬é—æ¼äº†ä¸€ä¸ªæ··æ·†å˜é‡ Xï¼Œæœ´ç´ ä¼°è®¡å’ŒçœŸå®æ•ˆåº”ä¹‹é—´çš„åå·®ä¸ºï¼š\n",
    "\n",
    "$$\\text{Bias} = \\gamma \\times \\delta$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\gamma$ï¼šæ··æ·†å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n",
    "- $\\delta$ï¼šæ··æ·†å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n",
    "\n",
    "### ç›´è§‰ç†è§£\n",
    "\n",
    "åå·® = (X å¯¹ Y çš„å½±å“) Ã— (X ä¸ T çš„å…³è”)\n",
    "\n",
    "**ä¸¤ä¸ªä¹˜æ•°éƒ½ä¸ä¸ºé›¶æ—¶ï¼Œæ‰ä¼šæœ‰åå·®ï¼**\n",
    "\n",
    "| æƒ…å†µ | Î³ (Xâ†’Y) | Î´ (X-T) | åå·® | ä¾‹å­ |\n",
    "|-----|---------|---------|------|------|\n",
    "| æ— åå·® | 0 | ä»»æ„ | 0 | X ä¸å½±å“ Y |\n",
    "| æ— åå·® | ä»»æ„ | 0 | 0 | X ä¸ T æ— å…³ |\n",
    "| æ­£å‘åå·® | + | + | + | é«˜ä¼°æ•ˆåº” |\n",
    "| æ­£å‘åå·® | - | - | + | é«˜ä¼°æ•ˆåº” |\n",
    "| è´Ÿå‘åå·® | + | - | - | ä½ä¼°æ•ˆåº” |\n",
    "| è´Ÿå‘åå·® | - | + | - | ä½ä¼°æ•ˆåº” |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¬ åŠ¨æ‰‹éªŒè¯åå·®å…¬å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿæ•°æ®\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "# DAG: X â†’ T, X â†’ Y, T â†’ Y\n",
    "# X æ˜¯æ··æ·†å˜é‡\n",
    "\n",
    "# ç”Ÿæˆæ··æ·†å˜é‡ X\n",
    "X = np.random.randn(n)\n",
    "\n",
    "# ç”Ÿæˆå¤„ç† Tï¼ˆå— X å½±å“ï¼‰\n",
    "# delta = 1.5ï¼ˆX å¯¹ T çš„å½±å“ç³»æ•°ï¼‰\n",
    "T = (np.random.randn(n) + 1.5 * X > 0).astype(int)\n",
    "\n",
    "# ç”Ÿæˆç»“æœ Yï¼ˆå— T å’Œ X å½±å“ï¼‰\n",
    "# çœŸå® ATE = 2, gamma = 1.5\n",
    "true_ate = 2.0\n",
    "gamma_true = 1.5\n",
    "Y = 5 + true_ate * T + gamma_true * X + np.random.randn(n) * 0.5\n",
    "\n",
    "df = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "\n",
    "print(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "print(f\"   æ ·æœ¬é‡: {n}\")\n",
    "print(f\"   çœŸå® ATE: {true_ate}\")\n",
    "print(f\"   X å¯¹ Y çš„çœŸå®æ•ˆåº” (Î³): {gamma_true}\")\n",
    "print(f\"   å¤„ç†ç»„æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confounding_bias(df: pd.DataFrame, confound_var: str = 'X') -> dict:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ··æ·†åå·®çš„å„ä¸ªç»„æˆéƒ¨åˆ†\n",
    "    \n",
    "    Omitted Variable Bias å…¬å¼: bias = Î³ Ã— Î´\n",
    "    \n",
    "    - Î³ (gamma): æ··æ·†å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n",
    "    - Î´ (delta): æ··æ·†å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n",
    "    \"\"\"\n",
    "    # æœ´ç´ ä¼°è®¡ï¼ˆä¸æ§åˆ¶ Xï¼‰\n",
    "    model_naive = LinearRegression()\n",
    "    model_naive.fit(df[['T']], df['Y'])\n",
    "    naive_estimate = model_naive.coef_[0]\n",
    "    \n",
    "    # è°ƒæ•´ä¼°è®¡ï¼ˆæ§åˆ¶ Xï¼‰\n",
    "    model_adjusted = LinearRegression()\n",
    "    model_adjusted.fit(df[['T', confound_var]], df['Y'])\n",
    "    adjusted_estimate = model_adjusted.coef_[0]\n",
    "    \n",
    "    # TODO: è®¡ç®— gammaï¼ˆX å¯¹ Y çš„æ•ˆåº”ï¼Œæ§åˆ¶ Tï¼‰\n",
    "    # æç¤º: è¿™æ˜¯ model_adjusted ä¸­ X çš„ç³»æ•°\n",
    "    gamma = None  # ğŸ‘ˆ ä½ çš„ä»£ç : model_adjusted.coef_[1]\n",
    "    \n",
    "    # TODO: è®¡ç®— deltaï¼ˆX ä¸ T çš„å…³è”ï¼‰\n",
    "    # å›å½’ T ~ Xï¼Œå– X çš„ç³»æ•°\n",
    "    model_delta = LinearRegression()\n",
    "    model_delta.fit(df[[confound_var]], df['T'])\n",
    "    delta = None  # ğŸ‘ˆ ä½ çš„ä»£ç : model_delta.coef_[0]\n",
    "    \n",
    "    # TODO: è®¡ç®—ç†è®ºåå·®\n",
    "    theoretical_bias = None  # ğŸ‘ˆ ä½ çš„ä»£ç : gamma * delta\n",
    "    \n",
    "    # å®é™…åå·®ï¼ˆä»ä¼°è®¡å€¼ç›´æ¥ç®—ï¼‰\n",
    "    actual_bias = naive_estimate - adjusted_estimate\n",
    "    \n",
    "    return {\n",
    "        'gamma': gamma,\n",
    "        'delta': delta,\n",
    "        'theoretical_bias': theoretical_bias,\n",
    "        'actual_bias': actual_bias,\n",
    "        'naive_estimate': naive_estimate,\n",
    "        'adjusted_estimate': adjusted_estimate\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef calculate_confounding_bias(df: pd.DataFrame, confound_var: str = 'X') -> dict:\n    \"\"\"è®¡ç®—æ··æ·†åå·®çš„å„ä¸ªç»„æˆéƒ¨åˆ†\"\"\"\n    from sklearn.linear_model import LinearRegression\n    \n    # æœ´ç´ ä¼°è®¡ï¼ˆä¸æ§åˆ¶ Xï¼‰\n    model_naive = LinearRegression()\n    model_naive.fit(df[['T']], df['Y'])\n    naive_estimate = model_naive.coef_[0]\n    \n    # è°ƒæ•´ä¼°è®¡ï¼ˆæ§åˆ¶ Xï¼‰\n    model_adjusted = LinearRegression()\n    model_adjusted.fit(df[['T', confound_var]], df['Y'])\n    adjusted_estimate = model_adjusted.coef_[0]\n    \n    # gammaï¼ˆX å¯¹ Y çš„æ•ˆåº”ï¼Œæ§åˆ¶ Tï¼‰\n    gamma = model_adjusted.coef_[1]\n    \n    # deltaï¼ˆX ä¸ T çš„å…³è”ï¼‰\n    model_delta = LinearRegression()\n    model_delta.fit(df[[confound_var]], df['T'])\n    delta = model_delta.coef_[0]\n    \n    # ç†è®ºåå·® = gamma Ã— delta\n    theoretical_bias = gamma * delta\n    \n    # å®é™…åå·®\n    actual_bias = naive_estimate - adjusted_estimate\n    \n    return {\n        'gamma': gamma,\n        'delta': delta,\n        'theoretical_bias': theoretical_bias,\n        'actual_bias': actual_bias,\n        'naive_estimate': naive_estimate,\n        'adjusted_estimate': adjusted_estimate\n    }\n```\n\n**è§£é‡Š**ï¼š\n- **Î³ (gamma)**: X å¯¹ Y çš„åå›å½’ç³»æ•°ï¼Œè¡¡é‡æ§åˆ¶ T å X å¯¹ Y çš„å½±å“\n- **Î´ (delta)**: X å¯¹ T çš„å›å½’ç³»æ•°ï¼Œè¡¡é‡ X å’Œ T çš„å…³è”å¼ºåº¦  \n- **åå·®å…¬å¼**: Bias = Î³ Ã— Î´ï¼ˆé—æ¼å˜é‡åå·®çš„ç»å…¸å…¬å¼ï¼‰\n- **éªŒè¯**: theoretical_bias åº”è¯¥éå¸¸æ¥è¿‘ actual_bias\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä½ çš„ä»£ç \n",
    "result = calculate_confounding_bias(df)\n",
    "\n",
    "if result['gamma'] is not None:\n",
    "    print(\"ğŸ”¬ æ··æ·†åå·®åˆ†è§£:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nğŸ“ åå·®å…¬å¼ç»„æˆéƒ¨åˆ†:\")\n",
    "    print(f\"   Î³ (X å¯¹ Y çš„æ•ˆåº”): {result['gamma']:.4f}\")\n",
    "    print(f\"   Î´ (X ä¸ T çš„å…³è”): {result['delta']:.4f}\")\n",
    "    print(f\"\\nğŸ“Š åå·®è®¡ç®—:\")\n",
    "    print(f\"   ç†è®ºåå·® (Î³ Ã— Î´): {result['theoretical_bias']:.4f}\")\n",
    "    print(f\"   å®é™…åå·®: {result['actual_bias']:.4f}\")\n",
    "    print(f\"\\nğŸ“ˆ æ•ˆåº”ä¼°è®¡:\")\n",
    "    print(f\"   æœ´ç´ ä¼°è®¡: {result['naive_estimate']:.4f}\")\n",
    "    print(f\"   è°ƒæ•´ä¼°è®¡: {result['adjusted_estimate']:.4f}\")\n",
    "    print(f\"   çœŸå®æ•ˆåº”: {true_ate:.4f}\")\n",
    "    \n",
    "    # éªŒè¯å…¬å¼\n",
    "    if abs(result['theoretical_bias'] - result['actual_bias']) < 0.1:\n",
    "        print(f\"\\nâœ… å…¬å¼éªŒè¯æˆåŠŸï¼ç†è®ºåå·® â‰ˆ å®é™…åå·®\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ calculate_confounding_bias å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ å…³é”®æ´å¯Ÿ\n",
    "\n",
    "ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹åˆ°ï¼š\n",
    "\n",
    "1. **åå·®çš„æ–¹å‘**ï¼šÎ³ å’Œ Î´ åŒå·æ—¶ï¼Œåå·®ä¸ºæ­£ï¼ˆé«˜ä¼°æ•ˆåº”ï¼‰\n",
    "2. **åå·®çš„å¤§å°**ï¼šå–å†³äº Î³ å’Œ Î´ çš„ä¹˜ç§¯\n",
    "3. **å…¬å¼çš„ç²¾ç¡®æ€§**ï¼šç†è®ºåå·®å’Œå®é™…åå·®å‡ ä¹å®Œå…¨ä¸€è‡´ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ Part 2: æ··æ·†å¼ºåº¦å®éªŒ\n",
    "\n",
    "è®©æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶ï¼šæ··æ·†å¼ºåº¦å¦‚ä½•å½±å“ä¼°è®¡åå·®ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef experiment_confounding_strength(n: int = 2000, true_ate: float = 2.0,\n                                   confounding_strengths: list = None, seed: int = 42):\n    \"\"\"å®éªŒä¸åŒæ··æ·†å¼ºåº¦å¯¹ä¼°è®¡çš„å½±å“\"\"\"\n    if confounding_strengths is None:\n        confounding_strengths = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n    \n    np.random.seed(seed)\n    results = []\n    \n    for strength in confounding_strengths:\n        X = np.random.randn(n)\n        \n        # ç”Ÿæˆå¤„ç† Tï¼ˆå— X å½±å“ï¼Œå¼ºåº¦ç”± strength æ§åˆ¶ï¼‰\n        propensity = 1 / (1 + np.exp(-strength * X))\n        T = np.random.binomial(1, propensity)\n        \n        # ç”Ÿæˆç»“æœ Yï¼ˆå— T å’Œ X å½±å“ï¼‰\n        Y = 5 + true_ate * T + strength * X + np.random.randn(n) * 0.5\n        \n        df_temp = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n        \n        # æœ´ç´ ä¼°è®¡\n        naive_est = df_temp[df_temp['T']==1]['Y'].mean() - df_temp[df_temp['T']==0]['Y'].mean()\n        \n        # è°ƒæ•´ä¼°è®¡\n        from sklearn.linear_model import LinearRegression\n        model = LinearRegression()\n        model.fit(df_temp[['T', 'X']], df_temp['Y'])\n        adjusted_est = model.coef_[0]\n        \n        results.append({\n            'confounding_strength': strength,\n            'naive_estimate': naive_est,\n            'adjusted_estimate': adjusted_est,\n            'naive_bias': naive_est - true_ate,\n            'adjusted_bias': adjusted_est - true_ate\n        })\n    \n    return pd.DataFrame(results)\n```\n\n**è§£é‡Š**ï¼š\n- **æ··æ·†å¼ºåº¦å‚æ•°åŒ–**: strength åŒæ—¶æ§åˆ¶ Xâ†’T å’Œ Xâ†’Y çš„å¼ºåº¦\n- **è§‚å¯Ÿ**: æ··æ·†å¼ºåº¦è¶Šå¤§ï¼Œæœ´ç´ ä¼°è®¡åå·®è¶Šå¤§\n- **ç¨³å¥æ€§**: è°ƒæ•´ä¼°è®¡åœ¨å„ç§å¼ºåº¦ä¸‹éƒ½æ¥è¿‘çœŸå®å€¼\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_confounding_strength(\n",
    "    n: int = 2000,\n",
    "    true_ate: float = 2.0,\n",
    "    confounding_strengths: list = None,\n",
    "    seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å®éªŒä¸åŒæ··æ·†å¼ºåº¦å¯¹ä¼°è®¡çš„å½±å“\n",
    "    \n",
    "    æ··æ·†å¼ºåº¦åŒæ—¶å½±å“:\n",
    "    - P(T=1|X): å€¾å‘å¾—åˆ†\n",
    "    - Y ä¸­ X çš„ç³»æ•°\n",
    "    \"\"\"\n",
    "    if confounding_strengths is None:\n",
    "        confounding_strengths = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    results = []\n",
    "    \n",
    "    for strength in confounding_strengths:\n",
    "        # ç”Ÿæˆæ··æ·†æ•°æ®\n",
    "        X = np.random.randn(n)\n",
    "        \n",
    "        # TODO: ç”Ÿæˆå¤„ç† T\n",
    "        # P(T=1|X) = sigmoid(strength * X)\n",
    "        propensity = 1 / (1 + np.exp(-strength * X))\n",
    "        T = None  # ğŸ‘ˆ ä½ çš„ä»£ç : np.random.binomial(1, propensity)\n",
    "        \n",
    "        # TODO: ç”Ÿæˆç»“æœ Y\n",
    "        # Y = 5 + true_ate * T + strength * X + noise\n",
    "        Y = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "        \n",
    "        if T is not None and Y is not None:\n",
    "            df_temp = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "            \n",
    "            # è®¡ç®—æœ´ç´ ä¼°è®¡\n",
    "            naive_est = df_temp[df_temp['T']==1]['Y'].mean() - df_temp[df_temp['T']==0]['Y'].mean()\n",
    "            \n",
    "            # è®¡ç®—è°ƒæ•´ä¼°è®¡\n",
    "            model = LinearRegression()\n",
    "            model.fit(df_temp[['T', 'X']], df_temp['Y'])\n",
    "            adjusted_est = model.coef_[0]\n",
    "            \n",
    "            results.append({\n",
    "                'confounding_strength': strength,\n",
    "                'naive_estimate': naive_est,\n",
    "                'adjusted_estimate': adjusted_est,\n",
    "                'naive_bias': naive_est - true_ate,\n",
    "                'adjusted_bias': adjusted_est - true_ate\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results) if results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œå®éªŒ\n",
    "exp_results = experiment_confounding_strength(n=3000)\n",
    "\n",
    "if exp_results is not None and not exp_results.empty:\n",
    "    print(\"ğŸ“Š æ··æ·†å¼ºåº¦å®éªŒç»“æœ:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(exp_results.round(3).to_string(index=False))\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å›¾1: ä¼°è®¡å€¼ vs æ··æ·†å¼ºåº¦\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(exp_results['confounding_strength'], exp_results['naive_estimate'], \n",
    "             'o-', label='æœ´ç´ ä¼°è®¡', color='red', markersize=8)\n",
    "    ax1.plot(exp_results['confounding_strength'], exp_results['adjusted_estimate'], \n",
    "             's-', label='è°ƒæ•´ä¼°è®¡', color='green', markersize=8)\n",
    "    ax1.axhline(2.0, color='blue', linestyle='--', label='çœŸå® ATE = 2', linewidth=2)\n",
    "    ax1.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax1.set_ylabel('ATE ä¼°è®¡', fontsize=12)\n",
    "    ax1.set_title('ä¼°è®¡å€¼éšæ··æ·†å¼ºåº¦çš„å˜åŒ–', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å›¾2: åå·® vs æ··æ·†å¼ºåº¦\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(exp_results['confounding_strength'] - 0.1, exp_results['naive_bias'], \n",
    "            width=0.2, label='æœ´ç´ ä¼°è®¡åå·®', color='red', alpha=0.7)\n",
    "    ax2.bar(exp_results['confounding_strength'] + 0.1, exp_results['adjusted_bias'], \n",
    "            width=0.2, label='è°ƒæ•´ä¼°è®¡åå·®', color='green', alpha=0.7)\n",
    "    ax2.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "    ax2.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax2.set_ylabel('åå·®', fontsize=12)\n",
    "    ax2.set_title('åå·®éšæ··æ·†å¼ºåº¦çš„å˜åŒ–', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "    print(\"   1. æ··æ·†å¼ºåº¦ä¸º 0 æ—¶ï¼Œæœ´ç´ ä¼°è®¡ä¹Ÿæ˜¯æ— åçš„\")\n",
    "    print(\"   2. æ··æ·†å¼ºåº¦è¶Šå¤§ï¼Œæœ´ç´ ä¼°è®¡åå·®è¶Šå¤§\")\n",
    "    print(\"   3. è°ƒæ•´ä¼°è®¡å§‹ç»ˆæ¥è¿‘çœŸå®å€¼ï¼\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ experiment_confounding_strength å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸª Part 3: Simpson's Paradoxï¼ˆè¾›æ™®æ£®æ‚–è®ºï¼‰\n",
    "\n",
    "### ä¸€ä¸ªçœŸå®çš„æ•…äº‹\n",
    "\n",
    "1973 å¹´ï¼Œä¼¯å…‹åˆ©å¤§å­¦è¢«æŒ‡æ§æ€§åˆ«æ­§è§†ï¼š\n",
    "\n",
    "| | ç”³è¯·äºº | å½•å–ç‡ |\n",
    "|--|--------|--------|\n",
    "| ç”·æ€§ | 8442 | 44% |\n",
    "| å¥³æ€§ | 4321 | 35% |\n",
    "\n",
    "çœ‹èµ·æ¥ç¡®å®æ­§è§†å¥³æ€§ï¼\n",
    "\n",
    "ä½†ä»”ç»†åˆ†ææ¯ä¸ªç³»çš„æ•°æ®åï¼Œå‘ç°**å¤§å¤šæ•°ç³»çš„å¥³æ€§å½•å–ç‡åè€Œæ›´é«˜**ï¼\n",
    "\n",
    "æ€ä¹ˆå›äº‹ï¼ŸğŸ¤¯\n",
    "\n",
    "åŸæ¥ï¼š\n",
    "- å¥³æ€§å€¾å‘äºç”³è¯·å½•å–ç‡ä½çš„ã€Œçƒ­é—¨ç³»ã€ï¼ˆå¦‚å¿ƒç†å­¦ï¼‰\n",
    "- ç”·æ€§å€¾å‘äºç”³è¯·å½•å–ç‡é«˜çš„ã€Œå†·é—¨ç³»ã€ï¼ˆå¦‚å·¥ç¨‹å­¦ï¼‰\n",
    "\n",
    "ã€Œé™¢ç³»ã€æ˜¯æ··æ·†å˜é‡ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simpson_paradox_data(n_per_group: int = 500, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºå±•ç¤º Simpson's Paradox çš„æ•°æ®\n",
    "    \n",
    "    åœºæ™¯: ç ”ç©¶æŸè¯ç‰©å¯¹åº·å¤ç‡çš„å½±å“\n",
    "    - æœ‰ä¸¤ä¸ªåŒ»é™¢ (A å’Œ B)\n",
    "    - åŒ»é™¢ A æ¥æ”¶é‡ç—‡æ‚£è€…å¤šï¼Œè¯ç‰©ä½¿ç”¨ç‡é«˜\n",
    "    - åŒ»é™¢ B æ¥æ”¶è½»ç—‡æ‚£è€…å¤šï¼Œè¯ç‰©ä½¿ç”¨ç‡ä½\n",
    "    - è¯ç‰©å®é™…ä¸Šæœ‰æ­£æ•ˆåº”ï¼\n",
    "    \n",
    "    è®¾è®¡æ•°æ®ä½¿å¾—:\n",
    "    - æ•´ä½“: ç”¨è¯ç»„åº·å¤ç‡ < æœªç”¨è¯ç»„åº·å¤ç‡ï¼ˆçœ‹èµ·æ¥è¯ç‰©æœ‰å®³!ï¼‰\n",
    "    - åˆ†åŒ»é™¢: ç”¨è¯ç»„åº·å¤ç‡ > æœªç”¨è¯ç»„åº·å¤ç‡ï¼ˆè¯ç‰©å®é™…æœ‰ç›Šï¼‰\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data = []\n",
    "    \n",
    "    # TODO: åŒ»é™¢ Aï¼ˆé‡ç—‡å¤šï¼Œç”¨è¯å¤šï¼‰\n",
    "    # é‡ç—‡åŸºç¡€åº·å¤ç‡ä½ï¼ˆ30%ï¼‰ï¼Œç”¨è¯æé«˜åˆ° 50%\n",
    "    # å¤§éƒ¨åˆ†é‡ç—‡æ‚£è€…åœ¨è¿™é‡Œï¼Œå¤§éƒ¨åˆ†æ¥å—æ²»ç–—\n",
    "    \n",
    "    # åŒ»é™¢ A - ç”¨è¯ç»„\n",
    "    n_A_treated = int(n_per_group * 1.5)  # ç”¨è¯äººæ•°å¤š\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : ç”¨è¯ååº·å¤ç‡ 50%\n",
    "    recovery_A_treated = None  # np.random.binomial(1, 0.50, n_A_treated)\n",
    "    \n",
    "    for i in range(n_A_treated):\n",
    "        if recovery_A_treated is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'A (é‡ç—‡)',\n",
    "                'ç”¨è¯': 1,\n",
    "                'åº·å¤': recovery_A_treated[i],\n",
    "                'ç—…æƒ…': 'é‡ç—‡'\n",
    "            })\n",
    "    \n",
    "    # åŒ»é™¢ A - æœªç”¨è¯ç»„\n",
    "    n_A_control = int(n_per_group * 0.3)  # æœªç”¨è¯äººæ•°å°‘\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : æœªç”¨è¯åº·å¤ç‡ 30%\n",
    "    recovery_A_control = None  # np.random.binomial(1, 0.30, n_A_control)\n",
    "    \n",
    "    for i in range(n_A_control):\n",
    "        if recovery_A_control is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'A (é‡ç—‡)',\n",
    "                'ç”¨è¯': 0,\n",
    "                'åº·å¤': recovery_A_control[i],\n",
    "                'ç—…æƒ…': 'é‡ç—‡'\n",
    "            })\n",
    "    \n",
    "    # TODO: åŒ»é™¢ Bï¼ˆè½»ç—‡å¤šï¼Œç”¨è¯å°‘ï¼‰\n",
    "    # è½»ç—‡åŸºç¡€åº·å¤ç‡é«˜ï¼ˆ70%ï¼‰ï¼Œç”¨è¯æé«˜åˆ° 90%\n",
    "    \n",
    "    # åŒ»é™¢ B - ç”¨è¯ç»„\n",
    "    n_B_treated = int(n_per_group * 0.3)  # ç”¨è¯äººæ•°å°‘\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : ç”¨è¯ååº·å¤ç‡ 90%\n",
    "    recovery_B_treated = None  # np.random.binomial(1, 0.90, n_B_treated)\n",
    "    \n",
    "    for i in range(n_B_treated):\n",
    "        if recovery_B_treated is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'B (è½»ç—‡)',\n",
    "                'ç”¨è¯': 1,\n",
    "                'åº·å¤': recovery_B_treated[i],\n",
    "                'ç—…æƒ…': 'è½»ç—‡'\n",
    "            })\n",
    "    \n",
    "    # åŒ»é™¢ B - æœªç”¨è¯ç»„\n",
    "    n_B_control = int(n_per_group * 1.5)  # æœªç”¨è¯äººæ•°å¤š\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : æœªç”¨è¯åº·å¤ç‡ 70%\n",
    "    recovery_B_control = None  # np.random.binomial(1, 0.70, n_B_control)\n",
    "    \n",
    "    for i in range(n_B_control):\n",
    "        if recovery_B_control is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'B (è½»ç—‡)',\n",
    "                'ç”¨è¯': 0,\n",
    "                'åº·å¤': recovery_B_control[i],\n",
    "                'ç—…æƒ…': 'è½»ç—‡'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_simpson_paradox(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    åˆ†æ Simpson's Paradox\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: æ•´ä½“æ•ˆåº”\n",
    "    overall_treated = df[df['ç”¨è¯'] == 1]['åº·å¤'].mean()\n",
    "    overall_control = df[df['ç”¨è¯'] == 0]['åº·å¤'].mean()\n",
    "    results['æ•´ä½“-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['æ•´ä½“-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['æ•´ä½“-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç : overall_treated - overall_control\n",
    "    \n",
    "    # TODO: åŒ»é™¢ A æ•ˆåº”\n",
    "    df_A = df[df['åŒ»é™¢'] == 'A (é‡ç—‡)']\n",
    "    results['åŒ»é™¢A-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢A-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢A-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: åŒ»é™¢ B æ•ˆåº”\n",
    "    df_B = df[df['åŒ»é™¢'] == 'B (è½»ç—‡)']\n",
    "    results['åŒ»é™¢B-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢B-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢B-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¹¶åˆ†æ Simpson's Paradox æ•°æ®\n",
    "simpson_df = create_simpson_paradox_data(n_per_group=500)\n",
    "\n",
    "if simpson_df is not None and not simpson_df.empty:\n",
    "    print(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"   æ€»æ ·æœ¬é‡: {len(simpson_df)}\")\n",
    "    print(f\"\\nå„ç»„äººæ•°:\")\n",
    "    print(simpson_df.groupby(['åŒ»é™¢', 'ç”¨è¯']).size().unstack())\n",
    "    \n",
    "    analysis = analyze_simpson_paradox(simpson_df)\n",
    "    \n",
    "    if analysis.get('æ•´ä½“-æ•ˆåº”') is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸª Simpson's Paradox åˆ†æç»“æœ:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ æ•´ä½“åˆ†æï¼ˆä¸åˆ†åŒ»é™¢ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['æ•´ä½“-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['æ•´ä½“-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['æ•´ä½“-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['æ•´ä½“-æ•ˆåº”'] < 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯ä¼¼ä¹æœ‰å®³ï¼âŒ\")\n",
    "        \n",
    "        print(f\"\\nğŸ¥ åŒ»é™¢ Aï¼ˆé‡ç—‡æ‚£è€…ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢A-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢A-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['åŒ»é™¢A-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['åŒ»é™¢A-æ•ˆåº”'] > 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯æœ‰æ•ˆï¼âœ…\")\n",
    "        \n",
    "        print(f\"\\nğŸ¥ åŒ»é™¢ Bï¼ˆè½»ç—‡æ‚£è€…ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢B-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢B-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['åŒ»é™¢B-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['åŒ»é™¢B-æ•ˆåº”'] > 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯æœ‰æ•ˆï¼âœ…\")\n",
    "        \n",
    "        print(f\"\\nğŸ­ æ‚–è®ºè§£é‡Š:\")\n",
    "        print(f\"   æ•´ä½“çœ‹èµ·æ¥ç”¨è¯æœ‰å®³ï¼Œä½†åˆ†å±‚åæ¯ä¸ªåŒ»é™¢ç”¨è¯éƒ½æœ‰æ•ˆï¼\")\n",
    "        print(f\"   åŸå› : é‡ç—‡æ‚£è€…ç”¨è¯å¤šï¼Œè½»ç—‡æ‚£è€…ä¸ç”¨è¯å¤š\")\n",
    "        print(f\"   'ç—…æƒ…ä¸¥é‡ç¨‹åº¦'æ˜¯æ··æ·†å˜é‡ï¼\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ create_simpson_paradox_data å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– Simpson's Paradox\n",
    "if simpson_df is not None and not simpson_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å›¾1: æ•´ä½“å¯¹æ¯”\n",
    "    ax1 = axes[0]\n",
    "    overall_data = simpson_df.groupby('ç”¨è¯')['åº·å¤'].mean()\n",
    "    colors = ['steelblue', 'coral']\n",
    "    bars = ax1.bar(['æœªç”¨è¯', 'ç”¨è¯'], [overall_data[0], overall_data[1]], color=colors)\n",
    "    ax1.set_ylabel('åº·å¤ç‡', fontsize=12)\n",
    "    ax1.set_title('æ•´ä½“åº·å¤ç‡\\n(çœ‹èµ·æ¥ç”¨è¯æœ‰å®³!)', fontsize=14)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    for bar, val in zip(bars, [overall_data[0], overall_data[1]]):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{val:.1%}', ha='center', fontsize=12)\n",
    "    \n",
    "    # å›¾2: åˆ†å±‚å¯¹æ¯”\n",
    "    ax2 = axes[1]\n",
    "    stratified_data = simpson_df.groupby(['åŒ»é™¢', 'ç”¨è¯'])['åº·å¤'].mean().unstack()\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    bars1 = ax2.bar(x - width/2, stratified_data[0], width, label='æœªç”¨è¯', color='steelblue')\n",
    "    bars2 = ax2.bar(x + width/2, stratified_data[1], width, label='ç”¨è¯', color='coral')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(['åŒ»é™¢A (é‡ç—‡)', 'åŒ»é™¢B (è½»ç—‡)'])\n",
    "    ax2.set_ylabel('åº·å¤ç‡', fontsize=12)\n",
    "    ax2.set_title('åˆ†å±‚åº·å¤ç‡\\n(ä¸¤ä¸ªåŒ»é™¢ç”¨è¯éƒ½æœ‰æ•ˆ!)', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    for bars, col in [(bars1, 0), (bars2, 1)]:\n",
    "        for bar, hospital in zip(bars, stratified_data.index):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{stratified_data.loc[hospital, col]:.1%}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Simpson's Paradox çš„æœ¬è´¨\n",
    "\n",
    "Simpson's Paradox ä¸æ˜¯çœŸæ­£çš„ã€Œæ‚–è®ºã€ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ï¼š\n",
    "\n",
    "1. **æ•´ä½“è¶‹åŠ¿ â‰  åˆ†å±‚è¶‹åŠ¿**: å½“å­˜åœ¨æ··æ·†å˜é‡æ—¶ï¼Œæ•´ä½“æ•°æ®å¯èƒ½ç»™å‡ºè¯¯å¯¼æ€§çš„ç»“è®º\n",
    "2. **å› æœæ–¹å‘å¾ˆé‡è¦**: åº”è¯¥æ§åˆ¶æ··æ·†å˜é‡ï¼ˆç—…æƒ…ï¼‰ï¼Œè€Œä¸æ˜¯è¢«å®ƒè¯¯å¯¼\n",
    "3. **æ•°æ®ä¼šè¯´è°**: æ²¡æœ‰æ­£ç¡®çš„å› æœåˆ†æï¼Œæ•°æ®å¯èƒ½å‘Šè¯‰ä½ å®Œå…¨ç›¸åçš„ç»“è®ºï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”® Part 4: æ•æ„Ÿæ€§åˆ†æ\n",
    "\n",
    "### å½“æ··æ·†å˜é‡ä¸å¯è§‚æµ‹æ—¶æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "åœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸æ— æ³•è§‚æµ‹åˆ°æ‰€æœ‰æ··æ·†å˜é‡ã€‚æ¯”å¦‚ï¼š\n",
    "\n",
    "- ç ”ç©¶å¸çƒŸå¯¹è‚ºç™Œçš„å½±å“ï¼Œä½†æ— æ³•è§‚æµ‹ã€Œé—ä¼ å› ç´ ã€\n",
    "- ç ”ç©¶æ•™è‚²å¯¹æ”¶å…¥çš„å½±å“ï¼Œä½†æ— æ³•è§‚æµ‹ã€Œå¤©èµ‹ã€\n",
    "\n",
    "è¿™æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦é—®ï¼š**å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·†ï¼Œæˆ‘ä»¬çš„ç»“è®ºè¿˜å¯é å—ï¼Ÿ**\n",
    "\n",
    "æ•æ„Ÿæ€§åˆ†æå°±æ˜¯ç”¨æ¥å›ç­”è¿™ä¸ªé—®é¢˜çš„ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_to_unmeasured_confounding(\n",
    "    df: pd.DataFrame,\n",
    "    gamma_range: np.ndarray = None,\n",
    "    delta_range: np.ndarray = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æ•æ„Ÿæ€§åˆ†æ: å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·†ï¼Œä¼°è®¡ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ\n",
    "    \n",
    "    å‡è®¾å­˜åœ¨æœªè§‚æµ‹æ··æ·†å˜é‡ U:\n",
    "    - U å¯¹ Y çš„æ•ˆåº”ä¸º gamma_u\n",
    "    - U ä¸ T çš„å…³è”ä¸º delta_u\n",
    "    - åå·® = gamma_u * delta_u\n",
    "    \n",
    "    å¯¹ä¸åŒçš„ (gamma_u, delta_u) ç»„åˆï¼Œè®¡ç®—å¯èƒ½çš„çœŸå®æ•ˆåº”\n",
    "    \"\"\"\n",
    "    if gamma_range is None:\n",
    "        gamma_range = np.linspace(-2, 2, 9)\n",
    "    if delta_range is None:\n",
    "        delta_range = np.linspace(-1, 1, 9)\n",
    "    \n",
    "    # å½“å‰ä¼°è®¡ï¼ˆå‡è®¾å·²æ§åˆ¶è§‚æµ‹åˆ°çš„æ··æ·†ï¼‰\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[['T', 'X']], df['Y'])\n",
    "    current_estimate = model.coef_[0]\n",
    "    \n",
    "    results = []\n",
    "    for gamma_u in gamma_range:\n",
    "        for delta_u in delta_range:\n",
    "            # å¯èƒ½çš„åå·®ï¼ˆå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼‰\n",
    "            possible_bias = gamma_u * delta_u\n",
    "            # å¯èƒ½çš„çœŸå®æ•ˆåº”\n",
    "            possible_true_effect = current_estimate - possible_bias\n",
    "            \n",
    "            results.append({\n",
    "                'gamma_u': gamma_u,\n",
    "                'delta_u': delta_u,\n",
    "                'possible_bias': possible_bias,\n",
    "                'possible_true_effect': possible_true_effect\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ•æ„Ÿæ€§åˆ†æ\n",
    "sensitivity = sensitivity_to_unmeasured_confounding(df)\n",
    "\n",
    "print(\"ğŸ”® æ•æ„Ÿæ€§åˆ†æç»“æœ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å½“å‰ä¼°è®¡\n",
    "model = LinearRegression()\n",
    "model.fit(df[['T', 'X']], df['Y'])\n",
    "current_est = model.coef_[0]\n",
    "\n",
    "print(f\"å½“å‰ä¼°è®¡ï¼ˆæ§åˆ¶ X åï¼‰: {current_est:.4f}\")\n",
    "print(f\"\\nå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† U:\")\n",
    "print(f\"   å¯èƒ½çš„çœŸå®æ•ˆåº”èŒƒå›´: [{sensitivity['possible_true_effect'].min():.2f}, {sensitivity['possible_true_effect'].max():.2f}]\")\n",
    "\n",
    "# æ‰¾å‡ºä½¿æ•ˆåº”å˜ä¸º 0 æˆ–è´Ÿæ•°çš„æ¡ä»¶\n",
    "zero_effect = sensitivity[sensitivity['possible_true_effect'] < 0]\n",
    "if len(zero_effect) > 0:\n",
    "    print(f\"\\nâš ï¸ ä»¥ä¸‹æƒ…å†µä¼šä½¿æ•ˆåº”å˜ä¸ºè´Ÿæ•°:\")\n",
    "    print(f\"   éœ€è¦ Î³_u Ã— Î´_u > {current_est:.2f}\")\n",
    "    print(f\"   ä¾‹å¦‚: Î³_u = 2, Î´_u = 1.5 (å¼ºæ··æ·†)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ•æ„Ÿæ€§åˆ†æ\n",
    "pivot = sensitivity.pivot_table(\n",
    "    index='gamma_u', \n",
    "    columns='delta_u', \n",
    "    values='possible_true_effect'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    pivot, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap='RdYlGn',\n",
    "    center=0,\n",
    "    vmin=-2,\n",
    "    vmax=4\n",
    ")\n",
    "plt.xlabel('Î´_u (U ä¸ T çš„å…³è”)', fontsize=12)\n",
    "plt.ylabel('Î³_u (U å¯¹ Y çš„æ•ˆåº”)', fontsize=12)\n",
    "plt.title('æ•æ„Ÿæ€§åˆ†æ: å¯èƒ½çš„çœŸå®æ•ˆåº”\\n(çº¢è‰² = è´Ÿæ•ˆåº”, ç»¿è‰² = æ­£æ•ˆåº”)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š çƒ­åŠ›å›¾è§£è¯»:\")\n",
    "print(\"   - ä¸­å¿ƒ (0, 0): æ— æœªè§‚æµ‹æ··æ·†ï¼Œæ•ˆåº” = å½“å‰ä¼°è®¡\")\n",
    "print(\"   - ç»¿è‰²åŒºåŸŸ: å³ä½¿æœ‰æœªè§‚æµ‹æ··æ·†ï¼Œæ•ˆåº”ä»ä¸ºæ­£\")\n",
    "print(\"   - çº¢è‰²åŒºåŸŸ: æœªè§‚æµ‹æ··æ·†å¯èƒ½ä½¿æ•ˆåº”ä¸ºè´Ÿ\")\n",
    "print(\"   - æ•ˆåº”ä¸º 0 çš„è¾¹ç•Œ: Î³_u Ã— Î´_u = å½“å‰ä¼°è®¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Part 5: æ€è€ƒé¢˜\n",
    "\n",
    "### é—®é¢˜ 1: ä¸ºä»€ä¹ˆ Simpson's Paradox ä¸æ˜¯çœŸæ­£çš„æ‚–è®ºï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 2: åå·®å…¬å¼ Bias = Î³ Ã— Î´ ä¸­ï¼Œä»€ä¹ˆæƒ…å†µä¸‹åå·®ä¸º 0ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 3: åœ¨å®è·µä¸­ï¼Œå¦‚ä½•åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ··æ·†ï¼Ÿ\n",
    "\n",
    "**æç¤º:** æƒ³æƒ³é¢†åŸŸçŸ¥è¯†ã€æ•æ„Ÿæ€§åˆ†æã€é˜´æ€§å¯¹ç…§...\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ¯ é¢è¯•é¢˜æ¨¡æ‹Ÿ\n\n### æ¦‚å¿µé¢˜\n\n**Q1: è§£é‡Šé—æ¼å˜é‡åå·®ï¼ˆOVBï¼‰å…¬å¼ Bias = Î³ Ã— Î´ï¼Œå¹¶è¯´æ˜ä»€ä¹ˆæƒ…å†µä¸‹åå·®ä¸ºé›¶ã€‚**\n\n<details>\n<summary>ç­”æ¡ˆ</summary>\n\n**OVB å…¬å¼**: Bias = Î³ Ã— Î´\n\n- **Î³ (gamma)**: é—æ¼å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n- **Î´ (delta)**: é—æ¼å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n\n**åå·®ä¸ºé›¶çš„æƒ…å†µ**:\n\n1. **Î³ = 0**: X å¯¹ Y æ²¡æœ‰ç›´æ¥å½±å“\n   - å³ä½¿ X ä¸ T ç›¸å…³ï¼Œé—æ¼ X ä¹Ÿä¸ä¼šé€ æˆåå·®\n   - ä¾‹å­ï¼šX åªå½±å“ Tï¼Œä¸å½±å“ Y\n\n2. **Î´ = 0**: X ä¸ T ä¸ç›¸å…³ï¼ˆç‹¬ç«‹ï¼‰\n   - å³ä½¿ X å½±å“ Yï¼Œé—æ¼ X ä¹Ÿä¸ä¼šé€ æˆåå·®\n   - ä¾‹å­ï¼šéšæœºå®éªŒä¸­ï¼ŒT ç‹¬ç«‹äºæ‰€æœ‰åå˜é‡\n\n**å…³é”®æ´å¯Ÿ**: \n- åªæœ‰å½“ X åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶æ—¶æ‰ä¼šäº§ç”Ÿåå·®ï¼š\n  1. X å½±å“ Yï¼ˆÎ³ â‰  0ï¼‰\n  2. X ä¸ T ç›¸å…³ï¼ˆÎ´ â‰  0ï¼‰\n- è¿™æ­£æ˜¯æ··æ·†å˜é‡çš„å®šä¹‰ï¼\n\n**å®è·µæ„ä¹‰**:\n- ä¸æ˜¯æ‰€æœ‰é—æ¼å˜é‡éƒ½ä¼šé€ æˆåå·®\n- åªéœ€æ§åˆ¶åŒæ—¶å½±å“ T å’Œ Y çš„å˜é‡\n- è¿™æ˜¯åé—¨å‡†åˆ™çš„ç†è®ºåŸºç¡€\n\n</details>\n\n---\n\n**Q2: Simpson's Paradox åœ¨å› æœæ¨æ–­ä¸­çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ**\n\n<details>\n<summary>ç­”æ¡ˆ</summary>\n\n**Simpson's Paradox**: æ•´ä½“è¶‹åŠ¿å’Œåˆ†å±‚è¶‹åŠ¿å¯èƒ½ç›¸åçš„ç°è±¡\n\n**æ ¸å¿ƒæ•™è®­**:\n\n1. **èšåˆæ•°æ®ä¼šè¯¯å¯¼**: ä¸åˆ†å±‚çš„æ•´ä½“æ•°æ®å¯èƒ½ç»™å‡ºé”™è¯¯ç»“è®º\n2. **æ··æ·†æ˜¯ç½ªé­ç¥¸é¦–**: æ‚–è®ºçš„æ ¹æºæ˜¯å­˜åœ¨æ··æ·†å˜é‡\n3. **å› æœæ–¹å‘å¾ˆé‡è¦**: éœ€è¦çŸ¥é“å˜é‡ä¹‹é—´çš„å› æœå…³ç³»æ‰èƒ½æ­£ç¡®åˆ†æ\n\n**ç»å…¸ä¾‹å­**: \n\nä¼¯å…‹åˆ©æ€§åˆ«æ­§è§†æ¡ˆï¼ˆ1973ï¼‰:\n- æ•´ä½“ï¼šç”·æ€§å½•å–ç‡ 44%ï¼Œå¥³æ€§ 35% â†’ çœ‹èµ·æ¥æ­§è§†å¥³æ€§\n- åˆ†é™¢ç³»ï¼šå¤§å¤šæ•°é™¢ç³»å¥³æ€§å½•å–ç‡æ›´é«˜æˆ–ç›¸åŒ\n- åŸå› ï¼šå¥³æ€§å€¾å‘ç”³è¯·ç«äº‰æ›´æ¿€çƒˆçš„é™¢ç³»\n\n**åˆ¤æ–­æ ‡å‡†**: åº”è¯¥åˆ†å±‚è¿˜æ˜¯èšåˆï¼Ÿ\n\nå…³é”®åœ¨äºå› æœç»“æ„ï¼š\n- å¦‚æœé™¢ç³»æ˜¯**æ··æ·†å˜é‡**ï¼ˆå½±å“æ€§åˆ«åˆ†å¸ƒå’Œå½•å–ç‡ï¼‰â†’ åº”è¯¥åˆ†å±‚\n- å¦‚æœé™¢ç³»æ˜¯**ç¢°æ’å˜é‡**æˆ–**ä¸­ä»‹** â†’ ä¸åº”è¯¥åˆ†å±‚\n\n**ä¸€èˆ¬åŸåˆ™**:\n1. å…ˆç”»å› æœå›¾\n2. åº”ç”¨åé—¨å‡†åˆ™\n3. æ§åˆ¶æ··æ·†å˜é‡ï¼Œä¸æ§åˆ¶ä¸­ä»‹/ç¢°æ’å˜é‡\n\n**å®è·µè­¦ç¤º**:\n- ä¸è¦ç›²ç›®ç›¸ä¿¡èšåˆç»Ÿè®¡\n- ä¹Ÿä¸è¦ç›²ç›®åˆ†å±‚ï¼ˆå¯èƒ½å¼•å…¥ç¢°æ’åå·®ï¼‰\n- éœ€è¦é¢†åŸŸçŸ¥è¯†å’Œå› æœæ€ç»´\n\n</details>\n\n---\n\n**Q3: ä»€ä¹ˆæ˜¯æ•æ„Ÿæ€§åˆ†æï¼Ÿä¸ºä»€ä¹ˆåœ¨å› æœæ¨æ–­ä¸­å¾ˆé‡è¦ï¼Ÿ**\n\n<details>\n<summary>ç­”æ¡ˆ</summary>\n\n**æ•æ„Ÿæ€§åˆ†æ**: è¯„ä¼°ä¼°è®¡ç»“æœå¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§\n\n**æ ¸å¿ƒæ€æƒ³**: \n- æˆ‘ä»¬æ°¸è¿œæ— æ³•ç¡®å®šæ˜¯å¦è§‚æµ‹åˆ°äº†æ‰€æœ‰æ··æ·†å˜é‡\n- æ•æ„Ÿæ€§åˆ†æé—®ï¼š\"å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼Œç»“è®ºä¼šæ”¹å˜å—ï¼Ÿ\"\n\n**æ–¹æ³•**:\n\n1. **å‚æ•°åŒ–æœªè§‚æµ‹æ··æ·†**:\n   - Î³_U: U å¯¹ Y çš„å½±å“\n   - Î´_U: U ä¸ T çš„å…³è”\n\n2. **è®¡ç®—å¯èƒ½çš„åå·®èŒƒå›´**:\n   - çœŸå®æ•ˆåº” âˆˆ [ä¼°è®¡å€¼ - Î³_UÃ—Î´_U, ä¼°è®¡å€¼ + Î³_UÃ—Î´_U]\n\n3. **è¯„ä¼°ç¨³å¥æ€§**:\n   - éœ€è¦å¤šå¼ºçš„æ··æ·†æ‰èƒ½æ¨ç¿»ç»“è®ºï¼Ÿ\n   - è¿™æ ·çš„æ··æ·†åˆç†å—ï¼Ÿ\n\n**Rosenbaum Bounds ä¾‹å­**:\n\nå‡è®¾å½“å‰ä¼°è®¡ ATE = 5ï¼Œæ˜¾è‘—æ€§ p < 0.05\n\n| Î“å€¼ | å«ä¹‰ | på€¼èŒƒå›´ | ç»“è®º |\n|-----|------|---------|------|\n| 1.0 | æ— æœªè§‚æµ‹æ··æ·† | [0.01, 0.01] | æ˜¾è‘— |\n| 1.5 | å€¾å‘å¾—åˆ†å¯ç›¸å·®50% | [0.01, 0.08] | ä»æ˜¾è‘— |\n| 2.0 | å€¾å‘å¾—åˆ†å¯ç›¸å·®100% | [0.01, 0.15] | ä¸ç¨³å¥ |\n\nè§£è¯»ï¼š\n- Î“ = 1.5 æ—¶ç»“è®ºä»æ˜¾è‘— â†’ ç»“æœç›¸å¯¹ç¨³å¥\n- Î“ = 2.0 æ—¶ç»“è®ºå¯èƒ½ä¸æ˜¾è‘— â†’ å­˜åœ¨é£é™©\n\n**å®è·µä»·å€¼**:\n\n1. **è¯šå®æ€§**: æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œä¸è¿‡åº¦è‡ªä¿¡\n2. **è¯´æœåŠ›**: å±•ç¤ºç»“æœå¯¹å‡è®¾çš„ä¾èµ–ç¨‹åº¦\n3. **æŒ‡å¯¼æ”¶é›†æ•°æ®**: æŒ‡å‡ºå“ªäº›æœªè§‚æµ‹å˜é‡æœ€å…³é”®\n\n**ä½•æ—¶ä½¿ç”¨**:\n- è§‚æµ‹ç ”ç©¶ï¼ˆééšæœºï¼‰\n- å·²æ§åˆ¶å·²çŸ¥æ··æ·†ï¼Œä½†æ‹…å¿ƒé—æ¼\n- éœ€è¦å‘åˆ©ç›Šç›¸å…³è€…è¯æ˜ç»“è®ºçš„ç¨³å¥æ€§\n\n</details>\n\n---\n\n### ç¼–ç¨‹é¢˜\n\n**Q4: å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ¨¡æ‹Ÿ Simpson's Paradoxï¼Œå±•ç¤ºæ•´ä½“å’Œåˆ†å±‚çš„æ•ˆåº”ä¼°è®¡ã€‚**\n\n<details>\n<summary>å‚è€ƒä»£ç </summary>\n\n```python\ndef create_simpsons_paradox_example(n_per_group=500, seed=42):\n    \"\"\"\n    åˆ›å»º Simpson's Paradox ç¤ºä¾‹\n    \n    åœºæ™¯ï¼šè¯„ä¼°æ–°è¯æ•ˆæœ\n    - ä¸¤ä¸ªåŒ»é™¢ A å’Œ B\n    - åŒ»é™¢ A ä¸»è¦æ”¶æ²»é‡ç—‡ï¼ˆç”¨è¯å¤šï¼‰\n    - åŒ»é™¢ B ä¸»è¦æ”¶æ²»è½»ç—‡ï¼ˆç”¨è¯å°‘ï¼‰\n    - æ¯ä¸ªåŒ»é™¢å†…ç”¨è¯éƒ½æœ‰æ­£æ•ˆåº”\n    - ä½†æ•´ä½“çœ‹èµ·æ¥ç”¨è¯æ•ˆæœä¸ºè´Ÿï¼\n    \"\"\"\n    np.random.seed(seed)\n    data = []\n    \n    # åŒ»é™¢ Aï¼ˆé‡ç—‡ä¸ºä¸»ï¼‰\n    # åŸºç¡€åº·å¤ç‡ä½ï¼Œä½†ç”¨è¯æé«˜åº·å¤ç‡\n    n_A_treated = int(n_per_group * 1.6)  # åŒ»é™¢Aç”¨è¯å¤š\n    n_A_control = int(n_per_group * 0.4)\n    \n    recovery_A_treated = np.random.binomial(1, 0.50, n_A_treated)  # ç”¨è¯åº·å¤ç‡50%\n    recovery_A_control = np.random.binomial(1, 0.30, n_A_control)  # ä¸ç”¨è¯30%\n    \n    for r in recovery_A_treated:\n        data.append({'åŒ»é™¢': 'A(é‡ç—‡)', 'ç”¨è¯': 1, 'åº·å¤': r})\n    for r in recovery_A_control:\n        data.append({'åŒ»é™¢': 'A(é‡ç—‡)', 'ç”¨è¯': 0, 'åº·å¤': r})\n    \n    # åŒ»é™¢ Bï¼ˆè½»ç—‡ä¸ºä¸»ï¼‰\n    # åŸºç¡€åº·å¤ç‡é«˜ï¼Œç”¨è¯è¿›ä¸€æ­¥æé«˜\n    n_B_treated = int(n_per_group * 0.4)  # åŒ»é™¢Bç”¨è¯å°‘\n    n_B_control = int(n_per_group * 1.6)\n    \n    recovery_B_treated = np.random.binomial(1, 0.90, n_B_treated)  # ç”¨è¯åº·å¤ç‡90%\n    recovery_B_control = np.random.binomial(1, 0.70, n_B_control)  # ä¸ç”¨è¯70%\n    \n    for r in recovery_B_treated:\n        data.append({'åŒ»é™¢': 'B(è½»ç—‡)', 'ç”¨è¯': 1, 'åº·å¤': r})\n    for r in recovery_B_control:\n        data.append({'åŒ»é™¢': 'B(è½»ç—‡)', 'ç”¨è¯': 0, 'åº·å¤': r})\n    \n    df = pd.DataFrame(data)\n    \n    # è®¡ç®—æ•ˆåº”\n    # æ•´ä½“æ•ˆåº”ï¼ˆé”™è¯¯çš„ï¼ï¼‰\n    overall_treated = df[df['ç”¨è¯']==1]['åº·å¤'].mean()\n    overall_control = df[df['ç”¨è¯']==0]['åº·å¤'].mean()\n    overall_effect = overall_treated - overall_control\n    \n    # åˆ†å±‚æ•ˆåº”ï¼ˆæ­£ç¡®çš„ï¼‰\n    df_A = df[df['åŒ»é™¢']=='A(é‡ç—‡)']\n    effect_A = df_A[df_A['ç”¨è¯']==1]['åº·å¤'].mean() - df_A[df_A['ç”¨è¯']==0]['åº·å¤'].mean()\n    \n    df_B = df[df['åŒ»é™¢']=='B(è½»ç—‡)']\n    effect_B = df_B[df_B['ç”¨è¯']==1]['åº·å¤'].mean() - df_B[df_B['ç”¨è¯']==0]['åº·å¤'].mean()\n    \n    # æ ‡å‡†åŒ–æ•ˆåº”ï¼ˆæŒ‰æ€»ä½“åŒ»é™¢åˆ†å¸ƒåŠ æƒï¼‰\n    weight_A = (len(df_A) / len(df))\n    weight_B = (len(df_B) / len(df))\n    standardized_effect = effect_A * weight_A + effect_B * weight_B\n    \n    print(f\"ğŸ“Š Simpson's Paradox ç¤ºä¾‹åˆ†æ\")\n    print(f\"=\" * 60)\n    print(f\"\\næ•´ä½“æ•ˆåº”ï¼ˆä¸åˆ†å±‚ï¼‰: {overall_effect:+.1%}\")\n    print(f\"  ç”¨è¯ç»„åº·å¤ç‡: {overall_treated:.1%}\")\n    print(f\"  å¯¹ç…§ç»„åº·å¤ç‡: {overall_control:.1%}\")\n    if overall_effect < 0:\n        print(f\"  âŒ é”™è¯¯ç»“è®ºï¼šç”¨è¯æœ‰å®³ï¼\")\n    \n    print(f\"\\nåˆ†å±‚æ•ˆåº”:\")\n    print(f\"  åŒ»é™¢Aï¼ˆé‡ç—‡ï¼‰: {effect_A:+.1%} âœ…\")\n    print(f\"  åŒ»é™¢Bï¼ˆè½»ç—‡ï¼‰: {effect_B:+.1%} âœ…\")\n    print(f\"  âœ… æ­£ç¡®ç»“è®ºï¼šæ¯ä¸ªåŒ»é™¢å†…ç”¨è¯éƒ½æœ‰ç›Šï¼\")\n    \n    print(f\"\\næ ‡å‡†åŒ–æ•ˆåº”ï¼ˆæ­£ç¡®åŠ æƒï¼‰: {standardized_effect:+.1%}\")\n    print(f\"\\nğŸ” æ‚–è®ºåŸå› :\")\n    print(f\"  - åŒ»é™¢Aï¼ˆé‡ç—‡ï¼‰ç”¨è¯æ¯”ä¾‹é«˜: {df_A['ç”¨è¯'].mean():.1%}\")\n    print(f\"  - åŒ»é™¢Bï¼ˆè½»ç—‡ï¼‰ç”¨è¯æ¯”ä¾‹ä½: {df_B['ç”¨è¯'].mean():.1%}\")\n    print(f\"  - ç”¨è¯ç»„æ··å…¥äº†æ›´å¤šé‡ç—‡æ‚£è€…ï¼Œæ‹‰ä½äº†æ•´ä½“åº·å¤ç‡\")\n    \n    return df\n\n# è¿è¡Œç¤ºä¾‹\ndf_simpson = create_simpsons_paradox_example()\n```\n\n**å…³é”®ç‚¹**:\n- æ•´ä½“æ•ˆåº”è¢«æ··æ·†å˜é‡ï¼ˆåŒ»é™¢/ç—…æƒ…ï¼‰è¯¯å¯¼\n- æ¯ä¸ªå±‚å†…æ•ˆåº”éƒ½æ˜¯æ­£çš„ï¼Œä½†æ•´ä½“å¯èƒ½æ˜¯è´Ÿçš„\n- å¿…é¡»æŒ‰æ­£ç¡®æƒé‡æ ‡å‡†åŒ–\n\n</details>\n\n---\n\n**Q5: å®ç°æ•æ„Ÿæ€§åˆ†æå‡½æ•°ï¼Œè¯„ä¼°ä¼°è®¡å¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§ã€‚**\n\n<details>\n<summary>å‚è€ƒä»£ç ï¼ˆå·²åœ¨notebookä¸­å®ç°ï¼‰</summary>\n\nè§ notebook ä¸­çš„ `sensitivity_to_unmeasured_confounding` å‡½æ•°ã€‚\n\n**ä½¿ç”¨ç¤ºä¾‹**:\n\n```python\n# å‡è®¾å½“å‰ä¼°è®¡ ATE = 5ï¼ˆæ§åˆ¶äº†è§‚æµ‹åˆ°çš„ Xï¼‰\n# è¯„ä¼°ï¼šå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼Œæ•ˆåº”ä¼šåœ¨ä»€ä¹ˆèŒƒå›´ï¼Ÿ\n\ngamma_range = np.linspace(-3, 3, 13)  # U å¯¹ Y çš„å¯èƒ½å½±å“\ndelta_range = np.linspace(-1, 1, 11)  # U ä¸ T çš„å¯èƒ½å…³è”\n\nsensitivity = sensitivity_to_unmeasured_confounding(\n    df, gamma_range=gamma_range, delta_range=delta_range\n)\n\n# æŸ¥çœ‹åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æ•ˆåº”ä¼šå˜å·\nprint(\"ä½¿æ•ˆåº”å˜ä¸ºè´Ÿæ•°çš„æƒ…å†µ:\")\nnegative_cases = sensitivity[sensitivity['possible_true_effect'] < 0]\nprint(negative_cases[['gamma_u', 'delta_u', 'possible_true_effect']])\n\n# åˆ¤æ–­ç¨³å¥æ€§\nif len(negative_cases) > 0:\n    min_gamma = negative_cases['gamma_u'].abs().min()\n    min_delta = negative_cases['delta_u'].abs().min()\n    print(f\"\\néœ€è¦ |Î³_U| â‰¥ {min_gamma:.2f} ä¸” |Î´_U| â‰¥ {min_delta:.2f} æ‰èƒ½æ¨ç¿»ç»“è®º\")\n    print(f\"åˆ¤æ–­ï¼šè¿™æ ·çš„æœªè§‚æµ‹æ··æ·†æ˜¯å¦åˆç†ï¼Ÿ\")\nelse:\n    print(f\"\\nåœ¨ç»™å®šå‚æ•°èŒƒå›´å†…ï¼Œæ•ˆåº”å§‹ç»ˆä¸ºæ­£\")\n    print(f\"ç»“è®ºè¾ƒç¨³å¥\")\n```\n\n**å®è·µæŠ€å·§**:\n1. æ ¹æ®é¢†åŸŸçŸ¥è¯†è®¾å®šåˆç†çš„ Î³ å’Œ Î´ èŒƒå›´\n2. å¯è§†åŒ–æ•æ„Ÿæ€§çƒ­å›¾ï¼ˆå¦‚notebookä¸­æ‰€ç¤ºï¼‰\n3. å…³æ³¨ä½¿ç»“è®ºå‘ç”Ÿè´¨å˜çš„ä¸´ç•Œå€¼\n4. ä¸å·²è§‚æµ‹æ··æ·†çš„å½±å“å¯¹æ¯”ï¼Œè¯„ä¼°åˆç†æ€§\n\n</details>\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ“ æ•°å­¦æ¨å¯¼\n\n### 1. é—æ¼å˜é‡åå·®å…¬å¼ï¼ˆOVBï¼‰\n\n**åœºæ™¯**: çœŸå®æ¨¡å‹ä¸º $Y = \\beta_0 + \\beta_T T + \\beta_X X + \\varepsilon$ï¼Œä½†æˆ‘ä»¬é—æ¼äº† Xï¼Œåªä¼°è®¡ $Y = \\alpha_0 + \\alpha_T T + u$\n\n**å®šç†**: é—æ¼ X å¯¼è‡´çš„åå·®ä¸ºï¼š\n\n$$\\text{Bias}(\\hat{\\alpha}_T) = E[\\hat{\\alpha}_T] - \\beta_T = \\beta_X \\cdot \\delta$$\n\nå…¶ä¸­ $\\delta$ æ˜¯ $T$ å¯¹ $X$ å›å½’çš„ç³»æ•°ï¼š$X = \\gamma_0 + \\delta T + v$\n\n**è¯æ˜**:\n\n1. çœŸå®æ¨¡å‹ï¼š\n   $$Y = \\beta_0 + \\beta_T T + \\beta_X X + \\varepsilon$$\n\n2. å°† $X = \\gamma_0 + \\delta T + v$ ä»£å…¥ï¼š\n   $$Y = \\beta_0 + \\beta_T T + \\beta_X(\\gamma_0 + \\delta T + v) + \\varepsilon$$\n   $$= (\\beta_0 + \\beta_X \\gamma_0) + (\\beta_T + \\beta_X \\delta) T + (\\beta_X v + \\varepsilon)$$\n\n3. ç®€åŒ–æ¨¡å‹çš„ç³»æ•°ï¼š\n   $$\\alpha_T = \\beta_T + \\beta_X \\delta$$\n\n4. å› æ­¤åå·®ï¼š\n   $$\\text{Bias} = \\alpha_T - \\beta_T = \\beta_X \\delta$$\n\n**ç¬¦å·è¯´æ˜**:\n- $\\beta_X$ (gamma): X å¯¹ Y çš„çœŸå®æ•ˆåº”\n- $\\delta$: X ä¸ T çš„å…³è”ï¼ˆT å¯¹ X å›å½’çš„ç³»æ•°ï¼Œæˆ– X å¯¹ T å›å½’çš„ç³»æ•°ä¹˜ä»¥æ–¹å·®æ¯”ï¼‰\n\n**åå·®æ–¹å‘**:\n| $\\beta_X$ | $\\delta$ | åå·® | è§£é‡Š |\n|----------|---------|------|------|\n| + | + | + | é«˜ä¼° |\n| + | - | - | ä½ä¼° |\n| - | + | - | ä½ä¼° |\n| - | - | + | é«˜ä¼° |\n\n---\n\n### 2. é€‰æ‹©åå·®çš„æ•°å­¦è¡¨è¾¾\n\n**å®šä¹‰**: å½“å¤„ç†åˆ†é…ä¸æ½œåœ¨ç»“æœç›¸å…³æ—¶äº§ç”Ÿçš„åå·®ã€‚\n\n$$\\begin{align}\nE[Y|T=1] - E[Y|T=0] &= E[Y(1)|T=1] - E[Y(0)|T=0] \\\\\n&= \\underbrace{E[Y(1) - Y(0)|T=1]}_{\\text{ATTï¼ˆçœŸå®æ•ˆåº”ï¼‰}} \\\\\n&\\quad + \\underbrace{E[Y(0)|T=1] - E[Y(0)|T=0]}_{\\text{é€‰æ‹©åå·®}}\n\\end{align}$$\n\n**é€‰æ‹©åå·®é¡¹**: $E[Y(0)|T=1] - E[Y(0)|T=0]$\n\n**è§£é‡Š**: å³ä½¿ä¸æ¥å—å¤„ç†ï¼Œå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„ç»“æœä¹Ÿä¸åŒã€‚\n\n**ä¾‹å­**:\n- T = ä¸Šå¤§å­¦ï¼ŒY = æ”¶å…¥\n- é€‰æ‹©åå·® = \"ä¸Šå¤§å­¦çš„äººå³ä½¿ä¸ä¸Šå¤§å­¦æ”¶å…¥ä¹Ÿä¼šæ›´é«˜\"\n- åŸå› ï¼šä¸Šå¤§å­¦çš„äººå¯èƒ½æœ¬æ¥å°±æ›´èªæ˜ã€æ›´åŠªåŠ›\n\n**æ¶ˆé™¤æ–¹æ³•**:\n1. éšæœºåŒ–ï¼šä½¿ $E[Y(0)|T=1] = E[Y(0)|T=0]$\n2. æ¡ä»¶ç‹¬ç«‹å‡è®¾ï¼šæ§åˆ¶æ‰€æœ‰å½±å“é€‰æ‹©çš„åå˜é‡ X\n3. å·¥å…·å˜é‡ã€DID ç­‰å‡†å®éªŒæ–¹æ³•\n\n---\n\n### 3. Simpson's Paradox çš„æ¦‚ç‡è®ºè§£é‡Š\n\n**ç°è±¡**: æ•´ä½“è¶‹åŠ¿å’Œåˆ†å±‚è¶‹åŠ¿ç›¸å\n\n**æ•°å­¦æ¡ä»¶**: å­˜åœ¨å˜é‡ Z ä½¿å¾—ï¼š\n\n$$\\text{sign}\\left(\\frac{P(Y=1|T=1)}{P(Y=1|T=0)}\\right) \\neq \\text{sign}\\left(\\frac{P(Y=1|T=1, Z=z)}{P(Y=1|T=0, Z=z)}\\right)$$\n\nå¯¹æ‰€æœ‰ z æˆç«‹ã€‚\n\n**ä¾‹å­**: åŒ»é™¢æ•°æ®\n\n|  | ç”¨è¯åº·å¤ç‡ | æœªç”¨è¯åº·å¤ç‡ | ç”¨è¯æ›´å¥½ï¼Ÿ |\n|--|-----------|-------------|-----------|\n| æ•´ä½“ | 40% | 50% | âœ— |\n| åŒ»é™¢Aï¼ˆé‡ç—‡ï¼‰ | 50% | 30% | âœ“ |\n| åŒ»é™¢Bï¼ˆè½»ç—‡ï¼‰ | 90% | 70% | âœ“ |\n\n**æ•°å­¦éªŒè¯**:\n\nè®¾ï¼š\n- $n_{A,1} = 200$ (åŒ»é™¢Aç”¨è¯), $n_{A,0} = 100$ (åŒ»é™¢Aä¸ç”¨è¯)\n- $n_{B,1} = 100$ (åŒ»é™¢Bç”¨è¯), $n_{B,0} = 200$ (åŒ»é™¢Bä¸ç”¨è¯)\n\næ•´ä½“åº·å¤ç‡ï¼š\n$$P(Y=1|T=1) = \\frac{200 \\times 0.5 + 100 \\times 0.9}{300} = \\frac{190}{300} \\approx 0.63$$\n\nä½†è¿™**é”™è¯¯åœ°**æ··åˆäº†ä¸åŒä¸¥é‡ç¨‹åº¦çš„æ‚£è€…ï¼\n\næ­£ç¡®æ–¹æ³•ï¼ˆæ ‡å‡†åŒ–ï¼‰ï¼š\n$$P(Y(1)=1) = P(Y=1|T=1, Z=A) \\cdot P(Z=A) + P(Y=1|T=1, Z=B) \\cdot P(Z=B)$$\n\nä½¿ç”¨**æ€»ä½“**çš„åŒ»é™¢åˆ†å¸ƒä½œä¸ºæƒé‡ï¼Œè€Œéç”¨è¯ç»„çš„åˆ†å¸ƒã€‚\n\n---\n\n### 4. æ•æ„Ÿæ€§åˆ†æçš„æ•°å­¦æ¡†æ¶\n\n**é—®é¢˜**: å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼Œä¼°è®¡ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ\n\n**æ¨¡å‹**:\n\n$$Y = \\beta_0 + \\beta_T T + \\beta_X X + \\beta_U U + \\varepsilon$$\n\nå…¶ä¸­ U æœªè§‚æµ‹ã€‚\n\n**å½“å‰ä¼°è®¡**ï¼ˆæ§åˆ¶ X åï¼‰:\n\n$$\\hat{\\beta}_T^{obs} = \\beta_T + \\beta_U \\delta_U$$\n\nå…¶ä¸­ $\\delta_U$ æ˜¯ U å¯¹ T å›å½’æ—¶çš„ç³»æ•°ï¼ˆæ§åˆ¶ X åï¼‰ã€‚\n\n**æ•æ„Ÿæ€§å‚æ•°**:\n- $\\Gamma$ = æœ€å¤§å€¾å‘å¾—åˆ†æ¯”å€¼ï¼š$\\frac{P(T=1|X,U)/(1-P(T=1|X,U))}{P(T=1|X)/(1-P(T=1|X))}$\n- $\\Lambda$ = U å¯¹ Y çš„æœ€å¤§å½±å“\n\n**Rosenbaum bounds**: \n$$\\beta_T \\in [\\hat{\\beta}_T^{obs} - \\beta_U \\delta_U, \\hat{\\beta}_T^{obs} + \\beta_U \\delta_U]$$\n\n**å®è·µåº”ç”¨**:\n- å¦‚æœ $\\Gamma = 1.5$ï¼ˆå€¾å‘å¾—åˆ†æœ€å¤šç›¸å·® 50%ï¼‰ï¼Œç»“è®ºä»ç„¶æ˜¾è‘— â†’ ç»“æœç¨³å¥\n- å¦‚æœ $\\Gamma = 2$ å°±ä¼šæ”¹å˜ç»“è®º â†’ ç»“æœä¸ç¨³å¥\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 4: å®é™…æ¡ˆä¾‹åˆ†æ\n",
    "\n",
    "**åœºæ™¯**: è¯„ä¼°ä¿ƒé”€é‚®ä»¶å¯¹è´­ä¹°çš„å½±å“\n",
    "\n",
    "- T: æ˜¯å¦æ”¶åˆ°ä¿ƒé”€é‚®ä»¶\n",
    "- Y: æ˜¯å¦è´­ä¹°\n",
    "\n",
    "è¯·åˆ†æ:\n",
    "1. åˆ—å‡ºå¯èƒ½çš„æ··æ·†å˜é‡\n",
    "2. æ¯ä¸ªæ··æ·†å˜é‡å¦‚ä½•åŒæ—¶å½±å“ T å’Œ Yï¼Ÿ\n",
    "3. æœ´ç´ ä¼°è®¡ä¼šé«˜ä¼°è¿˜æ˜¯ä½ä¼°çœŸå®æ•ˆåº”ï¼Ÿ\n",
    "4. å¦‚ä½•æ§åˆ¶è¿™äº›æ··æ·†ï¼Ÿ\n",
    "\n",
    "**ä½ çš„åˆ†æ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„åˆ†æ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "$$\\text{åå·®} = \\gamma \\times \\delta = \\text{(Xå¯¹Yçš„æ•ˆåº”)} \\times \\text{(Xä¸Tçš„å…³è”)}$$\n",
    "\n",
    "### å…³é”®çŸ¥è¯†ç‚¹\n",
    "\n",
    "| æ¦‚å¿µ | å®šä¹‰ | å¯ç¤º |\n",
    "|-----|------|------|\n",
    "| æ··æ·†åå·® | é—æ¼æ··æ·†å˜é‡å¯¼è‡´çš„ä¼°è®¡åå·® | å¯ä»¥ç²¾ç¡®è®¡ç®—ï¼ |\n",
    "| Simpson's Paradox | æ•´ä½“è¶‹åŠ¿ä¸åˆ†å±‚è¶‹åŠ¿ç›¸å | ä¸æ˜¯æ‚–è®ºï¼Œæ˜¯æ··æ·† |\n",
    "| æ•æ„Ÿæ€§åˆ†æ | è¯„ä¼°æœªè§‚æµ‹æ··æ·†çš„å½±å“ | å³ä½¿æœ‰é—æ¼ï¼Œä¹Ÿèƒ½è¯„ä¼° |\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "1. **ç”»å› æœå›¾**: åœ¨åˆ†æå‰å…ˆç”»å‡ºå‡è®¾çš„å› æœç»“æ„\n",
    "2. **è¯†åˆ«æ··æ·†**: æ‰¾å‡ºåŒæ—¶å½±å“å¤„ç†å’Œç»“æœçš„å˜é‡\n",
    "3. **åšæ•æ„Ÿæ€§åˆ†æ**: è¯„ä¼°ç»“è®ºå¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§\n",
    "4. **è°¨æ…ä¸‹ç»“è®º**: é™¤éæœ‰å¼ºæœ‰åŠ›çš„è¯æ®ï¼Œå¦åˆ™ä¿æŒè°¨æ…\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬æ·±åˆ»ç†è§£äº†æ··æ·†åå·®ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ å…·ä½“çš„**å¤„ç†æ–¹æ³•**â€”â€”å€¾å‘å¾—åˆ†åŒ¹é… (PSM)ï¼\n",
    "\n",
    "---\n",
    "\n",
    "**ã€ŒçŸ¥é“åå·®æœ‰å¤šå¤§ï¼Œæ¯”ä¸çŸ¥é“åå·®å­˜åœ¨è¦å¥½å¾—å¤šã€‚ã€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}