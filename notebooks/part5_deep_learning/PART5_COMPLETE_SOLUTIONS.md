# Part 5 Deep Learning - Complete Solutions & Interview Guide

## ç›®å½•

1. [Part 5.1: Representation Learning - å®Œæ•´è§£ç­”](#part-51)
2. [Part 5.2: TARNet & DragonNet - å®Œæ•´è§£ç­”](#part-52)
3. [Part 5.3: CEVAE - å®Œæ•´è§£ç­”](#part-53)
4. [Part 5.4: GANITE - å®Œæ•´è§£ç­”](#part-54)
5. [Part 5.5: VCNet - å®Œæ•´è§£ç­”](#part-55)
6. [ç»¼åˆé¢è¯•é¢˜åº“](#interview-questions)
7. [ä»é›¶å®ç°ç¤ºä¾‹](#from-scratch)

---

## Part 5.1: Representation Learning

### TODO å®Œæ•´ç­”æ¡ˆ

#### ç»ƒä¹  1.1: æ•°æ®ç”Ÿæˆ

```python
def generate_nonlinear_data(n: int = 1000, seed: int = 42):
    np.random.seed(seed)

    # ç­”æ¡ˆ: ç”ŸæˆåŸå§‹ç‰¹å¾
    X1 = np.random.randn(n)
    X2 = np.random.randn(n)

    # ç­”æ¡ˆ: æœ‰ç”¨ç‰¹å¾
    Phi1 = np.sin(X1)
    Phi2 = X1 * X2

    # ç­”æ¡ˆ: å¤„ç†åˆ†é…
    logit = Phi1 + 0.5 * Phi2
    propensity = 1 / (1 + np.exp(-logit))
    T = np.random.binomial(1, propensity, n)

    # ç­”æ¡ˆ: ç»“æœç”Ÿæˆ
    noise = np.random.randn(n) * 0.5
    Y = 1 + 2*T + Phi1 + 0.5*Phi2 + noise

    X = np.column_stack([X1, X2])
    return X, T, Y
```

#### ç»ƒä¹  1.2: è¡¨ç¤ºå­¦ä¹ ç½‘ç»œ

```python
class SimpleRepresentation(nn.Module):
    def __init__(self, input_dim: int, repr_dim: int = 10, hidden_dim: int = 20):
        super().__init__()

        # ç­”æ¡ˆ: å®šä¹‰ç½‘ç»œ
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, repr_dim),
        )

    def forward(self, x):
        return self.network(x)


def train_representation(X, T, Y, repr_dim=10, n_epochs=100):
    X_tensor = torch.FloatTensor(X)
    Y_tensor = torch.FloatTensor(Y).unsqueeze(1)

    repr_model = SimpleRepresentation(input_dim=X.shape[1], repr_dim=repr_dim)

    # ç­”æ¡ˆ: é¢„æµ‹å¤´
    prediction_head = nn.Linear(repr_dim, 1)

    # ç­”æ¡ˆ: ä¼˜åŒ–å™¨
    params = list(repr_model.parameters()) + list(prediction_head.parameters())
    optimizer = optim.Adam(params, lr=0.01)
    criterion = nn.MSELoss()

    # ç­”æ¡ˆ: è®­ç»ƒå¾ªç¯
    for epoch in range(n_epochs):
        optimizer.zero_grad()
        phi = repr_model(X_tensor)
        y_pred = prediction_head(phi)
        loss = criterion(y_pred, Y_tensor)
        loss.backward()
        optimizer.step()

    return repr_model
```

#### ç»ƒä¹  1.3-1.4: å¯è§†åŒ–ä¸å¹³è¡¡æ£€æŸ¥

```python
def visualize_representation(repr_model, X, T):
    repr_model.eval()
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X)
        # ç­”æ¡ˆ: æå–è¡¨ç¤º
        phi = repr_model(X_tensor).numpy()

    # ç­”æ¡ˆ: PCA é™ç»´
    if phi.shape[1] > 2:
        pca = PCA(n_components=2)
        phi = pca.fit_transform(phi)

    return phi, T


def check_representation_balance(phi, T):
    phi_treated = phi[T == 1]
    phi_control = phi[T == 0]

    mean_t = phi_treated.mean(axis=0)
    mean_c = phi_control.mean(axis=0)
    std_all = phi.std(axis=0) + 1e-8

    # ç­”æ¡ˆ: SMD
    smd = np.abs(mean_t - mean_c) / std_all

    # ç­”æ¡ˆ: MMD (ç®€åŒ–ç‰ˆ)
    mmd = np.sum((mean_t - mean_c)**2)

    return {
        'smd_mean': np.mean(smd),
        'smd_max': np.max(smd),
        'mmd': mmd
    }
```

#### ç»ƒä¹  6.1: MMD å®ç°

```python
def rbf_kernel(X, Y, gamma=1.0):
    # ç­”æ¡ˆ: æ¬§æ°è·ç¦»å¹³æ–¹
    XX = np.sum(X**2, axis=1).reshape(-1, 1)
    YY = np.sum(Y**2, axis=1).reshape(1, -1)
    XY = X @ Y.T

    dist_sq = XX + YY - 2 * XY

    # ç­”æ¡ˆ: é«˜æ–¯æ ¸
    K = np.exp(-gamma * dist_sq)

    return K


def compute_mmd(X, Y, kernel='rbf', gamma=1.0):
    n = X.shape[0]
    m = Y.shape[0]

    # ç­”æ¡ˆ: æ ¸çŸ©é˜µ
    K_XX = rbf_kernel(X, X, gamma)
    K_XY = rbf_kernel(X, Y, gamma)
    K_YY = rbf_kernel(Y, Y, gamma)

    # ç­”æ¡ˆ: MMD^2 (æ— åä¼°è®¡)
    term1 = (K_XX.sum() - np.trace(K_XX)) / (n * (n - 1))
    term2 = K_XY.sum() / (n * m)
    term3 = (K_YY.sum() - np.trace(K_YY)) / (m * (m - 1))

    mmd_sq = term1 - 2 * term2 + term3

    return np.sqrt(max(mmd_sq, 0))
```

### æ•°å­¦æ¨å¯¼

#### 1. IPM å®šä¹‰ä¸æ€§è´¨

**å®šä¹‰**:
$$\text{IPM}_{\mathcal{F}}(P, Q) = \sup_{f \in \mathcal{F}} \left| \mathbb{E}_{x \sim P}[f(x)] - \mathbb{E}_{x \sim Q}[f(x)] \right|$$

**æ€§è´¨**:
1. **éè´Ÿæ€§**: $\text{IPM}(P, Q) \geq 0$
2. **å¯¹ç§°æ€§**: $\text{IPM}(P, Q) = \text{IPM}(Q, P)$
3. **ä¸‰è§’ä¸ç­‰å¼**: $\text{IPM}(P, R) \leq \text{IPM}(P, Q) + \text{IPM}(Q, R)$

#### 2. MMD æ ¸æŠ€å·§æ¨å¯¼

**åŸå§‹å½¢å¼**:
$$\text{MMD}^2(P, Q) = \left\| \mathbb{E}_{x \sim P}[\phi(x)] - \mathbb{E}_{y \sim Q}[\phi(y)] \right\|^2$$

**å±•å¼€**:
$$\begin{align}
\text{MMD}^2(P, Q) &= \left\langle \mathbb{E}_P[\phi(x)], \mathbb{E}_P[\phi(x)] \right\rangle \\
&\quad - 2 \left\langle \mathbb{E}_P[\phi(x)], \mathbb{E}_Q[\phi(y)] \right\rangle \\
&\quad + \left\langle \mathbb{E}_Q[\phi(y)], \mathbb{E}_Q[\phi(y)] \right\rangle
\end{align}$$

**åº”ç”¨æ ¸æŠ€å·§** ($k(x,y) = \langle \phi(x), \phi(y) \rangle$):
$$\begin{align}
\text{MMD}^2(P, Q) &= \mathbb{E}_{x, x' \sim P}[k(x, x')] \\
&\quad - 2\mathbb{E}_{x \sim P, y \sim Q}[k(x, y)] \\
&\quad + \mathbb{E}_{y, y' \sim Q}[k(y, y')]
\end{align}$$

**æ— åä¼°è®¡å™¨**:
$$\widehat{\text{MMD}}^2 = \frac{1}{n(n-1)}\sum_{i \neq i'} k(x_i, x_{i'}) - \frac{2}{nm}\sum_{i,j} k(x_i, y_j) + \frac{1}{m(m-1)}\sum_{j \neq j'} k(y_j, y_{j'})$$

#### 3. Wasserstein è·ç¦»çš„å¯¹å¶å½¢å¼

**Kantorovich-Rubinstein å®šç†**:
$$W_1(P, Q) = \sup_{\|f\|_L \leq 1} \left| \mathbb{E}_{x \sim P}[f(x)] - \mathbb{E}_{y \sim Q}[f(y)] \right|$$

å…¶ä¸­ $\|f\|_L \leq 1$ è¡¨ç¤º $f$ æ˜¯ 1-Lipschitz å‡½æ•°ã€‚

**è¯æ˜æ¦‚è¦**:
1. åŸå§‹å®šä¹‰: $W_1(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|]$
2. å¼•å…¥å¯¹å¶å˜é‡ (Lagrange å¯¹å¶)
3. åº”ç”¨ Fenchel-Rockafellar å¯¹å¶å®šç†
4. å¾—åˆ°å¯¹å¶å½¢å¼

**Sliced Wasserstein**:
$$\text{SWD}(P, Q) = \int_{\mathbb{S}^{d-1}} W_1(P_\theta, Q_\theta) d\theta$$

å…¶ä¸­ $P_\theta$ æ˜¯ $P$ åœ¨æ–¹å‘ $\theta$ ä¸Šçš„ä¸€ç»´æŠ•å½±ã€‚

#### 4. å› æœæ¨æ–­è¯¯å·®ç•Œ

**å®šç† (Shalit et al., 2017)**:

å‡è®¾è¡¨ç¤ºå‡½æ•° $\Phi: \mathcal{X} \to \mathcal{R}$ å’Œå‡è®¾å‡½æ•° $h_0, h_1: \mathcal{R} \to \mathbb{R}$ã€‚å®šä¹‰:
- é¢„æµ‹è¯¯å·®: $\epsilon_h = \mathbb{E}_{(x,t,y) \sim P} [(y - h_t(\Phi(x)))^2]$
- è¡¨ç¤ºå¹³è¡¡: $\text{IPM}(P_\Phi^{t=0}, P_\Phi^{t=1})$

åˆ™ **ATE ä¼°è®¡è¯¯å·®ä¸Šç•Œ**:
$$\epsilon_{\text{ATE}} \leq \sqrt{\epsilon_h} + \lambda \cdot \text{IPM}(P_\Phi^{t=0}, P_\Phi^{t=1})$$

å…¶ä¸­ $\lambda$ æ˜¯å‡è®¾å‡½æ•°çš„ Lipschitz å¸¸æ•°ã€‚

**æ¨è®º**:
1. è¦å‡å°‘ ATE ä¼°è®¡è¯¯å·®ï¼Œéœ€è¦åŒæ—¶:
   - å‡å°‘é¢„æµ‹è¯¯å·® $\epsilon_h$
   - å‡å°‘è¡¨ç¤ºä¸å¹³è¡¡ IPM
2. æœ€ä¼˜æƒè¡¡ç”±å‚æ•° $\alpha$ æ§åˆ¶

---

## Part 5.2: TARNet & DragonNet

### æ•°å­¦æ¨å¯¼

#### 1. Factual Loss æ¨å¯¼

**é—®é¢˜**: å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬åªè§‚æµ‹åˆ°ä¸€ä¸ªç»“æœã€‚

**Factual Loss å®šä¹‰**:
$$\mathcal{L}_{\text{factual}} = \frac{1}{N}\sum_{i=1}^{N} (Y_i - \hat{Y}_i^{\text{factual}})^2$$

å…¶ä¸­:
$$\hat{Y}_i^{\text{factual}} = \begin{cases}
\hat{Y}_i(1) & \text{if } T_i = 1 \\
\hat{Y}_i(0) & \text{if } T_i = 0
\end{cases}$$

**ç®€æ´å½¢å¼**:
$$\hat{Y}_i^{\text{factual}} = T_i \cdot \hat{Y}_i(1) + (1 - T_i) \cdot \hat{Y}_i(0)$$

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ?**
- ä½¿ç”¨è§‚æµ‹åˆ°çš„ç»“æœè¿›è¡Œç›‘ç£å­¦ä¹ 
- åŒæ—¶å­¦ä¹  $\mu_0(x)$ å’Œ $\mu_1(x)$
- åäº‹å®é¢„æµ‹é€šè¿‡å…±äº«è¡¨ç¤ºæ³›åŒ–

#### 2. Targeted Regularization ç†è®ºæ¨å¯¼

**åŠå‚æ•°æ•ˆç‡ç†è®º** (Semiparametric Efficiency Theory):

åœ¨å› æœæ¨æ–­ä¸­ï¼Œefficient influence function (EIF) ä¸º:
$$\psi(X, T, Y) = h(X) + \frac{T}{e(X)}(Y - \mu_1(X)) - \frac{1-T}{1-e(X)}(Y - \mu_0(X))$$

å…¶ä¸­:
- $h(X) = \mu_1(X) - \mu_0(X)$ æ˜¯ CATE
- $e(X) = P(T=1|X)$ æ˜¯å€¾å‘å¾—åˆ†

**Targeted Regularization** åŸºäº TMLE (Targeted Maximum Likelihood Estimation):

$$\mathcal{L}_{\text{targeted}} = \frac{1}{N}\sum_{i=1}^{N} \left(Y_i - \hat{Y}_i - \epsilon \cdot h_i\right)^2$$

å…¶ä¸­:
$$h_i = \frac{T_i}{\hat{e}(X_i)} - \frac{1-T_i}{1-\hat{e}(X_i)}$$

**ç›´è§‰**:
- $h_i$ æ˜¯æ ·æœ¬çš„"æƒé‡"
- å€¾å‘å¾—åˆ†ä½çš„æ ·æœ¬æƒé‡é«˜ï¼ˆæ›´é‡è¦ï¼‰
- $\epsilon$ æ˜¯å¯å­¦ä¹ çš„è°ƒæ•´å‚æ•°

#### 3. å€¾å‘å¾—åˆ†æ­£åˆ™åŒ–çš„ä½œç”¨

**DragonNet æŸå¤±**:
$$\mathcal{L} = \mathcal{L}_{\text{factual}} + \alpha \cdot \mathcal{L}_{\text{propensity}} + \beta \cdot \mathcal{L}_{\text{targeted}}$$

**å€¾å‘å¾—åˆ†æŸå¤±**:
$$\mathcal{L}_{\text{propensity}} = -\frac{1}{N}\sum_{i=1}^{N} [T_i \log \hat{e}_i + (1-T_i) \log (1-\hat{e}_i)]$$

**ä½œç”¨**:
1. **è¯†åˆ«æ··æ·†å› å­**: å¼ºè¿«è¡¨ç¤ºå±‚å­¦ä¹ ä¸å¤„ç†åˆ†é…ç›¸å…³çš„ç‰¹å¾
2. **æ­£åˆ™åŒ–æ•ˆæœ**: é˜²æ­¢è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šå¤„ç†ç»„
3. **åŒé‡é²æ£’æ€§**: ç»“åˆç»“æœå›å½’å’Œå€¾å‘å¾—åˆ†æ¨¡å‹çš„ä¼˜åŠ¿

### ä»é›¶å®ç°: TARNet

```python
import torch
import torch.nn as nn

class TARNet(nn.Module):
    """TARNet ä»é›¶å®ç°"""

    def __init__(self, input_dim, hidden_dim=64, repr_dim=32):
        super().__init__()

        # å…±äº«è¡¨ç¤ºå±‚
        self.shared = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, repr_dim),
            nn.ReLU()
        )

        # å¯¹ç…§ç»„å¤´ (T=0)
        self.head_0 = nn.Sequential(
            nn.Linear(repr_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        # å¤„ç†ç»„å¤´ (T=1)
        self.head_1 = nn.Sequential(
            nn.Linear(repr_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, x, t=None):
        phi = self.shared(x)
        y0 = self.head_0(phi).squeeze()
        y1 = self.head_1(phi).squeeze()

        if t is not None:
            y = torch.where(t == 1, y1, y0)
            return y, y0, y1, phi
        else:
            return y0, y1, phi

    def predict_ite(self, x):
        y0, y1, _ = self.forward(x)
        return y1 - y0


def train_tarnet(model, X, T, Y, n_epochs=200, lr=1e-3):
    """è®­ç»ƒ TARNet"""
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    X_tensor = torch.FloatTensor(X)
    T_tensor = torch.FloatTensor(T)
    Y_tensor = torch.FloatTensor(Y)

    for epoch in range(n_epochs):
        optimizer.zero_grad()

        # Factual loss
        y_pred, y0, y1, phi = model(X_tensor, T_tensor)
        loss = criterion(y_pred, Y_tensor)

        loss.backward()
        optimizer.step()

        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

    return model
```

### ä»é›¶å®ç°: DragonNet

```python
class DragonNet(nn.Module):
    """DragonNet ä»é›¶å®ç°"""

    def __init__(self, input_dim, hidden_dim=64, repr_dim=32):
        super().__init__()

        # å…±äº«è¡¨ç¤ºå±‚
        self.shared = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, repr_dim),
            nn.ELU()
        )

        # ä¸‰ä¸ªå¤´
        self.head_0 = nn.Linear(repr_dim, 1)  # Y(0)
        self.head_1 = nn.Linear(repr_dim, 1)  # Y(1)
        self.head_prop = nn.Linear(repr_dim, 1)  # å€¾å‘å¾—åˆ†

        # Epsilon å‚æ•°
        self.epsilon = nn.Parameter(torch.zeros(1))

    def forward(self, x, t=None):
        phi = self.shared(x)
        y0 = self.head_0(phi).squeeze()
        y1 = self.head_1(phi).squeeze()
        prop = torch.sigmoid(self.head_prop(phi).squeeze())

        if t is not None:
            y = torch.where(t == 1, y1, y0)
            return y, y0, y1, prop, phi
        else:
            return y0, y1, prop, phi


def dragonnet_loss(y_true, t_true, y_pred, y0, y1, prop, epsilon, alpha=1.0, beta=1.0):
    """DragonNet å¤åˆæŸå¤±"""

    # 1. Factual loss
    factual_loss = torch.mean((y_true - y_pred) ** 2)

    # 2. Propensity loss
    eps = 1e-8
    prop_loss = -torch.mean(
        t_true * torch.log(prop + eps) +
        (1 - t_true) * torch.log(1 - prop + eps)
    )

    # 3. Targeted regularization
    h = t_true / (prop + eps) - (1 - t_true) / (1 - prop + eps)
    targeted_loss = torch.mean((y_true - y_pred - epsilon * h) ** 2)

    # æ€»æŸå¤±
    total_loss = factual_loss + alpha * prop_loss + beta * targeted_loss

    return total_loss, factual_loss, prop_loss, targeted_loss


def train_dragonnet(model, X, T, Y, alpha=1.0, beta=1.0, n_epochs=200, lr=1e-3):
    """è®­ç»ƒ DragonNet"""
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    X_tensor = torch.FloatTensor(X)
    T_tensor = torch.FloatTensor(T)
    Y_tensor = torch.FloatTensor(Y)

    for epoch in range(n_epochs):
        optimizer.zero_grad()

        y_pred, y0, y1, prop, phi = model(X_tensor, T_tensor)

        total_loss, factual_loss, prop_loss, targeted_loss = dragonnet_loss(
            Y_tensor, T_tensor, y_pred, y0, y1, prop, model.epsilon, alpha, beta
        )

        total_loss.backward()
        optimizer.step()

        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch+1}: Total={total_loss.item():.4f}, "
                  f"Factual={factual_loss.item():.4f}, "
                  f"Prop={prop_loss.item():.4f}, "
                  f"Targeted={targeted_loss.item():.4f}, "
                  f"Epsilon={model.epsilon.item():.4f}")

    return model
```

---

## Part 5.3: CEVAE

### æ•°å­¦æ¨å¯¼

#### 1. ELBO å®Œæ•´æ¨å¯¼

**ç›®æ ‡**: æœ€å¤§åŒ–è¾¹é™…å¯¹æ•°ä¼¼ç„¶ $\log p_\theta(X, T, Y)$

**å¼•å…¥å˜åˆ†åˆ†å¸ƒ** $q_\phi(Z | X, T, Y)$:

$$\begin{align}
\log p_\theta(X, T, Y) &= \log \int p_\theta(X, T, Y, Z) dZ \\
&= \log \int \frac{p_\theta(X, T, Y, Z)}{q_\phi(Z | X, T, Y)} q_\phi(Z | X, T, Y) dZ \\
&= \log \mathbb{E}_{q_\phi} \left[\frac{p_\theta(X, T, Y, Z)}{q_\phi(Z | X, T, Y)}\right]
\end{align}$$

**åº”ç”¨ Jensen ä¸ç­‰å¼** ($\log \mathbb{E}[Â·] \geq \mathbb{E}[\log Â·]$):

$$\begin{align}
\log p_\theta(X, T, Y) &\geq \mathbb{E}_{q_\phi(Z|X,T,Y)} \left[\log \frac{p_\theta(X, T, Y, Z)}{q_\phi(Z | X, T, Y)}\right] \\
&= \mathbb{E}_{q_\phi} [\log p_\theta(X, T, Y, Z)] - \mathbb{E}_{q_\phi} [\log q_\phi(Z | X, T, Y)] \\
&\equiv \mathcal{L}_{\text{ELBO}}
\end{align}$$

**å±•å¼€ ELBO**:

$$\begin{align}
\mathcal{L}_{\text{ELBO}} &= \mathbb{E}_{q_\phi(Z|X,T,Y)} [\log p_\theta(X | Z) + \log p_\theta(T | X, Z) + \log p_\theta(Y | T, X, Z) + \log p(Z)] \\
&\quad - \mathbb{E}_{q_\phi(Z|X,T,Y)} [\log q_\phi(Z | X, T, Y)] \\
&= \underbrace{\mathbb{E}_{q_\phi} [\log p_\theta(X | Z)]}_{\text{X é‡æ„}} + \underbrace{\mathbb{E}_{q_\phi} [\log p_\theta(T | X, Z)]}_{\text{T é‡æ„}} + \underbrace{\mathbb{E}_{q_\phi} [\log p_\theta(Y | T, X, Z)]}_{\text{Y é‡æ„}} \\
&\quad - \underbrace{\text{KL}(q_\phi(Z | X, T, Y) \| p(Z))}_{\text{KL æ•£åº¦}}
\end{align}$$

#### 2. é‡å‚æ•°åŒ–æŠ€å·§ (Reparameterization Trick)

**é—®é¢˜**: é‡‡æ ·æ“ä½œä¸å¯å¾®ï¼Œæ— æ³•åå‘ä¼ æ’­ã€‚

å‡è®¾ $Z \sim q_\phi(Z | X) = \mathcal{N}(\mu_\phi(X), \sigma_\phi^2(X))$

**æœ´ç´ é‡‡æ ·** (ä¸å¯å¾®):
$$Z = \text{sample from } \mathcal{N}(\mu_\phi(X), \sigma_\phi^2(X))$$

**é‡å‚æ•°åŒ–** (å¯å¾®):
$$\begin{align}
\epsilon &\sim \mathcal{N}(0, I) \\
Z &= \mu_\phi(X) + \sigma_\phi(X) \odot \epsilon
\end{align}$$

ç°åœ¨ $Z$ å…³äº $\mu_\phi$ å’Œ $\sigma_\phi$ å¯å¾®ï¼

**æ¢¯åº¦è®¡ç®—**:
$$\begin{align}
\nabla_\phi \mathbb{E}_{q_\phi(Z|X)}[f(Z)] &= \nabla_\phi \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)}[f(\mu_\phi(X) + \sigma_\phi(X) \odot \epsilon)] \\
&= \mathbb{E}_{\epsilon}[\nabla_\phi f(\mu_\phi(X) + \sigma_\phi(X) \odot \epsilon)]
\end{align}$$

#### 3. KL æ•£åº¦çš„è§£æå½¢å¼

å¯¹äº $q_\phi(Z | X) = \mathcal{N}(\mu, \Sigma)$ å’Œ $p(Z) = \mathcal{N}(0, I)$:

$$\begin{align}
\text{KL}(q_\phi \| p) &= \mathbb{E}_{Z \sim q_\phi} \left[\log \frac{q_\phi(Z|X)}{p(Z)}\right] \\
&= \mathbb{E}_Z \left[\log q_\phi(Z|X) - \log p(Z)\right] \\
&= -\frac{1}{2} \sum_{j=1}^{d_z} \left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right)
\end{align}$$

**ç®€åŒ–**:
$$\text{KL}(q_\phi \| p) = \frac{1}{2} \sum_{j=1}^{d_z} \left(\mu_j^2 + \sigma_j^2 - \log \sigma_j^2 - 1\right)$$

### ä»é›¶å®ç°: CEVAE

```python
class CEVAE(nn.Module):
    """CEVAE ä»é›¶å®ç°"""

    def __init__(self, x_dim, latent_dim=20, hidden_dim=200):
        super().__init__()

        self.latent_dim = latent_dim

        # ç¼–ç å™¨ q(Z | X, T, Y)
        self.encoder = nn.Sequential(
            nn.Linear(x_dim + 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

        # è§£ç å™¨ p(X | Z)
        self.decoder_x = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, x_dim)
        )

        # è§£ç å™¨ p(T | X, Z)
        self.decoder_t = nn.Sequential(
            nn.Linear(x_dim + latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

        # è§£ç å™¨ p(Y | T, X, Z)
        self.decoder_y = nn.Sequential(
            nn.Linear(x_dim + latent_dim + 1, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def encode(self, x, t, y):
        """ç¼–ç : q(Z | X, T, Y)"""
        inputs = torch.cat([x, t.unsqueeze(1), y.unsqueeze(1)], dim=1)
        h = self.encoder(inputs)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z, x, t):
        """è§£ç """
        x_recon = self.decoder_x(z)
        t_recon = self.decoder_t(torch.cat([x, z], dim=1)).squeeze()
        y_recon = self.decoder_y(torch.cat([x, t.unsqueeze(1), z], dim=1)).squeeze()
        return x_recon, t_recon, y_recon

    def forward(self, x, t, y):
        """å®Œæ•´å‰å‘ä¼ æ’­"""
        mu, logvar = self.encode(x, t, y)
        z = self.reparameterize(mu, logvar)
        x_recon, t_recon, y_recon = self.decode(z, x, t)

        return {
            'x_recon': x_recon,
            't_recon': t_recon,
            'y_recon': y_recon,
            'mu': mu,
            'logvar': logvar,
            'z': z
        }

    def predict_counterfactual(self, x, t, y, n_samples=100):
        """é¢„æµ‹åäº‹å®"""
        self.eval()
        with torch.no_grad():
            mu, logvar = self.encode(x, t, y)

            y0_samples = []
            y1_samples = []

            for _ in range(n_samples):
                z = self.reparameterize(mu, logvar)

                t0 = torch.zeros_like(t)
                t1 = torch.ones_like(t)

                _, _, y0 = self.decode(z, x, t0)
                _, _, y1 = self.decode(z, x, t1)

                y0_samples.append(y0)
                y1_samples.append(y1)

            y0_pred = torch.stack(y0_samples).mean(dim=0)
            y1_pred = torch.stack(y1_samples).mean(dim=0)

        return y0_pred, y1_pred


def cevae_loss(outputs, x, t, y, beta=1.0):
    """CEVAE æŸå¤±å‡½æ•°"""

    # X é‡æ„æŸå¤±
    x_recon_loss = torch.mean((outputs['x_recon'] - x) ** 2)

    # T é‡æ„æŸå¤± (BCE)
    eps = 1e-8
    t_recon_loss = -torch.mean(
        t * torch.log(outputs['t_recon'] + eps) +
        (1 - t) * torch.log(1 - outputs['t_recon'] + eps)
    )

    # Y é‡æ„æŸå¤±
    y_recon_loss = torch.mean((outputs['y_recon'] - y) ** 2)

    # KL æ•£åº¦
    kl_loss = -0.5 * torch.sum(
        1 + outputs['logvar'] - outputs['mu'].pow(2) - outputs['logvar'].exp()
    ) / x.size(0)

    # æ€»æŸå¤±
    total_loss = x_recon_loss + t_recon_loss + y_recon_loss + beta * kl_loss

    return total_loss, x_recon_loss, t_recon_loss, y_recon_loss, kl_loss
```

---

## ç»¼åˆé¢è¯•é¢˜åº“

### æ·±åº¦å› æœæ¨æ–­é¢è¯•é¢˜

#### ç†è®ºé¢˜

**1. ä¸ºä»€ä¹ˆ TARNet éœ€è¦å…±äº«è¡¨ç¤ºå±‚ï¼Ÿ**

**ç­”æ¡ˆ**:
- **æ ·æœ¬æ•ˆç‡**: ä¸¤ç»„å…±äº«ç‰¹å¾æå–å™¨ï¼Œå¢åŠ æœ‰æ•ˆè®­ç»ƒæ ·æœ¬
- **æ³›åŒ–èƒ½åŠ›**: å…±äº«å‚æ•°é˜²æ­¢è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šç»„
- **åäº‹å®é¢„æµ‹**: é€šè¿‡å…±äº«è¡¨ç¤ºæ³›åŒ–åˆ°æœªè§‚æµ‹çš„åäº‹å®
- **è¡¨ç¤ºå¹³è¡¡**: å…±äº«å±‚ä½¿ä¸¤ç»„åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ›´æ¥è¿‘

**2. Factual Loss ä¸æ™®é€šç›‘ç£å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

**ç­”æ¡ˆ**:
- **Factual Loss**: $\mathcal{L} = \sum_i (Y_i - [T_i \hat{Y}_i(1) + (1-T_i) \hat{Y}_i(0)])^2$
  - æ¯ä¸ªæ ·æœ¬åªç”¨è§‚æµ‹åˆ°çš„ç»“æœ
  - åŒæ—¶è®­ç»ƒä¸¤ä¸ªå¤´ ($\mu_0$ å’Œ $\mu_1$)
  - æŸå¤±"é€‰æ‹©"å¯¹åº”çš„å¤´

- **æ™®é€šç›‘ç£å­¦ä¹ **: $\mathcal{L} = \sum_i (Y_i - f(X_i))^2$
  - å•ä¸€é¢„æµ‹å‡½æ•°
  - æ— åäº‹å®æ¦‚å¿µ

**3. DragonNet çš„å€¾å‘å¾—åˆ†å¤´åœ¨æ¨¡å‹ä¸­èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ**

**ç­”æ¡ˆ**:
1. **è¯†åˆ«æ··æ·†**: å¼ºè¿«è¡¨ç¤ºå±‚å­¦ä¹ ä¸å¤„ç†åˆ†é…ç›¸å…³çš„ç‰¹å¾ï¼ˆå³æ··æ·†å› å­ï¼‰
2. **æ­£åˆ™åŒ–**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–
3. **Targeted Regularization**: é…åˆ epsilon å‚æ•°å®ç°åŒé‡é²æ£’ä¼°è®¡
4. **ç†è®ºä¿è¯**: åŸºäºåŠå‚æ•°æ•ˆç‡ç†è®º

**4. CEVAE å¦‚ä½•å¤„ç†éšæ··æ·†ï¼Ÿ**

**ç­”æ¡ˆ**:
- **å»ºæ¨¡éšå˜é‡**: ç”¨ VAE å­¦ä¹ éšå˜é‡ $Z$ çš„åˆ†å¸ƒ
- **ä»£ç†å˜é‡å‡è®¾**: è§‚æµ‹åˆ°çš„ $X$ åŒ…å«å…³äº $Z$ çš„ä¿¡æ¯
- **æ¡ä»¶ç‹¬ç«‹**: ç»™å®š $Z$ï¼Œå¤„ç†å’Œç»“æœæ¡ä»¶ç‹¬ç«‹
- **è¾¹ç¼˜åŒ–**: é€šè¿‡ç§¯åˆ† $\int p(Y|T,X,Z)p(Z|X) dZ$ è·å¾—å› æœæ•ˆåº”

**5. GANITE ä¸ºä»€ä¹ˆç”¨ GAN è€Œä¸æ˜¯ VAE ç”Ÿæˆåäº‹å®ï¼Ÿ**

**ç­”æ¡ˆ**:
- **åˆ†å¸ƒè´¨é‡**: GAN ç”Ÿæˆçš„æ ·æœ¬æ›´sharpï¼ŒVAE å€¾å‘äºæ¨¡ç³Š
- **å¤šæ¨¡æ€**: GAN å¯ä»¥æ•è·å¤šæ¨¡æ€åˆ†å¸ƒ
- **å¯¹æŠ—è®­ç»ƒ**: åˆ¤åˆ«å™¨å¸®åŠ©ç”Ÿæˆæ›´çœŸå®çš„åäº‹å®
- **ç¼ºç‚¹**: GAN è®­ç»ƒä¸ç¨³å®šï¼ŒVAE æœ‰ç†è®ºä¿è¯ï¼ˆELBOï¼‰

**6. VCNet å¦‚ä½•å¤„ç†è¿ç»­å¤„ç†ï¼Ÿ**

**ç­”æ¡ˆ**:
- **å˜ç³»æ•°ç½‘ç»œ**: $W(t) \cdot \phi(X)$ï¼Œæƒé‡éšå¤„ç†å¼ºåº¦å˜åŒ–
- **æ ·æ¡åŸºå‡½æ•°**: ä¿è¯å‰‚é‡-å“åº”æ›²çº¿å…‰æ»‘
- **å¹¿ä¹‰å€¾å‘å¾—åˆ†**: $e(t|X) = f_{T|X}(t|x)$ æ¦‚ç‡å¯†åº¦
- **è¾¹é™…å¤„ç†æ•ˆåº”**: $\frac{\partial \mu(t,x)}{\partial t}$

#### ç¼–ç¨‹é¢˜

**é¢˜ç›® 1: å®ç° TARNet çš„ Factual Loss**

```python
def factual_loss(y_true, t_true, y0_pred, y1_pred):
    """
    å®ç° Factual Loss

    Args:
        y_true: çœŸå®ç»“æœ (N,)
        t_true: å¤„ç†æ ‡ç­¾ (N,)
        y0_pred: Y(0) é¢„æµ‹ (N,)
        y1_pred: Y(1) é¢„æµ‹ (N,)

    Returns:
        loss: Factual Loss
    """
    # ç­”æ¡ˆ:
    y_pred = torch.where(t_true == 1, y1_pred, y0_pred)
    loss = torch.mean((y_true - y_pred) ** 2)
    return loss
```

**é¢˜ç›® 2: å®ç° VAE çš„é‡å‚æ•°åŒ–**

```python
def reparameterize(mu, logvar):
    """
    å®ç° VAE é‡å‚æ•°åŒ–æŠ€å·§

    Args:
        mu: å‡å€¼ (batch, latent_dim)
        logvar: log æ–¹å·® (batch, latent_dim)

    Returns:
        z: é‡‡æ ·çš„éšå˜é‡ (batch, latent_dim)
    """
    # ç­”æ¡ˆ:
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    z = mu + eps * std
    return z
```

**é¢˜ç›® 3: å®ç° MMD çš„ PyTorch å¯å¾®ç‰ˆæœ¬**

```python
def mmd_loss_pytorch(phi_t, phi_c, gamma=1.0):
    """
    PyTorch å¯å¾® MMD Loss

    Args:
        phi_t: å¤„ç†ç»„è¡¨ç¤º (n, d)
        phi_c: å¯¹ç…§ç»„è¡¨ç¤º (m, d)
        gamma: RBF æ ¸å‚æ•°

    Returns:
        mmd: MMD æŸå¤±
    """
    def rbf_kernel(X, Y):
        XX = torch.sum(X**2, dim=1, keepdim=True)
        YY = torch.sum(Y**2, dim=1, keepdim=True)
        XY = X @ Y.T
        dist_sq = XX + YY.T - 2 * XY
        return torch.exp(-gamma * dist_sq)

    n = phi_t.shape[0]
    m = phi_c.shape[0]

    K_TT = rbf_kernel(phi_t, phi_t)
    K_TC = rbf_kernel(phi_t, phi_c)
    K_CC = rbf_kernel(phi_c, phi_c)

    term1 = (K_TT.sum() - torch.trace(K_TT)) / (n * (n - 1))
    term2 = K_TC.sum() / (n * m)
    term3 = (K_CC.sum() - torch.trace(K_CC)) / (m * (m - 1))

    mmd_sq = term1 - 2 * term2 + term3

    return mmd_sq  # è¿”å›å¹³æ–¹é¿å… sqrt æ¢¯åº¦é—®é¢˜
```

### ç³»ç»Ÿè®¾è®¡é¢˜

**é¢˜ç›®: è®¾è®¡ä¸€ä¸ªä¼˜æƒ åˆ¸é¢é¢ä¼˜åŒ–ç³»ç»Ÿ**

**è¦æ±‚**:
1. è¾“å…¥: ç”¨æˆ·ç‰¹å¾ X
2. è¾“å‡º: æœ€ä¼˜ä¼˜æƒ åˆ¸é¢é¢
3. è€ƒè™‘: ROIã€é¢„ç®—çº¦æŸã€ABæµ‹è¯•

**ç­”æ¡ˆæ¡†æ¶**:

```python
class CouponOptimizationSystem:
    """ä¼˜æƒ åˆ¸ä¼˜åŒ–ç³»ç»Ÿ"""

    def __init__(self):
        # æ¨¡å‹: VCNet æˆ– DRNet
        self.dose_response_model = VCNet(input_dim=user_feature_dim)

        # çº¦æŸ
        self.budget = total_budget
        self.cost_per_yuan = cost_per_yuan

    def train(self, X, T, Y):
        """è®­ç»ƒå‰‚é‡-å“åº”æ¨¡å‹"""
        # ä½¿ç”¨å†å² AB æµ‹è¯•æ•°æ®
        train_vcnet(self.dose_response_model, X, T, Y)

    def find_optimal_coupon(self, user_features):
        """ä¸ºå•ä¸ªç”¨æˆ·æ‰¾æœ€ä¼˜é¢é¢"""
        # æœç´¢ç©ºé—´: [0, 50] å…ƒ
        t_values = np.linspace(0, 50, 100)

        # é¢„æµ‹å“åº”
        y_pred = self.dose_response_model.predict_dose_response(
            user_features, t_values
        )

        # è®¡ç®— ROI
        costs = t_values * self.cost_per_yuan
        net_profit = y_pred - costs

        # æ‰¾æœ€ä¼˜
        optimal_idx = np.argmax(net_profit)
        optimal_amount = t_values[optimal_idx]

        return optimal_amount

    def batch_optimize(self, user_features_batch):
        """æ‰¹é‡ä¼˜åŒ–ï¼ˆè€ƒè™‘é¢„ç®—çº¦æŸï¼‰"""
        n_users = len(user_features_batch)

        # ä¸ºæ¯ä¸ªç”¨æˆ·æ‰¾æœ€ä¼˜é¢é¢
        optimal_amounts = []
        expected_profits = []

        for features in user_features_batch:
            amount = self.find_optimal_coupon(features)
            profit = self.estimate_profit(features, amount)

            optimal_amounts.append(amount)
            expected_profits.append(profit)

        # é¢„ç®—çº¦æŸ: é€‰æ‹© ROI æœ€é«˜çš„ç”¨æˆ·å‘åˆ¸
        total_cost = sum(optimal_amounts)

        if total_cost > self.budget:
            # æŒ‰ ROI æ’åº
            roi = np.array(expected_profits) / np.array(optimal_amounts)
            sorted_idx = np.argsort(roi)[::-1]

            # è´ªå¿ƒé€‰æ‹©
            selected = []
            remaining_budget = self.budget

            for idx in sorted_idx:
                if optimal_amounts[idx] <= remaining_budget:
                    selected.append(idx)
                    remaining_budget -= optimal_amounts[idx]

            return selected, [optimal_amounts[i] for i in selected]
        else:
            return list(range(n_users)), optimal_amounts
```

---

## å­¦ä¹ è·¯å¾„å»ºè®®

### åˆå­¦è€… (0-3 ä¸ªæœˆ)

1. **åŸºç¡€ç†è®º** (2 å‘¨)
   - æ½œåœ¨ç»“æœæ¡†æ¶
   - å› æœå›¾ (DAG)
   - æ··æ·†ã€é€‰æ‹©åå·®

2. **ä¼ ç»Ÿæ–¹æ³•** (4 å‘¨)
   - PSM, IPW, DR
   - Meta-Learners (S/T/X-Learner)
   - Causal Forest

3. **æ·±åº¦å­¦ä¹ åŸºç¡€** (2 å‘¨)
   - PyTorch åŸºç¡€
   - ç¥ç»ç½‘ç»œè®­ç»ƒ
   - æ­£åˆ™åŒ–æŠ€å·§

4. **Part 5: æ·±åº¦å› æœæ¨¡å‹** (4 å‘¨)
   - Week 1: è¡¨ç¤ºå­¦ä¹  + IPM
   - Week 2: TARNet + DragonNet
   - Week 3: CEVAE
   - Week 4: GANITE + VCNet

### è¿›é˜¶è€… (3-6 ä¸ªæœˆ)

1. **ç†è®ºæ·±å…¥**
   - åŠå‚æ•°æ•ˆç‡ç†è®º
   - åŒé‡é²æ£’ä¼°è®¡
   - æ•æ„Ÿæ€§åˆ†æ

2. **é«˜çº§æ¨¡å‹**
   - TEDVAE, Perfect Match
   - SITE, Causal Transformer
   - Continuous Treatment

3. **å®æˆ˜é¡¹ç›®**
   - è¥é”€ä¼˜åŒ–
   - ä¸ªæ€§åŒ–æ¨è
   - åŒ»ç–—å†³ç­–æ”¯æŒ

### é¢è¯•å‡†å¤‡ (1-2 ä¸ªæœˆ)

1. **ç†è®ºå¤ä¹ **
   - æ¯ä¸ªæ¨¡å‹çš„æ•°å­¦æ¨å¯¼
   - å‡è®¾æ¡ä»¶å’Œé€‚ç”¨åœºæ™¯
   - ä¼˜ç¼ºç‚¹å¯¹æ¯”

2. **ç¼–ç¨‹ç»ƒä¹ **
   - ä»é›¶å®ç°æ‰€æœ‰æ¨¡å‹
   - LeetCode é£æ ¼ç¼–ç¨‹é¢˜
   - ç³»ç»Ÿè®¾è®¡é¢˜

3. **è®ºæ–‡é˜…è¯»**
   - TARNet (ICML 2017)
   - DragonNet (2019)
   - CEVAE (ICLR 2018)
   - GANITE (ICLR 2018)
   - VCNet (ICLR 2021)

---

## å‚è€ƒæ–‡çŒ®

1. Shalit, U., Johansson, F. D., & Sontag, D. (2017). Estimating individual treatment effect: generalization bounds and algorithms. ICML.

2. Shi, C., Blei, D., & Veitch, V. (2019). Adapting neural networks for the estimation of treatment effects. NeurIPS.

3. Louizos, C., Shalit, U., Mooij, J. M., Sontag, D., Zemel, R., & Welling, M. (2017). Causal effect inference with deep latent-variable models. NeurIPS.

4. Yoon, J., Jordon, J., & van der Schaar, M. (2018). GANITE: Estimation of individualized treatment effects using generative adversarial nets. ICLR.

5. Nie, X., Ye, M., Liu, Q., & Nicolae, D. (2021). VCNet and functional targeted regularization for learning causal effects of continuous treatments. ICLR.

---

**æœ¬æ–‡æ¡£æä¾›äº† Part 5 æ‰€æœ‰ Notebook çš„å®Œæ•´è§£ç­”ã€æ•°å­¦æ¨å¯¼å’Œé¢è¯•å‡†å¤‡ææ–™ã€‚**

**å»ºè®®å­¦ä¹ è·¯å¾„**:
1. å…ˆç†è§£ç†è®ºæ¨å¯¼
2. å®Œæˆæ‰€æœ‰ç¼–ç¨‹ç»ƒä¹ 
3. åšé¢è¯•é¢˜å·©å›º
4. åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨

**Good luck! ğŸš€**
