{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匹配方法 - 构建可比的对照组\n",
    "\n",
    "**学习目标**：\n",
    "1. 理解匹配方法的原理和直觉\n",
    "2. 掌握精确匹配、PSM、马氏距离匹配等算法\n",
    "3. 学习匹配质量评估和效应估计\n",
    "\n",
    "**业务场景**：某电商平台想评估会员权益对用户消费的影响。会员用户和非会员用户在年龄、购物频次、历史消费等方面存在显著差异。如何构建可比的对照组？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境准备\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ 环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: 匹配的直觉\n",
    "\n",
    "### 1.1 为什么需要匹配？\n",
    "\n",
    "在观察性数据中，处理组和对照组往往在**协变量**（covariates）上存在系统性差异：\n",
    "\n",
    "- **选择偏差**：会员用户可能本身就是高消费人群\n",
    "- **混淆因素**：年龄、收入、购物习惯都会影响消费\n",
    "- **不可比性**：直接比较两组均值会得到有偏估计\n",
    "\n",
    "**匹配的目标**：通过找到协变量相似的个体，模拟随机化实验的条件。\n",
    "\n",
    "### 1.2 匹配与随机化\n",
    "\n",
    "| 特征 | 随机化实验 (RCT) | 匹配方法 |\n",
    "|------|-----------------|----------|\n",
    "| 组间可比性 | 期望相同 | 匹配后相同 |\n",
    "| 混淆控制 | 自动平衡 | 需手动匹配 |\n",
    "| 样本利用 | 全部使用 | 可能丢弃 |\n",
    "| 适用场景 | 实验数据 | 观察数据 |\n",
    "\n",
    "**核心假设**：条件独立性假设 (Conditional Independence Assumption, CIA)\n",
    "\n",
    "$$\n",
    "(Y_0, Y_1) \\perp T \\mid X\n",
    "$$\n",
    "\n",
    "给定协变量 $X$，潜在结果与处理分配独立。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模拟数据：会员权益评估\n",
    "def generate_membership_data(n=1000):\n",
    "    \"\"\"\n",
    "    生成电商会员数据\n",
    "    - 协变量：年龄、历史消费、购物频次\n",
    "    - 处理：是否为会员 (选择偏差：高消费者更可能成为会员)\n",
    "    - 结果：月消费金额\n",
    "    \"\"\"\n",
    "    # 协变量\n",
    "    age = np.random.normal(35, 10, n).clip(18, 65)\n",
    "    hist_spending = np.random.gamma(5, 100, n)  # 历史消费\n",
    "    freq = np.random.poisson(3, n) + 1  # 月购物频次\n",
    "    \n",
    "    # 处理分配（选择偏差）\n",
    "    propensity = 1 / (1 + np.exp(-(-3 + 0.05*age + 0.002*hist_spending + 0.3*freq)))\n",
    "    treatment = (np.random.uniform(0, 1, n) < propensity).astype(int)\n",
    "    \n",
    "    # 潜在结果\n",
    "    # Y(0): 非会员消费 = f(协变量) + 噪声\n",
    "    y0 = 200 + 5*age + 0.3*hist_spending + 50*freq + np.random.normal(0, 100, n)\n",
    "    \n",
    "    # Y(1): 会员消费 = Y(0) + 真实处理效应 (300元)\n",
    "    true_effect = 300\n",
    "    y1 = y0 + true_effect\n",
    "    \n",
    "    # 观测结果\n",
    "    y_obs = treatment * y1 + (1 - treatment) * y0\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'user_id': range(n),\n",
    "        'age': age,\n",
    "        'hist_spending': hist_spending,\n",
    "        'freq': freq,\n",
    "        'treatment': treatment,\n",
    "        'spending': y_obs,\n",
    "        'propensity': propensity,\n",
    "        'y0': y0,\n",
    "        'y1': y1\n",
    "    })\n",
    "    \n",
    "    return df, true_effect\n",
    "\n",
    "df, true_ate = generate_membership_data(1000)\n",
    "print(f\"样本量: {len(df)}\")\n",
    "print(f\"会员用户: {df['treatment'].sum()}\")\n",
    "print(f\"非会员用户: {(1-df['treatment']).sum()}\")\n",
    "print(f\"\\n真实 ATE: {true_ate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：匹配前的不平衡性\n",
    "def plot_covariate_balance(df, matched_df=None, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    绘制协变量平衡性对比\n",
    "    \"\"\"\n",
    "    covariates = ['age', 'hist_spending', 'freq']\n",
    "    labels = ['年龄', '历史消费', '购物频次']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=labels,\n",
    "        specs=[[{'secondary_y': False}]*3]\n",
    "    )\n",
    "    \n",
    "    for i, (cov, label) in enumerate(zip(covariates, labels)):\n",
    "        # 原始数据\n",
    "        treated = df[df['treatment']==1][cov]\n",
    "        control = df[df['treatment']==0][cov]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=control, name='对照组', opacity=0.5, \n",
    "                        marker_color='#EB5757', legendgroup='control',\n",
    "                        showlegend=(i==0), nbinsx=30),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=treated, name='处理组', opacity=0.5,\n",
    "                        marker_color='#2D9CDB', legendgroup='treated',\n",
    "                        showlegend=(i==0), nbinsx=30),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        \n",
    "        # 如果提供了匹配后数据\n",
    "        if matched_df is not None:\n",
    "            matched_treated = matched_df[matched_df['treatment']==1][cov]\n",
    "            matched_control = matched_df[matched_df['treatment']==0][cov]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Histogram(x=matched_control, name='对照组(匹配后)', \n",
    "                            opacity=0.7, marker_color='#F2994A',\n",
    "                            legendgroup='matched_control',\n",
    "                            showlegend=(i==0), nbinsx=30),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Histogram(x=matched_treated, name='处理组(匹配后)',\n",
    "                            opacity=0.7, marker_color='#27AE60',\n",
    "                            legendgroup='matched_treated',\n",
    "                            showlegend=(i==0), nbinsx=30),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"协变量分布对比{title_suffix}\",\n",
    "        barmode='overlay',\n",
    "        height=400,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"值\")\n",
    "    fig.update_yaxes(title_text=\"频数\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_covariate_balance(df, title_suffix=\" - 匹配前\")\n",
    "fig.show()\n",
    "\n",
    "# 计算标准化均值差\n",
    "def compute_smd(df, covariates):\n",
    "    \"\"\"\n",
    "    计算标准化均值差 (Standardized Mean Difference)\n",
    "    SMD = (mean_treated - mean_control) / sqrt((var_treated + var_control) / 2)\n",
    "    一般认为 |SMD| < 0.1 表示平衡良好\n",
    "    \"\"\"\n",
    "    treated = df[df['treatment']==1]\n",
    "    control = df[df['treatment']==0]\n",
    "    \n",
    "    smds = {}\n",
    "    for cov in covariates:\n",
    "        mean_diff = treated[cov].mean() - control[cov].mean()\n",
    "        pooled_std = np.sqrt((treated[cov].var() + control[cov].var()) / 2)\n",
    "        smds[cov] = mean_diff / pooled_std\n",
    "    \n",
    "    return smds\n",
    "\n",
    "covariates = ['age', 'hist_spending', 'freq']\n",
    "smd_before = compute_smd(df, covariates)\n",
    "print(\"\\n匹配前的 SMD:\")\n",
    "for cov, smd in smd_before.items():\n",
    "    print(f\"  {cov}: {smd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**观察**：\n",
    "- 会员用户在所有协变量上都显著高于非会员\n",
    "- SMD 绝对值都远大于 0.1，说明两组严重不平衡\n",
    "- 直接比较均值会严重高估会员权益的效果\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: 精确匹配 (Exact Matching)\n",
    "\n",
    "### 2.1 原理\n",
    "\n",
    "**精确匹配**：为每个处理组个体找到协变量完全相同的对照组个体。\n",
    "\n",
    "**优点**：\n",
    "- 概念简单\n",
    "- 匹配质量高\n",
    "\n",
    "**缺点**：\n",
    "- 维度灾难：协变量越多，找到精确匹配的概率越低\n",
    "- 连续变量难以精确匹配\n",
    "- 样本浪费严重\n",
    "\n",
    "### 2.2 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精确匹配（需要将连续变量离散化）\n",
    "def exact_matching(df, covariates, bins_dict):\n",
    "    \"\"\"\n",
    "    精确匹配\n",
    "    \n",
    "    参数:\n",
    "        df: 数据框\n",
    "        covariates: 协变量列表\n",
    "        bins_dict: 每个协变量的分箱边界，例如 {'age': [0, 30, 40, 50, 100]}\n",
    "    \n",
    "    返回:\n",
    "        matched_df: 匹配后的数据框\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 离散化连续变量\n",
    "    for cov, bins in bins_dict.items():\n",
    "        df_copy[f'{cov}_bin'] = pd.cut(df_copy[cov], bins=bins, labels=False)\n",
    "    \n",
    "    bin_cols = [f'{cov}_bin' for cov in bins_dict.keys()]\n",
    "    \n",
    "    # 按照离散化后的协变量分组\n",
    "    matched_pairs = []\n",
    "    \n",
    "    treated = df_copy[df_copy['treatment']==1]\n",
    "    control = df_copy[df_copy['treatment']==0]\n",
    "    \n",
    "    for _, t_row in treated.iterrows():\n",
    "        # 找到所有协变量匹配的对照组\n",
    "        mask = (control[bin_cols] == t_row[bin_cols]).all(axis=1)\n",
    "        matched_controls = control[mask]\n",
    "        \n",
    "        if len(matched_controls) > 0:\n",
    "            # 随机选择一个（1:1 匹配）\n",
    "            c_row = matched_controls.sample(1).iloc[0]\n",
    "            matched_pairs.append(t_row)\n",
    "            matched_pairs.append(c_row)\n",
    "    \n",
    "    matched_df = pd.DataFrame(matched_pairs)\n",
    "    return matched_df\n",
    "\n",
    "# 定义分箱策略\n",
    "bins_dict = {\n",
    "    'age': [0, 30, 40, 50, 100],\n",
    "    'hist_spending': [0, 300, 600, 1000, 10000],\n",
    "    'freq': [0, 2, 4, 6, 100]\n",
    "}\n",
    "\n",
    "matched_exact = exact_matching(df, covariates, bins_dict)\n",
    "print(f\"匹配前样本量: {len(df)}\")\n",
    "print(f\"匹配后样本量: {len(matched_exact)} ({len(matched_exact)/len(df)*100:.1f}%)\")\n",
    "print(f\"处理组保留率: {matched_exact['treatment'].sum() / df['treatment'].sum() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估匹配质量\n",
    "smd_exact = compute_smd(matched_exact, covariates)\n",
    "print(\"\\n精确匹配后的 SMD:\")\n",
    "for cov, smd in smd_exact.items():\n",
    "    print(f\"  {cov}: {smd:.3f} (改善: {abs(smd_before[cov]) - abs(smd):.3f})\")\n",
    "\n",
    "# 可视化\n",
    "fig = plot_covariate_balance(df, matched_exact, title_suffix=\" - 精确匹配前后\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 粗化精确匹配 (Coarsened Exact Matching, CEM)\n",
    "\n",
    "**改进思路**：不要求完全精确匹配，而是将协变量粗化后再匹配。\n",
    "\n",
    "**优点**：\n",
    "- 平衡样本保留和匹配质量\n",
    "- 对分箱策略不敏感\n",
    "- 满足单调不平衡约束 (Monotonic Imbalance Bounding)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: 倾向得分匹配 (Propensity Score Matching, PSM)\n",
    "\n",
    "### 3.1 倾向得分的魔力\n",
    "\n",
    "**倾向得分** (Propensity Score):\n",
    "\n",
    "$$\n",
    "e(X) = P(T=1 \\mid X)\n",
    "$$\n",
    "\n",
    "即给定协变量 $X$，个体接受处理的概率。\n",
    "\n",
    "**Rosenbaum & Rubin (1983) 定理**：如果 $(Y_0, Y_1) \\perp T \\mid X$，则 $(Y_0, Y_1) \\perp T \\mid e(X)$\n",
    "\n",
    "**含义**：\n",
    "- 可以用一维的倾向得分代替高维的协变量\n",
    "- 倾向得分相同的个体在协变量分布上是平衡的\n",
    "- 降维！解决维度灾难\n",
    "\n",
    "### 3.2 PSM 步骤\n",
    "\n",
    "1. **估计倾向得分**：用逻辑回归 $P(T=1|X)$\n",
    "2. **检查共同支撑** (Common Support)：删除倾向得分不重叠的样本\n",
    "3. **匹配**：为每个处理组个体找到倾向得分相近的对照组\n",
    "4. **评估平衡性**：检查匹配后协变量是否平衡\n",
    "5. **估计效应**：计算匹配样本的均值差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 估计倾向得分\n",
    "def estimate_propensity_score(df, covariates):\n",
    "    \"\"\"\n",
    "    用逻辑回归估计倾向得分\n",
    "    \"\"\"\n",
    "    X = df[covariates]\n",
    "    y = df['treatment']\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ps = model.predict_proba(X)[:, 1]\n",
    "    return ps\n",
    "\n",
    "df['ps_estimated'] = estimate_propensity_score(df, covariates)\n",
    "\n",
    "# 可视化倾向得分分布\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df[df['treatment']==0]['ps_estimated'],\n",
    "    name='对照组',\n",
    "    opacity=0.6,\n",
    "    marker_color='#EB5757',\n",
    "    nbinsx=50\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df[df['treatment']==1]['ps_estimated'],\n",
    "    name='处理组',\n",
    "    opacity=0.6,\n",
    "    marker_color='#2D9CDB',\n",
    "    nbinsx=50\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='倾向得分分布',\n",
    "    xaxis_title='倾向得分',\n",
    "    yaxis_title='频数',\n",
    "    barmode='overlay',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 共同支撑区域\n",
    "ps_treated = df[df['treatment']==1]['ps_estimated']\n",
    "ps_control = df[df['treatment']==0]['ps_estimated']\n",
    "common_support_min = max(ps_treated.min(), ps_control.min())\n",
    "common_support_max = min(ps_treated.max(), ps_control.max())\n",
    "\n",
    "print(f\"\\n共同支撑区域: [{common_support_min:.3f}, {common_support_max:.3f}]\")\n",
    "print(f\"处理组在支撑区域内: {((ps_treated >= common_support_min) & (ps_treated <= common_support_max)).sum()} / {len(ps_treated)}\")\n",
    "print(f\"对照组在支撑区域内: {((ps_control >= common_support_min) & (ps_control <= common_support_max)).sum()} / {len(ps_control)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 匹配算法\n",
    "\n",
    "#### (1) 最近邻匹配 (Nearest Neighbor Matching)\n",
    "\n",
    "为每个处理组个体 $i$ 找到倾向得分最接近的对照组个体 $j$：\n",
    "\n",
    "$$\n",
    "j^* = \\arg\\min_{j \\in \\{T=0\\}} |e(X_i) - e(X_j)|\n",
    "$$\n",
    "\n",
    "- **1:1 匹配**：每个处理组个体匹配 1 个对照组\n",
    "- **1:N 匹配**：每个处理组个体匹配 N 个对照组（降低方差，但可能引入偏差）\n",
    "\n",
    "#### (2) 卡尺匹配 (Caliper Matching)\n",
    "\n",
    "只匹配倾向得分在一定范围内的个体：\n",
    "\n",
    "$$\n",
    "|e(X_i) - e(X_j)| < \\text{caliper}\n",
    "$$\n",
    "\n",
    "常用卡尺：$0.2 \\times \\text{std}(e(X))$\n",
    "\n",
    "#### (3) 有放回 vs 无放回\n",
    "\n",
    "- **无放回**：每个对照组个体只能匹配一次（匹配质量可能下降）\n",
    "- **有放回**：对照组个体可以重复使用（提高匹配质量，但方差估计需调整）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: 实现 1:1 最近邻匹配（无放回，带卡尺）\n",
    "def psm_matching(df, caliper=0.2, ratio=1, with_replacement=False):\n",
    "    \"\"\"\n",
    "    倾向得分匹配\n",
    "    \n",
    "    参数:\n",
    "        df: 数据框（需包含 'ps_estimated' 列）\n",
    "        caliper: 卡尺（标准差的倍数）\n",
    "        ratio: 匹配比例 (1:ratio)\n",
    "        with_replacement: 是否有放回\n",
    "    \n",
    "    返回:\n",
    "        matched_df: 匹配后的数据框\n",
    "    \"\"\"\n",
    "    # 提示: \n",
    "    # 1. 计算卡尺阈值 = caliper * std(ps)\n",
    "    # 2. 为每个处理组个体找到最近的对照组个体\n",
    "    # 3. 检查距离是否在卡尺范围内\n",
    "    # 4. 如果无放回，需要记录已匹配的对照组个体\n",
    "    \n",
    "    caliper_threshold = caliper * df['ps_estimated'].std()\n",
    "    \n",
    "    treated = df[df['treatment']==1].copy()\n",
    "    control = df[df['treatment']==0].copy()\n",
    "    \n",
    "    matched_pairs = []\n",
    "    used_controls = set()\n",
    "    \n",
    "    for _, t_row in treated.iterrows():\n",
    "        # 过滤已使用的对照组（如果无放回）\n",
    "        if not with_replacement:\n",
    "            available_controls = control[~control['user_id'].isin(used_controls)]\n",
    "        else:\n",
    "            available_controls = control\n",
    "        \n",
    "        if len(available_controls) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 计算倾向得分距离\n",
    "        distances = np.abs(available_controls['ps_estimated'] - t_row['ps_estimated'])\n",
    "        \n",
    "        # 找到最近的 ratio 个\n",
    "        n_matches = min(ratio, len(available_controls))\n",
    "        closest_indices = distances.nsmallest(n_matches).index\n",
    "        \n",
    "        # 检查卡尺\n",
    "        for idx in closest_indices:\n",
    "            if distances[idx] <= caliper_threshold:\n",
    "                matched_pairs.append(t_row)\n",
    "                matched_pairs.append(available_controls.loc[idx])\n",
    "                used_controls.add(available_controls.loc[idx, 'user_id'])\n",
    "            else:\n",
    "                break  # 如果第一个都超出卡尺，后面的也不用看了\n",
    "    \n",
    "    matched_df = pd.DataFrame(matched_pairs)\n",
    "    return matched_df\n",
    "\n",
    "# 测试不同参数\n",
    "matched_psm_11 = psm_matching(df, caliper=0.2, ratio=1, with_replacement=False)\n",
    "print(f\"1:1 匹配（无放回）: {len(matched_psm_11)} 样本\")\n",
    "\n",
    "matched_psm_13 = psm_matching(df, caliper=0.2, ratio=3, with_replacement=True)\n",
    "print(f\"1:3 匹配（有放回）: {len(matched_psm_13)} 样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估 1:1 PSM 匹配质量\n",
    "smd_psm = compute_smd(matched_psm_11, covariates)\n",
    "print(\"\\nPSM (1:1) 匹配后的 SMD:\")\n",
    "for cov, smd in smd_psm.items():\n",
    "    print(f\"  {cov}: {smd:.3f} (改善: {abs(smd_before[cov]) - abs(smd):.3f})\")\n",
    "\n",
    "# 可视化\n",
    "fig = plot_covariate_balance(df, matched_psm_11, title_suffix=\" - PSM 匹配前后\")\n",
    "fig.show()\n",
    "\n",
    "# 倾向得分分布对比\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df[df['treatment']==0]['ps_estimated'],\n",
    "    name='对照组（原始）',\n",
    "    opacity=0.4,\n",
    "    marker_color='#EB5757',\n",
    "    nbinsx=50\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df[df['treatment']==1]['ps_estimated'],\n",
    "    name='处理组（原始）',\n",
    "    opacity=0.4,\n",
    "    marker_color='#2D9CDB',\n",
    "    nbinsx=50\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=matched_psm_11[matched_psm_11['treatment']==0]['ps_estimated'],\n",
    "    name='对照组（匹配后）',\n",
    "    opacity=0.7,\n",
    "    marker_color='#F2994A',\n",
    "    nbinsx=50\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=matched_psm_11[matched_psm_11['treatment']==1]['ps_estimated'],\n",
    "    name='处理组（匹配后）',\n",
    "    opacity=0.7,\n",
    "    marker_color='#27AE60',\n",
    "    nbinsx=50\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='PSM 匹配前后的倾向得分分布',\n",
    "    xaxis_title='倾向得分',\n",
    "    yaxis_title='频数',\n",
    "    barmode='overlay',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 马氏距离匹配 (Mahalanobis Distance Matching)\n",
    "\n",
    "### 4.1 为什么需要马氏距离？\n",
    "\n",
    "PSM 只考虑倾向得分这一个维度，可能忽略协变量之间的相关性。\n",
    "\n",
    "**马氏距离** (Mahalanobis Distance):\n",
    "\n",
    "$$\n",
    "d_M(i, j) = \\sqrt{(X_i - X_j)^T \\Sigma^{-1} (X_i - X_j)}\n",
    "$$\n",
    "\n",
    "其中 $\\Sigma$ 是协变量的协方差矩阵。\n",
    "\n",
    "**特点**：\n",
    "- 考虑变量的尺度差异和相关性\n",
    "- 对多维数据更鲁棒\n",
    "- 计算成本高\n",
    "\n",
    "### 4.2 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: 实现马氏距离匹配\n",
    "def mahalanobis_matching(df, covariates, caliper=None, ratio=1):\n",
    "    \"\"\"\n",
    "    马氏距离匹配\n",
    "    \n",
    "    参数:\n",
    "        df: 数据框\n",
    "        covariates: 协变量列表\n",
    "        caliper: 卡尺（可选）\n",
    "        ratio: 匹配比例\n",
    "    \n",
    "    返回:\n",
    "        matched_df: 匹配后的数据框\n",
    "    \"\"\"\n",
    "    # 提示:\n",
    "    # 1. 计算协方差矩阵的逆\n",
    "    # 2. 为每个处理组个体计算与所有对照组的马氏距离\n",
    "    # 3. 选择距离最近的 ratio 个对照组\n",
    "    \n",
    "    treated = df[df['treatment']==1].copy()\n",
    "    control = df[df['treatment']==0].copy()\n",
    "    \n",
    "    # 计算协方差矩阵及其逆\n",
    "    X = df[covariates]\n",
    "    cov_matrix = np.cov(X.T)\n",
    "    cov_inv = np.linalg.inv(cov_matrix)\n",
    "    \n",
    "    matched_pairs = []\n",
    "    \n",
    "    for _, t_row in treated.iterrows():\n",
    "        t_cov = t_row[covariates].values\n",
    "        \n",
    "        # 计算与所有对照组的马氏距离\n",
    "        distances = []\n",
    "        for _, c_row in control.iterrows():\n",
    "            c_cov = c_row[covariates].values\n",
    "            dist = mahalanobis(t_cov, c_cov, cov_inv)\n",
    "            distances.append((c_row, dist))\n",
    "        \n",
    "        # 按距离排序\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # 选择最近的 ratio 个\n",
    "        for c_row, dist in distances[:ratio]:\n",
    "            if caliper is None or dist <= caliper:\n",
    "                matched_pairs.append(t_row)\n",
    "                matched_pairs.append(c_row)\n",
    "    \n",
    "    matched_df = pd.DataFrame(matched_pairs)\n",
    "    return matched_df\n",
    "\n",
    "matched_maha = mahalanobis_matching(df, covariates, ratio=1)\n",
    "print(f\"马氏距离匹配: {len(matched_maha)} 样本\")\n",
    "\n",
    "# 评估匹配质量\n",
    "smd_maha = compute_smd(matched_maha, covariates)\n",
    "print(\"\\n马氏距离匹配后的 SMD:\")\n",
    "for cov, smd in smd_maha.items():\n",
    "    print(f\"  {cov}: {smd:.3f} (改善: {abs(smd_before[cov]) - abs(smd):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PSM vs 马氏距离\n",
    "\n",
    "| 方法 | 优点 | 缺点 | 适用场景 |\n",
    "|------|------|------|----------|\n",
    "| PSM | 降维；理论基础强 | 可能丢失信息 | 协变量多；处理分配机制清晰 |\n",
    "| 马氏距离 | 保留多维信息 | 计算复杂；样本需求大 | 协变量相关性强 |\n",
    "\n",
    "**实践建议**：可以结合两者，先用 PSM 粗筛，再用马氏距离精匹配。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: 匹配质量评估\n",
    "\n",
    "### 5.1 平衡性检验\n",
    "\n",
    "#### (1) 标准化均值差 (SMD)\n",
    "\n",
    "$$\n",
    "\\text{SMD} = \\frac{\\bar{X}_{\\text{treated}} - \\bar{X}_{\\text{control}}}{\\sqrt{(s^2_{\\text{treated}} + s^2_{\\text{control}}) / 2}}\n",
    "$$\n",
    "\n",
    "**判断标准**：\n",
    "- $|\\text{SMD}| < 0.1$：平衡良好\n",
    "- $0.1 \\leq |\\text{SMD}| < 0.25$：可接受\n",
    "- $|\\text{SMD}| \\geq 0.25$：不平衡\n",
    "\n",
    "#### (2) 方差比 (Variance Ratio)\n",
    "\n",
    "$$\n",
    "\\text{VR} = \\frac{s^2_{\\text{treated}}}{s^2_{\\text{control}}}\n",
    "$$\n",
    "\n",
    "**判断标准**：$0.5 < \\text{VR} < 2$\n",
    "\n",
    "#### (3) 假设检验\n",
    "\n",
    "- t 检验：检验均值差异\n",
    "- KS 检验：检验分布差异\n",
    "\n",
    "**注意**：匹配后样本不独立，p 值仅供参考！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的平衡性检验\n",
    "def balance_assessment(df_before, df_after, covariates):\n",
    "    \"\"\"\n",
    "    全面评估匹配质量\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for cov in covariates:\n",
    "        # 匹配前\n",
    "        before_t = df_before[df_before['treatment']==1][cov]\n",
    "        before_c = df_before[df_before['treatment']==0][cov]\n",
    "        \n",
    "        mean_diff_before = before_t.mean() - before_c.mean()\n",
    "        pooled_std_before = np.sqrt((before_t.var() + before_c.var()) / 2)\n",
    "        smd_before = mean_diff_before / pooled_std_before\n",
    "        vr_before = before_t.var() / before_c.var()\n",
    "        \n",
    "        # 匹配后\n",
    "        after_t = df_after[df_after['treatment']==1][cov]\n",
    "        after_c = df_after[df_after['treatment']==0][cov]\n",
    "        \n",
    "        mean_diff_after = after_t.mean() - after_c.mean()\n",
    "        pooled_std_after = np.sqrt((after_t.var() + after_c.var()) / 2)\n",
    "        smd_after = mean_diff_after / pooled_std_after\n",
    "        vr_after = after_t.var() / after_c.var()\n",
    "        \n",
    "        # KS 检验\n",
    "        ks_before = stats.ks_2samp(before_t, before_c)\n",
    "        ks_after = stats.ks_2samp(after_t, after_c)\n",
    "        \n",
    "        results.append({\n",
    "            '协变量': cov,\n",
    "            'SMD (前)': f\"{smd_before:.3f}\",\n",
    "            'SMD (后)': f\"{smd_after:.3f}\",\n",
    "            'VR (前)': f\"{vr_before:.3f}\",\n",
    "            'VR (后)': f\"{vr_after:.3f}\",\n",
    "            'KS p值 (后)': f\"{ks_after.pvalue:.3f}\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "balance_table = balance_assessment(df, matched_psm_11, covariates)\n",
    "print(\"\\n匹配质量评估 (PSM 1:1):\")\n",
    "print(balance_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 SMD 改善\n",
    "def plot_smd_comparison(df_before, df_after, covariates, labels=None):\n",
    "    \"\"\"\n",
    "    Love Plot: 可视化匹配前后的 SMD\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = covariates\n",
    "    \n",
    "    smd_before = compute_smd(df_before, covariates)\n",
    "    smd_after = compute_smd(df_after, covariates)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # 匹配前\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[abs(smd_before[cov]) for cov in covariates],\n",
    "        y=labels,\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='#EB5757', symbol='circle'),\n",
    "        name='匹配前'\n",
    "    ))\n",
    "    \n",
    "    # 匹配后\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[abs(smd_after[cov]) for cov in covariates],\n",
    "        y=labels,\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='#27AE60', symbol='diamond'),\n",
    "        name='匹配后'\n",
    "    ))\n",
    "    \n",
    "    # 阈值线\n",
    "    fig.add_vline(x=0.1, line_dash=\"dash\", line_color=\"gray\",\n",
    "                  annotation_text=\"阈值 (0.1)\", annotation_position=\"top right\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Love Plot: 匹配前后的标准化均值差',\n",
    "        xaxis_title='|SMD|',\n",
    "        yaxis_title='协变量',\n",
    "        template='plotly_white',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_smd_comparison(df, matched_psm_11, covariates, \n",
    "                          labels=['年龄', '历史消费', '购物频次'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: 效应估计\n",
    "\n",
    "### 6.1 ATT vs ATE\n",
    "\n",
    "匹配方法通常估计的是 **处理组平均处理效应 (ATT)**：\n",
    "\n",
    "$$\n",
    "\\text{ATT} = E[Y_1 - Y_0 \\mid T=1]\n",
    "$$\n",
    "\n",
    "而非总体平均处理效应 (ATE)：\n",
    "\n",
    "$$\n",
    "\\text{ATE} = E[Y_1 - Y_0]\n",
    "$$\n",
    "\n",
    "**原因**：匹配是以处理组为基准，为其寻找对照组。\n",
    "\n",
    "**估计**：\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{ATT}} = \\frac{1}{N_T} \\sum_{i: T_i=1} \\left( Y_i - \\frac{1}{M_i} \\sum_{j \\in \\mathcal{M}(i)} Y_j \\right)\n",
    "$$\n",
    "\n",
    "其中 $\\mathcal{M}(i)$ 是个体 $i$ 的匹配集，$M_i$ 是匹配个数。\n",
    "\n",
    "### 6.2 方差估计\n",
    "\n",
    "**挑战**：\n",
    "- 匹配样本不独立（尤其是有放回匹配）\n",
    "- 倾向得分是估计出来的（估计误差）\n",
    "\n",
    "**解决方案**：\n",
    "1. **Abadie-Imbens 方差估计**（考虑匹配不确定性）\n",
    "2. **Bootstrap**（重抽样）\n",
    "3. **稳健标准误**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: 实现 ATT 估计和 Bootstrap 方差估计\n",
    "def estimate_att(matched_df):\n",
    "    \"\"\"\n",
    "    估计 ATT\n",
    "    \"\"\"\n",
    "    treated = matched_df[matched_df['treatment']==1]\n",
    "    control = matched_df[matched_df['treatment']==0]\n",
    "    \n",
    "    att = treated['spending'].mean() - control['spending'].mean()\n",
    "    return att\n",
    "\n",
    "def bootstrap_att(matched_df, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap 估计 ATT 的标准误和置信区间\n",
    "    \n",
    "    参数:\n",
    "        matched_df: 匹配后的数据框\n",
    "        n_bootstrap: Bootstrap 次数\n",
    "    \n",
    "    返回:\n",
    "        att, se, ci_lower, ci_upper\n",
    "    \"\"\"\n",
    "    # 提示:\n",
    "    # 1. 对匹配后的数据进行有放回抽样（保持配对结构）\n",
    "    # 2. 计算每次 Bootstrap 样本的 ATT\n",
    "    # 3. 用 Bootstrap 分布估计标准误和置信区间\n",
    "    \n",
    "    att_point = estimate_att(matched_df)\n",
    "    \n",
    "    # 识别匹配对（假设按顺序排列：处理-对照-处理-对照...）\n",
    "    treated_indices = matched_df[matched_df['treatment']==1].index\n",
    "    control_indices = matched_df[matched_df['treatment']==0].index\n",
    "    \n",
    "    # 确保配对\n",
    "    n_pairs = min(len(treated_indices), len(control_indices))\n",
    "    \n",
    "    att_bootstrap = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # 有放回抽样配对\n",
    "        sampled_pair_indices = np.random.choice(n_pairs, n_pairs, replace=True)\n",
    "        \n",
    "        sampled_treated = matched_df.loc[treated_indices[sampled_pair_indices]]\n",
    "        sampled_control = matched_df.loc[control_indices[sampled_pair_indices]]\n",
    "        \n",
    "        att_b = sampled_treated['spending'].mean() - sampled_control['spending'].mean()\n",
    "        att_bootstrap.append(att_b)\n",
    "    \n",
    "    att_bootstrap = np.array(att_bootstrap)\n",
    "    se = att_bootstrap.std()\n",
    "    ci_lower = np.percentile(att_bootstrap, 2.5)\n",
    "    ci_upper = np.percentile(att_bootstrap, 97.5)\n",
    "    \n",
    "    return att_point, se, ci_lower, ci_upper\n",
    "\n",
    "# 估计不同匹配方法的 ATT\n",
    "methods = {\n",
    "    '精确匹配': matched_exact,\n",
    "    'PSM (1:1)': matched_psm_11,\n",
    "    'PSM (1:3)': matched_psm_13,\n",
    "    '马氏距离': matched_maha\n",
    "}\n",
    "\n",
    "print(f\"真实 ATE: {true_ate}\\n\")\n",
    "print(\"ATT 估计结果:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, matched_df in methods.items():\n",
    "    if len(matched_df) > 0:\n",
    "        att, se, ci_lower, ci_upper = bootstrap_att(matched_df, n_bootstrap=500)\n",
    "        bias = att - true_ate\n",
    "        print(f\"{name:15} ATT={att:7.2f}  SE={se:6.2f}  95% CI=[{ci_lower:7.2f}, {ci_upper:7.2f}]  Bias={bias:7.2f}\")\n",
    "    else:\n",
    "        print(f\"{name:15} 无法匹配\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化效应估计\n",
    "def plot_treatment_effect_estimates(methods_dict, true_ate):\n",
    "    \"\"\"\n",
    "    可视化不同方法的效应估计\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, matched_df in methods_dict.items():\n",
    "        if len(matched_df) > 0:\n",
    "            att, se, ci_lower, ci_upper = bootstrap_att(matched_df, n_bootstrap=500)\n",
    "            results.append({\n",
    "                'method': name,\n",
    "                'att': att,\n",
    "                'se': se,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # 点估计\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=results_df['att'],\n",
    "        y=results_df['method'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='#2D9CDB'),\n",
    "        error_x=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=results_df['ci_upper'] - results_df['att'],\n",
    "            arrayminus=results_df['att'] - results_df['ci_lower']\n",
    "        ),\n",
    "        name='ATT 估计'\n",
    "    ))\n",
    "    \n",
    "    # 真实值\n",
    "    fig.add_vline(x=true_ate, line_dash=\"dash\", line_color=\"#27AE60\",\n",
    "                  annotation_text=f\"真实 ATE = {true_ate}\",\n",
    "                  annotation_position=\"top right\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='不同匹配方法的 ATT 估计及 95% 置信区间',\n",
    "        xaxis_title='ATT',\n",
    "        yaxis_title='匹配方法',\n",
    "        template='plotly_white',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_treatment_effect_estimates(methods, true_ate)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 敏感性分析\n",
    "\n",
    "**问题**：如果存在未观测的混淆因素怎么办？\n",
    "\n",
    "**Rosenbaum 边界 (Rosenbaum Bounds)**：评估未观测混淆对结果的影响。\n",
    "\n",
    "**思路**：假设存在一个未观测变量 $U$，使得两个协变量相同的个体在处理概率上相差 $\\Gamma$ 倍：\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\Gamma} \\leq \\frac{P(T=1 \\mid X, U)}{P(T=1 \\mid X, U')} \\leq \\Gamma\n",
    "$$\n",
    "\n",
    "计算在不同 $\\Gamma$ 下，结论是否会改变（p 值边界）。\n",
    "\n",
    "**解读**：\n",
    "- $\\Gamma = 1$：无未观测混淆\n",
    "- $\\Gamma = 2$：未观测混淆使处理概率相差 2 倍\n",
    "- 如果在 $\\Gamma = 2$ 下结论仍显著，说明结果对未观测混淆较稳健"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化版 Rosenbaum 敏感性分析\n",
    "def rosenbaum_bounds_simulation(matched_df, gamma_range=np.arange(1, 3.1, 0.2)):\n",
    "    \"\"\"\n",
    "    模拟 Rosenbaum 边界\n",
    "    \n",
    "    思路：\n",
    "    - 对于每个 gamma，模拟未观测混淆导致的处理分配偏差\n",
    "    - 重新分配处理，计算效应估计\n",
    "    - 看效应是否仍显著\n",
    "    \"\"\"\n",
    "    treated = matched_df[matched_df['treatment']==1]\n",
    "    control = matched_df[matched_df['treatment']==0]\n",
    "    \n",
    "    observed_effect = treated['spending'].mean() - control['spending'].mean()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for gamma in gamma_range:\n",
    "        # 模拟 1000 次\n",
    "        effects = []\n",
    "        for _ in range(1000):\n",
    "            # 为每对匹配个体，以概率 p 互换处理状态\n",
    "            # p 的范围由 gamma 决定\n",
    "            p = gamma / (1 + gamma)\n",
    "            \n",
    "            # 随机翻转一部分配对\n",
    "            n_pairs = min(len(treated), len(control))\n",
    "            flip = np.random.binomial(1, p, n_pairs)\n",
    "            \n",
    "            # 重新计算效应（这里简化处理）\n",
    "            perturbed_effect = observed_effect * (1 - 0.1 * (gamma - 1) * np.random.uniform(0.5, 1.5))\n",
    "            effects.append(perturbed_effect)\n",
    "        \n",
    "        effects = np.array(effects)\n",
    "        p_value = (effects <= 0).mean()  # 单侧检验\n",
    "        \n",
    "        results.append({\n",
    "            'gamma': gamma,\n",
    "            'p_value': p_value,\n",
    "            'effect_lower': np.percentile(effects, 5),\n",
    "            'effect_upper': np.percentile(effects, 95)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results), observed_effect\n",
    "\n",
    "sensitivity_df, obs_effect = rosenbaum_bounds_simulation(matched_psm_11)\n",
    "\n",
    "# 可视化\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=['P-value vs Gamma', 'Effect Bounds vs Gamma'])\n",
    "\n",
    "# P-value\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sensitivity_df['gamma'], y=sensitivity_df['p_value'],\n",
    "               mode='lines+markers', name='P-value',\n",
    "               line=dict(color='#2D9CDB')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=0.05, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=\"α=0.05\", row=1, col=1)\n",
    "\n",
    "# Effect bounds\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sensitivity_df['gamma'], y=sensitivity_df['effect_upper'],\n",
    "               mode='lines', name='Upper bound',\n",
    "               line=dict(color='#27AE60', dash='dash')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sensitivity_df['gamma'], y=sensitivity_df['effect_lower'],\n",
    "               mode='lines', name='Lower bound',\n",
    "               line=dict(color='#EB5757', dash='dash'),\n",
    "               fill='tonexty', fillcolor='rgba(45, 156, 219, 0.2)'),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=2)\n",
    "fig.add_hline(y=obs_effect, line_dash=\"dot\", line_color=\"#2D9CDB\",\n",
    "              annotation_text=f\"Observed = {obs_effect:.1f}\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Γ (未观测混淆强度)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Γ (未观测混淆强度)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"P-value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"ATT\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Rosenbaum 敏感性分析（模拟）',\n",
    "    template='plotly_white',\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n敏感性分析结果:\")\n",
    "print(sensitivity_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: 业务案例\n",
    "\n",
    "### 案例 1: 评估会员权益效果\n",
    "\n",
    "**背景**：电商平台推出付费会员服务，需要评估会员权益对用户消费的真实影响。\n",
    "\n",
    "**挑战**：\n",
    "- 会员用户本身就是高价值用户（选择偏差）\n",
    "- 无法进行随机化实验（商业考虑）\n",
    "\n",
    "**方案**：使用 PSM 匹配\n",
    "\n",
    "**步骤**：\n",
    "1. 收集协变量：年龄、历史消费、购物频次、注册时长等\n",
    "2. 估计倾向得分\n",
    "3. 1:3 匹配（提高统计效率）\n",
    "4. 检查平衡性\n",
    "5. 估计 ATT\n",
    "6. 敏感性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的会员权益评估流程\n",
    "print(\"=\" * 70)\n",
    "print(\"会员权益效果评估报告\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. 数据概览\")\n",
    "print(f\"   总样本量: {len(df)}\")\n",
    "print(f\"   会员用户: {df['treatment'].sum()} ({df['treatment'].mean()*100:.1f}%)\")\n",
    "print(f\"   非会员用户: {(1-df['treatment']).sum()} ({(1-df['treatment']).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n2. 协变量平衡性（匹配前）\")\n",
    "for cov in covariates:\n",
    "    t_mean = df[df['treatment']==1][cov].mean()\n",
    "    c_mean = df[df['treatment']==0][cov].mean()\n",
    "    print(f\"   {cov:15} 处理组={t_mean:8.2f}  对照组={c_mean:8.2f}  差异={t_mean-c_mean:8.2f}\")\n",
    "\n",
    "print(\"\\n3. PSM 1:3 匹配\")\n",
    "print(f\"   匹配后样本量: {len(matched_psm_13)}\")\n",
    "print(f\"   处理组保留率: {matched_psm_13['treatment'].sum() / df['treatment'].sum() * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n4. 协变量平衡性（匹配后）\")\n",
    "for cov in covariates:\n",
    "    t_mean = matched_psm_13[matched_psm_13['treatment']==1][cov].mean()\n",
    "    c_mean = matched_psm_13[matched_psm_13['treatment']==0][cov].mean()\n",
    "    smd = abs(compute_smd(matched_psm_13, [cov])[cov])\n",
    "    status = \"✓\" if smd < 0.1 else (\"⚠\" if smd < 0.25 else \"✗\")\n",
    "    print(f\"   {cov:15} 处理组={t_mean:8.2f}  对照组={c_mean:8.2f}  SMD={smd:.3f} {status}\")\n",
    "\n",
    "att, se, ci_lower, ci_upper = bootstrap_att(matched_psm_13, n_bootstrap=1000)\n",
    "print(\"\\n5. 效应估计\")\n",
    "print(f\"   ATT: {att:.2f} 元/月\")\n",
    "print(f\"   标准误: {se:.2f}\")\n",
    "print(f\"   95% 置信区间: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"   是否显著: {'是 (p<0.05)' if ci_lower > 0 else '否'}\")\n",
    "\n",
    "print(\"\\n6. 业务解读\")\n",
    "print(f\"   - 会员权益使用户月消费平均增加 {att:.0f} 元\")\n",
    "print(f\"   - 如果会员年费为 {12*50} 元，ROI = {att*12 / (12*50):.2f}\")\n",
    "print(f\"   - 推荐: {'值得推广' if att > 200 else '需要优化权益'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例 2: 广告投放效果评估\n",
    "\n",
    "**背景**：某 App 在部分城市投放了户外广告，想评估广告对新用户注册的影响。\n",
    "\n",
    "**数据**：\n",
    "- 处理组：投放广告的城市\n",
    "- 对照组：未投放广告的城市\n",
    "- 协变量：城市人口、GDP、互联网渗透率、竞品数量等\n",
    "\n",
    "**挑战**：\n",
    "- 样本量小（城市维度）\n",
    "- 协变量维度高\n",
    "- 地理相关性\n",
    "\n",
    "**方案**：马氏距离匹配 + 地理加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: 实现广告投放效果评估\n",
    "def generate_advertising_data(n_cities=100):\n",
    "    \"\"\"\n",
    "    生成广告投放数据（城市维度）\n",
    "    \"\"\"\n",
    "    # 协变量\n",
    "    population = np.random.gamma(5, 100, n_cities)  # 人口（万）\n",
    "    gdp = np.random.gamma(10, 500, n_cities)  # GDP（亿）\n",
    "    internet_penetration = np.random.beta(8, 2, n_cities)  # 互联网渗透率\n",
    "    competitors = np.random.poisson(3, n_cities)  # 竞品数量\n",
    "    \n",
    "    # 处理分配（倾向于人口多、GDP高的城市）\n",
    "    propensity = 1 / (1 + np.exp(-(-2 + 0.01*population + 0.001*gdp)))\n",
    "    treatment = (np.random.uniform(0, 1, n_cities) < propensity).astype(int)\n",
    "    \n",
    "    # 潜在结果（新用户注册数）\n",
    "    y0 = (10*population + 0.5*gdp + 100*internet_penetration - 20*competitors + \n",
    "          np.random.normal(0, 50, n_cities))\n",
    "    true_effect = 100  # 广告带来 100 个新用户\n",
    "    y1 = y0 + true_effect\n",
    "    \n",
    "    y_obs = treatment * y1 + (1 - treatment) * y0\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'city_id': range(n_cities),\n",
    "        'population': population,\n",
    "        'gdp': gdp,\n",
    "        'internet_penetration': internet_penetration,\n",
    "        'competitors': competitors,\n",
    "        'treatment': treatment,\n",
    "        'new_users': y_obs,\n",
    "        'y0': y0,\n",
    "        'y1': y1\n",
    "    })\n",
    "    \n",
    "    return df, true_effect\n",
    "\n",
    "df_ad, true_effect_ad = generate_advertising_data(100)\n",
    "\n",
    "# 使用马氏距离匹配\n",
    "ad_covariates = ['population', 'gdp', 'internet_penetration', 'competitors']\n",
    "matched_ad = mahalanobis_matching(df_ad, ad_covariates, ratio=2)\n",
    "\n",
    "# 评估\n",
    "print(\"广告投放效果评估\")\n",
    "print(f\"真实效果: {true_effect_ad} 新用户\")\n",
    "print(f\"\\n匹配样本量: {len(matched_ad)} (城市对)\")\n",
    "\n",
    "# 平衡性\n",
    "balance_ad = balance_assessment(df_ad, matched_ad, ad_covariates)\n",
    "print(\"\\n平衡性检验:\")\n",
    "print(balance_ad.to_string(index=False))\n",
    "\n",
    "# 效应估计\n",
    "att_ad, se_ad, ci_lower_ad, ci_upper_ad = bootstrap_att(matched_ad, n_bootstrap=500)\n",
    "print(f\"\\nATT: {att_ad:.2f} 新用户\")\n",
    "print(f\"95% CI: [{ci_lower_ad:.2f}, {ci_upper_ad:.2f}]\")\n",
    "print(f\"偏差: {att_ad - true_effect_ad:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: 练习与思考题\n",
    "\n",
    "### 练习 1: 完善卡尺选择\n",
    "\n",
    "实现一个函数，自动选择最优卡尺（通过交叉验证平衡性和样本量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: 实现最优卡尺选择\n",
    "def optimal_caliper_selection(df, caliper_range=np.arange(0.05, 0.5, 0.05)):\n",
    "    \"\"\"\n",
    "    选择最优卡尺\n",
    "    \n",
    "    目标：平衡匹配质量（SMD）和样本保留率\n",
    "    \n",
    "    返回:\n",
    "        optimal_caliper, results_df\n",
    "    \"\"\"\n",
    "    # 提示:\n",
    "    # 1. 对每个卡尺值进行匹配\n",
    "    # 2. 计算平均 SMD 和样本保留率\n",
    "    # 3. 定义一个综合指标（如：样本保留率 - λ * 平均SMD）\n",
    "    # 4. 选择综合指标最大的卡尺\n",
    "    \n",
    "    pass  # 你的代码\n",
    "\n",
    "# 测试\n",
    "# optimal_caliper, results = optimal_caliper_selection(df)\n",
    "# print(f\"最优卡尺: {optimal_caliper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思考题\n",
    "\n",
    "1. **匹配 vs 回归**：什么时候用匹配，什么时候用回归？能否结合两者？\n",
    "\n",
    "2. **多值处理**：如果处理不是二元的（如：广告投放强度 0/50%/100%），如何匹配？\n",
    "\n",
    "3. **时间维度**：如果需要评估处理的动态效应（如：会员第 1 个月 vs 第 6 个月的效果），匹配方法如何调整？\n",
    "\n",
    "4. **网络效应**：如果个体之间存在网络效应（如：用户的朋友也是会员），匹配假设是否成立？\n",
    "\n",
    "5. **外部有效性**：匹配方法估计的 ATT 能否推广到未匹配的总体？\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **匹配的本质**：通过构建可比的对照组，模拟随机化\n",
    "\n",
    "2. **关键假设**：条件独立性假设 (CIA) + 共同支撑\n",
    "\n",
    "3. **方法选择**：\n",
    "   - 精确匹配：协变量少且离散\n",
    "   - PSM：协变量多，处理分配机制清晰\n",
    "   - 马氏距离：协变量相关性强\n",
    "\n",
    "4. **匹配质量**：必须检查平衡性（SMD < 0.1）\n",
    "\n",
    "5. **敏感性分析**：评估未观测混淆的影响\n",
    "\n",
    "### 最佳实践\n",
    "\n",
    "1. **预分析**：先可视化协变量分布，了解不平衡程度\n",
    "2. **多种方法**：尝试不同匹配算法，比较稳健性\n",
    "3. **保留诊断**：报告匹配前后的平衡性统计\n",
    "4. **透明报告**：说明样本损失、匹配参数选择\n",
    "5. **结合方法**：匹配后可以再做回归调整（doubly robust）\n",
    "\n",
    "### 延伸阅读\n",
    "\n",
    "- Rosenbaum & Rubin (1983): \"The Central Role of the Propensity Score\"\n",
    "- Stuart (2010): \"Matching Methods for Causal Inference: A Review\"\n",
    "- Imbens & Rubin (2015): *Causal Inference for Statistics, Social, and Biomedical Sciences*\n",
    "\n",
    "---\n",
    "\n",
    "**下一讲预告**：工具变量法 - 当存在未观测混淆时如何识别因果效应？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
