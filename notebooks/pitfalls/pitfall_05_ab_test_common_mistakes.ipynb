{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš ï¸ Pitfall 05: A/B æµ‹è¯•å››å¤§å¤©å‘\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç»ƒä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. **è¯Šæ–­ SRM (Sample Ratio Mismatch)** - è¯†åˆ«åˆ†æµå¼‚å¸¸ï¼Œè¿½è¸ªæ•°æ®ç®¡é“é—®é¢˜\n",
    "2. **é¿å… Peeking Problem** - ç†è§£æ—©åœçš„ç»Ÿè®¡ä»£ä»·ï¼Œå®ç°æ­£ç¡®çš„åºè´¯æ£€éªŒ\n",
    "3. **å¤„ç† Multiple Testing** - æ§åˆ¶å®¶æ—é”™è¯¯ç‡ (FWER) å’Œå‡å‘ç°ç‡ (FDR)\n",
    "4. **åº”å¯¹ Network Effects (SUTVA è¿èƒŒ)** - è®¾è®¡ Cluster Randomization å’Œ Switchback å®éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¬ çœŸå®åœºæ™¯ï¼šä¸€åœº"æˆåŠŸ"å®éªŒçš„ç¿»è½¦ç°åœº\n",
    "\n",
    "ä½ æ˜¯å­—èŠ‚è·³åŠ¨çš„æ•°æ®ç§‘å­¦å®¶ï¼Œè´Ÿè´£åˆ†æä¸€ä¸ªæ–°æ¨èç®—æ³•çš„ A/B æµ‹è¯•ï¼š\n",
    "\n",
    "**å®éªŒè®¾è®¡**ï¼š\n",
    "- ç›®æ ‡ï¼šæå‡ç”¨æˆ·ç•™å­˜ç‡\n",
    "- æ ·æœ¬ï¼š1000 ä¸‡æ—¥æ´»ç”¨æˆ·ï¼Œ50%/50% åˆ†æµ\n",
    "- æŒ‡æ ‡ï¼š7 æ—¥ç•™å­˜ã€DAUã€äººå‡æ—¶é•¿ç­‰ 10 ä¸ªæŒ‡æ ‡\n",
    "- å‘¨æœŸï¼šè®¡åˆ’ 2 å‘¨\n",
    "\n",
    "**ç¬¬ 3 å¤©ï¼ŒPM å‘æ¥æ¶ˆæ¯**ï¼š\n",
    "> \"æ•°æ®çœ‹èµ·æ¥å¾ˆæ£’ï¼7 æ—¥ç•™å­˜æå‡ 0.5%ï¼Œp å€¼æ˜¯ 0.03ï¼Œæˆ‘ä»¬æå‰ä¸Šçº¿å§ï¼\"\n",
    "\n",
    "**ä½ å‘ç°çš„é—®é¢˜**ï¼š\n",
    "1. ğŸ“Š **æ ·æœ¬æ¯”ä¾‹**ï¼šå®éªŒç»„ 480 ä¸‡ï¼Œå¯¹ç…§ç»„ 520 ä¸‡ (åº”è¯¥æ˜¯ 500 ä¸‡ vs 500 ä¸‡)\n",
    "2. ğŸ‘€ **æ—©åœå†³ç­–**ï¼šç¬¬ 3 å¤©å°±çœ‹æ˜¾è‘—æ€§ï¼Œç»Ÿè®¡ä¸Šæ˜¯å¦åˆç†ï¼Ÿ\n",
    "3. ğŸ“ˆ **å¤šé‡æŒ‡æ ‡**ï¼š10 ä¸ªæŒ‡æ ‡ä¸­æœ‰ 2 ä¸ªæ˜¾è‘—ï¼Œè¿™ä¸ªç»“è®ºå¯é å—ï¼Ÿ\n",
    "4. ğŸ”— **ç”¨æˆ·äº¤äº’**ï¼šç”¨æˆ·ä¹‹é—´ä¼šåˆ†äº«å†…å®¹ï¼Œæ¨èç®—æ³•æ”¹å˜ä¼šå½±å“å…¶ä»–ç”¨æˆ·\n",
    "\n",
    "**è¿™ä¸ª notebook å°†é€ä¸€æ‹†è§£è¿™å››å¤§å¤©å‘ï¼Œè®©ä½ æˆä¸º A/B æµ‹è¯•çš„æ’é›·ä¸“å®¶ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: SRM (Sample Ratio Mismatch) - åˆ†æµå¼‚å¸¸æ£€æµ‹\n",
    "\n",
    "### ğŸ“ ä»€ä¹ˆæ˜¯ SRMï¼Ÿ\n",
    "\n",
    "SRM æŒ‡çš„æ˜¯å®éªŒä¸­å®é™…è§‚å¯Ÿåˆ°çš„æ ·æœ¬æ¯”ä¾‹ä¸é¢„æœŸæ¯”ä¾‹æ˜¾è‘—ä¸åŒã€‚\n",
    "\n",
    "**ä¸ºä»€ä¹ˆè¿™æ˜¯å¤§é—®é¢˜ï¼Ÿ**\n",
    "\n",
    "å¦‚æœåˆ†æµæœ¬èº«æœ‰åå·®ï¼Œé‚£ä¹ˆä¸¤ç»„ç”¨æˆ·å°±ä¸å†å¯æ¯”ï¼Œå®éªŒç»“è®ºæ¯«æ— æ„ä¹‰ï¼\n",
    "\n",
    "### å¸¸è§ SRM åŸå› \n",
    "\n",
    "| åŸå›  | å…·ä½“è¡¨ç° | æ£€æµ‹æ–¹æ³• |\n",
    "|------|----------|----------|\n",
    "| **åˆ†æµ Bug** | å“ˆå¸Œå‡½æ•°ä¸å‡åŒ€ | æ£€æŸ¥åˆ†æµä»£ç  |\n",
    "| **æ•°æ®ä¸¢å¤±** | æŸç»„æ—¥å¿—é‡‡é›†å¤±è´¥ | æ£€æŸ¥æ—¥å¿—å®Œæ•´æ€§ |\n",
    "| **Bot è¿‡æ»¤** | æŸç»„ Bot æ¯”ä¾‹ä¸åŒ | æ£€æŸ¥ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ |\n",
    "| **è§¦å‘æ¡ä»¶** | åªæœ‰éƒ¨åˆ†ç”¨æˆ·è§¦å‘å®éªŒ | æ£€æŸ¥è§¦å‘æ—¥å¿— |\n",
    "| **æ®‹ç•™æ•ˆåº”** | ä¹‹å‰å®éªŒå½±å“ç”¨æˆ·è¡Œä¸º | æ£€æŸ¥æ–°ç”¨æˆ·å•ç‹¬åˆ†æ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# é¢œè‰²æ–¹æ¡ˆ\n",
    "COLORS = {\n",
    "    'primary': '#2D9CDB',\n",
    "    'success': '#27AE60',\n",
    "    'danger': '#EB5757',\n",
    "    'warning': '#F2994A',\n",
    "    'neutral': '#828282',\n",
    "}\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRM æ£€æµ‹ï¼šå¡æ–¹æ£€éªŒ\n",
    "\n",
    "**åŸå‡è®¾ Hâ‚€**ï¼šå®é™…åˆ†æµæ¯”ä¾‹ = é¢„æœŸåˆ†æµæ¯”ä¾‹\n",
    "\n",
    "$$\\chi^2 = \\sum_{i} \\frac{(O_i - E_i)^2}{E_i}$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $O_i$: è§‚å¯Ÿåˆ°çš„å„ç»„æ ·æœ¬é‡\n",
    "- $E_i$: æœŸæœ›çš„å„ç»„æ ·æœ¬é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: å®ç° SRM æ£€æµ‹å‡½æ•°\n",
    "# æç¤ºï¼š\n",
    "# - è®¡ç®—æœŸæœ›æ ·æœ¬é‡ E = total * expected_ratio\n",
    "# - ä½¿ç”¨å¡æ–¹æ£€éªŒæ¯”è¾ƒè§‚å¯Ÿå€¼ä¸æœŸæœ›å€¼\n",
    "# - è¿”å› chi2 ç»Ÿè®¡é‡ã€p å€¼å’Œæ˜¯å¦é€šè¿‡æ£€éªŒ\n",
    "\n",
    "def detect_srm(n_treatment: int, n_control: int, \n",
    "               expected_ratio: float = 0.5,\n",
    "               alpha: float = 0.001) -> dict:\n",
    "    \"\"\"\n",
    "    æ£€æµ‹ Sample Ratio Mismatch\n",
    "    \n",
    "    å‚æ•°:\n",
    "        n_treatment: å®éªŒç»„æ ·æœ¬é‡\n",
    "        n_control: å¯¹ç…§ç»„æ ·æœ¬é‡\n",
    "        expected_ratio: æœŸæœ›çš„å®éªŒç»„æ¯”ä¾‹ (é»˜è®¤ 0.5)\n",
    "        alpha: æ˜¾è‘—æ€§æ°´å¹³ (SRM æ£€æµ‹é€šå¸¸ç”¨æ›´ä¸¥æ ¼çš„ 0.001)\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å« chi2, p_value, has_srm, actual_ratio çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    return {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'has_srm': has_srm,\n",
    "        'actual_ratio': actual_ratio,\n",
    "        'expected_ratio': expected_ratio\n",
    "    }\n",
    "\n",
    "# ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼ï¼‰\n",
    "# ================================\n",
    "# total = n_treatment + n_control\n",
    "# actual_ratio = n_treatment / total\n",
    "# \n",
    "# # æœŸæœ›æ ·æœ¬é‡\n",
    "# e_treatment = total * expected_ratio\n",
    "# e_control = total * (1 - expected_ratio)\n",
    "# \n",
    "# # å¡æ–¹æ£€éªŒ\n",
    "# observed = [n_treatment, n_control]\n",
    "# expected = [e_treatment, e_control]\n",
    "# chi2, p_value = stats.chisquare(observed, expected)\n",
    "# \n",
    "# has_srm = p_value < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• SRM æ£€æµ‹\n",
    "print(\"=\" * 60)\n",
    "print(\"SRM æ£€æµ‹ç¤ºä¾‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åœºæ™¯ 1: æ­£å¸¸åˆ†æµ\n",
    "result_normal = detect_srm(4_980_000, 5_020_000)\n",
    "print(f\"\\nã€æ­£å¸¸åˆ†æµã€‘4,980,000 vs 5,020,000\")\n",
    "print(f\"  å®é™…æ¯”ä¾‹: {result_normal['actual_ratio']:.4f}\")\n",
    "print(f\"  Ï‡Â² = {result_normal['chi2']:.2f}, p = {result_normal['p_value']:.4f}\")\n",
    "print(f\"  SRM æ£€æµ‹: {'âš ï¸ å¼‚å¸¸!' if result_normal['has_srm'] else 'âœ… æ­£å¸¸'}\")\n",
    "\n",
    "# åœºæ™¯ 2: æ˜æ˜¾åå·®\n",
    "result_biased = detect_srm(4_800_000, 5_200_000)\n",
    "print(f\"\\nã€æ˜æ˜¾åå·®ã€‘4,800,000 vs 5,200,000\")\n",
    "print(f\"  å®é™…æ¯”ä¾‹: {result_biased['actual_ratio']:.4f}\")\n",
    "print(f\"  Ï‡Â² = {result_biased['chi2']:.2f}, p = {result_biased['p_value']:.2e}\")\n",
    "print(f\"  SRM æ£€æµ‹: {'âš ï¸ å¼‚å¸¸!' if result_biased['has_srm'] else 'âœ… æ­£å¸¸'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è§†åŒ–ï¼šSRM çš„åæœ\n",
    "\n",
    "è®©æˆ‘ä»¬æ¨¡æ‹Ÿ SRM å¦‚ä½•å¯¼è‡´é”™è¯¯ç»“è®ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_srm_impact(n_simulations: int = 1000,\n",
    "                        n_per_group: int = 10000,\n",
    "                        srm_ratio: float = 0.48) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿ SRM å¯¹å®éªŒç»“è®ºçš„å½±å“\n",
    "    \n",
    "    è®¾å®šï¼šçœŸå®æ•ˆåº”ä¸º 0ï¼ˆå³å¤„ç†æ— æ•ˆæœï¼‰\n",
    "    ä½†ç”±äº SRMï¼Œé«˜ä»·å€¼ç”¨æˆ·æ›´å¤šè¿›å…¥å¯¹ç…§ç»„\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # ç”Ÿæˆç”¨æˆ·ä»·å€¼ï¼ˆå‡è®¾ä¸åˆ†æµç›¸å…³ï¼‰\n",
    "        user_value = np.random.normal(100, 20, n_per_group * 2)\n",
    "        \n",
    "        # SRM åœºæ™¯ï¼šé«˜ä»·å€¼ç”¨æˆ·æ›´å¯èƒ½è¿›å…¥å¯¹ç…§ç»„\n",
    "        prob_treatment = 0.5 - 0.002 * (user_value - 100)  # ä»·å€¼è¶Šé«˜ï¼Œè¿›å®éªŒç»„æ¦‚ç‡è¶Šä½\n",
    "        prob_treatment = np.clip(prob_treatment, 0.3, 0.7)\n",
    "        treatment = np.random.binomial(1, prob_treatment)\n",
    "        \n",
    "        # ç»“æœï¼šçœŸå®æ•ˆåº”ä¸º 0ï¼Œä½†ç»“æœä¸ç”¨æˆ·ä»·å€¼ç›¸å…³\n",
    "        outcome = user_value + np.random.normal(0, 10, len(user_value))\n",
    "        \n",
    "        # è®¡ç®—ç»„é—´å·®å¼‚\n",
    "        mean_treatment = outcome[treatment == 1].mean()\n",
    "        mean_control = outcome[treatment == 0].mean()\n",
    "        \n",
    "        # t æ£€éªŒ\n",
    "        t_stat, p_value = stats.ttest_ind(\n",
    "            outcome[treatment == 1],\n",
    "            outcome[treatment == 0]\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'mean_diff': mean_treatment - mean_control,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'actual_ratio': treatment.mean()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "srm_results = simulate_srm_impact()\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=['æ•ˆåº”ä¼°è®¡åˆ†å¸ƒï¼ˆçœŸå®æ•ˆåº”=0ï¼‰', 'å‡é˜³æ€§ç‡'])\n",
    "\n",
    "# æ•ˆåº”åˆ†å¸ƒ\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=srm_results['mean_diff'], nbinsx=50,\n",
    "                 marker_color=COLORS['danger'], opacity=0.7,\n",
    "                 name='ä¼°è®¡æ•ˆåº”'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_vline(x=0, line_dash='dash', line_color='black', row=1, col=1)\n",
    "fig.add_annotation(x=0, y=100, text='çœŸå®æ•ˆåº”=0', showarrow=False, row=1, col=1)\n",
    "\n",
    "# å‡é˜³æ€§ç‡\n",
    "false_positive_rate = srm_results['significant'].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['æœŸæœ›å‡é˜³æ€§ç‡', 'å®é™…å‡é˜³æ€§ç‡ï¼ˆSRMï¼‰'],\n",
    "           y=[0.05, false_positive_rate],\n",
    "           marker_color=[COLORS['success'], COLORS['danger']],\n",
    "           text=[f'{0.05:.1%}', f'{false_positive_rate:.1%}'],\n",
    "           textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='SRM çš„å±å®³ï¼šçœŸå®æ•ˆåº”ä¸º 0ï¼Œä½† SRM å¯¼è‡´ä¸¥é‡åå·®',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š SRM å½±å“åˆ†æï¼š\")\n",
    "print(f\"  çœŸå®æ•ˆåº”: 0\")\n",
    "print(f\"  ä¼°è®¡æ•ˆåº”å‡å€¼: {srm_results['mean_diff'].mean():.3f}\")\n",
    "print(f\"  æœŸæœ›å‡é˜³æ€§ç‡: 5%\")\n",
    "print(f\"  å®é™…å‡é˜³æ€§ç‡: {false_positive_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Peeking Problem - æ—©åœçš„ç»Ÿè®¡ä»£ä»·\n",
    "\n",
    "### ğŸ“ ä»€ä¹ˆæ˜¯ Peeking Problemï¼Ÿ\n",
    "\n",
    "**åœºæ™¯**ï¼šå®éªŒè·‘åˆ°ç¬¬ 3 å¤©ï¼ŒPM æ€¥ç€è¦ç»“æœï¼Œä½ çœ‹äº†ä¸€çœ¼ p å€¼æ˜¯ 0.03ï¼Œå°±å®£å¸ƒå®éªŒæˆåŠŸã€‚\n",
    "\n",
    "**é—®é¢˜**ï¼šåå¤æ£€æŸ¥æ˜¾è‘—æ€§ä¼šä¸¥é‡è†¨èƒ€å‡é˜³æ€§ç‡ï¼\n",
    "\n",
    "$$\\text{å®é™…å‡é˜³æ€§ç‡} = 1 - (1-\\alpha)^k > \\alpha$$\n",
    "\n",
    "å…¶ä¸­ $k$ æ˜¯æ£€æŸ¥æ¬¡æ•°ã€‚å¦‚æœæ¯å¤©æ£€æŸ¥ä¸€æ¬¡ï¼Œè·‘ 14 å¤©ï¼š\n",
    "\n",
    "$$\\text{å®é™…å‡é˜³æ€§ç‡} = 1 - (1-0.05)^{14} \\approx 51\\%$$\n",
    "\n",
    "**åä¹‰ä¸Š 5% çš„æ˜¾è‘—æ€§æ°´å¹³ï¼Œå®é™…ä¸Šå˜æˆäº† 51% çš„å‡é˜³æ€§ç‡ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: æ¨¡æ‹Ÿ Peeking Problem\n",
    "# æç¤ºï¼š\n",
    "# - ç”ŸæˆçœŸå®æ•ˆåº”ä¸º 0 çš„æ•°æ®\n",
    "# - æ¯å¤©æ£€æŸ¥ä¸€æ¬¡ p å€¼\n",
    "# - è®°å½•æ˜¯å¦åœ¨ä»»æ„ä¸€å¤©è¾¾åˆ°æ˜¾è‘—\n",
    "\n",
    "def simulate_peeking(n_simulations: int = 1000,\n",
    "                     n_per_day: int = 1000,\n",
    "                     n_days: int = 14,\n",
    "                     alpha: float = 0.05) -> dict:\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿ Peeking Problem\n",
    "    \n",
    "    å‚æ•°:\n",
    "        n_simulations: æ¨¡æ‹Ÿæ¬¡æ•°\n",
    "        n_per_day: æ¯å¤©æ¯ç»„æ–°å¢æ ·æœ¬é‡\n",
    "        n_days: å®éªŒå¤©æ•°\n",
    "        alpha: æ˜¾è‘—æ€§æ°´å¹³\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å« false_positive_rate, p_value_trajectories çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    false_positive_count = 0\n",
    "    p_trajectories = []\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        # === ä½ çš„ä»£ç  ===\n",
    "        # æç¤ºï¼š\n",
    "        # 1. åˆå§‹åŒ–ç´¯ç§¯æ•°æ®\n",
    "        # 2. æ¯å¤©æ–°å¢æ•°æ®ï¼Œç´¯ç§¯è®¡ç®—\n",
    "        # 3. æ¯å¤©åšä¸€æ¬¡ t æ£€éªŒ\n",
    "        # 4. è®°å½•æ˜¯å¦åœ¨ä»»æ„ä¸€å¤©æ˜¾è‘—\n",
    "        \n",
    "        \n",
    "        # === ä»£ç ç»“æŸ ===\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'false_positive_rate': false_positive_count / n_simulations,\n",
    "        'p_trajectories': np.array(p_trajectories[:100]),  # åªä¿ç•™å‰ 100 æ¡ç”¨äºå¯è§†åŒ–\n",
    "        'expected_fpr': 1 - (1 - alpha) ** n_days\n",
    "    }\n",
    "\n",
    "# ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼ï¼‰\n",
    "# ================================\n",
    "# treatment_data = []\n",
    "# control_data = []\n",
    "# p_values = []\n",
    "# ever_significant = False\n",
    "# \n",
    "# for day in range(n_days):\n",
    "#     # æ¯å¤©æ–°å¢æ•°æ®ï¼ˆçœŸå®æ•ˆåº”ä¸º 0ï¼‰\n",
    "#     treatment_data.extend(np.random.normal(0, 1, n_per_day))\n",
    "#     control_data.extend(np.random.normal(0, 1, n_per_day))\n",
    "#     \n",
    "#     # åš t æ£€éªŒ\n",
    "#     t_stat, p_value = stats.ttest_ind(treatment_data, control_data)\n",
    "#     p_values.append(p_value)\n",
    "#     \n",
    "#     if p_value < alpha:\n",
    "#         ever_significant = True\n",
    "# \n",
    "# if ever_significant:\n",
    "#     false_positive_count += 1\n",
    "# p_trajectories.append(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ¨¡æ‹Ÿï¼ˆå¦‚æœä¸Šé¢çš„ TODO æœªå®Œæˆï¼Œä½¿ç”¨ä¸‹é¢çš„å‚è€ƒå®ç°ï¼‰\n",
    "def simulate_peeking_reference(n_simulations=1000, n_per_day=1000, n_days=14, alpha=0.05):\n",
    "    false_positive_count = 0\n",
    "    p_trajectories = []\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        treatment_data = []\n",
    "        control_data = []\n",
    "        p_values = []\n",
    "        ever_significant = False\n",
    "        \n",
    "        for day in range(n_days):\n",
    "            treatment_data.extend(np.random.normal(0, 1, n_per_day))\n",
    "            control_data.extend(np.random.normal(0, 1, n_per_day))\n",
    "            \n",
    "            t_stat, p_value = stats.ttest_ind(treatment_data, control_data)\n",
    "            p_values.append(p_value)\n",
    "            \n",
    "            if p_value < alpha:\n",
    "                ever_significant = True\n",
    "        \n",
    "        if ever_significant:\n",
    "            false_positive_count += 1\n",
    "        p_trajectories.append(p_values)\n",
    "    \n",
    "    return {\n",
    "        'false_positive_rate': false_positive_count / n_simulations,\n",
    "        'p_trajectories': np.array(p_trajectories[:100]),\n",
    "        'expected_fpr': 1 - (1 - alpha) ** n_days\n",
    "    }\n",
    "\n",
    "peeking_results = simulate_peeking_reference()\n",
    "\n",
    "print(f\"\\nğŸ“Š Peeking Problem æ¨¡æ‹Ÿç»“æœï¼š\")\n",
    "print(f\"  åä¹‰æ˜¾è‘—æ€§æ°´å¹³: 5%\")\n",
    "print(f\"  ç†è®ºå‡é˜³æ€§ç‡ï¼ˆ14 æ¬¡æ£€æŸ¥ï¼‰: {peeking_results['expected_fpr']:.1%}\")\n",
    "print(f\"  æ¨¡æ‹Ÿå‡é˜³æ€§ç‡: {peeking_results['false_positive_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– p å€¼è½¨è¿¹\n",
    "fig = go.Figure()\n",
    "\n",
    "# ç»˜åˆ¶å¤šæ¡ p å€¼è½¨è¿¹\n",
    "for i in range(min(50, len(peeking_results['p_trajectories']))):\n",
    "    trajectory = peeking_results['p_trajectories'][i]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(trajectory) + 1)),\n",
    "        y=trajectory,\n",
    "        mode='lines',\n",
    "        line=dict(width=0.5, color=COLORS['primary']),\n",
    "        opacity=0.3,\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "# æ·»åŠ  0.05 é˜ˆå€¼çº¿\n",
    "fig.add_hline(y=0.05, line_dash='dash', line_color=COLORS['danger'],\n",
    "              annotation_text='Î± = 0.05')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Peeking Problemï¼šp å€¼è½¨è¿¹ï¼ˆçœŸå®æ•ˆåº” = 0ï¼‰',\n",
    "    xaxis_title='å®éªŒå¤©æ•°',\n",
    "    yaxis_title='p å€¼',\n",
    "    yaxis_type='log',\n",
    "    template='plotly_white',\n",
    "    height=400,\n",
    "    annotations=[\n",
    "        dict(x=10, y=0.01, text='æ¯æ¡çº¿ä»£è¡¨ä¸€æ¬¡å®éªŒ<br>è§¦ç¢°çº¢çº¿å°±ä¼š\"æ˜¾è‘—\"', showarrow=False)\n",
    "    ]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§£å†³æ–¹æ¡ˆï¼šåºè´¯æ£€éªŒ (Sequential Testing)\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šè°ƒæ•´æ¯æ¬¡æ£€éªŒçš„é˜ˆå€¼ï¼Œä½¿å¾—æ•´ä½“å‡é˜³æ€§ç‡æ§åˆ¶åœ¨ Î±\n",
    "\n",
    "#### æ–¹æ³• 1: Alpha Spending Function (Lan-DeMets)\n",
    "\n",
    "$$\\alpha^*(t) = 2 - 2\\Phi\\left(z_{\\alpha/2} / \\sqrt{t}\\right)$$\n",
    "\n",
    "å…¶ä¸­ $t$ æ˜¯ä¿¡æ¯æ¯”ä¾‹ï¼ˆå·²æ”¶é›†æ ·æœ¬/è®¡åˆ’æ ·æœ¬ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: å®ç° Alpha Spending Function\n",
    "# æç¤ºï¼š\n",
    "# - O'Brien-Fleming é£æ ¼çš„ spending function\n",
    "# - æ—©æœŸæ›´ä¿å®ˆï¼ˆæ›´å°çš„ alphaï¼‰ï¼ŒåæœŸæ›´å®½æ¾\n",
    "\n",
    "def alpha_spending_obf(information_fraction: np.ndarray, \n",
    "                       alpha: float = 0.05) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    O'Brien-Fleming é£æ ¼çš„ Alpha Spending Function\n",
    "    \n",
    "    å‚æ•°:\n",
    "        information_fraction: ä¿¡æ¯æ¯”ä¾‹ (0 åˆ° 1)\n",
    "        alpha: æ€»ä½“æ˜¾è‘—æ€§æ°´å¹³\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç´¯ç§¯æ¶ˆè€—çš„ alpha\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    return spent_alpha\n",
    "\n",
    "# ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼ï¼‰\n",
    "# ================================\n",
    "# z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "# spent_alpha = 2 - 2 * stats.norm.cdf(z_alpha / np.sqrt(information_fraction))\n",
    "# return spent_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‚è€ƒå®ç°\n",
    "def alpha_spending_obf_ref(information_fraction, alpha=0.05):\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    spent_alpha = 2 - 2 * stats.norm.cdf(z_alpha / np.sqrt(np.maximum(information_fraction, 0.001)))\n",
    "    return spent_alpha\n",
    "\n",
    "# å¯è§†åŒ– alpha spending\n",
    "info_frac = np.linspace(0.01, 1, 100)\n",
    "spent = alpha_spending_obf_ref(info_frac)\n",
    "\n",
    "# è®¡ç®—æ¯æ¬¡æ£€æŸ¥çš„å¢é‡ alpha\n",
    "n_looks = 14\n",
    "look_points = np.linspace(1/n_looks, 1, n_looks)\n",
    "cumulative_alpha = alpha_spending_obf_ref(look_points)\n",
    "incremental_alpha = np.diff(np.concatenate([[0], cumulative_alpha]))\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=['ç´¯ç§¯ Alpha Spending', 'æ¯æ¬¡æ£€æŸ¥çš„é˜ˆå€¼'])\n",
    "\n",
    "# ç´¯ç§¯ alpha\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=info_frac, y=spent, mode='lines',\n",
    "               line=dict(color=COLORS['primary'], width=2),\n",
    "               name='ç´¯ç§¯æ¶ˆè€—'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=0.05, line_dash='dash', line_color=COLORS['danger'], row=1, col=1)\n",
    "\n",
    "# æ¯æ¬¡æ£€æŸ¥çš„é˜ˆå€¼\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(1, n_looks + 1)), y=incremental_alpha,\n",
    "           marker_color=COLORS['success'],\n",
    "           name='å¢é‡ alpha'),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_hline(y=0.05/n_looks, line_dash='dash', line_color=COLORS['warning'],\n",
    "              annotation_text='Bonferroni æ ¡æ­£', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='O\\'Brien-Fleming Alpha Spending Function',\n",
    "    template='plotly_white',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_xaxes(title_text='ä¿¡æ¯æ¯”ä¾‹', row=1, col=1)\n",
    "fig.update_xaxes(title_text='æ£€æŸ¥æ¬¡æ•°', row=1, col=2)\n",
    "fig.update_yaxes(title_text='ç´¯ç§¯ Alpha', row=1, col=1)\n",
    "fig.update_yaxes(title_text='å•æ¬¡æ£€æŸ¥é˜ˆå€¼', row=1, col=2)\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Alpha Spending ç»“æœï¼š\")\n",
    "print(f\"  ç¬¬ 1 æ¬¡æ£€æŸ¥é˜ˆå€¼: {incremental_alpha[0]:.6f}ï¼ˆéå¸¸ä¸¥æ ¼ï¼‰\")\n",
    "print(f\"  ç¬¬ 7 æ¬¡æ£€æŸ¥é˜ˆå€¼: {incremental_alpha[6]:.6f}\")\n",
    "print(f\"  ç¬¬ 14 æ¬¡æ£€æŸ¥é˜ˆå€¼: {incremental_alpha[13]:.6f}ï¼ˆç›¸å¯¹å®½æ¾ï¼‰\")\n",
    "print(f\"  æ€» alpha æ¶ˆè€—: {cumulative_alpha[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Multiple Testing - å¤šé‡æ£€éªŒé—®é¢˜\n",
    "\n",
    "### ğŸ“ å¤šé‡æ£€éªŒçš„é™·é˜±\n",
    "\n",
    "å¦‚æœä½ åŒæ—¶æ£€éªŒ 10 ä¸ªæŒ‡æ ‡ï¼Œå³ä½¿å¤„ç†å®Œå…¨æ— æ•ˆï¼ŒæœŸæœ›æœ‰å¤šå°‘ä¸ªä¼š\"æ˜¾è‘—\"ï¼Ÿ\n",
    "\n",
    "$$E[\\text{å‡é˜³æ€§æ•°}] = 10 \\times 0.05 = 0.5$$\n",
    "\n",
    "**å®é™…ä¸Š**ï¼šè‡³å°‘æœ‰ä¸€ä¸ªå‡é˜³æ€§çš„æ¦‚ç‡ï¼š\n",
    "\n",
    "$$P(\\text{è‡³å°‘ä¸€ä¸ªå‡é˜³æ€§}) = 1 - (1-0.05)^{10} \\approx 40\\%$$\n",
    "\n",
    "### ä¸¤ç§é”™è¯¯ç‡æ§åˆ¶\n",
    "\n",
    "| æ–¹æ³• | æ§åˆ¶ç›®æ ‡ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|----------|----------|\n",
    "| **FWER** (Bonferroni) | ä»»ä½•å‡é˜³æ€§çš„æ¦‚ç‡ | å…³é”®å†³ç­–ã€åŒ»å­¦è¯•éªŒ |\n",
    "| **FDR** (BH) | å‡é˜³æ€§åœ¨æ‰€æœ‰é˜³æ€§ä¸­çš„æ¯”ä¾‹ | æ¢ç´¢æ€§åˆ†æã€å¤§è§„æ¨¡ç­›é€‰ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: å®ç° Bonferroni å’Œ Benjamini-Hochberg æ ¡æ­£\n",
    "# æç¤ºï¼š\n",
    "# - Bonferroni: ç®€å•é™¤ä»¥æ£€éªŒæ•°é‡\n",
    "# - BH: æŒ‰ p å€¼æ’åºï¼Œæ‰¾åˆ°æœ€å¤§çš„ k ä½¿å¾— p(k) â‰¤ k/m * Î±\n",
    "\n",
    "def bonferroni_correction(p_values: np.ndarray, alpha: float = 0.05) -> dict:\n",
    "    \"\"\"\n",
    "    Bonferroni æ ¡æ­£ï¼ˆæ§åˆ¶ FWERï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å« adjusted_alpha, significant çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    return {'adjusted_alpha': adjusted_alpha, 'significant': significant}\n",
    "\n",
    "def benjamini_hochberg(p_values: np.ndarray, alpha: float = 0.05) -> dict:\n",
    "    \"\"\"\n",
    "    Benjamini-Hochberg æ ¡æ­£ï¼ˆæ§åˆ¶ FDRï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å« adjusted_p_values, significant çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    return {'adjusted_p_values': adjusted_p, 'significant': significant}\n",
    "\n",
    "# ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼ï¼‰\n",
    "# ================================\n",
    "# Bonferroni:\n",
    "# m = len(p_values)\n",
    "# adjusted_alpha = alpha / m\n",
    "# significant = p_values < adjusted_alpha\n",
    "#\n",
    "# Benjamini-Hochberg:\n",
    "# m = len(p_values)\n",
    "# sorted_idx = np.argsort(p_values)\n",
    "# sorted_p = p_values[sorted_idx]\n",
    "# \n",
    "# # BH ä¸´ç•Œå€¼\n",
    "# thresholds = (np.arange(1, m + 1) / m) * alpha\n",
    "# \n",
    "# # æ‰¾åˆ°æœ€å¤§çš„ k ä½¿å¾— p(k) â‰¤ threshold(k)\n",
    "# significant_sorted = sorted_p <= thresholds\n",
    "# if significant_sorted.any():\n",
    "#     max_k = np.max(np.where(significant_sorted)[0]) + 1\n",
    "#     significant_sorted = np.arange(m) < max_k\n",
    "# \n",
    "# # æ¢å¤åŸé¡ºåº\n",
    "# significant = np.zeros(m, dtype=bool)\n",
    "# significant[sorted_idx] = significant_sorted\n",
    "# \n",
    "# # è°ƒæ•´åçš„ p å€¼\n",
    "# adjusted_p = np.minimum(sorted_p * m / np.arange(1, m + 1), 1)\n",
    "# adjusted_p_final = np.zeros(m)\n",
    "# adjusted_p_final[sorted_idx] = adjusted_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‚è€ƒå®ç°\n",
    "def bonferroni_ref(p_values, alpha=0.05):\n",
    "    m = len(p_values)\n",
    "    adjusted_alpha = alpha / m\n",
    "    return {'adjusted_alpha': adjusted_alpha, 'significant': p_values < adjusted_alpha}\n",
    "\n",
    "def bh_ref(p_values, alpha=0.05):\n",
    "    m = len(p_values)\n",
    "    sorted_idx = np.argsort(p_values)\n",
    "    sorted_p = p_values[sorted_idx]\n",
    "    \n",
    "    thresholds = (np.arange(1, m + 1) / m) * alpha\n",
    "    significant_sorted = sorted_p <= thresholds\n",
    "    \n",
    "    if significant_sorted.any():\n",
    "        max_k = np.max(np.where(significant_sorted)[0]) + 1\n",
    "        significant_sorted = np.arange(m) < max_k\n",
    "    \n",
    "    significant = np.zeros(m, dtype=bool)\n",
    "    significant[sorted_idx] = significant_sorted\n",
    "    \n",
    "    adjusted_p = np.minimum.accumulate((sorted_p * m / np.arange(1, m + 1))[::-1])[::-1]\n",
    "    adjusted_p_final = np.zeros(m)\n",
    "    adjusted_p_final[sorted_idx] = adjusted_p\n",
    "    \n",
    "    return {'adjusted_p_values': adjusted_p_final, 'significant': significant}\n",
    "\n",
    "# æ¨¡æ‹Ÿå¤šæŒ‡æ ‡åœºæ™¯\n",
    "np.random.seed(123)\n",
    "n_metrics = 10\n",
    "\n",
    "# å‡è®¾ï¼š8 ä¸ªæŒ‡æ ‡çœŸå®æ•ˆåº”ä¸º 0ï¼Œ2 ä¸ªæŒ‡æ ‡æœ‰çœŸå®æ•ˆåº”\n",
    "# ç”Ÿæˆ p å€¼\n",
    "p_values = np.concatenate([\n",
    "    np.random.uniform(0, 1, 8),  # 8 ä¸ªæ— æ•ˆæŒ‡æ ‡\n",
    "    np.array([0.001, 0.01])       # 2 ä¸ªæœ‰æ•ˆæŒ‡æ ‡\n",
    "])\n",
    "\n",
    "metric_names = ['ç•™å­˜ç‡', 'DAU', 'äººå‡æ—¶é•¿', 'å¯åŠ¨æ¬¡æ•°', 'é¡µé¢æµè§ˆ', \n",
    "                'ç‚¹å‡»ç‡', 'äº’åŠ¨ç‡', 'åˆ†äº«ç‡', 'æ ¸å¿ƒè½¬åŒ–', 'ä»˜è´¹ç‡']\n",
    "\n",
    "# åº”ç”¨æ ¡æ­£\n",
    "bonf_result = bonferroni_ref(p_values)\n",
    "bh_result = bh_ref(p_values)\n",
    "\n",
    "# å±•ç¤ºç»“æœ\n",
    "results_df = pd.DataFrame({\n",
    "    'æŒ‡æ ‡': metric_names,\n",
    "    'p å€¼': p_values,\n",
    "    'æœªæ ¡æ­£æ˜¾è‘—': p_values < 0.05,\n",
    "    'Bonferroni æ˜¾è‘—': bonf_result['significant'],\n",
    "    'BH æ˜¾è‘—': bh_result['significant'],\n",
    "    'BH è°ƒæ•´ p å€¼': bh_result['adjusted_p_values']\n",
    "}).sort_values('p å€¼')\n",
    "\n",
    "print(\"\\nğŸ“Š å¤šé‡æ£€éªŒæ ¡æ­£ç»“æœï¼š\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nè¯´æ˜ï¼š\")\n",
    "print(f\"  - æœªæ ¡æ­£ï¼š{(p_values < 0.05).sum()} ä¸ªæ˜¾è‘—ï¼ˆå¯èƒ½åŒ…å«å‡é˜³æ€§ï¼‰\")\n",
    "print(f\"  - Bonferroniï¼ˆÎ±={bonf_result['adjusted_alpha']:.4f}ï¼‰ï¼š{bonf_result['significant'].sum()} ä¸ªæ˜¾è‘—\")\n",
    "print(f\"  - BHï¼ˆFDR æ§åˆ¶ï¼‰ï¼š{bh_result['significant'].sum()} ä¸ªæ˜¾è‘—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– p å€¼ä¸é˜ˆå€¼\n",
    "sorted_idx = np.argsort(p_values)\n",
    "sorted_p = p_values[sorted_idx]\n",
    "sorted_names = [metric_names[i] for i in sorted_idx]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# åŸå§‹ p å€¼\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, n_metrics + 1)),\n",
    "    y=sorted_p,\n",
    "    mode='markers+lines',\n",
    "    marker=dict(size=12, color=COLORS['primary']),\n",
    "    name='p å€¼ï¼ˆæ’åºåï¼‰',\n",
    "    text=sorted_names,\n",
    "    hovertemplate='%{text}<br>p = %{y:.4f}'\n",
    "))\n",
    "\n",
    "# æœªæ ¡æ­£é˜ˆå€¼\n",
    "fig.add_hline(y=0.05, line_dash='solid', line_color=COLORS['danger'],\n",
    "              annotation_text='æœªæ ¡æ­£ Î±=0.05')\n",
    "\n",
    "# Bonferroni é˜ˆå€¼\n",
    "fig.add_hline(y=0.05/n_metrics, line_dash='dash', line_color=COLORS['warning'],\n",
    "              annotation_text=f'Bonferroni Î±={0.05/n_metrics:.4f}')\n",
    "\n",
    "# BH é˜ˆå€¼çº¿\n",
    "bh_thresholds = (np.arange(1, n_metrics + 1) / n_metrics) * 0.05\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, n_metrics + 1)),\n",
    "    y=bh_thresholds,\n",
    "    mode='lines',\n",
    "    line=dict(dash='dot', color=COLORS['success']),\n",
    "    name='BH é˜ˆå€¼çº¿'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='å¤šé‡æ£€éªŒæ ¡æ­£ï¼šp å€¼ä¸å„æ–¹æ³•é˜ˆå€¼æ¯”è¾ƒ',\n",
    "    xaxis_title='æ’åºåæŒ‡æ ‡åºå·',\n",
    "    yaxis_title='p å€¼',\n",
    "    yaxis_type='log',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    legend=dict(x=0.7, y=0.95)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Network Effects - SUTVA è¿èƒŒ\n",
    "\n",
    "### ğŸ“ ä»€ä¹ˆæ˜¯ SUTVAï¼Ÿ\n",
    "\n",
    "**SUTVA (Stable Unit Treatment Value Assumption)**ï¼š\n",
    "\n",
    "1. **æ— å¹²æ‰°å‡è®¾**ï¼šä¸€ä¸ªç”¨æˆ·çš„å¤„ç†ä¸å½±å“å¦ä¸€ä¸ªç”¨æˆ·çš„ç»“æœ\n",
    "2. **å¤„ç†ä¸€è‡´æ€§**ï¼šæ‰€æœ‰ç”¨æˆ·æ¥å—çš„æ˜¯åŒä¸€ç§å¤„ç†\n",
    "\n",
    "**ä»€ä¹ˆæ—¶å€™ SUTVA ä¼šè¢«è¿èƒŒï¼Ÿ**\n",
    "\n",
    "| åœºæ™¯ | SUTVA è¿èƒŒåŸå›  |\n",
    "|------|----------------|\n",
    "| ç¤¾äº¤ç½‘ç»œ | ç”¨æˆ· A çœ‹åˆ°çš„å†…å®¹å½±å“ç”¨æˆ· B çœ‹åˆ°çš„å†…å®¹ |\n",
    "| åŒè¾¹å¸‚åœº | å¸æœºç«¯ä¼˜æƒ å½±å“ä¹˜å®¢ç«¯ä½“éªŒ |\n",
    "| æ¨èç³»ç»Ÿ | ç»™ A æ¨èçš„å†…å®¹ä¼šå‡å°‘ B çš„æ›å…‰æœºä¼š |\n",
    "| å…±äº«èµ„æº | æœåŠ¡å™¨èµ„æºè¢«æŸç»„ç”¨æˆ·å ç”¨å½±å“å…¶ä»–ç»„ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: æ¨¡æ‹Ÿç½‘ç»œæ•ˆåº”å¯¼è‡´çš„ä¼°è®¡åå·®\n",
    "# åœºæ™¯ï¼šç¤¾äº¤å¹³å°ä¸Šçš„å†…å®¹æ¨èå®éªŒ\n",
    "# - å®éªŒç»„ç”¨æˆ·çœ‹åˆ°æ›´å¤šä¼˜è´¨å†…å®¹\n",
    "# - ä½†ä¼˜è´¨å†…å®¹æ˜¯ç¨€ç¼ºçš„ï¼Œå®éªŒç»„å¤šçœ‹äº†ï¼Œå¯¹ç…§ç»„å°±å°‘çœ‹äº†\n",
    "\n",
    "def simulate_network_effects(n_users: int = 10000,\n",
    "                             n_content: int = 1000,\n",
    "                             treatment_boost: float = 0.3) -> dict:\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿå­˜åœ¨ç½‘ç»œæ•ˆåº”çš„ A/B æµ‹è¯•\n",
    "    \n",
    "    å‚æ•°:\n",
    "        n_users: ç”¨æˆ·æ•°\n",
    "        n_content: ä¼˜è´¨å†…å®¹æ€»é‡ï¼ˆç¨€ç¼ºèµ„æºï¼‰\n",
    "        treatment_boost: å®éªŒç»„è·å–ä¼˜è´¨å†…å®¹çš„æå‡æ¯”ä¾‹\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å«çœŸå®æ•ˆåº”ã€ä¼°è®¡æ•ˆåº”ã€åå·®çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    # æç¤ºï¼š\n",
    "    # 1. éšæœºåˆ†é…ç”¨æˆ·åˆ°å®éªŒç»„/å¯¹ç…§ç»„\n",
    "    # 2. åˆ†é…ä¼˜è´¨å†…å®¹ï¼ˆæ€»é‡æœ‰é™ï¼‰\n",
    "    # 3. å®éªŒç»„è·å–æ›´å¤šä¼˜è´¨å†…å®¹ï¼Œä½†è¿™ä¼šæŒ¤å‹å¯¹ç…§ç»„\n",
    "    # 4. ç”¨æˆ·æ»¡æ„åº¦ = f(è·å¾—çš„ä¼˜è´¨å†…å®¹æ•°é‡)\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    return {\n",
    "        'true_effect': true_effect,\n",
    "        'naive_estimate': naive_estimate,\n",
    "        'bias': naive_estimate - true_effect\n",
    "    }\n",
    "\n",
    "# ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼ï¼‰\n",
    "# ================================\n",
    "# # åˆ†é…å¤„ç†\n",
    "# treatment = np.random.binomial(1, 0.5, n_users)\n",
    "# n_treatment = treatment.sum()\n",
    "# n_control = n_users - n_treatment\n",
    "# \n",
    "# # åŸºç¡€ç”¨æˆ·åå¥½ï¼ˆä¸å¤„ç†æ— å…³ï¼‰\n",
    "# base_preference = np.random.normal(50, 10, n_users)\n",
    "# \n",
    "# # ä¼˜è´¨å†…å®¹åˆ†é…ï¼ˆé›¶å’Œåšå¼ˆï¼‰\n",
    "# # å®éªŒç»„ç”¨æˆ·å¹³å‡è·å¾—æ›´å¤šå†…å®¹ï¼Œä½†æ€»é‡æœ‰é™\n",
    "# treatment_content_share = (1 + treatment_boost) / (1 + treatment_boost * 0.5)\n",
    "# control_content_share = 1 / (1 + treatment_boost * 0.5)\n",
    "# \n",
    "# content_per_user = np.where(\n",
    "#     treatment == 1,\n",
    "#     n_content * treatment_content_share / n_treatment,\n",
    "#     n_content * control_content_share / n_control\n",
    "# )\n",
    "# \n",
    "# # ç”¨æˆ·æ»¡æ„åº¦ = åŸºç¡€åå¥½ + 0.1 * è·å¾—çš„å†…å®¹æ•°\n",
    "# satisfaction = base_preference + 0.1 * content_per_user + np.random.normal(0, 5, n_users)\n",
    "# \n",
    "# # æœ´ç´ ä¼°è®¡ï¼ˆæœ‰åï¼‰\n",
    "# naive_estimate = satisfaction[treatment == 1].mean() - satisfaction[treatment == 0].mean()\n",
    "# \n",
    "# # çœŸå®æ•ˆåº”ï¼ˆå¦‚æœæ²¡æœ‰ç½‘ç»œæ•ˆåº”ï¼Œæ¯ä¸ªç”¨æˆ·è·å¾—åŒç­‰å†…å®¹ï¼‰\n",
    "# # çœŸå®æ•ˆåº”åº”è¯¥åªæ¥è‡ªæ¨èç®—æ³•æœ¬èº«çš„æ”¹è¿›ï¼Œè¿™é‡Œå‡è®¾ä¸º 0\n",
    "# true_effect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‚è€ƒå®ç°\n",
    "def simulate_network_effects_ref(n_users=10000, n_content=1000, treatment_boost=0.3):\n",
    "    treatment = np.random.binomial(1, 0.5, n_users)\n",
    "    n_treatment = treatment.sum()\n",
    "    n_control = n_users - n_treatment\n",
    "    \n",
    "    base_preference = np.random.normal(50, 10, n_users)\n",
    "    \n",
    "    # å†…å®¹åˆ†é…æ˜¯é›¶å’Œçš„\n",
    "    avg_content = n_content / n_users\n",
    "    treatment_extra = avg_content * treatment_boost\n",
    "    control_loss = treatment_extra * n_treatment / n_control  # å®ˆæ’\n",
    "    \n",
    "    content_per_user = np.where(\n",
    "        treatment == 1,\n",
    "        avg_content + treatment_extra,\n",
    "        avg_content - control_loss\n",
    "    )\n",
    "    \n",
    "    satisfaction = base_preference + 0.5 * content_per_user + np.random.normal(0, 5, n_users)\n",
    "    \n",
    "    naive_estimate = satisfaction[treatment == 1].mean() - satisfaction[treatment == 0].mean()\n",
    "    true_effect = 0  # ç®—æ³•æœ¬èº«æ— æ•ˆï¼Œåªæ˜¯é‡æ–°åˆ†é…\n",
    "    \n",
    "    return {\n",
    "        'true_effect': true_effect,\n",
    "        'naive_estimate': naive_estimate,\n",
    "        'bias': naive_estimate - true_effect,\n",
    "        'treatment_content': content_per_user[treatment == 1].mean(),\n",
    "        'control_content': content_per_user[treatment == 0].mean()\n",
    "    }\n",
    "\n",
    "# è¿è¡Œæ¨¡æ‹Ÿ\n",
    "network_results = simulate_network_effects_ref()\n",
    "\n",
    "print(\"\\nğŸ“Š ç½‘ç»œæ•ˆåº”æ¨¡æ‹Ÿç»“æœï¼š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  å®éªŒç»„å¹³å‡è·å¾—å†…å®¹: {network_results['treatment_content']:.2f}\")\n",
    "print(f\"  å¯¹ç…§ç»„å¹³å‡è·å¾—å†…å®¹: {network_results['control_content']:.2f}\")\n",
    "print(f\"  çœŸå®æ•ˆåº”: {network_results['true_effect']:.3f}ï¼ˆç®—æ³•æ— æ•ˆï¼‰\")\n",
    "print(f\"  æœ´ç´ ä¼°è®¡: {network_results['naive_estimate']:.3f}\")\n",
    "print(f\"  ä¼°è®¡åå·®: {network_results['bias']:.3f}\")\n",
    "print(f\"\\nâš ï¸ é—®é¢˜ï¼šæœ´ç´ ä¼°è®¡å¤§å¹…é«˜ä¼°äº†çœŸå®æ•ˆåº”ï¼\")\n",
    "print(f\"   å› ä¸ºå®éªŒç»„çš„'æå‡'æ˜¯ä»¥å¯¹ç…§ç»„çš„'ä¸‹é™'ä¸ºä»£ä»·çš„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§£å†³æ–¹æ¡ˆï¼šCluster Randomization\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šä»¥ç¾¤ä½“ï¼ˆè€Œéä¸ªä½“ï¼‰ä¸ºå•ä½è¿›è¡ŒéšæœºåŒ–ï¼Œå‡å°‘ç»„é—´å¹²æ‰°\n",
    "\n",
    "**é€‚ç”¨åœºæ™¯**ï¼š\n",
    "- åœ°ç†åŒºåŸŸï¼ˆåŸå¸‚ã€åŒºå¿ï¼‰\n",
    "- æ—¶é—´æ®µï¼ˆSwitchback å®éªŒï¼‰\n",
    "- ç¤¾äº¤åœˆï¼ˆå¥½å‹ç¾¤ç»„ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: å®ç° Cluster Randomization åˆ†æ\n",
    "# æç¤ºï¼š\n",
    "# - ä»¥åŸå¸‚ä¸ºå•ä½éšæœºåˆ†é…\n",
    "# - éœ€è¦è€ƒè™‘èšç±»å†…ç›¸å…³æ€§ï¼ˆICCï¼‰\n",
    "# - æ ‡å‡†è¯¯éœ€è¦èšç±»è°ƒæ•´\n",
    "\n",
    "def cluster_randomization_analysis(df: pd.DataFrame,\n",
    "                                   cluster_col: str,\n",
    "                                   treatment_col: str,\n",
    "                                   outcome_col: str) -> dict:\n",
    "    \"\"\"\n",
    "    èšç±»éšæœºåŒ–å®éªŒåˆ†æ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        df: æ•°æ®æ¡†\n",
    "        cluster_col: èšç±»åˆ—å\n",
    "        treatment_col: å¤„ç†åˆ—å\n",
    "        outcome_col: ç»“æœåˆ—å\n",
    "    \n",
    "    è¿”å›:\n",
    "        åŒ…å«æ•ˆåº”ä¼°è®¡ã€èšç±»è°ƒæ•´æ ‡å‡†è¯¯ã€ICC çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    # æç¤ºï¼š\n",
    "    # 1. è®¡ç®—ç»„é—´å·®å¼‚ï¼ˆæ•ˆåº”ä¼°è®¡ï¼‰\n",
    "    # 2. è®¡ç®— ICCï¼ˆç»„å†…ç›¸å…³ç³»æ•°ï¼‰\n",
    "    # 3. è®¡ç®—èšç±»è°ƒæ•´çš„æ ‡å‡†è¯¯\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    return {\n",
    "        'effect': effect,\n",
    "        'naive_se': naive_se,\n",
    "        'cluster_se': cluster_se,\n",
    "        'icc': icc,\n",
    "        'design_effect': design_effect\n",
    "    }\n",
    "\n",
    "# ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼ï¼‰\n",
    "# ================================\n",
    "# # è®¡ç®—æ•ˆåº”\n",
    "# treatment_mean = df[df[treatment_col] == 1][outcome_col].mean()\n",
    "# control_mean = df[df[treatment_col] == 0][outcome_col].mean()\n",
    "# effect = treatment_mean - control_mean\n",
    "# \n",
    "# # æœ´ç´ æ ‡å‡†è¯¯ï¼ˆå¿½ç•¥èšç±»ï¼‰\n",
    "# n_treatment = (df[treatment_col] == 1).sum()\n",
    "# n_control = (df[treatment_col] == 0).sum()\n",
    "# var_treatment = df[df[treatment_col] == 1][outcome_col].var()\n",
    "# var_control = df[df[treatment_col] == 0][outcome_col].var()\n",
    "# naive_se = np.sqrt(var_treatment/n_treatment + var_control/n_control)\n",
    "# \n",
    "# # è®¡ç®— ICC\n",
    "# cluster_means = df.groupby(cluster_col)[outcome_col].mean()\n",
    "# grand_mean = df[outcome_col].mean()\n",
    "# \n",
    "# # ç»„é—´æ–¹å·®\n",
    "# between_var = np.var(cluster_means)\n",
    "# # ç»„å†…æ–¹å·®\n",
    "# within_var = df.groupby(cluster_col)[outcome_col].var().mean()\n",
    "# \n",
    "# icc = between_var / (between_var + within_var)\n",
    "# \n",
    "# # è®¾è®¡æ•ˆåº”\n",
    "# avg_cluster_size = len(df) / df[cluster_col].nunique()\n",
    "# design_effect = 1 + (avg_cluster_size - 1) * icc\n",
    "# \n",
    "# # èšç±»è°ƒæ•´æ ‡å‡†è¯¯\n",
    "# cluster_se = naive_se * np.sqrt(design_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆèšç±»éšæœºåŒ–æ•°æ®\n",
    "np.random.seed(42)\n",
    "\n",
    "n_clusters = 50  # 50 ä¸ªåŸå¸‚\n",
    "users_per_cluster = 200  # æ¯åŸå¸‚ 200 ç”¨æˆ·\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "data = []\n",
    "for cluster_id in range(n_clusters):\n",
    "    # åŸå¸‚çº§åˆ«çš„å¤„ç†åˆ†é…\n",
    "    treatment = 1 if cluster_id < n_clusters // 2 else 0\n",
    "    \n",
    "    # åŸå¸‚çº§åˆ«çš„éšæœºæ•ˆåº”\n",
    "    cluster_effect = np.random.normal(0, 5)\n",
    "    \n",
    "    for user_id in range(users_per_cluster):\n",
    "        # ä¸ªä½“ç»“æœ = å¤„ç†æ•ˆåº” + åŸå¸‚æ•ˆåº” + ä¸ªä½“å™ªå£°\n",
    "        outcome = 50 + 2 * treatment + cluster_effect + np.random.normal(0, 10)\n",
    "        data.append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'user_id': f\"{cluster_id}_{user_id}\",\n",
    "            'treatment': treatment,\n",
    "            'outcome': outcome\n",
    "        })\n",
    "\n",
    "df_cluster = pd.DataFrame(data)\n",
    "\n",
    "# å‚è€ƒå®ç°\n",
    "def cluster_analysis_ref(df, cluster_col, treatment_col, outcome_col):\n",
    "    treatment_mean = df[df[treatment_col] == 1][outcome_col].mean()\n",
    "    control_mean = df[df[treatment_col] == 0][outcome_col].mean()\n",
    "    effect = treatment_mean - control_mean\n",
    "    \n",
    "    n_treatment = (df[treatment_col] == 1).sum()\n",
    "    n_control = (df[treatment_col] == 0).sum()\n",
    "    var_treatment = df[df[treatment_col] == 1][outcome_col].var()\n",
    "    var_control = df[df[treatment_col] == 0][outcome_col].var()\n",
    "    naive_se = np.sqrt(var_treatment/n_treatment + var_control/n_control)\n",
    "    \n",
    "    # ICC\n",
    "    cluster_means = df.groupby(cluster_col)[outcome_col].mean()\n",
    "    between_var = cluster_means.var()\n",
    "    within_var = df.groupby(cluster_col)[outcome_col].var().mean()\n",
    "    icc = between_var / (between_var + within_var)\n",
    "    \n",
    "    avg_cluster_size = len(df) / df[cluster_col].nunique()\n",
    "    design_effect = 1 + (avg_cluster_size - 1) * icc\n",
    "    cluster_se = naive_se * np.sqrt(design_effect)\n",
    "    \n",
    "    return {\n",
    "        'effect': effect,\n",
    "        'naive_se': naive_se,\n",
    "        'cluster_se': cluster_se,\n",
    "        'icc': icc,\n",
    "        'design_effect': design_effect\n",
    "    }\n",
    "\n",
    "cluster_result = cluster_analysis_ref(df_cluster, 'cluster_id', 'treatment', 'outcome')\n",
    "\n",
    "print(\"\\nğŸ“Š èšç±»éšæœºåŒ–åˆ†æç»“æœï¼š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  çœŸå®æ•ˆåº”: 2.0\")\n",
    "print(f\"  ä¼°è®¡æ•ˆåº”: {cluster_result['effect']:.3f}\")\n",
    "print(f\"  æœ´ç´ æ ‡å‡†è¯¯: {cluster_result['naive_se']:.4f}\")\n",
    "print(f\"  èšç±»è°ƒæ•´æ ‡å‡†è¯¯: {cluster_result['cluster_se']:.4f}\")\n",
    "print(f\"  ICC (ç»„å†…ç›¸å…³ç³»æ•°): {cluster_result['icc']:.3f}\")\n",
    "print(f\"  è®¾è®¡æ•ˆåº”: {cluster_result['design_effect']:.2f}\")\n",
    "print(f\"\\nâš ï¸ èšç±»è°ƒæ•´åæ ‡å‡†è¯¯æ˜¯æœ´ç´ ä¼°è®¡çš„ {cluster_result['cluster_se']/cluster_result['naive_se']:.1f} å€ï¼\")\n",
    "print(f\"   å¦‚æœå¿½ç•¥èšç±»ï¼Œä¼šä¸¥é‡ä½ä¼°ä¸ç¡®å®šæ€§ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switchback å®éªŒè®¾è®¡\n",
    "\n",
    "**é€‚ç”¨åœºæ™¯**ï¼šåŒè¾¹å¸‚åœºï¼ˆå¦‚æ‰“è½¦å¹³å°ï¼‰ï¼Œç”¨æˆ·å’Œä¾›ç»™è€…é«˜åº¦ç›¸å…³\n",
    "\n",
    "**è®¾è®¡æ€æƒ³**ï¼š\n",
    "- ä»¥æ—¶é—´æ®µä¸ºå•ä½åˆ‡æ¢å¤„ç†\n",
    "- ä¾‹å¦‚ï¼šæ¯ 30 åˆ†é’Ÿåˆ‡æ¢ä¸€æ¬¡\n",
    "- å‡å°‘ç»„é—´å¹²æ‰°ï¼Œå› ä¸ºåŒä¸€æ—¶é—´åªæœ‰ä¸€ç§ç­–ç•¥åœ¨è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switchback å®éªŒå¯è§†åŒ–\n",
    "n_days = 7\n",
    "slots_per_day = 48  # æ¯å¤© 48 ä¸ª 30 åˆ†é’Ÿæ—¶æ®µ\n",
    "\n",
    "# ç”Ÿæˆ switchback åˆ†é…\n",
    "np.random.seed(42)\n",
    "switchback_assignment = np.random.binomial(1, 0.5, n_days * slots_per_day)\n",
    "\n",
    "# é‡å¡‘ä¸º (days, slots)\n",
    "assignment_matrix = switchback_assignment.reshape(n_days, slots_per_day)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=assignment_matrix,\n",
    "    x=[f\"{i//2}:{(i%2)*30:02d}\" for i in range(slots_per_day)],\n",
    "    y=[f\"Day {i+1}\" for i in range(n_days)],\n",
    "    colorscale=[[0, COLORS['primary']], [1, COLORS['success']]],\n",
    "    showscale=False,\n",
    "    hovertemplate='Day %{y}<br>Time: %{x}<br>Treatment: %{z}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Switchback å®éªŒè®¾è®¡ï¼šæ—¶é—´æ®µçº§åˆ«éšæœºåŒ–',\n",
    "    xaxis_title='æ—¶é—´ï¼ˆæ¯ 30 åˆ†é’Ÿä¸€ä¸ªæ—¶æ®µï¼‰',\n",
    "    yaxis_title='æ—¥æœŸ',\n",
    "    template='plotly_white',\n",
    "    height=300,\n",
    "    annotations=[\n",
    "        dict(x=24, y=-0.5, text='è“è‰²=å¯¹ç…§ç»„ï¼Œç»¿è‰²=å®éªŒç»„', showarrow=False, yref='y')\n",
    "    ]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Switchback è®¾è®¡ï¼š\")\n",
    "print(f\"  æ€»æ—¶æ®µæ•°: {n_days * slots_per_day}\")\n",
    "print(f\"  å®éªŒç»„æ—¶æ®µ: {switchback_assignment.sum()}\")\n",
    "print(f\"  å¯¹ç…§ç»„æ—¶æ®µ: {len(switchback_assignment) - switchback_assignment.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ æ€è€ƒé¢˜\n",
    "\n",
    "### åŸºç¡€ç†è§£\n",
    "\n",
    "1. **SRM æ£€æµ‹**ï¼šä¸ºä»€ä¹ˆ SRM æ£€æµ‹é€šå¸¸ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ˜¾è‘—æ€§æ°´å¹³ï¼ˆå¦‚ 0.001 è€Œé 0.05ï¼‰ï¼Ÿå¦‚æœ SRM æ£€æµ‹æ˜¾è‘—ï¼Œåº”è¯¥æ€ä¹ˆå¤„ç†å®éªŒç»“æœï¼Ÿ\n",
    "\n",
    "2. **Peeking Problem**ï¼šå¦‚æœä¸šåŠ¡æ–¹åšæŒè¦æ¯å¤©çœ‹å®éªŒç»“æœï¼Œä½œä¸ºæ•°æ®ç§‘å­¦å®¶ä½ åº”è¯¥å¦‚ä½•è®¾è®¡å®éªŒå’ŒæŠ¥å‘Šæµç¨‹ï¼Ÿ\n",
    "\n",
    "### æ·±å…¥åˆ†æ\n",
    "\n",
    "3. **å¤šé‡æ£€éªŒæƒè¡¡**ï¼šå‡è®¾ä½ åŒæ—¶æµ‹è¯• 20 ä¸ªæŒ‡æ ‡ï¼Œå…¶ä¸­æœ‰ 3 ä¸ª p å€¼ä¸º 0.02, 0.03, 0.04ã€‚ä½¿ç”¨ Bonferroni æ ¡æ­£åå…¨éƒ¨ä¸æ˜¾è‘—ï¼Œä½¿ç”¨ BH æ ¡æ­£åæœ‰ 1 ä¸ªæ˜¾è‘—ã€‚ä½ ä¼šå¦‚ä½•å‘ä¸šåŠ¡æ–¹è§£é‡Šè¿™ä¸ªç»“æœï¼Ÿåº”è¯¥é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼Ÿ\n",
    "\n",
    "4. **ç½‘ç»œæ•ˆåº”è¯†åˆ«**ï¼šä½ æ­£åœ¨ä¸ºç¤¾äº¤å¹³å°è®¾è®¡ä¸€ä¸ªæ¨èç®—æ³•å®éªŒã€‚å¦‚ä½•åˆ¤æ–­è¿™ä¸ªå®éªŒæ˜¯å¦å­˜åœ¨æ˜¾è‘—çš„ç½‘ç»œæ•ˆåº”ï¼Ÿæœ‰å“ªäº›è¯Šæ–­æ–¹æ³•ï¼Ÿ\n",
    "\n",
    "### å®æˆ˜åº”ç”¨\n",
    "\n",
    "5. **ç»¼åˆåœºæ™¯**ï¼šå­—èŠ‚è·³åŠ¨è¦æµ‹è¯•ä¸€ä¸ªæ–°çš„è§†é¢‘æ¨èç®—æ³•ï¼Œç›®æ ‡ç”¨æˆ·ç¾¤æ˜¯ 5000 ä¸‡ DAUã€‚è¯·è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„å®éªŒæ–¹æ¡ˆï¼Œéœ€è¦è€ƒè™‘ï¼š\n",
    "   - å¦‚ä½•é¿å… SRMï¼Ÿ\n",
    "   - å¦‚ä½•å¤„ç†æ¯æ—¥ç›‘æ§ä¸ç»Ÿè®¡ä¸¥æ ¼æ€§çš„å¹³è¡¡ï¼Ÿ\n",
    "   - éœ€è¦ç›‘æ§å“ªäº›æŒ‡æ ‡ï¼Ÿå¦‚ä½•å¤„ç†å¤šé‡æ£€éªŒï¼Ÿ\n",
    "   - æ¨èç®—æ³•å®éªŒæ˜¯å¦å­˜åœ¨ç½‘ç»œæ•ˆåº”ï¼Ÿå¦‚ä½•ç¼“è§£ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ æœ¬ç« å°ç»“\n",
    "\n",
    "| é™·é˜± | é—®é¢˜ | æ£€æµ‹æ–¹æ³• | è§£å†³æ–¹æ¡ˆ |\n",
    "|------|------|----------|----------|\n",
    "| **SRM** | åˆ†æµæ¯”ä¾‹å¼‚å¸¸ï¼Œä¸¤ç»„ä¸å¯æ¯” | å¡æ–¹æ£€éªŒ (Î±=0.001) | æ’æŸ¥æ•°æ®ç®¡é“ã€åˆ†æµé€»è¾‘ |\n",
    "| **Peeking** | åå¤æ£€æŸ¥è†¨èƒ€å‡é˜³æ€§ | è·Ÿè¸ªæ£€æŸ¥æ¬¡æ•° | Alpha Spending, å›ºå®šæ ·æœ¬é‡ |\n",
    "| **Multiple Testing** | å¤šæŒ‡æ ‡å¢åŠ å‡é˜³æ€§ | è®¡ç®— FWER/FDR | Bonferroni, BH æ ¡æ­£ |\n",
    "| **Network Effects** | ç”¨æˆ·é—´å¹²æ‰°è¿èƒŒ SUTVA | æ£€æŸ¥æº¢å‡ºæ•ˆåº” | Cluster/Switchback å®éªŒ |\n",
    "\n",
    "### ğŸ¯ é¢è¯•å…³é”®ç‚¹\n",
    "\n",
    "1. **SRM æ˜¯çº¢çº¿**ï¼šä¸€æ—¦æ£€æµ‹åˆ° SRMï¼Œå®éªŒç»“è®ºæ— æ•ˆï¼Œå¿…é¡»å…ˆä¿®å¤æ•°æ®é—®é¢˜\n",
    "\n",
    "2. **ç»Ÿè®¡ä¸¥æ ¼æ€§ä¸ä¸šåŠ¡éœ€æ±‚çš„å¹³è¡¡**ï¼š\n",
    "   - ä½¿ç”¨åºè´¯æ£€éªŒå…è®¸æ—©åœï¼Œä½†è¦æå‰è§„åˆ’\n",
    "   - åŒºåˆ†æ¢ç´¢æ€§åˆ†æå’Œç¡®è¯æ€§åˆ†æ\n",
    "\n",
    "3. **ç½‘ç»œæ•ˆåº”æ˜¯å¤§å‚å®éªŒçš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š\n",
    "   - ç¤¾äº¤ã€åŒè¾¹å¸‚åœºã€æ¨èç³»ç»Ÿéƒ½å­˜åœ¨\n",
    "   - è®¾è®¡èšç±»/æ—¶é—´åˆ‡ç‰‡å®éªŒæ¥ç¼“è§£\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é˜…è¯»\n",
    "\n",
    "- **Trustworthy Online Controlled Experiments** (Kohavi et al.) - A/B æµ‹è¯•åœ£ç»\n",
    "- **Detecting Network Effects: Randomizing Over Randomized Experiments** (Blake & Coey, 2014)\n",
    "- **Switchback Tests and Randomized Experimentation Under Network Effects at DoorDash** (2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
