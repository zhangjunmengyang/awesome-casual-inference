{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.3: CEVAE - 用变分推断处理隐混淆\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解变分自编码器 (VAE) 基础原理\n",
    "2. 掌握 CEVAE (Causal Effect VAE) 的架构设计\n",
    "3. 学习如何处理隐变量的因果推断问题\n",
    "4. 实现完整的 CEVAE 模型并进行评估\n",
    "\n",
    "---\n",
    "\n",
    "## 业务场景：医疗推荐系统中的隐性健康状况\n",
    "\n",
    "**问题背景**：\n",
    "\n",
    "某医疗平台想评估「个性化健康建议」对患者健康改善的因果效应。但面临一个严重问题：\n",
    "\n",
    "- **观测数据**：年龄、性别、BMI、运动频率、饮食习惯\n",
    "- **处理变量**：是否接受个性化建议\n",
    "- **结果变量**：3个月后的健康评分\n",
    "- **隐藏混淆**：患者的**真实健康状况**（未被完全观测）\n",
    "\n",
    "问题在于：\n",
    "1. 医生可能根据患者的**隐性健康状况**（如未确诊的慢性病）决定是否推荐\n",
    "2. 这些隐性状况同时影响最终的健康结果\n",
    "3. 我们无法直接观测到这些隐变量\n",
    "\n",
    "**传统方法的困境**：\n",
    "- PSM/IPW：只能处理观测到的混淆变量\n",
    "- DML：假设所有混淆变量都已观测\n",
    "- 工具变量：难以找到合适的工具\n",
    "\n",
    "**CEVAE 的解决方案**：\n",
    "用变分推断学习隐变量的分布，同时估计因果效应！\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✅ 环境准备完成！\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"设备: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: 问题背景 - 隐混淆变量\n",
    "\n",
    "## 1.1 因果图与隐变量\n",
    "\n",
    "### 完整因果图\n",
    "\n",
    "```\n",
    "       Z (隐变量：真实健康状况)\n",
    "      / | \\\n",
    "     /  |  \\\n",
    "    /   |   \\\n",
    "   X    T    Y\n",
    "   |    |    |\n",
    "   |    v    |\n",
    "   +----Y----+\n",
    "```\n",
    "\n",
    "其中：\n",
    "- **Z**: 隐变量（未观测的健康状况）\n",
    "- **X**: 观测协变量（年龄、BMI等）\n",
    "- **T**: 处理变量（是否接受建议）\n",
    "- **Y**: 结果变量（健康评分）\n",
    "\n",
    "### 数学形式\n",
    "\n",
    "生成过程：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z &\\sim p(Z) \\quad \\text{(隐变量的先验分布)} \\\\\n",
    "X &\\sim p(X | Z) \\quad \\text{(观测特征由隐变量生成)} \\\\\n",
    "T &\\sim p(T | X, Z) \\quad \\text{(处理受隐变量影响)} \\\\\n",
    "Y &\\sim p(Y | T, X, Z) \\quad \\text{(结果受隐变量混淆)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**核心挑战**：Z 未观测，但影响 T 和 Y，导致混淆偏差！\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 为什么需要深度学习？\n",
    "\n",
    "### 传统方法的局限\n",
    "\n",
    "| 方法 | 假设 | 局限 |\n",
    "|------|------|------|\n",
    "| **PSM** | 所有混淆变量已观测 | 无法处理隐变量 |\n",
    "| **IPW** | 倾向性得分可识别 | 需要正确的协变量 |\n",
    "| **工具变量** | 存在合适的工具 | 实践中难以找到 |\n",
    "| **DML** | Unconfoundedness | 忽略隐混淆 |\n",
    "\n",
    "### CEVAE 的优势\n",
    "\n",
    "1. **建模隐变量**：用 VAE 学习 Z 的分布\n",
    "2. **端到端优化**：联合学习表示和因果效应\n",
    "3. **灵活性**：处理复杂非线性关系\n",
    "4. **不确定性量化**：变分推断提供不确定性估计\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 CEVAE 的核心创新\n",
    "\n",
    "### 三大创新点\n",
    "\n",
    "#### 1. **因果生成模型**\n",
    "- 明确建模因果图结构\n",
    "- 区分处理前和处理后变量\n",
    "\n",
    "#### 2. **代理变量策略**\n",
    "- 利用观测到的 X 作为 Z 的代理\n",
    "- 通过 $p(X|Z)$ 间接推断 Z\n",
    "\n",
    "#### 3. **变分推断框架**\n",
    "- 用 ELBO 优化目标\n",
    "- 平衡重构和正则化\n",
    "\n",
    "### 核心公式\n",
    "\n",
    "后验推断：\n",
    "\n",
    "$$\n",
    "q(Z | X, T, Y) \\approx p(Z | X, T, Y)\n",
    "$$\n",
    "\n",
    "因果效应：\n",
    "\n",
    "$$\n",
    "\\tau(x) = \\mathbb{E}[Y | do(T=1), X=x] - \\mathbb{E}[Y | do(T=0), X=x]\n",
    "$$\n",
    "\n",
    "通过边缘化隐变量：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Y | do(T=t), X=x] = \\int p(Y | T=t, X=x, Z) p(Z | X) dZ\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: VAE 基础回顾\n",
    "\n",
    "## 2.1 变分自编码器原理\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "VAE 是一个生成模型，学习数据的**潜在表示** (latent representation)：\n",
    "\n",
    "```\n",
    "编码器: X → Z (推断)\n",
    "解码器: Z → X (生成)\n",
    "```\n",
    "\n",
    "### 数学框架\n",
    "\n",
    "**生成模型**：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_\\theta(X, Z) &= p(Z) \\cdot p_\\theta(X | Z) \\\\\n",
    "p(Z) &= \\mathcal{N}(0, I) \\quad \\text{(先验)} \\\\\n",
    "p_\\theta(X | Z) &= \\text{Decoder}(Z; \\theta) \\quad \\text{(似然)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**推断模型**：\n",
    "\n",
    "$$\n",
    "q_\\phi(Z | X) = \\mathcal{N}(\\mu_\\phi(X), \\sigma_\\phi^2(X)) \\quad \\text{(近似后验)}\n",
    "$$\n",
    "\n",
    "其中 $\\mu_\\phi$ 和 $\\sigma_\\phi$ 是编码器网络。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ELBO 目标函数\n",
    "\n",
    "### 推导过程\n",
    "\n",
    "我们想最大化边际对数似然：\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(X) = \\log \\int p_\\theta(X, Z) dZ\n",
    "$$\n",
    "\n",
    "但这个积分难以计算。引入变分分布 $q_\\phi(Z|X)$：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta(X) &= \\mathbb{E}_{q_\\phi(Z|X)} \\left[ \\log \\frac{p_\\theta(X, Z)}{q_\\phi(Z|X)} \\right] + \\text{KL}(q_\\phi(Z|X) \\| p_\\theta(Z|X)) \\\\\n",
    "&\\geq \\mathbb{E}_{q_\\phi(Z|X)} \\left[ \\log p_\\theta(X | Z) \\right] - \\text{KL}(q_\\phi(Z|X) \\| p(Z)) \\\\\n",
    "&\\equiv \\mathcal{L}_{\\text{ELBO}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### ELBO 的两个组成部分\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ELBO}} = \\underbrace{\\mathbb{E}_{q_\\phi(Z|X)} [\\log p_\\theta(X | Z)]}_{\\text{重构损失}} - \\underbrace{\\text{KL}(q_\\phi(Z|X) \\| p(Z))}_{\\text{正则化项}}\n",
    "$$\n",
    "\n",
    "1. **重构损失**：确保解码器能从 Z 重构 X\n",
    "2. **KL 散度**：让后验接近先验，防止过拟合\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 重参数化技巧 (Reparameterization Trick)\n",
    "\n",
    "### 问题\n",
    "\n",
    "ELBO 中的期望包含采样操作：\n",
    "\n",
    "$$\n",
    "Z \\sim q_\\phi(Z | X) = \\mathcal{N}(\\mu_\\phi(X), \\sigma_\\phi^2(X))\n",
    "$$\n",
    "\n",
    "采样操作**不可微**，无法反向传播！\n",
    "\n",
    "### 解决方案\n",
    "\n",
    "重参数化：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\epsilon &\\sim \\mathcal{N}(0, I) \\\\\n",
    "Z &= \\mu_\\phi(X) + \\sigma_\\phi(X) \\odot \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "现在随机性从 $\\epsilon$ 引入，而 $\\mu_\\phi$ 和 $\\sigma_\\phi$ 是确定性的，可以计算梯度！\n",
    "\n",
    "### PyTorch 实现\n",
    "\n",
    "```python\n",
    "def reparameterize(mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)  # 从标准正态采样\n",
    "    z = mu + eps * std\n",
    "    return z\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化重参数化技巧\n",
    "def visualize_reparameterization():\n",
    "    \"\"\"可视化重参数化过程\"\"\"\n",
    "    \n",
    "    # 设置参数\n",
    "    mu = 2.0\n",
    "    sigma = 1.5\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # 直接采样 (不可微)\n",
    "    direct_samples = np.random.normal(mu, sigma, n_samples)\n",
    "    \n",
    "    # 重参数化采样\n",
    "    eps = np.random.normal(0, 1, n_samples)\n",
    "    reparam_samples = mu + sigma * eps\n",
    "    \n",
    "    # 创建图表\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('直接采样 (不可微)', '重参数化采样 (可微)')\n",
    "    )\n",
    "    \n",
    "    # 直接采样分布\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=direct_samples, name='直接采样', \n",
    "                     nbinsx=50, opacity=0.7, marker_color='#E74C3C'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 重参数化采样分布\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=reparam_samples, name='重参数化', \n",
    "                     nbinsx=50, opacity=0.7, marker_color='#3498DB'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 添加理论分布\n",
    "    x = np.linspace(-2, 6, 100)\n",
    "    y = 1000 * (1/np.sqrt(2*np.pi*sigma**2)) * np.exp(-(x-mu)**2/(2*sigma**2)) * 0.2\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=y, mode='lines', name='理论分布',\n",
    "                   line=dict(color='black', width=2, dash='dash')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=y, mode='lines', name='理论分布',\n",
    "                   line=dict(color='black', width=2, dash='dash'),\n",
    "                   showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='重参数化技巧：两种采样方式的等价性',\n",
    "        height=400,\n",
    "        showlegend=True,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text='Z 值', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Z 值', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='频数', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='频数', row=1, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"直接采样均值: {direct_samples.mean():.3f}, 标准差: {direct_samples.std():.3f}\")\n",
    "    print(f\"重参数化均值: {reparam_samples.mean():.3f}, 标准差: {reparam_samples.std():.3f}\")\n",
    "    print(f\"理论值: μ={mu}, σ={sigma}\")\n",
    "\n",
    "visualize_reparameterization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: CEVAE 架构设计\n",
    "\n",
    "## 3.1 因果图与生成模型\n",
    "\n",
    "### CEVAE 的生成过程\n",
    "\n",
    "CEVAE 明确建模因果结构：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(Z) &= \\mathcal{N}(0, I) \\quad \\text{(隐变量先验)} \\\\\n",
    "p_\\theta(X | Z) &= \\mathcal{N}(\\mu_X(Z), \\sigma_X^2(Z)) \\quad \\text{(观测特征)} \\\\\n",
    "p_\\theta(T | X, Z) &= \\text{Bernoulli}(\\pi(X, Z)) \\quad \\text{(处理分配)} \\\\\n",
    "p_\\theta(Y | T, X, Z) &= \\mathcal{N}(\\mu_Y(T, X, Z), \\sigma_Y^2) \\quad \\text{(结果)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 关键区别\n",
    "\n",
    "| 标准 VAE | CEVAE |\n",
    "|----------|-------|\n",
    "| $p(X \\mid Z)$ | $p(X \\mid Z), p(T \\mid X,Z), p(Y \\mid T,X,Z)$ |\n",
    "| 只重构 X | 重构 X, T, Y |\n",
    "| 无因果结构 | 明确因果图 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 推断网络 (Encoder)\n",
    "\n",
    "### 目标\n",
    "\n",
    "学习后验分布：\n",
    "\n",
    "$$\n",
    "q_\\phi(Z | X, T, Y) \\approx p(Z | X, T, Y)\n",
    "$$\n",
    "\n",
    "### 架构\n",
    "\n",
    "```\n",
    "输入: [X, T, Y] → 全连接层 → ReLU → 全连接层 → [μ_Z, log_σ_Z]\n",
    "```\n",
    "\n",
    "### PyTorch 实现\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, t, y):\n",
    "        # 拼接所有输入\n",
    "        inputs = torch.cat([x, t.unsqueeze(1), y.unsqueeze(1)], dim=1)\n",
    "        h = F.relu(self.fc1(inputs))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 生成网络 (Decoder)\n",
    "\n",
    "CEVAE 的解码器包含**三个部分**，分别建模因果图中的三个条件分布：\n",
    "\n",
    "### 1. X 解码器\n",
    "\n",
    "$$\n",
    "p_\\theta(X | Z) = \\mathcal{N}(\\mu_X(Z), \\sigma_X^2(Z))\n",
    "$$\n",
    "\n",
    "```python\n",
    "class XDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, x_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, x_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        x_recon = self.fc2(h)\n",
    "        return x_recon\n",
    "```\n",
    "\n",
    "### 2. T 解码器\n",
    "\n",
    "$$\n",
    "p_\\theta(T | X, Z) = \\text{Bernoulli}(\\sigma(\\pi(X, Z)))\n",
    "$$\n",
    "\n",
    "```python\n",
    "class TDecoder(nn.Module):\n",
    "    def __init__(self, x_dim, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(x_dim + latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        h = torch.cat([x, z], dim=1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        t_logits = self.fc2(h)\n",
    "        return torch.sigmoid(t_logits)\n",
    "```\n",
    "\n",
    "### 3. Y 解码器\n",
    "\n",
    "$$\n",
    "p_\\theta(Y | T, X, Z) = \\mathcal{N}(\\mu_Y(T, X, Z), \\sigma_Y^2)\n",
    "$$\n",
    "\n",
    "```python\n",
    "class YDecoder(nn.Module):\n",
    "    def __init__(self, x_dim, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(x_dim + latent_dim + 1, hidden_dim)  # +1 for T\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, t, z):\n",
    "        h = torch.cat([x, t.unsqueeze(1), z], dim=1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        y_pred = self.fc2(h)\n",
    "        return y_pred\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 因果效应预测\n",
    "\n",
    "### 反事实推断\n",
    "\n",
    "对于给定样本 $(x_i, t_i, y_i)$：\n",
    "\n",
    "1. **推断隐变量**：\n",
    "   $$\n",
    "   Z_i \\sim q_\\phi(Z | X_i, T_i, Y_i)\n",
    "   $$\n",
    "\n",
    "2. **预测反事实结果**：\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   Y_i(1) &= \\mathbb{E}[Y | T=1, X=x_i, Z=z_i] \\\\\n",
    "   Y_i(0) &= \\mathbb{E}[Y | T=0, X=x_i, Z=z_i]\n",
    "   \\end{align}\n",
    "   $$\n",
    "\n",
    "3. **计算个体因果效应**：\n",
    "   $$\n",
    "   \\tau_i = Y_i(1) - Y_i(0)\n",
    "   $$\n",
    "\n",
    "### 平均因果效应 (ATE)\n",
    "\n",
    "通过蒙特卡洛采样：\n",
    "\n",
    "$$\n",
    "\\text{ATE} = \\frac{1}{N} \\sum_{i=1}^N \\tau_i = \\frac{1}{N} \\sum_{i=1}^N \\left[ Y_i(1) - Y_i(0) \\right]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: 训练与实现\n",
    "\n",
    "## 4.1 完整损失函数\n",
    "\n",
    "### ELBO for CEVAE\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\mathbb{E}_{q_\\phi(Z|X,T,Y)} &\\Big[ \\log p_\\theta(X | Z) + \\log p_\\theta(T | X, Z) + \\log p_\\theta(Y | T, X, Z) \\Big] \\\\\n",
    "&- \\text{KL}(q_\\phi(Z | X, T, Y) \\| p(Z))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 各项损失\n",
    "\n",
    "1. **重构损失 (X)**：\n",
    "   $$\n",
    "   \\mathcal{L}_X = -\\log p_\\theta(X | Z) = \\frac{1}{2} \\|X - \\hat{X}\\|^2\n",
    "   $$\n",
    "\n",
    "2. **处理损失 (T)**：\n",
    "   $$\n",
    "   \\mathcal{L}_T = -\\log p_\\theta(T | X, Z) = \\text{BCE}(T, \\hat{T})\n",
    "   $$\n",
    "\n",
    "3. **结果损失 (Y)**：\n",
    "   $$\n",
    "   \\mathcal{L}_Y = -\\log p_\\theta(Y | T, X, Z) = \\frac{1}{2} \\|Y - \\hat{Y}\\|^2\n",
    "   $$\n",
    "\n",
    "4. **KL 散度**：\n",
    "   $$\n",
    "   \\mathcal{L}_{KL} = \\text{KL}(q_\\phi(Z | X, T, Y) \\| \\mathcal{N}(0, I)) = \\frac{1}{2} \\sum_{j=1}^{d_z} \\left( \\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right)\n",
    "   $$\n",
    "\n",
    "### 总损失\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_X + \\mathcal{L}_T + \\mathcal{L}_Y + \\beta \\cdot \\mathcal{L}_{KL}\n",
    "$$\n",
    "\n",
    "其中 $\\beta$ 是权重系数（通常设为 1）。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PyTorch 完整实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(nn.Module):\n",
    "    \"\"\"Causal Effect Variational Autoencoder\"\"\"\n",
    "    \n",
    "    def __init__(self, x_dim: int, latent_dim: int = 20, hidden_dim: int = 200):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            x_dim: 观测协变量维度\n",
    "            latent_dim: 隐变量维度\n",
    "            hidden_dim: 隐藏层维度\n",
    "        \"\"\"\n",
    "        super(CEVAE, self).__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # ===== 编码器 (Inference Network) =====\n",
    "        # 输入: [X, T, Y], 输出: Z 的分布参数\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(x_dim + 2, hidden_dim),  # +2 for T and Y\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ===== 解码器 (Generative Network) =====\n",
    "        \n",
    "        # X 解码器: p(X | Z)\n",
    "        self.x_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, x_dim)\n",
    "        )\n",
    "        \n",
    "        # T 解码器: p(T | X, Z)\n",
    "        self.t_decoder = nn.Sequential(\n",
    "            nn.Linear(x_dim + latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Y 解码器: p(Y | T, X, Z)\n",
    "        self.y_decoder = nn.Sequential(\n",
    "            nn.Linear(x_dim + latent_dim + 1, hidden_dim),  # +1 for T\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"编码: 推断 q(Z | X, T, Y)\"\"\"\n",
    "        inputs = torch.cat([x, t.unsqueeze(1), y.unsqueeze(1)], dim=1)\n",
    "        h = self.encoder(inputs)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"重参数化技巧\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode_x(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"解码 X: p(X | Z)\"\"\"\n",
    "        return self.x_decoder(z)\n",
    "    \n",
    "    def decode_t(self, x: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"解码 T: p(T | X, Z)\"\"\"\n",
    "        inputs = torch.cat([x, z], dim=1)\n",
    "        return self.t_decoder(inputs)\n",
    "    \n",
    "    def decode_y(self, x: torch.Tensor, t: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"解码 Y: p(Y | T, X, Z)\"\"\"\n",
    "        inputs = torch.cat([x, t.unsqueeze(1), z], dim=1)\n",
    "        return self.y_decoder(inputs)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 编码\n",
    "        mu, logvar = self.encode(x, t, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # 解码\n",
    "        x_recon = self.decode_x(z)\n",
    "        t_recon = self.decode_t(x, z)\n",
    "        y_recon = self.decode_y(x, t, z)\n",
    "        \n",
    "        return {\n",
    "            'x_recon': x_recon,\n",
    "            't_recon': t_recon,\n",
    "            'y_recon': y_recon,\n",
    "            'mu': mu,\n",
    "            'logvar': logvar,\n",
    "            'z': z\n",
    "        }\n",
    "    \n",
    "    def predict_counterfactual(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor, \n",
    "                               n_samples: int = 100) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"预测反事实结果\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # 推断隐变量\n",
    "            mu, logvar = self.encode(x, t, y)\n",
    "            \n",
    "            # 多次采样以获得更稳定的估计\n",
    "            y0_samples = []\n",
    "            y1_samples = []\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                z = self.reparameterize(mu, logvar)\n",
    "                \n",
    "                # 预测 Y(0) 和 Y(1)\n",
    "                t0 = torch.zeros_like(t)\n",
    "                t1 = torch.ones_like(t)\n",
    "                \n",
    "                y0 = self.decode_y(x, t0, z)\n",
    "                y1 = self.decode_y(x, t1, z)\n",
    "                \n",
    "                y0_samples.append(y0)\n",
    "                y1_samples.append(y1)\n",
    "            \n",
    "            # 平均\n",
    "            y0_pred = torch.stack(y0_samples).mean(dim=0).squeeze()\n",
    "            y1_pred = torch.stack(y1_samples).mean(dim=0).squeeze()\n",
    "            \n",
    "        return y0_pred, y1_pred\n",
    "\n",
    "print(\"✅ CEVAE 模型定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cevae_loss(outputs: Dict[str, torch.Tensor], \n",
    "               x: torch.Tensor, \n",
    "               t: torch.Tensor, \n",
    "               y: torch.Tensor,\n",
    "               beta: float = 1.0) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    计算 CEVAE 损失\n",
    "    \n",
    "    参数:\n",
    "        outputs: 模型输出字典\n",
    "        x, t, y: 真实数据\n",
    "        beta: KL 散度权重\n",
    "    \n",
    "    返回:\n",
    "        损失字典\n",
    "    \"\"\"\n",
    "    # 1. X 重构损失 (MSE)\n",
    "    x_recon_loss = F.mse_loss(outputs['x_recon'], x, reduction='mean')\n",
    "    \n",
    "    # 2. T 重构损失 (Binary Cross-Entropy)\n",
    "    t_recon_loss = F.binary_cross_entropy(\n",
    "        outputs['t_recon'].squeeze(), \n",
    "        t, \n",
    "        reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # 3. Y 重构损失 (MSE)\n",
    "    y_recon_loss = F.mse_loss(outputs['y_recon'].squeeze(), y, reduction='mean')\n",
    "    \n",
    "    # 4. KL 散度\n",
    "    # KL(q(z|x,t,y) || N(0,1)) = 0.5 * sum(mu^2 + sigma^2 - log(sigma^2) - 1)\n",
    "    kl_loss = -0.5 * torch.sum(\n",
    "        1 + outputs['logvar'] - outputs['mu'].pow(2) - outputs['logvar'].exp()\n",
    "    ) / x.size(0)  # 平均到每个样本\n",
    "    \n",
    "    # 总损失\n",
    "    total_loss = x_recon_loss + t_recon_loss + y_recon_loss + beta * kl_loss\n",
    "    \n",
    "    return {\n",
    "        'total': total_loss,\n",
    "        'x_recon': x_recon_loss,\n",
    "        't_recon': t_recon_loss,\n",
    "        'y_recon': y_recon_loss,\n",
    "        'kl': kl_loss\n",
    "    }\n",
    "\n",
    "print(\"✅ 损失函数定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confounded_data(n_samples: int = 2000, \n",
    "                             x_dim: int = 10,\n",
    "                             latent_dim: int = 3,\n",
    "                             noise_level: float = 0.5,\n",
    "                             true_ate: float = 3.0) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    生成带隐混淆的模拟数据\n",
    "    \n",
    "    数据生成过程:\n",
    "    1. Z ~ N(0, 1)  (隐变量)\n",
    "    2. X = W_x @ Z + noise  (观测特征受隐变量影响)\n",
    "    3. T ~ Bernoulli(sigma(Z'a + X'b))  (处理受隐变量和观测特征影响)\n",
    "    4. Y = true_ate * T + Z'c + X'd + noise  (结果受所有变量影响)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 生成隐变量 Z\n",
    "    Z = np.random.randn(n_samples, latent_dim)\n",
    "    \n",
    "    # 2. 生成观测特征 X (由 Z 生成)\n",
    "    W_x = np.random.randn(latent_dim, x_dim) * 0.5\n",
    "    X = Z @ W_x + np.random.randn(n_samples, x_dim) * noise_level\n",
    "    \n",
    "    # 3. 生成处理 T (受 Z 和 X 影响)\n",
    "    a = np.random.randn(latent_dim) * 1.5  # Z 对 T 的影响\n",
    "    b = np.random.randn(x_dim) * 0.3       # X 对 T 的影响\n",
    "    \n",
    "    propensity_logit = Z @ a + X @ b\n",
    "    propensity = 1 / (1 + np.exp(-propensity_logit))\n",
    "    T = (np.random.rand(n_samples) < propensity).astype(float)\n",
    "    \n",
    "    # 4. 生成结果 Y (受 T, Z, X 影响)\n",
    "    c = np.random.randn(latent_dim) * 2.0  # Z 对 Y 的影响 (关键！)\n",
    "    d = np.random.randn(x_dim) * 0.5       # X 对 Y 的影响\n",
    "    \n",
    "    # Y(0) 的基础值\n",
    "    Y0 = Z @ c + X @ d + np.random.randn(n_samples) * noise_level\n",
    "    \n",
    "    # 处理效应\n",
    "    Y1 = Y0 + true_ate\n",
    "    \n",
    "    # 观测到的 Y\n",
    "    Y = T * Y1 + (1 - T) * Y0\n",
    "    \n",
    "    # 计算真实的 ITE\n",
    "    true_ite = Y1 - Y0\n",
    "    \n",
    "    return {\n",
    "        'X': X,\n",
    "        'T': T,\n",
    "        'Y': Y,\n",
    "        'Z': Z,  # 真实隐变量 (仅用于评估)\n",
    "        'Y0': Y0,\n",
    "        'Y1': Y1,\n",
    "        'true_ite': true_ite,\n",
    "        'true_ate': true_ate,\n",
    "        'propensity': propensity\n",
    "    }\n",
    "\n",
    "\n",
    "# 生成数据\n",
    "print(\"生成模拟数据...\")\n",
    "data = generate_confounded_data(n_samples=2000, x_dim=10, latent_dim=3, true_ate=3.0)\n",
    "\n",
    "print(f\"\\n数据形状:\")\n",
    "print(f\"  X: {data['X'].shape}\")\n",
    "print(f\"  T: {data['T'].shape}\")\n",
    "print(f\"  Y: {data['Y'].shape}\")\n",
    "print(f\"  Z (隐变量): {data['Z'].shape}\")\n",
    "print(f\"\\n真实 ATE: {data['true_ate']:.3f}\")\n",
    "print(f\"处理组比例: {data['T'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化数据生成过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化隐变量的影响\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        '隐变量 Z 的分布',\n",
    "        'Z 对处理 T 的影响',\n",
    "        'Z 对结果 Y 的影响',\n",
    "        '倾向性得分分布'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Z 的分布\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=data['Z'][:, 0], name='Z[0]', opacity=0.7, nbinsx=30),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Z vs T\n",
    "z_first = data['Z'][:, 0]\n",
    "fig.add_trace(\n",
    "    go.Box(x=data['T'], y=z_first, name='Z vs T', marker_color='#3498DB'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Z vs Y\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=z_first, y=data['Y'], mode='markers', \n",
    "               marker=dict(color=data['T'], colorscale='RdBu', showscale=True,\n",
    "                          colorbar=dict(title='T', x=1.15)),\n",
    "               name='Z vs Y'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. 倾向性得分\n",
    "treated = data['propensity'][data['T'] == 1]\n",
    "control = data['propensity'][data['T'] == 0]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=treated, name='处理组', opacity=0.6, marker_color='#E74C3C', nbinsx=30),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=control, name='对照组', opacity=0.6, marker_color='#3498DB', nbinsx=30),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    title_text='隐混淆数据生成过程可视化',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Z 值', row=1, col=1)\n",
    "fig.update_xaxes(title_text='处理组', row=1, col=2)\n",
    "fig.update_xaxes(title_text='Z[0]', row=2, col=1)\n",
    "fig.update_xaxes(title_text='倾向性得分', row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text='频数', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Z[0]', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Y', row=2, col=1)\n",
    "fig.update_yaxes(title_text='频数', row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.5 训练 CEVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 标准化 X 和 Y\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_x.fit_transform(data['X'])\n",
    "Y_scaled = scaler_y.fit_transform(data['Y'].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_tensor = torch.FloatTensor(X_scaled)\n",
    "T_tensor = torch.FloatTensor(data['T'])\n",
    "Y_tensor = torch.FloatTensor(Y_scaled)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = np.arange(len(X_tensor))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]\n",
    "T_train, T_test = T_tensor[train_idx], T_tensor[test_idx]\n",
    "Y_train, Y_test = Y_tensor[train_idx], Y_tensor[test_idx]\n",
    "\n",
    "print(f\"训练集大小: {len(train_idx)}\")\n",
    "print(f\"测试集大小: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "x_dim = X_train.shape[1]\n",
    "model = CEVAE(x_dim=x_dim, latent_dim=20, hidden_dim=200)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# 训练参数\n",
    "n_epochs = 200\n",
    "batch_size = 128\n",
    "beta = 1.0  # KL weight\n",
    "\n",
    "# 记录训练历史\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'x_recon': [],\n",
    "    't_recon': [],\n",
    "    'y_recon': [],\n",
    "    'kl': []\n",
    "}\n",
    "\n",
    "print(\"开始训练 CEVAE...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = {'total': [], 'x_recon': [], 't_recon': [], 'y_recon': [], 'kl': []}\n",
    "    \n",
    "    # Mini-batch training\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        x_batch = X_train[start_idx:end_idx]\n",
    "        t_batch = T_train[start_idx:end_idx]\n",
    "        y_batch = Y_train[start_idx:end_idx]\n",
    "        \n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch, t_batch, y_batch)\n",
    "        \n",
    "        # 计算损失\n",
    "        losses = cevae_loss(outputs, x_batch, t_batch, y_batch, beta=beta)\n",
    "        \n",
    "        # 反向传播\n",
    "        losses['total'].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录\n",
    "        for key in losses:\n",
    "            epoch_losses[key].append(losses[key].item())\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 测试集评估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test, T_test, Y_test)\n",
    "        test_losses = cevae_loss(test_outputs, X_test, T_test, Y_test, beta=beta)\n",
    "    \n",
    "    # 记录历史\n",
    "    train_loss = np.mean(epoch_losses['total'])\n",
    "    test_loss = test_losses['total'].item()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['x_recon'].append(np.mean(epoch_losses['x_recon']))\n",
    "    history['t_recon'].append(np.mean(epoch_losses['t_recon']))\n",
    "    history['y_recon'].append(np.mean(epoch_losses['y_recon']))\n",
    "    history['kl'].append(np.mean(epoch_losses['kl']))\n",
    "    \n",
    "    # 打印进度\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"  X Recon: {history['x_recon'][-1]:.4f} | T Recon: {history['t_recon'][-1]:.4f} | Y Recon: {history['y_recon'][-1]:.4f} | KL: {history['kl'][-1]:.4f}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n✅ 训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练曲线\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('总损失', '重构损失 (X, T, Y)', 'KL 散度', '学习曲线')\n",
    ")\n",
    "\n",
    "epochs = np.arange(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# 1. 总损失\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['train_loss'], mode='lines', name='训练损失',\n",
    "               line=dict(color='#3498DB', width=2)),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['test_loss'], mode='lines', name='测试损失',\n",
    "               line=dict(color='#E74C3C', width=2)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. 各项重构损失\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['x_recon'], mode='lines', name='X 重构',\n",
    "               line=dict(color='#2ECC71')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['t_recon'], mode='lines', name='T 重构',\n",
    "               line=dict(color='#F39C12')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['y_recon'], mode='lines', name='Y 重构',\n",
    "               line=dict(color='#9B59B6')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. KL 散度\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['kl'], mode='lines', name='KL 散度',\n",
    "               line=dict(color='#E67E22', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. 对数尺度学习曲线\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['train_loss'], mode='lines', name='训练 (log)',\n",
    "               line=dict(color='#3498DB', width=2)),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=epochs, y=history['test_loss'], mode='lines', name='测试 (log)',\n",
    "               line=dict(color='#E74C3C', width=2)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_yaxes(type='log', row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    title_text='CEVAE 训练过程',\n",
    "    showlegend=True,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Epoch')\n",
    "fig.update_yaxes(title_text='Loss')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: 模型评估\n",
    "\n",
    "## 5.1 因果效应估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测反事实结果\n",
    "print(\"预测反事实结果...\")\n",
    "y0_pred, y1_pred = model.predict_counterfactual(X_test, T_test, Y_test, n_samples=100)\n",
    "\n",
    "# 转换回原始尺度\n",
    "y0_pred_original = scaler_y.inverse_transform(y0_pred.numpy().reshape(-1, 1)).flatten()\n",
    "y1_pred_original = scaler_y.inverse_transform(y1_pred.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "# 计算 ITE\n",
    "ite_pred = y1_pred_original - y0_pred_original\n",
    "\n",
    "# 真实的 ITE (测试集)\n",
    "true_ite_test = data['true_ite'][test_idx]\n",
    "\n",
    "# 计算 ATE\n",
    "ate_pred = ite_pred.mean()\n",
    "ate_true = data['true_ate']\n",
    "\n",
    "print(f\"\\n预测 ATE: {ate_pred:.3f}\")\n",
    "print(f\"真实 ATE: {ate_true:.3f}\")\n",
    "print(f\"误差: {abs(ate_pred - ate_true):.3f}\")\n",
    "\n",
    "# ITE 的 RMSE\n",
    "ite_rmse = np.sqrt(np.mean((ite_pred - true_ite_test)**2))\n",
    "print(f\"\\nITE RMSE: {ite_rmse:.3f}\")\n",
    "\n",
    "# ITE 的相关系数\n",
    "ite_corr = np.corrcoef(ite_pred, true_ite_test)[0, 1]\n",
    "print(f\"ITE 相关系数: {ite_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 ITE 预测\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('ITE 预测 vs 真实', 'ITE 分布对比')\n",
    ")\n",
    "\n",
    "# 1. 散点图\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=true_ite_test, \n",
    "        y=ite_pred,\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, opacity=0.6, color='#3498DB'),\n",
    "        name='预测 ITE'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 添加对角线\n",
    "min_val = min(true_ite_test.min(), ite_pred.min())\n",
    "max_val = max(true_ite_test.max(), ite_pred.max())\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_val, max_val],\n",
    "        y=[min_val, max_val],\n",
    "        mode='lines',\n",
    "        line=dict(color='red', dash='dash', width=2),\n",
    "        name='完美预测'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. 分布对比\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=true_ite_test, name='真实 ITE', opacity=0.6, \n",
    "                 marker_color='#E74C3C', nbinsx=30),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=ite_pred, name='预测 ITE', opacity=0.6, \n",
    "                 marker_color='#3498DB', nbinsx=30),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='真实 ITE', row=1, col=1)\n",
    "fig.update_yaxes(title_text='预测 ITE', row=1, col=1)\n",
    "fig.update_xaxes(title_text='ITE 值', row=1, col=2)\n",
    "fig.update_yaxes(title_text='频数', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text=f'CEVAE 因果效应估计 (ATE 误差: {abs(ate_pred - ate_true):.3f}, 相关性: {ite_corr:.3f})',\n",
    "    showlegend=True,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.2 与基线方法对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 参考答案: 实现基线方法并对比\n\ndef naive_estimator(X, T, Y):\n    \"\"\"简单差异估计\"\"\"\n    ate = Y[T == 1].mean() - Y[T == 0].mean()\n    return ate\n\ndef linear_regression_estimator(X_train, T_train, Y_train, X_test, T_test):\n    \"\"\"线性回归估计\"\"\"\n    # 训练两个模型\n    X_t1 = X_train[T_train == 1]\n    Y_t1 = Y_train[T_train == 1]\n    X_t0 = X_train[T_train == 0]\n    Y_t0 = Y_train[T_train == 0]\n    \n    model_t1 = LinearRegression().fit(X_t1, Y_t1)\n    model_t0 = LinearRegression().fit(X_t0, Y_t0)\n    \n    # 预测\n    y1_pred = model_t1.predict(X_test)\n    y0_pred = model_t0.predict(X_test)\n    \n    ite = y1_pred - y0_pred\n    ate = ite.mean()\n    \n    return ate, ite\n\n# 计算基线方法的 ATE\nY_test_original = scaler_y.inverse_transform(Y_test.numpy().reshape(-1, 1)).flatten()\nY_train_original = scaler_y.inverse_transform(Y_train.numpy().reshape(-1, 1)).flatten()\n\nate_naive = naive_estimator(\n    X_test.numpy(), \n    T_test.numpy(), \n    Y_test_original\n)\n\nate_lr, ite_lr = linear_regression_estimator(\n    X_train.numpy(),\n    T_train.numpy(),\n    Y_train_original,\n    X_test.numpy(),\n    T_test.numpy()\n)\n\n# 结果汇总\nresults_df = pd.DataFrame({\n    '方法': ['真实值', 'Naive', '线性回归', 'CEVAE'],\n    'ATE': [ate_true, ate_naive, ate_lr, ate_pred],\n    '绝对误差': [0, abs(ate_naive - ate_true), abs(ate_lr - ate_true), abs(ate_pred - ate_true)],\n    'ITE RMSE': [0, np.sqrt(np.mean((np.full_like(true_ite_test, ate_naive) - true_ite_test)**2)),\n                 np.sqrt(np.mean((ite_lr - true_ite_test)**2)),\n                 ite_rmse]\n})\n\nprint(\"\\n=== 方法对比 ===\")\nprint(results_df.to_string(index=False))\n\n# 可视化对比\nfig = go.Figure()\n\nmethods = results_df['方法'][1:]  # 排除真实值\nerrors = results_df['绝对误差'][1:]\ncolors = ['#E74C3C', '#F39C12', '#2ECC71']\n\nfig.add_trace(go.Bar(\n    x=methods,\n    y=errors,\n    marker_color=colors,\n    text=[f'{e:.3f}' for e in errors],\n    textposition='auto'\n))\n\nfig.update_layout(\n    title='ATE 估计误差对比 (越低越好)',\n    xaxis_title='方法',\n    yaxis_title='绝对误差',\n    template='plotly_white',\n    height=400\n)\n\nfig.show()\n\nprint(\"\\n✅ CEVAE相比传统方法的优势:\")\nprint(\"1. 能处理未观测混淆变量\")\nprint(\"2. 提供不确定性量化\")\nprint(\"3. 学习更丰富的表示\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.3 隐变量质量评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估学到的隐变量 Z 与真实隐变量的相关性\n",
    "print(\"评估隐变量质量...\\n\")\n",
    "\n",
    "# 获取推断的隐变量\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu_test, _ = model.encode(X_test, T_test, Y_test)\n",
    "    z_inferred = mu_test.numpy()\n",
    "\n",
    "# 真实隐变量\n",
    "z_true = data['Z'][test_idx]\n",
    "\n",
    "# 计算 CCA (典型相关分析) 或简单的相关系数\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 对每个维度计算最大相关性\n",
    "max_corrs = []\n",
    "for i in range(z_true.shape[1]):\n",
    "    corrs = [abs(pearsonr(z_true[:, i], z_inferred[:, j])[0]) \n",
    "             for j in range(z_inferred.shape[1])]\n",
    "    max_corrs.append(max(corrs))\n",
    "\n",
    "print(\"真实隐变量各维度的最大相关性:\")\n",
    "for i, corr in enumerate(max_corrs):\n",
    "    print(f\"  Z[{i}]: {corr:.3f}\")\n",
    "print(f\"\\n平均最大相关性: {np.mean(max_corrs):.3f}\")\n",
    "\n",
    "# 可视化隐变量空间\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('真实隐变量 (前2维)', '推断隐变量 (前2维)')\n",
    ")\n",
    "\n",
    "# 真实 Z\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=z_true[:, 0],\n",
    "        y=z_true[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=T_test.numpy(), colorscale='RdBu', showscale=True,\n",
    "                   colorbar=dict(title='T', x=0.45)),\n",
    "        name='真实 Z'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 推断 Z\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=z_inferred[:, 0],\n",
    "        y=z_inferred[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=T_test.numpy(), colorscale='RdBu', showscale=False),\n",
    "        name='推断 Z'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Z[0]')\n",
    "fig.update_yaxes(title_text='Z[1]')\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text='隐变量空间对比 (按处理组着色)',\n",
    "    showlegend=False,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: 扩展与变体\n",
    "\n",
    "## 6.1 TEDVAE (Treatment Effect Disentangled VAE)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "TEDVAE 进一步**解耦**隐变量：\n",
    "\n",
    "$$\n",
    "Z = [Z_c, Z_t, Z_y]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $Z_c$: 共同因子 (影响 X, T, Y)\n",
    "- $Z_t$: 处理特异因子 (只影响 T)\n",
    "- $Z_y$: 结果特异因子 (只影响 Y)\n",
    "\n",
    "### 优势\n",
    "\n",
    "1. **更好的可解释性**: 明确区分不同类型的混淆\n",
    "2. **更精确的因果估计**: 减少不相关因素的干扰\n",
    "3. **支持多种处理**: 可扩展到多值处理\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 其他深度因果模型\n",
    "\n",
    "### 1. **GANITE** (Generative Adversarial Nets for Inference of ITE)\n",
    "\n",
    "- **架构**: 对抗训练\n",
    "- **优势**: 更强的生成能力\n",
    "- **适用**: 复杂的反事实分布\n",
    "\n",
    "### 2. **Perfect Match**\n",
    "\n",
    "- **架构**: 距离度量学习\n",
    "- **优势**: 在表示空间中进行匹配\n",
    "- **适用**: 高维协变量\n",
    "\n",
    "### 3. **VCNet** (Variable Continuous Network)\n",
    "\n",
    "- **架构**: 连续处理的神经网络\n",
    "- **优势**: 处理连续值处理变量\n",
    "- **适用**: 剂量-响应曲线\n",
    "\n",
    "### 4. **SITE** (Self-Supervised ITE)\n",
    "\n",
    "- **架构**: 自监督学习\n",
    "- **优势**: 利用未标注数据\n",
    "- **适用**: 数据稀缺场景\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 实践建议\n",
    "\n",
    "### 何时使用 CEVAE？\n",
    "\n",
    "**适合的场景**：\n",
    "1. ✅ 怀疑存在**未观测混淆**\n",
    "2. ✅ 有足够的**观测协变量**作为代理\n",
    "3. ✅ 数据量充足（通常 >1000 样本）\n",
    "4. ✅ 愿意接受**模型假设**（VAE 框架）\n",
    "\n",
    "**不适合的场景**：\n",
    "1. ❌ 数据量太小（<500 样本）\n",
    "2. ❌ 完全没有代理变量\n",
    "3. ❌ 需要强可解释性\n",
    "4. ❌ 计算资源受限\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "| 参数 | 推荐范围 | 影响 |\n",
    "|------|----------|------|\n",
    "| **latent_dim** | 10-50 | 隐变量容量 |\n",
    "| **hidden_dim** | 100-500 | 网络表达能力 |\n",
    "| **learning_rate** | 1e-4 ~ 1e-3 | 收敛速度 |\n",
    "| **beta** (KL weight) | 0.1 ~ 2.0 | 重构 vs 正则化 |\n",
    "| **batch_size** | 64-256 | 训练稳定性 |\n",
    "\n",
    "### 常见问题\n",
    "\n",
    "**Q1: KL 散度塌陷怎么办？**\n",
    "- A: 使用 KL annealing（逐渐增大 beta）\n",
    "- A: 增大 latent_dim\n",
    "- A: 使用 free bits 技巧\n",
    "\n",
    "**Q2: 重构质量不好？**\n",
    "- A: 增大 hidden_dim\n",
    "- A: 调整各项损失的权重\n",
    "- A: 使用更深的网络\n",
    "\n",
    "**Q3: ATE 估计不准？**\n",
    "- A: 检查数据质量和代理变量\n",
    "- A: 尝试不同的网络架构\n",
    "- A: 增加训练轮数\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 练习 1: 实现 Beta-VAE\n\nBeta-VAE 是 CEVAE 的一个变体，通过调整 KL 散度的权重 $\\beta$ 来控制解耦程度。\n\n**参考答案**:\n\n```python\n# Beta-VAE 通过改变 beta 参数来控制解耦\n# beta > 1: 更强的解耦，牺牲重构质量\n# beta < 1: 更好的重构，较弱的解耦\n\n# 测试不同 beta 值\nbeta_values = [0.5, 1.0, 2.0, 5.0]\n\nfor beta in beta_values:\n    print(f\"\\n训练 CEVAE (beta={beta})...\")\n    \n    model_beta = CEVAE(x_dim=x_dim, latent_dim=20, hidden_dim=200)\n    optimizer = optim.Adam(model_beta.parameters(), lr=1e-3)\n    \n    # 训练 50 epochs（演示用）\n    for epoch in range(50):\n        # ... 训练循环（使用 beta 参数）\n        pass\n    \n    # 评估\n    y0_pred, y1_pred = model_beta.predict_counterfactual(X_test, T_test, Y_test)\n    ate_pred = (y1_pred - y0_pred).mean()\n    print(f\"Beta={beta}, ATE={ate_pred:.3f}\")\n```\n\n**实验结果解释**:\n- beta=0.5: 重构好，但隐变量可能纠缠\n- beta=1.0: 标准 VAE设置\n- beta=2.0: 更好的解耦，可能过正则化\n- beta=5.0: 严重过正则化，重构质量差"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 练习 2: 消融实验\n\n**任务**: 验证 CEVAE 各个组件的重要性\n\n**参考答案**:\n\n```python\n# 1. No-Z 变体: 不使用隐变量，直接从 X 预测 Y\nclass NoZModel(nn.Module):\n    def __init__(self, x_dim, hidden_dim=200):\n        super().__init__()\n        self.y0_net = nn.Sequential(\n            nn.Linear(x_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.y1_net = nn.Sequential(\n            nn.Linear(x_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    \n    def forward(self, x, t):\n        y0 = self.y0_net(x)\n        y1 = self.y1_net(x)\n        y_pred = torch.where(t.unsqueeze(1) == 1, y1, y0)\n        return y_pred\n\n# 2. No-X-recon 变体: 不重构 X\ndef cevae_loss_no_x_recon(outputs, x, t, y, beta=1.0):\n    # 只包含 T, Y 重构和 KL 散度\n    t_recon_loss = F.binary_cross_entropy(outputs['t_recon'].squeeze(), t)\n    y_recon_loss = F.mse_loss(outputs['y_recon'].squeeze(), y)\n    kl_loss = -0.5 * torch.sum(\n        1 + outputs['logvar'] - outputs['mu'].pow(2) - outputs['logvar'].exp()\n    ) / x.size(0)\n    \n    return {\n        'total': t_recon_loss + y_recon_loss + beta * kl_loss,\n        't_recon': t_recon_loss,\n        'y_recon': y_recon_loss,\n        'kl': kl_loss\n    }\n\n# 3. 训练和对比\nprint(\"消融实验结果:\")\nprint(\"-\" * 60)\nprint(f\"{'模型':<20} {'ATE误差':<15} {'PEHE':<15}\")\nprint(\"-\" * 60)\nprint(f\"{'完整 CEVAE':<20} {0.15:<15.3f} {0.45:<15.3f}\")  # 填入你的实际结果\nprint(f\"{'No-Z (TARNet-like)':<20} {0.35:<15.3f} {0.78:<15.3f}\")\nprint(f\"{'No-X-recon':<20} {0.22:<15.3f} {0.58:<15.3f}\")\nprint(\"-\" * 60)\n\nprint(\"\\n关键发现:\")\nprint(\"1. X重构对隐变量学习至关重要（代理变量假设）\")\nprint(\"2. 隐变量Z能显著降低偏差（对比No-Z）\")\nprint(\"3. T的重构帮助模型理解处理分配机制\")\n```\n\n**思考**: \n\nX重构损失最重要！因为：\n1. 它确保隐变量Z能\"解释\"观测到的X\n2. 这是代理变量假设的关键（X包含关于Z的信息）\n3. 没有X重构，后验q(Z|X,T,Y)无法准确推断Z"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 练习 3: 真实数据集 - IHDP\n\nIHDP (Infant Health and Development Program) 是因果推断的经典基准数据集。\n\n**参考答案**:\n\n```python\n# IHDP 数据集加载\n# 注意：需要安装相关库 pip install cdt 或手动下载数据\n\n# 方法1: 使用 causalml 库（推荐）\ntry:\n    from causalml.dataset import make_uplift_classification\n    from sklearn.model_selection import train_test_split\n    \n    # 生成类似IHDP的合成数据\n    df, x_names = make_uplift_classification(\n        n_samples=1000,\n        n_features=25,\n        n_informative=10,\n        n_uplift_increase=5,\n        n_uplift_decrease=2,\n        treatment_name='treatment_group_key',\n        y_name='conversion'\n    )\n    \n    X = df[x_names].values\n    T = df['treatment_group_key'].values\n    Y = df['conversion'].values\n    \n    print(f\"数据加载成功: {X.shape[0]} 样本, {X.shape[1]} 特征\")\n    \nexcept ImportError:\n    print(\"causalml 未安装，使用模拟 IHDP 数据\")\n    # 生成模拟数据\n    n_samples = 1000\n    n_features = 25\n    X = np.random.randn(n_samples, n_features)\n    T = np.random.binomial(1, 0.5, n_samples)\n    Y = np.random.randn(n_samples) * 2 + 5\n\n# 训练 CEVAE\nprint(\"\\n在 IHDP 数据上训练 CEVAE...\")\n\n# 标准化\nscaler_x = StandardScaler()\nscaler_y = StandardScaler()\nX_scaled = scaler_x.fit_transform(X)\nY_scaled = scaler_y.fit_transform(Y.reshape(-1, 1)).flatten()\n\n# 划分数据\nX_train, X_test, T_train, T_test, Y_train, Y_test = train_test_split(\n    X_scaled, T, Y_scaled, test_size=0.2, random_state=42\n)\n\n# 转换为张量\nX_train_t = torch.FloatTensor(X_train)\nT_train_t = torch.FloatTensor(T_train)\nY_train_t = torch.FloatTensor(Y_train)\n\n# 训练模型\nmodel_ihdp = CEVAE(x_dim=X_train.shape[1], latent_dim=15, hidden_dim=150)\noptimizer = optim.Adam(model_ihdp.parameters(), lr=1e-3)\n\nprint(\"训练中...\")\nfor epoch in range(100):\n    # 简化训练（实际应使用完整训练循环）\n    optimizer.zero_grad()\n    outputs = model_ihdp(X_train_t, T_train_t, Y_train_t)\n    losses = cevae_loss(outputs, X_train_t, T_train_t, Y_train_t)\n    losses['total'].backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}/100, Loss: {losses['total'].item():.4f}\")\n\nprint(\"\\n训练完成！\")\nprint(\"IHDP基准测试：\")\nprint(f\"  PEHE: ~0.8-1.2 (论文报告)\")\nprint(f\"  ATE误差: ~0.3-0.5\")\n```\n\n**IHDP 数据集特点**:\n- 747个样本\n- 25个协变量（连续+二元）\n- 二元处理（早期干预 vs 对照）\n- 连续结果（儿童智力测试得分）\n\n**论文基准**:\n- CEVAE原文在IHDP上的PEHE约为0.8-1.0\n- 对比方法: TARNet(1.2), OLS(1.5), PSM(1.8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 思考题参考答案\n\n### 1. CEVAE 的识别假设是什么？\n\n**参考答案**:\n\nCEVAE依赖于**代理变量假设**（Proxy Variable Assumption）：\n\n**核心假设**:\n- 存在隐变量Z同时影响X、T、Y\n- 观测到的X包含关于Z的\"足够信息\"\n- 条件独立性: $Y(t) \\perp Z | X$\n\n**数学表达**:\n\n生成过程:\n```\nZ ~ p(Z)           # 隐变量先验\nX | Z ~ p(X|Z)      # X是Z的代理\nT | X,Z ~ p(T|X,Z)  # 处理分配\nY | T,X,Z ~ p(Y|T,X,Z)  # 结果生成\n```\n\n**为什么需要这个假设**:\n\n如果X完全不包含Z的信息，则:\n- 后验q(Z|X,T,Y)无法准确推断Z\n- CEVAE退化为随机猜测\n- 因果效应估计失效\n\n**如何验证**:\n1. 领域知识: 确保X包含相关代理指标\n2. X的重构质量: 高质量重构说明Z→X通路有效\n3. 敏感性分析: 测试不同隐变量维度的影响\n\n---\n\n### 2. CEVAE 能处理工具变量设定吗？\n\n**参考答案**:\n\n标准CEVAE **不适合**工具变量（IV）设定，原因:\n\n**IV假设 vs CEVAE假设对比**:\n\n| 假设类型 | 工具变量 (IV) | CEVAE |\n|---------|--------------|-------|\n| 核心假设 | Z ⊥ Y \\| T (外生性) | Z → Y (Z影响Y) |\n| 因果图 | Z → T → Y | Z → {X, T, Y} |\n| 识别策略 | 利用Z的外生变异 | 利用Z的代理信息 |\n\n**具体原因**:\n\n1. **因果结构不同**:\n   - IV: Z只通过T影响Y\n   - CEVAE: Z直接影响Y\n\n2. **生成模型冲突**:\n   - CEVAE建模 p(Y|T,X,Z)，假设Z对Y有直接效应\n   - IV要求 Y ⊥ Z | T\n\n**如果要用深度学习处理IV**:\n\n可以使用专门的模型，如:\n- **DeepIV** (Hartford et al., 2017)\n- **DFIV** (Xu et al., 2021)\n\n这些模型的架构:\n```\nZ → [Stage 1: 预测 T|Z,X] → T̂ → [Stage 2: 预测 Y|T̂,X] → Ŷ\n```\n\n---\n\n### 3. 如何在 CEVAE 中加入先验知识？\n\n**参考答案**:\n\n有多种方式将领域知识注入CEVAE:\n\n#### 方法 1: 结构化先验分布\n\n**默认**: $p(Z) = \\mathcal{N}(0, I)$\n\n**改进**: 使用领域知识设计先验\n\n例子 - 医疗场景:\n```python\n# 假设隐变量是健康状态，分为三类\nclass StructuredPrior(nn.Module):\n    def __init__(self, n_components=3, latent_dim=20):\n        self.means = nn.Parameter(torch.randn(n_components, latent_dim))\n        self.log_vars = nn.Parameter(torch.zeros(n_components, latent_dim))\n        \n    def sample(self, n_samples):\n        # Gaussian Mixture\n        component = torch.randint(0, self.n_components, (n_samples,))\n        mu = self.means[component]\n        sigma = torch.exp(0.5 * self.log_vars[component])\n        return mu + sigma * torch.randn_like(mu)\n```\n\n#### 方法 2: 约束解码器\n\n**单调性约束** - 如果已知\"剂量越高效果越强\":\n\n```python\nclass MonotonicDecoder(nn.Module):\n    def forward(self, x, t, z):\n        # 使用正权重确保单调性\n        h = F.linear(z, weight=F.softplus(self.weight1))\n        # ...\n```\n\n**稀疏性约束** - 如果已知只有少数特征重要:\n\n```python\nloss += lambda_sparse * torch.norm(encoder.weight, p=1)\n```\n\n#### 方法 3: 辅助损失\n\n**一致性损失** - 如果已知某些样本应该有相似效应:\n\n```python\n# 年龄相近的患者效应应该接近\nage_similar_pairs = find_similar_pairs(X, feature='age')\nconsistency_loss = torch.mean((ite[pair[:,0]] - ite[pair[:,1]])**2)\n\ntotal_loss = elbo_loss + beta * consistency_loss\n```\n\n**因果图约束** - 如果已知部分因果关系:\n\n```python\n# 已知: X1 → T, X2 ⊥ T\n# 在倾向得分头中只使用X1\npropensity = self.propensity_head(x[:, X1_indices])\n```\n\n#### 方法 4: 半监督学习\n\n如果有少量RCT数据（真实的T是随机分配的）:\n\n```python\n# RCT数据: e(X) = 0.5 (固定)\nrct_propensity_loss = F.mse_loss(\n    propensity_head(X_rct),\n    torch.full_like(X_rct[:, 0], 0.5)\n)\n```\n\n**实际建议**:\n\n1. 从简单先验开始（标准高斯）\n2. 如果领域知识强，逐步加入约束\n3. 使用验证集检验约束是否有帮助\n4. 过度约束可能伤害泛化能力\n\n---\n\n### 4. CEVAE 的不确定性量化\n\n**参考答案**:\n\nCEVAE通过**变分推断**自然提供不确定性估计，这是相比TARNet/DragonNet的重要优势。\n\n#### 不确定性来源\n\n1. **隐变量的不确定性**:\n   - 后验分布 q(Z|X,T,Y) 的方差\n   - 反映了我们对Z的推断信心\n\n2. **模型的不确定性**:\n   - 通过多次采样Z得到ITE分布\n   - 标准差反映预测不确定性\n\n#### 实现方法\n\n```python\ndef predict_with_uncertainty(model, x, t, y, n_samples=100):\n    \"\"\"\n    预测ITE及其不确定性\n    \n    Returns:\n        ite_mean: ITE的点估计\n        ite_std: ITE的标准差\n        ite_samples: 所有采样\n    \"\"\"\n    model.eval()\n    ite_samples = []\n    \n    with torch.no_grad():\n        # 获取后验分布参数\n        mu_z, logvar_z = model.encode(x, t, y)\n        \n        for _ in range(n_samples):\n            # 从后验采样\n            z = model.reparameterize(mu_z, logvar_z)\n            \n            # 预测反事实\n            y0 = model.decode_y(x, torch.zeros_like(t), z)\n            y1 = model.decode_y(x, torch.ones_like(t), z)\n            \n            ite = y1 - y0\n            ite_samples.append(ite)\n    \n    ite_samples = torch.stack(ite_samples)  # (n_samples, batch_size, 1)\n    \n    return {\n        'ite_mean': ite_samples.mean(dim=0),\n        'ite_std': ite_samples.std(dim=0),\n        'ite_95ci_lower': torch.quantile(ite_samples, 0.025, dim=0),\n        'ite_95ci_upper': torch.quantile(ite_samples, 0.975, dim=0),\n        'samples': ite_samples\n    }\n```\n\n#### 应用场景\n\n**1. 医疗决策支持**:\n\n```python\n# 只对高信心的预测采取行动\nuncertainty = predict_with_uncertainty(model, x, t, y)\nhigh_confidence = uncertainty['ite_std'] < threshold\n\nif high_confidence and uncertainty['ite_mean'] > 0:\n    print(\"推荐治疗\")\nelse:\n    print(\"需要更多检查\")\n```\n\n**2. 主动学习**:\n\n选择不确定性最大的样本进行额外标注:\n\n```python\n# 选择top-K最不确定的样本\nuncertainty_scores = uncertainty['ite_std']\ntopk_indices = torch.topk(uncertainty_scores, k=100).indices\n\n# 这些样本值得做更多实验/RCT\nsamples_to_annotate = X[topk_indices]\n```\n\n**3. 风险评估**:\n\n```python\n# 计算治疗有害的概率\np_harmful = (uncertainty['samples'] < 0).float().mean(dim=0)\n\nif p_harmful > 0.1:\n    print(f\"警告: {p_harmful:.1%}的概率治疗有害\")\n```\n\n**可视化不确定性**:\n\n```python\nimport matplotlib.pyplot as plt\n\n# 绘制ITE及其置信区间\nsorted_idx = torch.argsort(uncertainty['ite_mean'])\n\nplt.figure(figsize=(12, 4))\nplt.plot(uncertainty['ite_mean'][sorted_idx], label='ITE预测', color='blue')\nplt.fill_between(\n    range(len(sorted_idx)),\n    uncertainty['ite_95ci_lower'][sorted_idx],\n    uncertainty['ite_95ci_upper'][sorted_idx],\n    alpha=0.3, color='blue', label='95% CI'\n)\nplt.xlabel('样本（按ITE排序）')\nplt.ylabel('ITE')\nplt.legend()\nplt.title('ITE预测及不确定性')\nplt.show()\n```\n\n**与贝叶斯神经网络的对比**:\n\n| 方法 | 不确定性来源 | 计算成本 | 校准质量 |\n|------|-------------|---------|---------|\n| CEVAE | 隐变量Z的后验 | 低 (单次前向) | 中等 |\n| Dropout | 权重采样 | 中 (多次前向) | 中等 |\n| Ensemble | 模型采样 | 高 (训练多个) | 高 |\n| MC Dropout | Dropout采样 | 中 | 中等 |\n\n**最佳实践**:\n\n1. 总是报告不确定性，不只是点估计\n2. 在高风险应用中，不确定性量化至关重要\n3. 验证不确定性是否校准（calibration）\n4. 考虑与Ensemble结合获得更好的不确定性\n\n---\n\n*这些思考题的答案展示了CEVAE的理论深度和实践价值！*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "## 本节要点\n",
    "\n",
    "1. **CEVAE 解决了什么问题？**\n",
    "   - 处理**未观测混淆变量**的因果推断\n",
    "   - 利用观测协变量作为隐变量的代理\n",
    "\n",
    "2. **核心技术**\n",
    "   - 变分自编码器 (VAE) 框架\n",
    "   - 明确建模因果图结构\n",
    "   - 重参数化技巧实现端到端训练\n",
    "\n",
    "3. **优势与局限**\n",
    "   - ✅ 处理隐混淆\n",
    "   - ✅ 不确定性量化\n",
    "   - ✅ 灵活的架构\n",
    "   - ❌ 需要代理变量假设\n",
    "   - ❌ 模型复杂，难以调试\n",
    "   - ❌ 计算成本较高\n",
    "\n",
    "## 关键公式回顾\n",
    "\n",
    "**ELBO 损失**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{q_\\phi(Z|X,T,Y)} \\left[ \\log p_\\theta(X, T, Y | Z) \\right] - \\text{KL}(q_\\phi(Z | X, T, Y) \\| p(Z))\n",
    "$$\n",
    "\n",
    "**因果效应**:\n",
    "\n",
    "$$\n",
    "\\tau(x) = \\mathbb{E}_Z \\left[ \\mu_Y(1, x, Z) - \\mu_Y(0, x, Z) \\mid X=x \\right]\n",
    "$$\n",
    "\n",
    "## 下一步学习\n",
    "\n",
    "- **Part 5.4**: 对抗学习方法 (GANITE, CMGP)\n",
    "- **Part 6**: 因果发现与结构学习\n",
    "- **Part 7**: 实际应用案例分析\n",
    "\n",
    "---\n",
    "\n",
    "**恭喜你完成了 CEVAE 的学习！**\n",
    "\n",
    "你已经掌握了：\n",
    "- ✅ VAE 的核心原理\n",
    "- ✅ 如何用深度学习处理隐混淆\n",
    "- ✅ 完整的 PyTorch 实现\n",
    "- ✅ 模型评估与调试技巧\n",
    "\n",
    "继续加油！ 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}