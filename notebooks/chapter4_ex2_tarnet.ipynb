{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Exercise 2: TARNet - æ·±åº¦å› æœæ¨æ–­çš„å¼€å±±ä¹‹ä½œ\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£ TARNet çš„æ¶æ„è®¾è®¡\n",
    "2. å®ç°ç®€åŒ–ç‰ˆ TARNet\n",
    "3. ç†è§£ Factual Loss çš„å«ä¹‰\n",
    "4. è®­ç»ƒå’Œè¯„ä¼° TARNet\n",
    "\n",
    "---\n",
    "\n",
    "## TARNet: Treatment-Agnostic Representation Network\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "TARNet æ˜¯ 2017 å¹´æå‡ºçš„å¼€åˆ›æ€§å·¥ä½œï¼Œé¦–æ¬¡å°†æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ°åº”ç”¨äºå› æœæ•ˆåº”ä¼°è®¡ã€‚\n",
    "\n",
    "**æ ¸å¿ƒè®¾è®¡**ï¼š\n",
    "1. **å…±äº«è¡¨ç¤ºå±‚**ï¼šå­¦ä¹ å¯¹å¤„ç†ç»„å’Œæ§åˆ¶ç»„éƒ½æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤º\n",
    "2. **åŒå¤´è¾“å‡º**ï¼šåˆ†åˆ«é¢„æµ‹ Y(0) å’Œ Y(1)\n",
    "\n",
    "$$X \\xrightarrow{\\Phi} \\text{Representation} \\xrightarrow{\\begin{cases} h_0 \\to \\hat{Y}(0) \\\\ h_1 \\to \\hat{Y}(1) \\end{cases}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæ´»åŒ–ç±»æ¯”ï¼šåŒè¯­ç¿»è¯‘å®˜\n",
    "\n",
    "æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘å®˜ï¼Œéœ€è¦ç¿»è¯‘ä¸­æ–‡æ–‡ç« ç»™ä¸¤ç±»è¯»è€…ï¼š\n",
    "- **è‹±è¯­è¯»è€…** (å¤„ç†ç»„)\n",
    "- **æ³•è¯­è¯»è€…** (æ§åˆ¶ç»„)\n",
    "\n",
    "**ä¼ ç»Ÿæ–¹æ³• (T-Learner)**ï¼š\n",
    "- é›‡ä½£ä¸¤ä¸ªç‹¬ç«‹çš„ç¿»è¯‘å®˜\n",
    "- æ¯ä¸ªç¿»è¯‘å®˜åªæ‡‚ä¸€ç§è¯­è¨€\n",
    "- æ•ˆç‡ä½ï¼Œä¸”æ— æ³•åˆ©ç”¨å…±åŒçŸ¥è¯†\n",
    "\n",
    "**TARNet æ–¹æ³•**ï¼š\n",
    "- ä¸€ä¸ªç¿»è¯‘å®˜å…ˆç†è§£æ–‡ç« æ ¸å¿ƒå«ä¹‰ (å…±äº«è¡¨ç¤º)\n",
    "- ç„¶ååˆ†åˆ«ç¿»è¯‘æˆè‹±è¯­å’Œæ³•è¯­ (åŒå¤´è¾“å‡º)\n",
    "- æ•ˆç‡é«˜ï¼Œä¸”èƒ½åˆ©ç”¨è¯­è¨€é—´çš„å…±åŒç»“æ„\n",
    "\n",
    "### TARNet æ¶æ„å›¾\n",
    "\n",
    "```\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         å…±äº«è¡¨ç¤ºå±‚ (Shared Repr)          â”‚\n",
    "        â”‚  X â†’ [Hidden] â†’ [Hidden] â†’ Î¦(X)         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â–¼                           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    Head 0     â”‚           â”‚    Head 1     â”‚\n",
    "    â”‚ Î¦(X) â†’ Å¶(0)  â”‚           â”‚ Î¦(X) â†’ Å¶(1)  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚                           â”‚\n",
    "            â–¼                           â–¼\n",
    "      æ§åˆ¶ç»„é¢„æµ‹                    å¤„ç†ç»„é¢„æµ‹\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: TARNet æ¶æ„\n",
    "\n",
    "### ç½‘ç»œç»„ä»¶\n",
    "\n",
    "1. **Representation Network (å…±äº«)**\n",
    "   - Input: X (åŸå§‹ç‰¹å¾)\n",
    "   - Output: Î¦(X) (è¡¨ç¤º)\n",
    "   - ç»“æ„: å¤šå±‚å…¨è¿æ¥ + ReLU\n",
    "\n",
    "2. **Head 0 (æ§åˆ¶ç»„)**\n",
    "   - Input: Î¦(X)\n",
    "   - Output: Å¶(0)\n",
    "   - ç”¨äºé¢„æµ‹ä¸æ¥å—å¤„ç†æ—¶çš„ç»“æœ\n",
    "\n",
    "3. **Head 1 (å¤„ç†ç»„)**\n",
    "   - Input: Î¦(X)\n",
    "   - Output: Å¶(1)\n",
    "   - ç”¨äºé¢„æµ‹æ¥å—å¤„ç†æ—¶çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  2.1: ç†è§£ TARNet æ¶æ„\n",
    "\n",
    "class SimpleTARNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–ç‰ˆ TARNet\n",
    "    \n",
    "    æ¶æ„:\n",
    "    X -> [Shared Representation] -> Phi(X)\n",
    "                                      |\n",
    "                    +----------------+----------------+\n",
    "                    |                                 |\n",
    "                [Head 0]                         [Head 1]\n",
    "                    |                                 |\n",
    "                  Y(0)                              Y(1)\n",
    "    \n",
    "    TODO: å®Œæˆç½‘ç»œå®šä¹‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 50,\n",
    "        repr_dim: int = 25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: å®šä¹‰å…±äº«è¡¨ç¤ºå±‚\n",
    "        # Input -> Hidden (ReLU) -> Representation (ReLU)\n",
    "        self.representation = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç :\n",
    "            # nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, repr_dim),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # TODO: å®šä¹‰æ§åˆ¶ç»„è¾“å‡ºå¤´ (Y0)\n",
    "        # Representation -> Hidden (ReLU) -> Output (1ç»´)\n",
    "        self.head0 = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç :\n",
    "            # nn.Linear(repr_dim, hidden_dim // 2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "        \n",
    "        # TODO: å®šä¹‰å¤„ç†ç»„è¾“å‡ºå¤´ (Y1)\n",
    "        # Representation -> Hidden (ReLU) -> Output (1ç»´)\n",
    "        self.head1 = nn.Sequential(\n",
    "            # ä½ çš„ä»£ç :\n",
    "            # nn.Linear(repr_dim, hidden_dim // 2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        TODO: å®Œæˆå‰å‘ä¼ æ’­é€»è¾‘\n",
    "        \n",
    "        Returns:\n",
    "            (y0_pred, y1_pred, representation)\n",
    "        \"\"\"\n",
    "        # TODO: è®¡ç®—å…±äº«è¡¨ç¤º\n",
    "        phi = None  # ä½ çš„ä»£ç : self.representation(x)\n",
    "        \n",
    "        # TODO: é€šè¿‡ä¸¤ä¸ªå¤´è®¡ç®— Y(0) å’Œ Y(1)\n",
    "        y0 = None  # ä½ çš„ä»£ç : self.head0(phi)\n",
    "        y1 = None  # ä½ çš„ä»£ç : self.head1(phi)\n",
    "        \n",
    "        return y0, y1, phi\n",
    "    \n",
    "    def predict_ite(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        é¢„æµ‹ä¸ªä½“å¤„ç†æ•ˆåº” ITE = Y(1) - Y(0)\n",
    "        \n",
    "        TODO: å®Œæˆ ITE é¢„æµ‹\n",
    "        \"\"\"\n",
    "        y0, y1, _ = self.forward(x)\n",
    "        # ä½ çš„ä»£ç : return y1 - y0\n",
    "        pass\n",
    "\n",
    "# æµ‹è¯•æ¶æ„\n",
    "model = SimpleTARNet(input_dim=5)\n",
    "X_sample = torch.randn(10, 5)\n",
    "try:\n",
    "    y0, y1, phi = model(X_sample)\n",
    "    if y0 is not None:\n",
    "        print(f\"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: Y0={y0.shape}, Y1={y1.shape}, Phi={phi.shape}\")\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ SimpleTARNet.forward å‡½æ•°\")\n",
    "except Exception as e:\n",
    "    print(f\"[æœªå®Œæˆ] è¯·å®Œæˆ SimpleTARNet å®šä¹‰: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Factual Loss\n",
    "\n",
    "### æ ¸å¿ƒæŒ‘æˆ˜ï¼šåäº‹å®æ— æ³•è§‚æµ‹\n",
    "\n",
    "å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬åªèƒ½è§‚æµ‹åˆ°ä¸€ä¸ªç»“æœï¼š\n",
    "- å¦‚æœ T=1ï¼Œæˆ‘ä»¬åªè§‚æµ‹åˆ° Y(1)\n",
    "- å¦‚æœ T=0ï¼Œæˆ‘ä»¬åªè§‚æµ‹åˆ° Y(0)\n",
    "\n",
    "æ‰€ä»¥æˆ‘ä»¬åªèƒ½åœ¨ **è§‚æµ‹åˆ°çš„ç»“æœ** ä¸Šè®¡ç®—æŸå¤±ï¼\n",
    "\n",
    "### Factual Loss å…¬å¼\n",
    "\n",
    "$$L_{factual} = \\frac{1}{n}\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i^{factual})^2$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "$$\\hat{Y}_i^{factual} = \\begin{cases} \\hat{Y}_i(1) & \\text{if } T_i = 1 \\\\ \\hat{Y}_i(0) & \\text{if } T_i = 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  2.2: Factual Loss\n",
    "\n",
    "def compute_factual_loss(\n",
    "    y_true: torch.Tensor,\n",
    "    t_true: torch.Tensor,\n",
    "    y0_pred: torch.Tensor,\n",
    "    y1_pred: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    è®¡ç®— Factual Loss\n",
    "    \n",
    "    å…³é”®æ€æƒ³: åªåœ¨è§‚æµ‹åˆ°çš„ç»“æœä¸Šè®¡ç®—æŸå¤±\n",
    "    - å¦‚æœ T=1, æŸå¤± = (Y - Y1_pred)^2\n",
    "    - å¦‚æœ T=0, æŸå¤± = (Y - Y0_pred)^2\n",
    "    \n",
    "    TODO: å®ç° Factual Loss\n",
    "    \n",
    "    ä¼ªä»£ç :\n",
    "        y_pred = where(T == 1, Y1_pred, Y0_pred)\n",
    "        loss = MSE(y_true, y_pred)\n",
    "    \n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    # TODO: æ ¹æ®å¤„ç†çŠ¶æ€é€‰æ‹©é¢„æµ‹å€¼\n",
    "    # æç¤º: ä½¿ç”¨ torch.where(condition, if_true, if_false)\n",
    "    # t_true éœ€è¦ reshape æˆå’Œ y0_pred ç›¸åŒçš„å½¢çŠ¶\n",
    "    \n",
    "    t_expanded = t_true.unsqueeze(1) if len(t_true.shape) == 1 else t_true\n",
    "    y_pred = None  # ä½ çš„ä»£ç : torch.where(t_expanded == 1, y1_pred, y0_pred)\n",
    "    \n",
    "    if y_pred is None:\n",
    "        return None\n",
    "    \n",
    "    # TODO: è®¡ç®— MSE\n",
    "    y_true_expanded = y_true.unsqueeze(1) if len(y_true.shape) == 1 else y_true\n",
    "    loss = None  # ä½ çš„ä»£ç : torch.mean((y_true_expanded - y_pred)**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# æµ‹è¯•\n",
    "y_true = torch.FloatTensor([1.0, 2.0, 3.0])\n",
    "t_true = torch.FloatTensor([1.0, 0.0, 1.0])\n",
    "y0_pred = torch.FloatTensor([[1.5], [2.0], [2.5]])\n",
    "y1_pred = torch.FloatTensor([[1.0], [2.5], [3.0]])\n",
    "\n",
    "loss = compute_factual_loss(y_true, t_true, y0_pred, y1_pred)\n",
    "if loss is not None:\n",
    "    print(f\"Factual Loss: {loss.item():.4f}\")\n",
    "    print(\"\\nè§£é‡Š:\")\n",
    "    print(\"æ ·æœ¬1: T=1, Y=1.0, Y1_pred=1.0, è¯¯å·®=0\")\n",
    "    print(\"æ ·æœ¬2: T=0, Y=2.0, Y0_pred=2.0, è¯¯å·®=0\")\n",
    "    print(\"æ ·æœ¬3: T=1, Y=3.0, Y1_pred=3.0, è¯¯å·®=0\")\n",
    "else:\n",
    "    print(\"[æœªå®Œæˆ] è¯·å®Œæˆ compute_factual_loss å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: æ•°æ®ç”Ÿæˆ\n",
    "\n",
    "è®©æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªåŒ…å«çœŸå®æ½œåœ¨ç»“æœçš„æ•°æ®é›†ï¼Œä»¥ä¾¿è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  2.5: æ•°æ®ç”Ÿæˆ\n",
    "\n",
    "def generate_simple_data(\n",
    "    n: int = 1000,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆç®€å•çš„åŠåˆæˆæ•°æ®\n",
    "    \n",
    "    DGP:\n",
    "    - X ~ N(0, I) (5ç»´)\n",
    "    - T ~ Bernoulli(sigmoid(0.5*X1 + 0.3*X2))\n",
    "    - Y(0) = 1 + 0.5*X1 + 0.3*X2 + noise\n",
    "    - Y(1) = Y(0) + (2 + 0.5*X1)  # å¼‚è´¨æ€§æ•ˆåº”\n",
    "    \n",
    "    TODO: å®Œæˆæ•°æ®ç”Ÿæˆ\n",
    "    \n",
    "    Returns:\n",
    "        (X, T, Y, Y0, Y1)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # TODO: ç”Ÿæˆç‰¹å¾ (5 ç»´)\n",
    "    X = None  # ä½ çš„ä»£ç : np.random.randn(n, 5)\n",
    "    \n",
    "    if X is None:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # TODO: ç”Ÿæˆå¤„ç†\n",
    "    # propensity = sigmoid(0.5*X[:, 0] + 0.3*X[:, 1])\n",
    "    propensity = None  # ä½ çš„ä»£ç \n",
    "    T = None  # ä½ çš„ä»£ç : np.random.binomial(1, propensity, n)\n",
    "    \n",
    "    # TODO: ç”Ÿæˆæ½œåœ¨ç»“æœ\n",
    "    noise = np.random.randn(n) * 0.5\n",
    "    # Y(0) = 1 + 0.5*X1 + 0.3*X2 + noise\n",
    "    Y0 = None  # ä½ çš„ä»£ç \n",
    "    # Y(1) = Y(0) + (2 + 0.5*X1)  å¼‚è´¨æ€§æ•ˆåº”: åŸºç¡€æ•ˆåº” 2, åŠ ä¸Šä¸ X1 ç›¸å…³çš„éƒ¨åˆ†\n",
    "    Y1 = None  # ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: è§‚æµ‹ç»“æœ\n",
    "    Y = None  # ä½ çš„ä»£ç : np.where(T == 1, Y1, Y0)\n",
    "    \n",
    "    return X, T, Y, Y0, Y1\n",
    "\n",
    "# æµ‹è¯•\n",
    "X, T, Y, Y0, Y1 = generate_simple_data()\n",
    "if X is not None and X[0, 0] is not None:\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}\")\n",
    "    print(f\"å¤„ç†æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "    print(f\"çœŸå® ATE: {np.mean(Y1 - Y0):.4f}\")\n",
    "    print(f\"ITE èŒƒå›´: [{(Y1-Y0).min():.2f}, {(Y1-Y0).max():.2f}]\")\n",
    "else:\n",
    "    print(\"[æœªå®Œæˆ] è¯·å®Œæˆ generate_simple_data å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: è®­ç»ƒ TARNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  2.3: è®­ç»ƒ TARNet\n",
    "\n",
    "def train_tarnet(\n",
    "    X: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    n_epochs: int = 100,\n",
    "    batch_size: int = 64,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> Tuple[SimpleTARNet, dict]:\n",
    "    \"\"\"\n",
    "    è®­ç»ƒ TARNet\n",
    "    \n",
    "    TODO: å®Œæˆè®­ç»ƒè¿‡ç¨‹\n",
    "    \n",
    "    Returns:\n",
    "        (trained_model, training_history)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: è½¬æ¢ä¸º PyTorch Tensor\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    T_tensor = torch.FloatTensor(T)\n",
    "    Y_tensor = torch.FloatTensor(Y)\n",
    "    \n",
    "    # TODO: åˆ›å»º DataLoader\n",
    "    dataset = TensorDataset(X_tensor, T_tensor, Y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # TODO: åˆå§‹åŒ–æ¨¡å‹\n",
    "    model = SimpleTARNet(input_dim=X.shape[1])\n",
    "    \n",
    "    # TODO: å®šä¹‰ä¼˜åŒ–å™¨\n",
    "    optimizer = None  # ä½ çš„ä»£ç : optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if optimizer is None:\n",
    "        return model, {'loss': []}\n",
    "    \n",
    "    # è®­ç»ƒå†å²\n",
    "    history = {'loss': []}\n",
    "    \n",
    "    # TODO: è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch_x, batch_t, batch_y in dataloader:\n",
    "            # ä½ çš„ä»£ç :\n",
    "            # 1. æ¸…é›¶æ¢¯åº¦\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            # 2. å‰å‘ä¼ æ’­\n",
    "            # y0_pred, y1_pred, _ = model(batch_x)\n",
    "            \n",
    "            # 3. è®¡ç®— Factual Loss\n",
    "            # loss = compute_factual_loss(batch_y, batch_t, y0_pred, y1_pred)\n",
    "            \n",
    "            # 4. åå‘ä¼ æ’­\n",
    "            # loss.backward()\n",
    "            \n",
    "            # 5. æ›´æ–°å‚æ•°\n",
    "            # optimizer.step()\n",
    "            \n",
    "            # epoch_loss += loss.item()\n",
    "            # n_batches += 1\n",
    "            pass\n",
    "        \n",
    "        # è®°å½•æŸå¤±\n",
    "        history['loss'].append(epoch_loss / n_batches if n_batches > 0 else 0)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{n_epochs}, Loss: {history['loss'][-1]:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# è®­ç»ƒ\n",
    "if X is not None:\n",
    "    print(\"å¼€å§‹è®­ç»ƒ TARNet...\")\n",
    "    model, history = train_tarnet(X, T, Y, n_epochs=100, batch_size=64)\n",
    "    \n",
    "    if len(history['loss']) > 0 and history['loss'][-1] > 0:\n",
    "        print(f\"\\nè®­ç»ƒå®Œæˆ! æœ€ç»ˆæŸå¤±: {history['loss'][-1]:.4f}\")\n",
    "        \n",
    "        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history['loss'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Factual Loss')\n",
    "        plt.title('TARNet Training Curve')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ train_tarnet å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: è¯„ä¼° TARNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  2.4: è¯„ä¼° TARNet\n",
    "\n",
    "def evaluate_tarnet(\n",
    "    model: SimpleTARNet,\n",
    "    X: np.ndarray,\n",
    "    Y0_true: np.ndarray,\n",
    "    Y1_true: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    è¯„ä¼° TARNet æ€§èƒ½\n",
    "    \n",
    "    æŒ‡æ ‡:\n",
    "    1. PEHE (ITE è¯¯å·®): sqrt(E[(ITE_true - ITE_pred)^2])\n",
    "    2. ATE è¯¯å·®: |ATE_true - ATE_pred|\n",
    "    \n",
    "    TODO: å®Œæˆè¯„ä¼°é€»è¾‘\n",
    "    \n",
    "    Returns:\n",
    "        dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        \n",
    "        # TODO: é¢„æµ‹ Y(0) å’Œ Y(1)\n",
    "        y0_pred, y1_pred, _ = model(X_tensor)\n",
    "        \n",
    "        if y0_pred is None:\n",
    "            return {'pehe': None, 'ate_true': None, 'ate_pred': None, 'ate_error': None}\n",
    "        \n",
    "        y0_pred = y0_pred.squeeze().numpy()\n",
    "        y1_pred = y1_pred.squeeze().numpy()\n",
    "    \n",
    "    # TODO: è®¡ç®— PEHE\n",
    "    # PEHE = sqrt(mean((ITE_true - ITE_pred)^2))\n",
    "    ite_true = Y1_true - Y0_true\n",
    "    ite_pred = y1_pred - y0_pred\n",
    "    pehe = None  # ä½ çš„ä»£ç : np.sqrt(np.mean((ite_true - ite_pred)**2))\n",
    "    \n",
    "    # TODO: è®¡ç®— ATE è¯¯å·®\n",
    "    ate_true = np.mean(Y1_true - Y0_true)\n",
    "    ate_pred = np.mean(y1_pred - y0_pred)\n",
    "    ate_error = None  # ä½ çš„ä»£ç : np.abs(ate_true - ate_pred)\n",
    "    \n",
    "    return {\n",
    "        'pehe': pehe,\n",
    "        'ate_true': ate_true,\n",
    "        'ate_pred': ate_pred,\n",
    "        'ate_error': ate_error\n",
    "    }\n",
    "\n",
    "# è¯„ä¼°\n",
    "if X is not None and model is not None:\n",
    "    metrics = evaluate_tarnet(model, X, Y0, Y1)\n",
    "    if metrics['pehe'] is not None:\n",
    "        print(\"=\" * 40)\n",
    "        print(\"TARNet è¯„ä¼°ç»“æœ\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"PEHE: {metrics['pehe']:.4f}\")\n",
    "        print(f\"çœŸå® ATE: {metrics['ate_true']:.4f}\")\n",
    "        print(f\"é¢„æµ‹ ATE: {metrics['ate_pred']:.4f}\")\n",
    "        print(f\"ATE è¯¯å·®: {metrics['ate_error']:.4f}\")\n",
    "    else:\n",
    "        print(\"[æœªå®Œæˆ] è¯·å®Œæˆ evaluate_tarnet å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: å¯è§†åŒ–ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– ITE é¢„æµ‹\n",
    "\n",
    "if X is not None and model is not None:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y0_pred, y1_pred, _ = model(torch.FloatTensor(X))\n",
    "        if y0_pred is not None:\n",
    "            ite_pred = (y1_pred - y0_pred).squeeze().numpy()\n",
    "            ite_true = Y1 - Y0\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            \n",
    "            # å›¾1: ITE çœŸå® vs é¢„æµ‹\n",
    "            axes[0].scatter(ite_true, ite_pred, alpha=0.5, s=10)\n",
    "            axes[0].plot([ite_true.min(), ite_true.max()], \n",
    "                        [ite_true.min(), ite_true.max()], 'r--', lw=2)\n",
    "            axes[0].set_xlabel('True ITE')\n",
    "            axes[0].set_ylabel('Predicted ITE')\n",
    "            axes[0].set_title('ITE: True vs Predicted')\n",
    "            \n",
    "            # å›¾2: ITE åˆ†å¸ƒ\n",
    "            axes[1].hist(ite_true, bins=30, alpha=0.5, label='True', density=True)\n",
    "            axes[1].hist(ite_pred, bins=30, alpha=0.5, label='Predicted', density=True)\n",
    "            axes[1].set_xlabel('ITE')\n",
    "            axes[1].set_ylabel('Density')\n",
    "            axes[1].set_title('ITE Distribution')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            # å›¾3: ITE ä¸ç‰¹å¾çš„å…³ç³»\n",
    "            axes[2].scatter(X[:, 0], ite_true, alpha=0.5, s=10, label='True')\n",
    "            axes[2].scatter(X[:, 0], ite_pred, alpha=0.5, s=10, label='Predicted')\n",
    "            axes[2].set_xlabel('X1 (ç¬¬ä¸€ä¸ªç‰¹å¾)')\n",
    "            axes[2].set_ylabel('ITE')\n",
    "            axes[2].set_title('ITE vs X1')\n",
    "            axes[2].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒé¢˜\n",
    "\n",
    "### 1. ä¸ºä»€ä¹ˆ TARNet éœ€è¦ä¸¤ä¸ªç‹¬ç«‹çš„è¾“å‡ºå¤´?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 2. Factual Loss å’Œæ™®é€šçš„ç›‘ç£å­¦ä¹ æŸå¤±æœ‰ä»€ä¹ˆåŒºåˆ«?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 3. å¦‚æœæˆ‘ä»¬åªè®­ç»ƒä¸€ä¸ªå¤´ (æ¯”å¦‚åªè®­ç»ƒ Head 1)ï¼Œä¼šæœ‰ä»€ä¹ˆé—®é¢˜?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 4. å…±äº«è¡¨ç¤ºå±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆ? ä¸ºä»€ä¹ˆä¸ç»™ä¸¤ä¸ªå¤´åˆ†åˆ«çš„ç‰¹å¾æå–å™¨?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 5. TARNet å¦‚ä½•é¢„æµ‹åäº‹å®ç»“æœ (æ¯”å¦‚å¯¹äº T=1 çš„äººé¢„æµ‹ Y(0))?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "\n",
    "### 6. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ TARNet ä¼šæ¯”ä¼ ç»Ÿçš„ Meta-Learner (å¦‚ S/T-Learner) æ›´å¥½?\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "| æ¦‚å¿µ | è¦ç‚¹ |\n",
    "|------|------|\n",
    "| **TARNet** | é¦–ä¸ªç³»ç»ŸåŒ–çš„æ·±åº¦å› æœæ¨¡å‹ |\n",
    "| **å…±äº«è¡¨ç¤º** | è®©ä¸¤ä¸ªå¤´å…±äº«åº•å±‚ç‰¹å¾æå– |\n",
    "| **åŒå¤´è¾“å‡º** | åˆ†åˆ«é¢„æµ‹ Y(0) å’Œ Y(1) |\n",
    "| **Factual Loss** | åªåœ¨è§‚æµ‹ç»“æœä¸Šè®¡ç®—æŸå¤± |\n",
    "| **PEHE** | è¯„ä¼° ITE é¢„æµ‹ç²¾åº¦çš„é»„é‡‘æŒ‡æ ‡ |\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "åœ¨ä¸‹ä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  **DragonNet** â€”â€” TARNet çš„å¢å¼ºç‰ˆï¼ŒåŠ å…¥äº†å€¾å‘å¾—åˆ†é¢„æµ‹ï¼\n",
    "\n",
    "---\n",
    "\n",
    "*æ­å–œä½ å®Œæˆäº† TARNet çš„å­¦ä¹ ï¼* ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
