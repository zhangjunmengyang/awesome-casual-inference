{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŒé‡å·®åˆ† (Difference-in-Differences) - æ”¿ç­–è¯„ä¼°çš„åˆ©å™¨\n",
    "\n",
    "---\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. ç†è§£ DID çš„æ ¸å¿ƒæ€æƒ³ï¼šä¸ºä»€ä¹ˆéœ€è¦ã€Œå·®åˆ†çš„å·®åˆ†ã€\n",
    "2. æŒæ¡å¹³è¡Œè¶‹åŠ¿å‡è®¾çš„æ£€éªŒæ–¹æ³•\n",
    "3. å­¦ä¼šä½¿ç”¨ Event Study è®¾è®¡åˆ†æåŠ¨æ€æ•ˆåº”\n",
    "4. ç†è§£äº¤é”™ DID (Staggered DID) çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ\n",
    "5. åº”ç”¨ DID è§£å†³çœŸå®ä¸šåŠ¡é—®é¢˜\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸šåŠ¡åœºæ™¯å¼•å…¥\n",
    "\n",
    "### åœºæ™¯ 1: ç”µå•†å¹³å°çš„ä¼šå‘˜æ”¿ç­–\n",
    "\n",
    "ä½ æ˜¯ä¸€å®¶ç”µå•†å¹³å°çš„æ•°æ®ç§‘å­¦å®¶ã€‚2024å¹´1æœˆ1æ—¥ï¼Œå…¬å¸åœ¨**åŒ—äº¬åœ°åŒº**æ¨å‡ºäº†æ–°çš„ä¼šå‘˜æ”¿ç­–ï¼ˆå…è´¹åŒ…é‚®ï¼‰ï¼Œè€Œ**ä¸Šæµ·åœ°åŒº**æš‚æœªæ¨å‡ºã€‚\n",
    "\n",
    "ä½ éœ€è¦å›ç­”ï¼š**è¿™ä¸ªä¼šå‘˜æ”¿ç­–æå‡äº†å¤šå°‘ç”¨æˆ·æ¶ˆè´¹é¢ï¼Ÿ**\n",
    "\n",
    "### ç›´è§‰çš„é”™è¯¯åšæ³•\n",
    "\n",
    "âŒ **é”™è¯¯åšæ³• 1**ï¼šæ¯”è¾ƒåŒ—äº¬å®æ–½å‰åçš„å·®å¼‚\n",
    "- é—®é¢˜ï¼šå¯èƒ½å—å­£èŠ‚æ€§ã€èŠ‚å‡æ—¥ç­‰å› ç´ å½±å“\n",
    "\n",
    "âŒ **é”™è¯¯åšæ³• 2**ï¼šæ¯”è¾ƒåŒ—äº¬å’Œä¸Šæµ·çš„å·®å¼‚\n",
    "- é—®é¢˜ï¼šä¸¤ä¸ªåŸå¸‚æœ¬èº«æ¶ˆè´¹æ°´å¹³å°±ä¸åŒ\n",
    "\n",
    "âœ… **æ­£ç¡®åšæ³•**ï¼šä½¿ç”¨åŒé‡å·®åˆ† (DID)\n",
    "- ç¬¬ä¸€æ¬¡å·®åˆ†ï¼šå»é™¤æ—¶é—´è¶‹åŠ¿\n",
    "- ç¬¬äºŒæ¬¡å·®åˆ†ï¼šå»é™¤åœ°åŒºå·®å¼‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotly é…è‰²æ–¹æ¡ˆ\n",
    "COLORS = {\n",
    "    'primary': '#2D9CDB',\n",
    "    'success': '#27AE60',\n",
    "    'danger': '#EB5757',\n",
    "    'warning': '#F2994A',\n",
    "    'info': '#9B51E0',\n",
    "    'treatment': '#27AE60',\n",
    "    'control': '#2D9CDB'\n",
    "}\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 1: DID çš„æ ¸å¿ƒç›´è§‰\n\n### 1.1 ä»€ä¹ˆæ˜¯ã€Œå·®åˆ†çš„å·®åˆ†ã€\n\næƒ³è±¡ä½ åœ¨è§‚å¯Ÿä¸¤ä¸ªåŸå¸‚çš„ç”¨æˆ·æ¶ˆè´¹å˜åŒ–ï¼š\n\n```\n         æ”¿ç­–å‰    æ”¿ç­–å\nåŒ—äº¬     100       150      â†’ å¢é•¿ 50\nä¸Šæµ·      80       120      â†’ å¢é•¿ 40\n```\n\n**ç¬¬ä¸€æ¬¡å·®åˆ†ï¼ˆæ—¶é—´ç»´åº¦ï¼‰**ï¼š\n- åŒ—äº¬ï¼š150 - 100 = 50\n- ä¸Šæµ·ï¼š120 - 80 = 40\n\n**ç¬¬äºŒæ¬¡å·®åˆ†ï¼ˆç»„é—´å·®å¼‚ï¼‰**ï¼š\n- DID = (150 - 100) - (120 - 80) = 50 - 40 = **10**\n\nè¿™ä¸ª 10 å°±æ˜¯æ”¿ç­–çš„å› æœæ•ˆåº”ï¼\n\n### 1.2 DID çš„æ•°å­¦è¡¨è¾¾\n\n$$\n\\begin{aligned}\n\\text{DID} &= [E(Y_{\\text{treat}, \\text{post}}) - E(Y_{\\text{treat}, \\text{pre}})] \\\\\n           &\\quad - [E(Y_{\\text{control}, \\text{post}}) - E(Y_{\\text{control}, \\text{pre}})]\n\\end{aligned}\n$$\n\nå…¶ä¸­ï¼š\n- $Y_{\\text{treat}, \\text{post}}$: å¤„ç†ç»„åœ¨æ”¿ç­–åçš„ç»“æœ\n- $Y_{\\text{treat}, \\text{pre}}$: å¤„ç†ç»„åœ¨æ”¿ç­–å‰çš„ç»“æœ\n- $Y_{\\text{control}, \\text{post}}$: å¯¹ç…§ç»„åœ¨æ”¿ç­–åçš„ç»“æœ\n- $Y_{\\text{control}, \\text{pre}}$: å¯¹ç…§ç»„åœ¨æ”¿ç­–å‰çš„ç»“æœ\n\n---\n\n## ğŸ“ æ ¸å¿ƒæ•°å­¦æ¨å¯¼\n\n### DID ä¼°è®¡é‡çš„å½¢å¼åŒ–å®šä¹‰\n\nè®¾ $Y_{it}(1)$ å’Œ $Y_{it}(0)$ åˆ†åˆ«è¡¨ç¤ºä¸ªä½“ $i$ åœ¨æ—¶é—´ $t$ æ¥å—å¤„ç†å’Œä¸æ¥å—å¤„ç†çš„æ½œåœ¨ç»“æœã€‚\n\n**å¤„ç†æ•ˆåº”**ï¼ˆATT, Average Treatment effect on the Treatedï¼‰ï¼š\n$$\n\\tau_{ATT} = E[Y_{1,post}(1) - Y_{1,post}(0) | T=1]\n$$\n\n**é—®é¢˜**ï¼š$Y_{1,post}(0)$ ä¸å¯è§‚æµ‹ï¼ˆåäº‹å®ï¼‰ã€‚\n\n**å¹³è¡Œè¶‹åŠ¿å‡è®¾**ï¼ˆParallel Trends Assumptionï¼‰ï¼š\n$$\nE[Y_{1,post}(0) - Y_{1,pre}(0) | T=1] = E[Y_{0,post}(0) - Y_{0,pre}(0) | T=0]\n$$\n\n**å«ä¹‰**ï¼šå¦‚æœæ²¡æœ‰æ”¿ç­–å¹²é¢„ï¼Œå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¶‹åŠ¿åº”è¯¥æ˜¯å¹³è¡Œçš„ã€‚\n\n**DID ä¼°è®¡é‡æ¨å¯¼**ï¼š\n\nåŸºäºå¹³è¡Œè¶‹åŠ¿å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å¯¹ç…§ç»„çš„å˜åŒ–æ¥ä¼°è®¡å¤„ç†ç»„çš„åäº‹å®å˜åŒ–ï¼š\n\n$$\n\\begin{aligned}\n\\tau_{DID} &= [E(Y_{1,post} | T=1) - E(Y_{1,pre} | T=1)] \\\\\n           &\\quad - [E(Y_{0,post} | T=0) - E(Y_{0,pre} | T=0)] \\\\\n           &= E[Y_{1,post}(1) - Y_{1,pre}(0) | T=1] \\\\\n           &\\quad - E[Y_{0,post}(0) - Y_{0,pre}(0) | T=0] \\\\\n           &= E[Y_{1,post}(1) - Y_{1,post}(0) | T=1] \\quad \\text{(å¹³è¡Œè¶‹åŠ¿)} \\\\\n           &= \\tau_{ATT}\n\\end{aligned}\n$$\n\n### DID å›å½’æ¨¡å‹\n\nDID å¯ä»¥ç”¨å›å½’æ–¹ç¨‹è¡¨ç¤ºï¼š\n\n$$\nY_{it} = \\beta_0 + \\beta_1 \\text{Treat}_i + \\beta_2 \\text{Post}_t + \\beta_3 (\\text{Treat}_i \\times \\text{Post}_t) + \\epsilon_{it}\n$$\n\n**å‚æ•°è§£é‡Š**ï¼š\n- $\\beta_0$: å¯¹ç…§ç»„åœ¨æ”¿ç­–å‰çš„å¹³å‡å€¼\n- $\\beta_1$: æ”¿ç­–å‰å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„å·®å¼‚ï¼ˆç»„å›ºå®šæ•ˆåº”ï¼‰\n- $\\beta_2$: å¯¹ç…§ç»„åœ¨æ”¿ç­–åçš„å˜åŒ–ï¼ˆæ—¶é—´å›ºå®šæ•ˆåº”ï¼‰\n- $\\beta_3$: **DID ä¼°è®¡é‡**ï¼ˆæˆ‘ä»¬å…³å¿ƒçš„å› æœæ•ˆåº”ï¼‰\n\n**è¯æ˜** $\\beta_3 = \\tau_{DID}$ï¼š\n\nå„ç»„æœŸæœ›å€¼ï¼š\n- $E[Y | T=0, \\text{Post}=0] = \\beta_0$\n- $E[Y | T=1, \\text{Post}=0] = \\beta_0 + \\beta_1$\n- $E[Y | T=0, \\text{Post}=1] = \\beta_0 + \\beta_2$\n- $E[Y | T=1, \\text{Post}=1] = \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$\n\nè®¡ç®— DIDï¼š\n$$\n\\begin{aligned}\n\\tau_{DID} &= [(Î²_0 + Î²_1 + Î²_2 + Î²_3) - (Î²_0 + Î²_1)] - [(Î²_0 + Î²_2) - Î²_0] \\\\\n           &= (Î²_2 + Î²_3) - Î²_2 \\\\\n           &= Î²_3\n\\end{aligned}\n$$\n\n### å¸¦å®½é€‰æ‹©ï¼ˆIK optimal bandwidthï¼‰\n\nå¯¹äºå±€éƒ¨çº¿æ€§å›å½’çš„æœ€ä¼˜å¸¦å®½ï¼ˆç”¨äºè¿ç»­å¤„ç†çš„ DIDï¼‰ï¼š\n\n$$\nh_{opt} = \\left(\\frac{C \\cdot \\sigma^2}{n \\cdot m_2^2}\\right)^{1/5}\n$$\n\nå…¶ä¸­ï¼š\n- $C$: å¸¸æ•°ï¼ˆå–å†³äºæ ¸å‡½æ•°ï¼‰\n- $\\sigma^2$: æ¡ä»¶æ–¹å·®\n- $m_2$: äºŒé˜¶å¯¼æ•°\n- $n$: æ ·æœ¬é‡\n\n---\n\n### 1.3 DID èƒŒåçš„ç›´è§‰\n\nğŸ¯ **æ ¸å¿ƒæ€æƒ³**ï¼š\n- **å¯¹ç…§ç»„**å‘Šè¯‰æˆ‘ä»¬ã€Œæ²¡æœ‰æ”¿ç­–æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€\n- **å¤„ç†ç»„çš„é¢å¤–å¢é•¿**å°±æ˜¯æ”¿ç­–æ•ˆåº”\n\nğŸ“Š **ç”ŸåŠ¨æ¯”å–»**ï¼š\n- ä½ å’Œæœ‹å‹ä¸€èµ·å‡è‚¥\n- æœ‹å‹åƒäº†å‡è‚¥è¯ï¼Œä½ æ²¡åƒ\n- ä¸€ä¸ªæœˆåï¼Œæœ‹å‹å‡äº† 5kgï¼Œä½ å‡äº† 2kg\n- å‡è‚¥è¯çš„æ•ˆæœ = 5kg - 2kg = **3kg**\n- ä¸ºä»€ä¹ˆä¸æ˜¯ 5kgï¼Ÿå› ä¸ºä½ ä»¬éƒ½åœ¨è¿åŠ¨ã€æ§åˆ¶é¥®é£Ÿï¼ˆå…±åŒè¶‹åŠ¿ï¼‰\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– DID çš„ç›´è§‰\n",
    "def visualize_did_intuition():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ– DID çš„æ ¸å¿ƒç›´è§‰\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "    data = pd.DataFrame({\n",
    "        'period': ['æ”¿ç­–å‰', 'æ”¿ç­–å', 'æ”¿ç­–å‰', 'æ”¿ç­–å'],\n",
    "        'group': ['åŒ—äº¬', 'åŒ—äº¬', 'ä¸Šæµ·', 'ä¸Šæµ·'],\n",
    "        'value': [100, 150, 80, 120]\n",
    "    })\n",
    "    \n",
    "    # åˆ›å»ºå›¾è¡¨\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # åŒ—äº¬ï¼ˆå¤„ç†ç»„ï¼‰\n",
    "    beijing = data[data['group'] == 'åŒ—äº¬']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=beijing['period'],\n",
    "        y=beijing['value'],\n",
    "        mode='lines+markers',\n",
    "        name='åŒ—äº¬ï¼ˆå¤„ç†ç»„ï¼‰',\n",
    "        line=dict(color=COLORS['treatment'], width=3),\n",
    "        marker=dict(size=12)\n",
    "    ))\n",
    "    \n",
    "    # ä¸Šæµ·ï¼ˆå¯¹ç…§ç»„ï¼‰\n",
    "    shanghai = data[data['group'] == 'ä¸Šæµ·']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=shanghai['period'],\n",
    "        y=shanghai['value'],\n",
    "        mode='lines+markers',\n",
    "        name='ä¸Šæµ·ï¼ˆå¯¹ç…§ç»„ï¼‰',\n",
    "        line=dict(color=COLORS['control'], width=3),\n",
    "        marker=dict(size=12)\n",
    "    ))\n",
    "    \n",
    "    # åäº‹å®è¶‹åŠ¿ï¼ˆåŒ—äº¬å¦‚æœæ²¡æœ‰æ”¿ç­–ï¼‰\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=['æ”¿ç­–å‰', 'æ”¿ç­–å'],\n",
    "        y=[100, 140],  # 100 + (120-80) = 140\n",
    "        mode='lines',\n",
    "        name='åäº‹å®ï¼ˆåŒ—äº¬æ— æ”¿ç­–ï¼‰',\n",
    "        line=dict(color=COLORS['treatment'], width=2, dash='dash'),\n",
    "        opacity=0.5\n",
    "    ))\n",
    "    \n",
    "    # æ·»åŠ ç®­å¤´æ ‡æ³¨ DID æ•ˆåº”\n",
    "    fig.add_annotation(\n",
    "        x='æ”¿ç­–å',\n",
    "        y=145,\n",
    "        ax='æ”¿ç­–å',\n",
    "        ay=140,\n",
    "        xref='x',\n",
    "        yref='y',\n",
    "        axref='x',\n",
    "        ayref='y',\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        arrowsize=1,\n",
    "        arrowwidth=2,\n",
    "        arrowcolor=COLORS['danger']\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        x='æ”¿ç­–å',\n",
    "        y=145,\n",
    "        text='<b>DID æ•ˆåº” = 10</b>',\n",
    "        showarrow=False,\n",
    "        xshift=60,\n",
    "        font=dict(size=14, color=COLORS['danger'])\n",
    "    )\n",
    "    \n",
    "    # å¸ƒå±€\n",
    "    fig.update_layout(\n",
    "        title='åŒé‡å·®åˆ† (DID) çš„æ ¸å¿ƒç›´è§‰',\n",
    "        xaxis_title='æ—¶é—´',\n",
    "        yaxis_title='å¹³å‡æ¶ˆè´¹é¢ï¼ˆå…ƒï¼‰',\n",
    "        template='plotly_white',\n",
    "        height=500,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = visualize_did_intuition()\n",
    "fig.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š å›¾è¡¨è§£è¯»ï¼š\n",
    "1. ç»¿çº¿ï¼ˆåŒ—äº¬ï¼‰ï¼šæ”¿ç­–åå¢é•¿äº† 50\n",
    "2. è“çº¿ï¼ˆä¸Šæµ·ï¼‰ï¼šåŒæœŸå¢é•¿äº† 40ï¼ˆè‡ªç„¶è¶‹åŠ¿ï¼‰\n",
    "3. è™šçº¿ï¼šå¦‚æœåŒ—äº¬æ²¡æœ‰æ”¿ç­–ï¼Œåº”è¯¥ä¹Ÿå¢é•¿ 40\n",
    "4. çº¢è‰²ç®­å¤´ï¼šé¢å¤–å¢é•¿çš„ 10 å°±æ˜¯æ”¿ç­–æ•ˆåº”\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: åŸºæœ¬ DID è®¾è®¡ï¼ˆä¸¤æœŸä¸¤ç»„ï¼‰\n",
    "\n",
    "### 2.1 å›å½’æ¡†æ¶\n",
    "\n",
    "DID å¯ä»¥ç”¨å›å½’æ–¹ç¨‹è¡¨ç¤ºï¼š\n",
    "\n",
    "$$\n",
    "Y_{it} = \\beta_0 + \\beta_1 \\text{Treat}_i + \\beta_2 \\text{Post}_t + \\beta_3 (\\text{Treat}_i \\times \\text{Post}_t) + \\epsilon_{it}\n",
    "$$\n",
    "\n",
    "**ç¬¦å·è¯´æ˜**ï¼š\n",
    "- $Y_{it}$: ä¸ªä½“ $i$ åœ¨æ—¶é—´ $t$ çš„ç»“æœå˜é‡\n",
    "- $\\text{Treat}_i$: ä¸ªä½“ $i$ æ˜¯å¦åœ¨å¤„ç†ç»„ï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰\n",
    "- $\\text{Post}_t$: æ—¶é—´ $t$ æ˜¯å¦åœ¨æ”¿ç­–åï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰\n",
    "- $\\text{Treat}_i \\times \\text{Post}_t$: äº¤äº’é¡¹\n",
    "- $\\beta_3$: **DID ä¼°è®¡é‡**ï¼ˆæˆ‘ä»¬å…³å¿ƒçš„å› æœæ•ˆåº”ï¼‰\n",
    "\n",
    "### 2.2 å‚æ•°è§£é‡Š\n",
    "\n",
    "| å‚æ•° | å«ä¹‰ | è§£é‡Š |\n",
    "|------|------|------|\n",
    "| $\\beta_0$ | æˆªè· | å¯¹ç…§ç»„åœ¨æ”¿ç­–å‰çš„å¹³å‡å€¼ |\n",
    "| $\\beta_1$ | å¤„ç†ç»„æ•ˆåº” | æ”¿ç­–å‰å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„å·®å¼‚ |\n",
    "| $\\beta_2$ | æ—¶é—´æ•ˆåº” | å¯¹ç…§ç»„åœ¨æ”¿ç­–åçš„å˜åŒ– |\n",
    "| $\\beta_3$ | **DID æ•ˆåº”** | æ”¿ç­–çš„å› æœæ•ˆåº” |\n",
    "\n",
    "### 2.3 æ ‡å‡†è¯¯çš„é€‰æ‹©\n",
    "\n",
    "âš ï¸ **é‡è¦**ï¼šåœ¨ DID è®¾è®¡ä¸­ï¼Œå¿…é¡»ä½¿ç”¨**èšç±»æ ‡å‡†è¯¯** (Clustered Standard Errors)\n",
    "\n",
    "**ä¸ºä»€ä¹ˆï¼Ÿ**\n",
    "- åŒä¸€ä¸ªä½“çš„è§‚æµ‹å€¼åœ¨æ—¶é—´ä¸Šç›¸å…³\n",
    "- æ ‡å‡† OLS æ ‡å‡†è¯¯ä¼šä½ä¼°çœŸå®çš„ä¸ç¡®å®šæ€§\n",
    "- èšç±»åœ¨ä¸ªä½“å±‚é¢ï¼ˆæˆ–åœ°åŒºå±‚é¢ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "def generate_did_data(n_treated=500, n_control=500, treatment_effect=10, seed=42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆ DID æ¨¡æ‹Ÿæ•°æ®\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - n_treated: å¤„ç†ç»„æ ·æœ¬é‡\n",
    "    - n_control: å¯¹ç…§ç»„æ ·æœ¬é‡\n",
    "    - treatment_effect: çœŸå®å¤„ç†æ•ˆåº”\n",
    "    - seed: éšæœºç§å­\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    # å¤„ç†ç»„ï¼ˆåŒ—äº¬ï¼‰\n",
    "    for i in range(n_treated):\n",
    "        user_id = f\"beijing_{i}\"\n",
    "        baseline = np.random.normal(100, 15)  # åŸºçº¿æ¶ˆè´¹\n",
    "        \n",
    "        # æ”¿ç­–å‰\n",
    "        data_list.append({\n",
    "            'user_id': user_id,\n",
    "            'city': 'beijing',\n",
    "            'period': 'pre',\n",
    "            'treat': 1,\n",
    "            'post': 0,\n",
    "            'spending': baseline + np.random.normal(0, 5)\n",
    "        })\n",
    "        \n",
    "        # æ”¿ç­–å\n",
    "        time_trend = 40  # å…±åŒæ—¶é—´è¶‹åŠ¿\n",
    "        data_list.append({\n",
    "            'user_id': user_id,\n",
    "            'city': 'beijing',\n",
    "            'period': 'post',\n",
    "            'treat': 1,\n",
    "            'post': 1,\n",
    "            'spending': baseline + time_trend + treatment_effect + np.random.normal(0, 5)\n",
    "        })\n",
    "    \n",
    "    # å¯¹ç…§ç»„ï¼ˆä¸Šæµ·ï¼‰\n",
    "    for i in range(n_control):\n",
    "        user_id = f\"shanghai_{i}\"\n",
    "        baseline = np.random.normal(80, 15)  # ä¸Šæµ·åŸºçº¿æ¶ˆè´¹è¾ƒä½\n",
    "        \n",
    "        # æ”¿ç­–å‰\n",
    "        data_list.append({\n",
    "            'user_id': user_id,\n",
    "            'city': 'shanghai',\n",
    "            'period': 'pre',\n",
    "            'treat': 0,\n",
    "            'post': 0,\n",
    "            'spending': baseline + np.random.normal(0, 5)\n",
    "        })\n",
    "        \n",
    "        # æ”¿ç­–å\n",
    "        time_trend = 40  # ç›¸åŒçš„æ—¶é—´è¶‹åŠ¿\n",
    "        data_list.append({\n",
    "            'user_id': user_id,\n",
    "            'city': 'shanghai',\n",
    "            'period': 'post',\n",
    "            'treat': 0,\n",
    "            'post': 1,\n",
    "            'spending': baseline + time_trend + np.random.normal(0, 5)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    df['treat_post'] = df['treat'] * df['post']  # äº¤äº’é¡¹\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "df = generate_did_data(treatment_effect=10)\n",
    "\n",
    "print(\"æ•°æ®é¢„è§ˆï¼š\")\n",
    "print(df.head(10))\n",
    "print(f\"\\næ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"\\nåˆ†ç»„ç»Ÿè®¡:\")\n",
    "print(df.groupby(['city', 'period'])['spending'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨è®¡ç®— DID\n",
    "def manual_did(df):\n",
    "    \"\"\"\n",
    "    æ‰‹åŠ¨è®¡ç®— DID ä¼°è®¡é‡\n",
    "    \"\"\"\n",
    "    # è®¡ç®—å››ä¸ªå‡å€¼\n",
    "    treat_post = df[(df['treat']==1) & (df['post']==1)]['spending'].mean()\n",
    "    treat_pre = df[(df['treat']==1) & (df['post']==0)]['spending'].mean()\n",
    "    control_post = df[(df['treat']==0) & (df['post']==1)]['spending'].mean()\n",
    "    control_pre = df[(df['treat']==0) & (df['post']==0)]['spending'].mean()\n",
    "    \n",
    "    # ç¬¬ä¸€æ¬¡å·®åˆ†ï¼ˆæ—¶é—´ï¼‰\n",
    "    diff_treat = treat_post - treat_pre\n",
    "    diff_control = control_post - control_pre\n",
    "    \n",
    "    # ç¬¬äºŒæ¬¡å·®åˆ†ï¼ˆç»„é—´ï¼‰\n",
    "    did = diff_treat - diff_control\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"æ‰‹åŠ¨è®¡ç®— DID\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nå¤„ç†ç»„ï¼ˆåŒ—äº¬ï¼‰ï¼š\")\n",
    "    print(f\"  æ”¿ç­–å‰: {treat_pre:.2f}\")\n",
    "    print(f\"  æ”¿ç­–å: {treat_post:.2f}\")\n",
    "    print(f\"  å·®åˆ†:   {diff_treat:.2f}\")\n",
    "    \n",
    "    print(f\"\\nå¯¹ç…§ç»„ï¼ˆä¸Šæµ·ï¼‰ï¼š\")\n",
    "    print(f\"  æ”¿ç­–å‰: {control_pre:.2f}\")\n",
    "    print(f\"  æ”¿ç­–å: {control_post:.2f}\")\n",
    "    print(f\"  å·®åˆ†:   {diff_control:.2f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DID ä¼°è®¡é‡: {did:.2f}\")\n",
    "    print(f\"ï¼ˆçœŸå®æ•ˆåº”: 10.00ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return did\n",
    "\n",
    "manual_did_estimate = manual_did(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›å½’ä¼°è®¡ DID\n",
    "def regression_did(df):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å›å½’ä¼°è®¡ DID\n",
    "    \"\"\"\n",
    "    # OLS å›å½’\n",
    "    model = smf.ols('spending ~ treat + post + treat_post', data=df).fit()\n",
    "    \n",
    "    # èšç±»æ ‡å‡†è¯¯ï¼ˆæŒ‰ç”¨æˆ·èšç±»ï¼‰\n",
    "    model_clustered = smf.ols('spending ~ treat + post + treat_post', data=df).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df['user_id']}\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"å›å½’ä¼°è®¡ DID\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nå›å½’æ–¹ç¨‹: spending ~ treat + post + treat_post\")\n",
    "    print(\"\\næ ‡å‡† OLS æ ‡å‡†è¯¯:\")\n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    print(\"\\nèšç±»æ ‡å‡†è¯¯ï¼ˆæŒ‰ç”¨æˆ·èšç±»ï¼‰:\")\n",
    "    print(model_clustered.summary().tables[1])\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DID ä¼°è®¡é‡ (treat_post): {model.params['treat_post']:.2f}\")\n",
    "    print(f\"   æ ‡å‡†è¯¯ (OLS): {model.bse['treat_post']:.4f}\")\n",
    "    print(f\"   æ ‡å‡†è¯¯ (èšç±»): {model_clustered.bse['treat_post']:.4f}\")\n",
    "    print(f\"   p-value: {model_clustered.pvalues['treat_post']:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model_clustered\n",
    "\n",
    "model = regression_did(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ å…³é”®å‘ç°\n",
    "\n",
    "1. **æ‰‹åŠ¨è®¡ç®—å’Œå›å½’ä¼°è®¡ä¸€è‡´**ï¼šä¸¤ç§æ–¹æ³•å¾—åˆ°ç›¸åŒçš„ DID ä¼°è®¡é‡\n",
    "2. **èšç±»æ ‡å‡†è¯¯æ›´å¤§**ï¼šè€ƒè™‘äº†ä¸ªä½“å†…ç›¸å…³æ€§åï¼Œä¸ç¡®å®šæ€§å¢åŠ \n",
    "3. **å›å½’æ¡†æ¶çš„ä¼˜åŠ¿**ï¼šå¯ä»¥æ–¹ä¾¿åœ°åŠ å…¥æ§åˆ¶å˜é‡ã€è®¡ç®—ç½®ä¿¡åŒºé—´\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: å¹³è¡Œè¶‹åŠ¿å‡è®¾ (Parallel Trends Assumption)\n",
    "\n",
    "### 3.1 å‡è®¾çš„å«ä¹‰\n",
    "\n",
    "DID çš„æ ¸å¿ƒå‡è®¾ï¼š**å¦‚æœæ²¡æœ‰æ”¿ç­–å¹²é¢„ï¼Œå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¶‹åŠ¿åº”è¯¥æ˜¯å¹³è¡Œçš„**ã€‚\n",
    "\n",
    "$$\n",
    "E[Y_{1t}(0) - Y_{1,t-1}(0)] = E[Y_{0t}(0) - Y_{0,t-1}(0)]\n",
    "$$\n",
    "\n",
    "**é€šä¿—è§£é‡Š**ï¼š\n",
    "- å¦‚æœåŒ—äº¬æ²¡æœ‰æ¨å‡ºä¼šå‘˜æ”¿ç­–\n",
    "- åŒ—äº¬å’Œä¸Šæµ·çš„æ¶ˆè´¹å¢é•¿åº”è¯¥æ˜¯ä¸€æ ·çš„\n",
    "- è¿™æ˜¯ä¸€ä¸ª**åäº‹å®å‡è®¾**ï¼Œæ— æ³•ç›´æ¥éªŒè¯\n",
    "\n",
    "### 3.2 ä¸ºä»€ä¹ˆå¹³è¡Œè¶‹åŠ¿é‡è¦ï¼Ÿ\n",
    "\n",
    "å¦‚æœè¿åå¹³è¡Œè¶‹åŠ¿å‡è®¾ï¼ŒDID ä¼°è®¡æ˜¯**æœ‰å**çš„ï¼š\n",
    "\n",
    "- å¤„ç†ç»„æœ¬èº«å¢é•¿æ›´å¿« â†’ DID é«˜ä¼°æ”¿ç­–æ•ˆåº”\n",
    "- å¤„ç†ç»„æœ¬èº«å¢é•¿æ›´æ…¢ â†’ DID ä½ä¼°æ”¿ç­–æ•ˆåº”\n",
    "\n",
    "### 3.3 æ£€éªŒæ–¹æ³•\n",
    "\n",
    "è™½ç„¶æ— æ³•ç›´æ¥éªŒè¯ï¼Œä½†æˆ‘ä»¬å¯ä»¥ï¼š\n",
    "\n",
    "1. **å›¾å½¢åŒ–æ£€éªŒ**ï¼šè§‚å¯Ÿæ”¿ç­–å‰çš„è¶‹åŠ¿\n",
    "2. **ç»Ÿè®¡æ£€éªŒ**ï¼šæµ‹è¯•æ”¿ç­–å‰å„æœŸçš„å·®å¼‚\n",
    "3. **å®‰æ…°å‰‚æ£€éªŒ**ï¼šä½¿ç”¨å‡çš„æ”¿ç­–æ—¶é—´ç‚¹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå¤šæœŸæ•°æ®ç”¨äºå¹³è¡Œè¶‹åŠ¿æ£€éªŒ\n",
    "def generate_multi_period_data(n_treated=500, n_control=500, n_periods=10, \n",
    "                               treatment_time=6, treatment_effect=10, seed=42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå¤šæœŸ DID æ•°æ®\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - n_periods: æ€»æ—¶æœŸæ•°\n",
    "    - treatment_time: æ”¿ç­–å¼€å§‹æ—¶æœŸï¼ˆä»0å¼€å§‹ï¼‰\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    # å¤„ç†ç»„\n",
    "    for i in range(n_treated):\n",
    "        user_id = f\"beijing_{i}\"\n",
    "        baseline = np.random.normal(100, 15)\n",
    "        individual_trend = np.random.normal(0, 2)  # ä¸ªä½“ç‰¹å®šè¶‹åŠ¿\n",
    "        \n",
    "        for t in range(n_periods):\n",
    "            time_effect = 4 * t  # å…±åŒæ—¶é—´è¶‹åŠ¿\n",
    "            is_post = 1 if t >= treatment_time else 0\n",
    "            treatment = treatment_effect if is_post else 0\n",
    "            \n",
    "            data_list.append({\n",
    "                'user_id': user_id,\n",
    "                'city': 'beijing',\n",
    "                'period': t,\n",
    "                'treat': 1,\n",
    "                'post': is_post,\n",
    "                'spending': baseline + time_effect + individual_trend * t + treatment + np.random.normal(0, 5)\n",
    "            })\n",
    "    \n",
    "    # å¯¹ç…§ç»„\n",
    "    for i in range(n_control):\n",
    "        user_id = f\"shanghai_{i}\"\n",
    "        baseline = np.random.normal(80, 15)\n",
    "        individual_trend = np.random.normal(0, 2)\n",
    "        \n",
    "        for t in range(n_periods):\n",
    "            time_effect = 4 * t\n",
    "            \n",
    "            data_list.append({\n",
    "                'user_id': user_id,\n",
    "                'city': 'shanghai',\n",
    "                'period': t,\n",
    "                'treat': 0,\n",
    "                'post': 0,  # å¯¹ç…§ç»„å§‹ç»ˆä¸º0\n",
    "                'spending': baseline + time_effect + individual_trend * t + np.random.normal(0, 5)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df, treatment_time\n",
    "\n",
    "# ç”Ÿæˆå¤šæœŸæ•°æ®\n",
    "df_multi, treatment_time = generate_multi_period_data()\n",
    "\n",
    "print(\"å¤šæœŸæ•°æ®é¢„è§ˆï¼š\")\n",
    "print(df_multi.head(20))\n",
    "print(f\"\\næ”¿ç­–æ—¶é—´ç‚¹: ç¬¬ {treatment_time} æœŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›¾å½¢åŒ–æ£€éªŒï¼šå¹³è¡Œè¶‹åŠ¿\n",
    "def plot_parallel_trends(df, treatment_time):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å¹³è¡Œè¶‹åŠ¿å‡è®¾\n",
    "    \"\"\"\n",
    "    # è®¡ç®—æ¯æœŸçš„å¹³å‡æ¶ˆè´¹\n",
    "    trends = df.groupby(['period', 'city'])['spending'].mean().reset_index()\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # åŒ—äº¬ï¼ˆå¤„ç†ç»„ï¼‰\n",
    "    beijing = trends[trends['city'] == 'beijing']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=beijing['period'],\n",
    "        y=beijing['spending'],\n",
    "        mode='lines+markers',\n",
    "        name='åŒ—äº¬ï¼ˆå¤„ç†ç»„ï¼‰',\n",
    "        line=dict(color=COLORS['treatment'], width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # ä¸Šæµ·ï¼ˆå¯¹ç…§ç»„ï¼‰\n",
    "    shanghai = trends[trends['city'] == 'shanghai']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=shanghai['period'],\n",
    "        y=shanghai['spending'],\n",
    "        mode='lines+markers',\n",
    "        name='ä¸Šæµ·ï¼ˆå¯¹ç…§ç»„ï¼‰',\n",
    "        line=dict(color=COLORS['control'], width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # æ·»åŠ æ”¿ç­–æ—¶é—´çº¿\n",
    "    fig.add_vline(\n",
    "        x=treatment_time - 0.5,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=COLORS['danger'],\n",
    "        line_width=2,\n",
    "        annotation_text=\"æ”¿ç­–å¼€å§‹\",\n",
    "        annotation_position=\"top\"\n",
    "    )\n",
    "    \n",
    "    # æ·»åŠ èƒŒæ™¯è‰²åŒºåˆ†æ”¿ç­–å‰å\n",
    "    fig.add_vrect(\n",
    "        x0=-0.5, x1=treatment_time-0.5,\n",
    "        fillcolor=\"lightgray\", opacity=0.1,\n",
    "        layer=\"below\", line_width=0,\n",
    "        annotation_text=\"æ”¿ç­–å‰\", annotation_position=\"top left\"\n",
    "    )\n",
    "    \n",
    "    fig.add_vrect(\n",
    "        x0=treatment_time-0.5, x1=9.5,\n",
    "        fillcolor=\"lightgreen\", opacity=0.1,\n",
    "        layer=\"below\", line_width=0,\n",
    "        annotation_text=\"æ”¿ç­–å\", annotation_position=\"top right\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='å¹³è¡Œè¶‹åŠ¿æ£€éªŒï¼šæ”¿ç­–å‰è¶‹åŠ¿æ˜¯å¦å¹³è¡Œï¼Ÿ',\n",
    "        xaxis_title='æ—¶æœŸ',\n",
    "        yaxis_title='å¹³å‡æ¶ˆè´¹é¢ï¼ˆå…ƒï¼‰',\n",
    "        template='plotly_white',\n",
    "        height=500,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_parallel_trends(df_multi, treatment_time)\n",
    "fig.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š å¦‚ä½•åˆ¤æ–­å¹³è¡Œè¶‹åŠ¿ï¼Ÿ\n",
    "1. è§‚å¯Ÿæ”¿ç­–å‰ï¼ˆç°è‰²åŒºåŸŸï¼‰ä¸¤æ¡çº¿æ˜¯å¦å¹³è¡Œ\n",
    "2. å¦‚æœæ”¿ç­–å‰è¶‹åŠ¿ç›¸ä¼¼ï¼Œåˆ™æ”¯æŒå¹³è¡Œè¶‹åŠ¿å‡è®¾\n",
    "3. æ”¿ç­–åï¼ˆç»¿è‰²åŒºåŸŸï¼‰å‡ºç°åˆ†ç¦»ï¼Œè¿™æ˜¯æ”¿ç­–æ•ˆåº”\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»Ÿè®¡æ£€éªŒï¼šLead æ£€éªŒ\n",
    "def lead_test(df, treatment_time):\n",
    "    \"\"\"\n",
    "    Lead æ£€éªŒï¼šæµ‹è¯•æ”¿ç­–å‰å„æœŸæ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚\n",
    "    \n",
    "    åŸç†ï¼šå¦‚æœå¹³è¡Œè¶‹åŠ¿æˆç«‹ï¼Œæ”¿ç­–å‰å„æœŸçš„å¤„ç†ç»„-å¯¹ç…§ç»„å·®å¼‚åº”è¯¥ç¨³å®š\n",
    "    \"\"\"\n",
    "    # åªä½¿ç”¨æ”¿ç­–å‰çš„æ•°æ®\n",
    "    df_pre = df[df['period'] < treatment_time].copy()\n",
    "    \n",
    "    # åˆ›å»ºæ¯æœŸçš„è™šæ‹Ÿå˜é‡\n",
    "    for t in df_pre['period'].unique():\n",
    "        df_pre[f'period_{t}'] = (df_pre['period'] == t).astype(int)\n",
    "        df_pre[f'treat_period_{t}'] = df_pre['treat'] * df_pre[f'period_{t}']\n",
    "    \n",
    "    # æ„å»ºå›å½’å…¬å¼\n",
    "    period_dummies = ' + '.join([f'period_{t}' for t in range(1, treatment_time)])\n",
    "    interaction_terms = ' + '.join([f'treat_period_{t}' for t in range(1, treatment_time)])\n",
    "    formula = f'spending ~ treat + {period_dummies} + {interaction_terms}'\n",
    "    \n",
    "    # å›å½’\n",
    "    model = smf.ols(formula, data=df_pre).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_pre['user_id']}\n",
    "    )\n",
    "    \n",
    "    # æå–äº¤äº’é¡¹ç³»æ•°\n",
    "    lead_coeffs = []\n",
    "    for t in range(1, treatment_time):\n",
    "        param_name = f'treat_period_{t}'\n",
    "        if param_name in model.params:\n",
    "            lead_coeffs.append({\n",
    "                'period': t,\n",
    "                'coef': model.params[param_name],\n",
    "                'se': model.bse[param_name],\n",
    "                'pvalue': model.pvalues[param_name]\n",
    "            })\n",
    "    \n",
    "    lead_df = pd.DataFrame(lead_coeffs)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Lead æ£€éªŒç»“æœ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\næ£€éªŒåŸç†ï¼šå¦‚æœå¹³è¡Œè¶‹åŠ¿æˆç«‹ï¼Œæ”¿ç­–å‰å„æœŸçš„å¤„ç†æ•ˆåº”åº”è¯¥ä¸º0\\n\")\n",
    "    print(lead_df.to_string(index=False))\n",
    "    \n",
    "    significant = lead_df[lead_df['pvalue'] < 0.05]\n",
    "    if len(significant) > 0:\n",
    "        print(f\"\\nâš ï¸  è­¦å‘Šï¼šæœ‰ {len(significant)} ä¸ªæ—¶æœŸçš„ç³»æ•°æ˜¾è‘—ä¸ä¸º0\")\n",
    "        print(\"   è¿™å¯èƒ½è¿åäº†å¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n",
    "    else:\n",
    "        print(\"\\nâœ… æ‰€æœ‰æ—¶æœŸçš„ç³»æ•°éƒ½ä¸æ˜¾è‘—ï¼Œæ”¯æŒå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return lead_df\n",
    "\n",
    "lead_results = lead_test(df_multi, treatment_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ TODO ç»ƒä¹  1: å®‰æ…°å‰‚æ£€éªŒ\n",
    "\n",
    "**ä»»åŠ¡**ï¼šå®ç°å®‰æ…°å‰‚æ£€éªŒ (Placebo Test)\n",
    "\n",
    "**æ€è·¯**ï¼š\n",
    "1. ä½¿ç”¨æ”¿ç­–å‰çš„æ•°æ®\n",
    "2. å‡è®¾ä¸€ä¸ªã€Œå‡çš„ã€æ”¿ç­–æ—¶é—´ç‚¹ï¼ˆæ¯”å¦‚çœŸå®æ”¿ç­–å‰2æœŸï¼‰\n",
    "3. ä¼°è®¡ DIDï¼Œå¦‚æœæ˜¾è‘—ï¼Œè¯´æ˜å¯èƒ½è¿åå¹³è¡Œè¶‹åŠ¿\n",
    "\n",
    "**æç¤º**ï¼š\n",
    "```python\n",
    "# åªä½¿ç”¨æ”¿ç­–å‰çš„æ•°æ®\n",
    "df_placebo = df_multi[df_multi['period'] < treatment_time].copy()\n",
    "\n",
    "# è®¾ç½®å‡çš„æ”¿ç­–æ—¶é—´ç‚¹\n",
    "fake_treatment_time = treatment_time - 2\n",
    "df_placebo['fake_post'] = (df_placebo['period'] >= fake_treatment_time).astype(int)\n",
    "df_placebo['treat_fake_post'] = df_placebo['treat'] * df_placebo['fake_post']\n",
    "\n",
    "# å›å½’ä¼°è®¡\n",
    "# TODO: è¡¥å…¨ä»£ç \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å®ç°å®‰æ…°å‰‚æ£€éªŒ\ndef placebo_test(df, treatment_time):\n    \"\"\"\n    å®‰æ…°å‰‚æ£€éªŒ - å®Œæ•´å®ç°\n    \"\"\"\n    # åªä½¿ç”¨æ”¿ç­–å‰çš„æ•°æ®\n    df_placebo = df[df['period'] < treatment_time].copy()\n    \n    # è®¾ç½®å‡çš„æ”¿ç­–æ—¶é—´ç‚¹ï¼ˆçœŸå®æ”¿ç­–å‰2æœŸï¼‰\n    fake_treatment_time = treatment_time - 2\n    \n    # åˆ›å»º fake_post å˜é‡\n    df_placebo['fake_post'] = (df_placebo['period'] >= fake_treatment_time).astype(int)\n    \n    # åˆ›å»ºäº¤äº’é¡¹\n    df_placebo['treat_fake_post'] = df_placebo['treat'] * df_placebo['fake_post']\n    \n    # å›å½’ä¼°è®¡\n    model = smf.ols('spending ~ treat + fake_post + treat_fake_post', \n                    data=df_placebo).fit(\n        cov_type='cluster',\n        cov_kwds={'groups': df_placebo['user_id']}\n    )\n    \n    # æå–ç»“æœ\n    placebo_effect = model.params['treat_fake_post']\n    placebo_se = model.bse['treat_fake_post']\n    placebo_pvalue = model.pvalues['treat_fake_post']\n    \n    print(\"=\" * 60)\n    print(\"å®‰æ…°å‰‚æ£€éªŒç»“æœ\")\n    print(\"=\" * 60)\n    print(f\"\\nå‡çš„æ”¿ç­–æ—¶é—´ç‚¹: ç¬¬ {fake_treatment_time} æœŸ\")\n    print(f\"çœŸå®æ”¿ç­–æ—¶é—´ç‚¹: ç¬¬ {treatment_time} æœŸ\\n\")\n    \n    print(f\"å®‰æ…°å‰‚ DID ä¼°è®¡é‡: {placebo_effect:.2f}\")\n    print(f\"æ ‡å‡†è¯¯: {placebo_se:.4f}\")\n    print(f\"p-value: {placebo_pvalue:.4f}\")\n    \n    if placebo_pvalue < 0.05:\n        print(\"\\nâš ï¸  è­¦å‘Šï¼šå®‰æ…°å‰‚æ•ˆåº”æ˜¾è‘—ï¼Œå¯èƒ½è¿åå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n        print(\"   è¿™è¡¨æ˜åœ¨çœŸå®æ”¿ç­–ä¹‹å‰ï¼Œä¸¤ç»„å·²ç»å­˜åœ¨å·®å¼‚è¶‹åŠ¿\")\n    else:\n        print(\"\\nâœ… å®‰æ…°å‰‚æ•ˆåº”ä¸æ˜¾è‘—ï¼Œæ”¯æŒå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n        print(\"   è¿™å¢å¼ºäº†æˆ‘ä»¬å¯¹çœŸå®DIDä¼°è®¡çš„ä¿¡å¿ƒ\")\n    \n    print(\"=\" * 60)\n    \n    return model\n\n# è¿è¡Œå®‰æ…°å‰‚æ£€éªŒ\nplacebo_result = placebo_test(df_multi, treatment_time)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‚è€ƒç­”æ¡ˆï¼ˆå…ˆè‡ªå·±å°è¯•ï¼Œå†æŸ¥çœ‹ï¼‰\n",
    "def placebo_test_solution(df, treatment_time):\n",
    "    \"\"\"\n",
    "    å®‰æ…°å‰‚æ£€éªŒ - å‚è€ƒç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    df_placebo = df[df['period'] < treatment_time].copy()\n",
    "    fake_treatment_time = treatment_time - 2\n",
    "    \n",
    "    df_placebo['fake_post'] = (df_placebo['period'] >= fake_treatment_time).astype(int)\n",
    "    df_placebo['treat_fake_post'] = df_placebo['treat'] * df_placebo['fake_post']\n",
    "    \n",
    "    model = smf.ols('spending ~ treat + fake_post + treat_fake_post', \n",
    "                    data=df_placebo).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_placebo['user_id']}\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"å®‰æ…°å‰‚æ£€éªŒç»“æœ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nå‡çš„æ”¿ç­–æ—¶é—´ç‚¹: ç¬¬ {fake_treatment_time} æœŸ\")\n",
    "    print(f\"çœŸå®æ”¿ç­–æ—¶é—´ç‚¹: ç¬¬ {treatment_time} æœŸ\\n\")\n",
    "    \n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    placebo_effect = model.params['treat_fake_post']\n",
    "    placebo_pvalue = model.pvalues['treat_fake_post']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å®‰æ…°å‰‚ DID ä¼°è®¡é‡: {placebo_effect:.2f}\")\n",
    "    print(f\"   p-value: {placebo_pvalue:.4f}\")\n",
    "    \n",
    "    if placebo_pvalue < 0.05:\n",
    "        print(\"\\nâš ï¸  è­¦å‘Šï¼šå®‰æ…°å‰‚æ•ˆåº”æ˜¾è‘—ï¼Œå¯èƒ½è¿åå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n",
    "    else:\n",
    "        print(\"\\nâœ… å®‰æ…°å‰‚æ•ˆåº”ä¸æ˜¾è‘—ï¼Œæ”¯æŒå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "placebo_test_solution(df_multi, treatment_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Event Study è®¾è®¡\n",
    "\n",
    "### 4.1 ä»€ä¹ˆæ˜¯ Event Studyï¼Ÿ\n",
    "\n",
    "Event Study æ˜¯ DID çš„æ‰©å±•ï¼Œå…è®¸æˆ‘ä»¬ï¼š\n",
    "1. **å¯è§†åŒ–å¹³è¡Œè¶‹åŠ¿**ï¼šè§‚å¯Ÿæ”¿ç­–å‰çš„è¶‹åŠ¿\n",
    "2. **ä¼°è®¡åŠ¨æ€æ•ˆåº”**ï¼šæ”¿ç­–æ•ˆåº”å¦‚ä½•éšæ—¶é—´å˜åŒ–\n",
    "3. **æ£€éªŒæ»åæ•ˆåº”**ï¼šæ”¿ç­–æ˜¯å¦æœ‰å»¶è¿Ÿå½±å“\n",
    "\n",
    "### 4.2 å›å½’æ–¹ç¨‹\n",
    "\n",
    "$$\n",
    "Y_{it} = \\alpha_i + \\lambda_t + \\sum_{k \\neq -1} \\beta_k \\cdot \\mathbb{1}[t - t^* = k] \\cdot \\text{Treat}_i + \\epsilon_{it}\n",
    "$$\n",
    "\n",
    "**ç¬¦å·è¯´æ˜**ï¼š\n",
    "- $\\alpha_i$: ä¸ªä½“å›ºå®šæ•ˆåº”\n",
    "- $\\lambda_t$: æ—¶é—´å›ºå®šæ•ˆåº”\n",
    "- $t^*$: æ”¿ç­–æ—¶é—´ç‚¹\n",
    "- $k$: ç›¸å¯¹æ—¶é—´ï¼ˆä¸æ”¿ç­–æ—¶é—´çš„è·ç¦»ï¼‰\n",
    "- $\\beta_k$: ç›¸å¯¹æ—¶é—´ $k$ çš„å¤„ç†æ•ˆåº”\n",
    "- $k = -1$ ä½œä¸ºåŸºå‡†æœŸï¼ˆå½’ä¸€åŒ–ä¸º0ï¼‰\n",
    "\n",
    "### 4.3 Event Study å›¾çš„è§£è¯»\n",
    "\n",
    "- **æ”¿ç­–å‰** ($k < 0$)ï¼šç³»æ•°åº”è¯¥æ¥è¿‘0ï¼ˆå¹³è¡Œè¶‹åŠ¿ï¼‰\n",
    "- **æ”¿ç­–æ—¶** ($k = 0$)ï¼šå¼€å§‹å‡ºç°æ•ˆåº”\n",
    "- **æ”¿ç­–å** ($k > 0$)ï¼šè§‚å¯Ÿæ•ˆåº”çš„æ¼”å˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event Study ä¼°è®¡\n",
    "def event_study(df, treatment_time):\n",
    "    \"\"\"\n",
    "    Event Study ä¼°è®¡\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # è®¡ç®—ç›¸å¯¹æ—¶é—´\n",
    "    df['rel_time'] = df['period'] - treatment_time\n",
    "    \n",
    "    # åˆ›å»ºç›¸å¯¹æ—¶é—´è™šæ‹Ÿå˜é‡ï¼ˆæ’é™¤ -1 ä½œä¸ºåŸºå‡†ï¼‰\n",
    "    rel_times = sorted(df['rel_time'].unique())\n",
    "    rel_times = [t for t in rel_times if t != -1]  # æ’é™¤åŸºå‡†æœŸ\n",
    "    \n",
    "    for t in rel_times:\n",
    "        df[f'rel_time_{t}'] = (df['rel_time'] == t).astype(int)\n",
    "        df[f'treat_rel_time_{t}'] = df['treat'] * df[f'rel_time_{t}']\n",
    "    \n",
    "    # æ„å»ºå›å½’å…¬å¼\n",
    "    period_dummies = ' + '.join([f'C(period)'])\n",
    "    interaction_terms = ' + '.join([f'treat_rel_time_{t}' for t in rel_times])\n",
    "    formula = f'spending ~ treat + {period_dummies} + {interaction_terms}'\n",
    "    \n",
    "    # å›å½’\n",
    "    model = smf.ols(formula, data=df).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df['user_id']}\n",
    "    )\n",
    "    \n",
    "    # æå–ç³»æ•°\n",
    "    coeffs = []\n",
    "    for t in rel_times:\n",
    "        param_name = f'treat_rel_time_{t}'\n",
    "        if param_name in model.params:\n",
    "            coeffs.append({\n",
    "                'rel_time': t,\n",
    "                'coef': model.params[param_name],\n",
    "                'se': model.bse[param_name],\n",
    "                'ci_lower': model.params[param_name] - 1.96 * model.bse[param_name],\n",
    "                'ci_upper': model.params[param_name] + 1.96 * model.bse[param_name]\n",
    "            })\n",
    "    \n",
    "    # æ·»åŠ åŸºå‡†æœŸï¼ˆç³»æ•°ä¸º0ï¼‰\n",
    "    coeffs.append({\n",
    "        'rel_time': -1,\n",
    "        'coef': 0,\n",
    "        'se': 0,\n",
    "        'ci_lower': 0,\n",
    "        'ci_upper': 0\n",
    "    })\n",
    "    \n",
    "    event_df = pd.DataFrame(coeffs).sort_values('rel_time')\n",
    "    \n",
    "    return event_df, model\n",
    "\n",
    "event_df, event_model = event_study(df_multi, treatment_time)\n",
    "\n",
    "print(\"Event Study ä¼°è®¡ç»“æœï¼š\")\n",
    "print(event_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶ Event Study å›¾\n",
    "def plot_event_study(event_df):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶ Event Study å›¾\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # ç‚¹ä¼°è®¡\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=event_df['rel_time'],\n",
    "        y=event_df['coef'],\n",
    "        mode='markers+lines',\n",
    "        name='DID ä¼°è®¡é‡',\n",
    "        marker=dict(size=10, color=COLORS['primary']),\n",
    "        line=dict(color=COLORS['primary'], width=2)\n",
    "    ))\n",
    "    \n",
    "    # 95% ç½®ä¿¡åŒºé—´\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(event_df['rel_time']) + list(event_df['rel_time'][::-1]),\n",
    "        y=list(event_df['ci_upper']) + list(event_df['ci_lower'][::-1]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(45, 156, 219, 0.2)',\n",
    "        line=dict(color='rgba(255,255,255,0)'),\n",
    "        name='95% ç½®ä¿¡åŒºé—´',\n",
    "        showlegend=True\n",
    "    ))\n",
    "    \n",
    "    # é›¶çº¿\n",
    "    fig.add_hline(\n",
    "        y=0,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"gray\",\n",
    "        line_width=1\n",
    "    )\n",
    "    \n",
    "    # æ”¿ç­–æ—¶é—´çº¿\n",
    "    fig.add_vline(\n",
    "        x=-0.5,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=COLORS['danger'],\n",
    "        line_width=2,\n",
    "        annotation_text=\"æ”¿ç­–å¼€å§‹\",\n",
    "        annotation_position=\"top\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Event Study å›¾ï¼šæ”¿ç­–æ•ˆåº”çš„åŠ¨æ€æ¼”å˜',\n",
    "        xaxis_title='ç›¸å¯¹æ—¶é—´ï¼ˆç›¸å¯¹äºæ”¿ç­–æ—¶é—´ï¼‰',\n",
    "        yaxis_title='å¤„ç†æ•ˆåº”ï¼ˆå…ƒï¼‰',\n",
    "        template='plotly_white',\n",
    "        height=500,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_event_study(event_df)\n",
    "fig.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š Event Study å›¾è§£è¯»ï¼š\n",
    "1. æ”¿ç­–å‰ï¼ˆè´Ÿå€¼ï¼‰ï¼šç³»æ•°åº”è¯¥æ¥è¿‘0ä¸”ä¸æ˜¾è‘—ï¼ˆæ”¯æŒå¹³è¡Œè¶‹åŠ¿ï¼‰\n",
    "2. æ”¿ç­–æ—¶ï¼ˆ0ï¼‰ï¼šå¼€å§‹å‡ºç°æ­£å‘æ•ˆåº”\n",
    "3. æ”¿ç­–åï¼ˆæ­£å€¼ï¼‰ï¼šè§‚å¯Ÿæ•ˆåº”æ˜¯å¦æŒç»­ã€å¢å¼ºæˆ–å‡å¼±\n",
    "4. ç½®ä¿¡åŒºé—´ï¼šå¦‚æœåŒ…å«0ï¼Œè¯´æ˜è¯¥æœŸæ•ˆåº”ä¸æ˜¾è‘—\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: äº¤é”™ DID (Staggered DID)\n",
    "\n",
    "### 5.1 ä»€ä¹ˆæ˜¯äº¤é”™ DIDï¼Ÿ\n",
    "\n",
    "åœ¨ç°å®ä¸­ï¼Œæ”¿ç­–å¾€å¾€ä¸æ˜¯åŒæ—¶åœ¨æ‰€æœ‰åœ°åŒºå®æ–½ï¼š\n",
    "\n",
    "```\n",
    "         2024-01  2024-02  2024-03  2024-04\n",
    "åŒ—äº¬       âœ…       âœ…       âœ…       âœ…\n",
    "ä¸Šæµ·       â­•       â­•       âœ…       âœ…\n",
    "æ·±åœ³       â­•       â­•       â­•       âœ…\n",
    "å¹¿å·       â­•       â­•       â­•       â­•\n",
    "```\n",
    "\n",
    "è¿™ç§**å¼‚è´¨æ€§å¤„ç†æ—¶é—´** (Staggered Treatment Timing) çš„è®¾è®¡ç§°ä¸ºäº¤é”™ DIDã€‚\n",
    "\n",
    "### 5.2 TWFE çš„é—®é¢˜\n",
    "\n",
    "ä¼ ç»Ÿçš„åŒå‘å›ºå®šæ•ˆåº” (Two-Way Fixed Effects, TWFE) ä¼°è®¡é‡ï¼š\n",
    "\n",
    "$$\n",
    "Y_{it} = \\alpha_i + \\lambda_t + \\beta \\cdot \\text{Treat}_{it} + \\epsilon_{it}\n",
    "$$\n",
    "\n",
    "åœ¨äº¤é”™ DID ä¸­å­˜åœ¨**ä¸¥é‡é—®é¢˜**ï¼š\n",
    "\n",
    "1. **è´Ÿæƒé‡é—®é¢˜**ï¼šæŸäº›å¤„ç†æ•ˆåº”è¢«èµ‹äºˆè´Ÿæƒé‡\n",
    "2. **å¼‚è´¨æ€§åå·®**ï¼šå¦‚æœå¤„ç†æ•ˆåº”éšæ—¶é—´å˜åŒ–ï¼ŒTWFE æœ‰å\n",
    "3. **ç¦å¿Œæ¯”è¾ƒ**ï¼šå·²å¤„ç†ç»„è¢«ç”¨ä½œæœªå¤„ç†ç»„çš„å¯¹ç…§\n",
    "\n",
    "### 5.3 ç°ä»£è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "**Callaway & Sant'Anna (2021)**ï¼š\n",
    "- é¿å…ç¦å¿Œæ¯”è¾ƒ\n",
    "- å…è®¸å¼‚è´¨æ€§å¤„ç†æ•ˆåº”\n",
    "- æä¾›å¤šç§èšåˆæ–¹å¼\n",
    "\n",
    "**Sun & Abraham (2021)**ï¼š\n",
    "- äº¤äº’åŠ æƒä¼°è®¡é‡\n",
    "- å…¼å®¹ Event Study æ¡†æ¶\n",
    "\n",
    "**De Chaisemartin & D'Haultfoeuille (2020)**ï¼š\n",
    "- DID_M ä¼°è®¡é‡\n",
    "- è¯Šæ–­å·¥å…·\n",
    "\n",
    "### 5.4 ç®€åŒ–ç¤ºä¾‹\n",
    "\n",
    "ç”±äºå®Œæ•´çš„äº¤é”™ DID å®ç°è¾ƒå¤æ‚ï¼ˆéœ€è¦ä¸“é—¨çš„åŒ…å¦‚ `did` æˆ– `csdid`ï¼‰ï¼Œè¿™é‡Œæˆ‘ä»¬å±•ç¤ºæ ¸å¿ƒæ€æƒ³ï¼š\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆäº¤é”™ DID æ•°æ®\n",
    "def generate_staggered_did_data(n_cities=4, n_users_per_city=200, n_periods=12, seed=42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆäº¤é”™ DID æ•°æ®\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    cities = ['Beijing', 'Shanghai', 'Shenzhen', 'Guangzhou']\n",
    "    treatment_times = [3, 6, 9, None]  # None è¡¨ç¤ºå§‹ç»ˆä¸å¤„ç†\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for city, treat_time in zip(cities, treatment_times):\n",
    "        for i in range(n_users_per_city):\n",
    "            user_id = f\"{city}_{i}\"\n",
    "            baseline = np.random.normal(100, 15)\n",
    "            \n",
    "            for t in range(n_periods):\n",
    "                time_effect = 3 * t\n",
    "                \n",
    "                # æ˜¯å¦å·²å¤„ç†\n",
    "                is_treated = 0\n",
    "                if treat_time is not None and t >= treat_time:\n",
    "                    is_treated = 1\n",
    "                \n",
    "                # å¤„ç†æ•ˆåº”ï¼ˆéšæ—¶é—´è¡°å‡ï¼‰\n",
    "                if is_treated:\n",
    "                    time_since_treatment = t - treat_time\n",
    "                    treatment_effect = 15 * np.exp(-0.1 * time_since_treatment)\n",
    "                else:\n",
    "                    treatment_effect = 0\n",
    "                \n",
    "                data_list.append({\n",
    "                    'user_id': user_id,\n",
    "                    'city': city,\n",
    "                    'period': t,\n",
    "                    'treat_time': treat_time if treat_time is not None else 999,\n",
    "                    'is_treated': is_treated,\n",
    "                    'spending': baseline + time_effect + treatment_effect + np.random.normal(0, 5)\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# ç”Ÿæˆäº¤é”™ DID æ•°æ®\n",
    "df_staggered = generate_staggered_did_data()\n",
    "\n",
    "print(\"äº¤é”™ DID æ•°æ®é¢„è§ˆï¼š\")\n",
    "print(df_staggered.head(20))\n",
    "print(\"\\nå„åŸå¸‚çš„å¤„ç†æ—¶é—´ï¼š\")\n",
    "print(df_staggered.groupby('city')['treat_time'].first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–äº¤é”™ DID\n",
    "def plot_staggered_did(df):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–äº¤é”™ DID çš„å¤„ç†æ¨¡å¼\n",
    "    \"\"\"\n",
    "    # è®¡ç®—æ¯ä¸ªåŸå¸‚æ¯æœŸçš„å¹³å‡æ¶ˆè´¹\n",
    "    trends = df.groupby(['period', 'city'])['spending'].mean().reset_index()\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors_map = {\n",
    "        'Beijing': COLORS['success'],\n",
    "        'Shanghai': COLORS['primary'],\n",
    "        'Shenzhen': COLORS['warning'],\n",
    "        'Guangzhou': COLORS['danger']\n",
    "    }\n",
    "    \n",
    "    for city in trends['city'].unique():\n",
    "        city_data = trends[trends['city'] == city]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=city_data['period'],\n",
    "            y=city_data['spending'],\n",
    "            mode='lines+markers',\n",
    "            name=city,\n",
    "            line=dict(color=colors_map[city], width=2),\n",
    "            marker=dict(size=6)\n",
    "        ))\n",
    "    \n",
    "    # æ·»åŠ å¤„ç†æ—¶é—´çº¿\n",
    "    treatment_times = df.groupby('city')['treat_time'].first()\n",
    "    for city, treat_time in treatment_times.items():\n",
    "        if treat_time < 999:\n",
    "            fig.add_vline(\n",
    "                x=treat_time - 0.5,\n",
    "                line_dash=\"dot\",\n",
    "                line_color=colors_map[city],\n",
    "                line_width=1,\n",
    "                annotation_text=f\"{city}\",\n",
    "                annotation_position=\"top\"\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='äº¤é”™ DIDï¼šä¸åŒåŸå¸‚åœ¨ä¸åŒæ—¶é—´æ¥å—å¤„ç†',\n",
    "        xaxis_title='æ—¶æœŸ',\n",
    "        yaxis_title='å¹³å‡æ¶ˆè´¹é¢ï¼ˆå…ƒï¼‰',\n",
    "        template='plotly_white',\n",
    "        height=500,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_staggered_did(df_staggered)\n",
    "fig.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š äº¤é”™ DID çš„ç‰¹ç‚¹ï¼š\n",
    "1. ä¸åŒåŸå¸‚åœ¨ä¸åŒæ—¶é—´æ¥å—å¤„ç†ï¼ˆè™šçº¿æ ‡è®°ï¼‰\n",
    "2. å¤„ç†åçš„è½¨è¿¹å‡ºç°è·³è·ƒ\n",
    "3. å¹¿å·å§‹ç»ˆæœªå¤„ç†ï¼Œå¯ä½œä¸ºå¯¹ç…§ç»„\n",
    "4. éœ€è¦å°å¿ƒå¤„ç†ã€Œå·²å¤„ç†ã€vsã€Œæœªå¤„ç†ã€çš„æ¯”è¾ƒ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWFE ä¼°è®¡ï¼ˆå¯èƒ½æœ‰åï¼‰\n",
    "def twfe_estimation(df):\n",
    "    \"\"\"\n",
    "    ä¼ ç»Ÿçš„åŒå‘å›ºå®šæ•ˆåº”ä¼°è®¡\n",
    "    \"\"\"\n",
    "    model = smf.ols('spending ~ C(city) + C(period) + is_treated', data=df).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df['user_id']}\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TWFE ä¼°è®¡ç»“æœ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nå›å½’æ–¹ç¨‹: spending ~ city_fe + period_fe + is_treated\\n\")\n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    twfe_effect = model.params['is_treated']\n",
    "    print(f\"\\nğŸ“Š TWFE ä¼°è®¡çš„å¤„ç†æ•ˆåº”: {twfe_effect:.2f}\")\n",
    "    print(\"\\nâš ï¸  æ³¨æ„ï¼šåœ¨äº¤é”™ DID è®¾è®¡ä¸­ï¼ŒTWFE ä¼°è®¡é‡å¯èƒ½æœ‰å\")\n",
    "    print(\"   åŸå› ï¼š\")\n",
    "    print(\"   1. å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ï¼ˆæ•ˆåº”éšæ—¶é—´è¡°å‡ï¼‰\")\n",
    "    print(\"   2. ç¦å¿Œæ¯”è¾ƒï¼ˆå·²å¤„ç†ç»„ä½œä¸ºå¯¹ç…§ï¼‰\")\n",
    "    print(\"   3. è´Ÿæƒé‡é—®é¢˜\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "twfe_model = twfe_estimation(df_staggered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 ç°ä»£ä¼°è®¡æ–¹æ³•çš„ç®€åŒ–å®ç°\n",
    "\n",
    "å®Œæ•´çš„ Callaway-Sant'Anna æˆ– Sun-Abraham ä¼°è®¡é‡éœ€è¦ä¸“é—¨çš„åŒ…ã€‚è¿™é‡Œæˆ‘ä»¬å±•ç¤ºæ ¸å¿ƒæ€æƒ³ï¼š\n",
    "\n",
    "**ç­–ç•¥**ï¼š\n",
    "1. å°†æ ·æœ¬åˆ†ç»„ï¼ˆæŒ‰å¤„ç†æ—¶é—´ï¼‰\n",
    "2. å¯¹æ¯ä¸ªç»„å•ç‹¬ä¼°è®¡ DID\n",
    "3. åŠ æƒå¹³å‡å¾—åˆ°æ€»ä½“æ•ˆåº”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_calls": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€åŒ–çš„ Callaway-Sant'Anna æ€æƒ³\n",
    "def simplified_callaway_santanna(df):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–ç‰ˆçš„ Callaway-Sant'Anna ä¼°è®¡\n",
    "    \n",
    "    æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "    1. å¯¹æ¯ä¸ªå¤„ç†ç»„ï¼ˆcohortï¼‰ï¼Œä½¿ç”¨æœªå¤„ç†ç»„ä½œä¸ºå¯¹ç…§\n",
    "    2. ä¼°è®¡æ¯ä¸ª cohort çš„ ATT\n",
    "    3. åŠ æƒå¹³å‡\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # è·å–æ‰€æœ‰å¤„ç†æ—¶é—´ï¼ˆæ’é™¤å§‹ç»ˆæœªå¤„ç†çš„ï¼‰\n",
    "    treat_times = df[df['treat_time'] < 999]['treat_time'].unique()\n",
    "    \n",
    "    for treat_time in sorted(treat_times):\n",
    "        # è¯¥ cohortï¼ˆåœ¨ treat_time æ¥å—å¤„ç†çš„åŸå¸‚ï¼‰\n",
    "        cohort = df[df['treat_time'] == treat_time].copy()\n",
    "        \n",
    "        # å¯¹ç…§ç»„ï¼ˆå§‹ç»ˆæœªå¤„ç†çš„åŸå¸‚ï¼‰\n",
    "        control = df[df['treat_time'] == 999].copy()\n",
    "        \n",
    "        # åˆå¹¶\n",
    "        did_data = pd.concat([cohort, control])\n",
    "        \n",
    "        # åˆ›å»º post å˜é‡\n",
    "        did_data['post'] = (did_data['period'] >= treat_time).astype(int)\n",
    "        did_data['treat'] = (did_data['treat_time'] == treat_time).astype(int)\n",
    "        did_data['treat_post'] = did_data['treat'] * did_data['post']\n",
    "        \n",
    "        # DID ä¼°è®¡\n",
    "        model = smf.ols('spending ~ treat + post + treat_post', data=did_data).fit(\n",
    "            cov_type='cluster',\n",
    "            cov_kwds={'groups': did_data['user_id']}\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'treat_time': treat_time,\n",
    "            'att': model.params['treat_post'],\n",
    "            'se': model.bse['treat_post'],\n",
    "            'n': len(cohort) // 12  # ç”¨æˆ·æ•°\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # åŠ æƒå¹³å‡ï¼ˆæŒ‰æ ·æœ¬é‡ï¼‰\n",
    "    total_n = results_df['n'].sum()\n",
    "    overall_att = (results_df['att'] * results_df['n']).sum() / total_n\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç®€åŒ–ç‰ˆ Callaway-Sant'Anna ä¼°è®¡\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nå„ Cohort çš„ ATTï¼š\\n\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åŠ æƒå¹³å‡ ATT: {overall_att:.2f}\")\n",
    "    print(\"\\nğŸ’¡ è¿™ä¸ªä¼°è®¡é¿å…äº† TWFE çš„é—®é¢˜ï¼š\")\n",
    "    print(\"   1. åªä½¿ç”¨æœªå¤„ç†ç»„ä½œä¸ºå¯¹ç…§\")\n",
    "    print(\"   2. å…è®¸æ¯ä¸ª cohort æœ‰ä¸åŒçš„å¤„ç†æ•ˆåº”\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return results_df, overall_att\n",
    "\n",
    "cs_results, cs_att = simplified_callaway_santanna(df_staggered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ä¸šåŠ¡æ¡ˆä¾‹\n",
    "\n",
    "### æ¡ˆä¾‹ 1: æœ€ä½å·¥èµ„æ”¿ç­–æ•ˆæœ\n",
    "\n",
    "**èƒŒæ™¯**ï¼š\n",
    "- 1992å¹´ï¼Œæ–°æ³½è¥¿å·æé«˜æœ€ä½å·¥èµ„ï¼Œå®¾å¤•æ³•å°¼äºšå·æœªæé«˜\n",
    "- ç ”ç©¶é—®é¢˜ï¼šæé«˜æœ€ä½å·¥èµ„æ˜¯å¦ä¼šå‡å°‘å°±ä¸šï¼Ÿ\n",
    "\n",
    "**æ•°æ®**ï¼š\n",
    "- å¿«é¤åº—çš„å°±ä¸šäººæ•°\n",
    "- æ”¿ç­–å‰åå„ä¸€æœŸ\n",
    "\n",
    "**ç»å…¸è®ºæ–‡**ï¼šCard & Krueger (1994), *Minimum Wages and Employment*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¡ˆä¾‹ 1: æœ€ä½å·¥èµ„æ”¿ç­–ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰\n",
    "def minimum_wage_case_study():\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿæœ€ä½å·¥èµ„æ”¿ç­–çš„ DID åˆ†æ\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    # æ–°æ³½è¥¿å·ï¼ˆå¤„ç†ç»„ï¼‰- 400å®¶å¿«é¤åº—\n",
    "    for i in range(400):\n",
    "        store_id = f\"NJ_{i}\"\n",
    "        baseline_emp = np.random.normal(20, 5)  # åŸºçº¿å°±ä¸šäººæ•°\n",
    "        \n",
    "        # æ”¿ç­–å‰\n",
    "        data_list.append({\n",
    "            'store_id': store_id,\n",
    "            'state': 'New Jersey',\n",
    "            'period': 'Before',\n",
    "            'treat': 1,\n",
    "            'post': 0,\n",
    "            'employment': baseline_emp + np.random.normal(0, 2)\n",
    "        })\n",
    "        \n",
    "        # æ”¿ç­–åï¼ˆæœ€ä½å·¥èµ„æé«˜åï¼Œå°±ä¸šåè€Œå¢åŠ äº†ï¼ï¼‰\n",
    "        data_list.append({\n",
    "            'store_id': store_id,\n",
    "            'state': 'New Jersey',\n",
    "            'period': 'After',\n",
    "            'treat': 1,\n",
    "            'post': 1,\n",
    "            'employment': baseline_emp + 0.5 + 2.5 + np.random.normal(0, 2)  # å¢åŠ 2.5äºº\n",
    "        })\n",
    "    \n",
    "    # å®¾å¤•æ³•å°¼äºšå·ï¼ˆå¯¹ç…§ç»„ï¼‰- 300å®¶å¿«é¤åº—\n",
    "    for i in range(300):\n",
    "        store_id = f\"PA_{i}\"\n",
    "        baseline_emp = np.random.normal(20, 5)\n",
    "        \n",
    "        # æ”¿ç­–å‰\n",
    "        data_list.append({\n",
    "            'store_id': store_id,\n",
    "            'state': 'Pennsylvania',\n",
    "            'period': 'Before',\n",
    "            'treat': 0,\n",
    "            'post': 0,\n",
    "            'employment': baseline_emp + np.random.normal(0, 2)\n",
    "        })\n",
    "        \n",
    "        # æ”¿ç­–åï¼ˆè‡ªç„¶å¢é•¿ï¼‰\n",
    "        data_list.append({\n",
    "            'store_id': store_id,\n",
    "            'state': 'Pennsylvania',\n",
    "            'period': 'After',\n",
    "            'treat': 0,\n",
    "            'post': 1,\n",
    "            'employment': baseline_emp + 0.5 + np.random.normal(0, 2)  # å¢åŠ 0.5äºº\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    df['treat_post'] = df['treat'] * df['post']\n",
    "    \n",
    "    # DID åˆ†æ\n",
    "    print(\"=\" * 60)\n",
    "    print(\"æ¡ˆä¾‹ 1: æœ€ä½å·¥èµ„æ”¿ç­–å¯¹å°±ä¸šçš„å½±å“\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nç ”ç©¶èƒŒæ™¯ï¼š\")\n",
    "    print(\"  1992å¹´ï¼Œæ–°æ³½è¥¿å·å°†æœ€ä½å·¥èµ„ä» $4.25 æé«˜åˆ° $5.05\")\n",
    "    print(\"  å®¾å¤•æ³•å°¼äºšå·ç»´æŒ $4.25 ä¸å˜\")\n",
    "    print(\"  ç ”ç©¶é—®é¢˜ï¼šæé«˜æœ€ä½å·¥èµ„æ˜¯å¦ä¼šå‡å°‘å°±ä¸šï¼Ÿ\")\n",
    "    \n",
    "    # æè¿°æ€§ç»Ÿè®¡\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"æè¿°æ€§ç»Ÿè®¡\")\n",
    "    print(\"=\" * 60)\n",
    "    stats = df.groupby(['state', 'period'])['employment'].agg(['mean', 'std', 'count'])\n",
    "    print(stats)\n",
    "    \n",
    "    # å›å½’ä¼°è®¡\n",
    "    model = smf.ols('employment ~ treat + post + treat_post', data=df).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df['store_id']}\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DID å›å½’ç»“æœ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    did_effect = model.params['treat_post']\n",
    "    pvalue = model.pvalues['treat_post']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DID ä¼°è®¡é‡: {did_effect:.2f} äºº\")\n",
    "    print(f\"   p-value: {pvalue:.4f}\")\n",
    "    \n",
    "    if pvalue < 0.05:\n",
    "        if did_effect > 0:\n",
    "            print(\"\\nâœ… ç»“è®ºï¼šæé«˜æœ€ä½å·¥èµ„æ˜¾è‘—å¢åŠ äº†å°±ä¸šï¼\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸  ç»“è®ºï¼šæé«˜æœ€ä½å·¥èµ„æ˜¾è‘—å‡å°‘äº†å°±ä¸š\")\n",
    "    else:\n",
    "        print(\"\\nç»“è®ºï¼šæé«˜æœ€ä½å·¥èµ„å¯¹å°±ä¸šæ²¡æœ‰æ˜¾è‘—å½±å“\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ç»æµå­¦å¯ç¤ºï¼š\")\n",
    "    print(\"   ä¼ ç»Ÿç†è®ºè®¤ä¸ºæé«˜æœ€ä½å·¥èµ„ä¼šå‡å°‘å°±ä¸šï¼ˆéœ€æ±‚æ›²çº¿ï¼‰\")\n",
    "    print(\"   ä½†å®è¯ç ”ç©¶å‘ç°æ•ˆåº”å¯èƒ½ä¸ºæ­£æˆ–ä¸æ˜¾è‘—\")\n",
    "    print(\"   å¯èƒ½åŸå› ï¼šå•ä¸€é›‡ä¸»æ¨¡å‹ã€æ•ˆç‡å·¥èµ„ã€å·¥äººæµåŠ¨ç‡é™ä½ç­‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return df, model\n",
    "\n",
    "df_minwage, model_minwage = minimum_wage_case_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¡ˆä¾‹ 2: å¹³å°æ”¿ç­–å˜æ›´è¯„ä¼°\n",
    "\n",
    "**èƒŒæ™¯**ï¼š\n",
    "- å¤–å–å¹³å°åœ¨éƒ¨åˆ†åŸå¸‚æ¨å‡ºã€Œæ— æ¥è§¦é…é€ã€åŠŸèƒ½\n",
    "- è¯„ä¼°è¯¥åŠŸèƒ½å¯¹è®¢å•é‡çš„å½±å“\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ TODO ç»ƒä¹  2: å¹³å°æ”¿ç­–å˜æ›´æ¡ˆä¾‹\n",
    "\n",
    "**ä»»åŠ¡**ï¼šåˆ†æã€Œæ— æ¥è§¦é…é€ã€åŠŸèƒ½å¯¹è®¢å•é‡çš„å½±å“\n",
    "\n",
    "**æ•°æ®ç”Ÿæˆ**ï¼š\n",
    "```python\n",
    "# å¤„ç†ç»„ï¼šåŒ—äº¬ã€ä¸Šæµ·ï¼ˆ2024-03-01 æ¨å‡ºï¼‰\n",
    "# å¯¹ç…§ç»„ï¼šæ·±åœ³ã€å¹¿å·ï¼ˆæœªæ¨å‡ºï¼‰\n",
    "# æ—¶é—´ï¼š2024-01 åˆ° 2024-06ï¼ˆå…±6ä¸ªæœˆï¼‰\n",
    "# çœŸå®æ•ˆåº”ï¼šè®¢å•é‡å¢åŠ  15%\n",
    "```\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "2. è¿›è¡Œ DID ä¼°è®¡\n",
    "3. ç»˜åˆ¶è¶‹åŠ¿å›¾\n",
    "4. æ£€éªŒå¹³è¡Œè¶‹åŠ¿å‡è®¾\n",
    "\n",
    "**æç¤º**ï¼šå‚è€ƒå‰é¢çš„æ¡ˆä¾‹ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¹³å°æ”¿ç­–å˜æ›´æ¡ˆä¾‹ - å®Œæ•´å®ç°\ndef contactless_delivery_case_study():\n    \"\"\"\n    æ— æ¥è§¦é…é€åŠŸèƒ½çš„ DID åˆ†æ - å‚è€ƒç­”æ¡ˆ\n    \"\"\"\n    np.random.seed(42)\n    \n    # æ—¶é—´è·¨åº¦ï¼š2022-01 åˆ° 2024-06ï¼ˆå…±30ä¸ªæœˆï¼‰\n    months = pd.date_range('2022-01', '2024-07', freq='M')\n    n_months = len(months)\n    treatment_month_idx = 14  # 2023å¹´3æœˆï¼ˆç´¢å¼•ä»0å¼€å§‹ï¼‰\n    \n    # åŸå¸‚åˆ—è¡¨\n    cities = {\n        'åŒ—äº¬': {'treated': True, 'baseline': 1000},\n        'ä¸Šæµ·': {'treated': True, 'baseline': 950},\n        'æ·±åœ³': {'treated': False, 'baseline': 800},\n        'å¹¿å·': {'treated': False, 'baseline': 820}\n    }\n    \n    data_list = []\n    \n    for city_name, city_info in cities.items():\n        baseline = city_info['baseline']\n        is_treated = city_info['treated']\n        \n        for t in range(n_months):\n            # å…±åŒæ—¶é—´è¶‹åŠ¿ï¼šæ¯æœˆå¢é•¿5%\n            time_effect = baseline * 0.05 * t / 12\n            \n            # å­£èŠ‚æ€§æ•ˆåº”\n            seasonal = 50 * np.sin(2 * np.pi * t / 12)\n            \n            # å¤„ç†æ•ˆåº”ï¼šæ— æ¥è§¦é…é€æå‡15%\n            is_post = t >= treatment_month_idx\n            treatment_effect = baseline * 0.15 if (is_treated and is_post) else 0\n            \n            # å™ªå£°\n            noise = np.random.normal(0, 30)\n            \n            # è®¢å•é‡\n            orders = baseline + time_effect + seasonal + treatment_effect + noise\n            \n            data_list.append({\n                'month': months[t],\n                'city': city_name,\n                'orders': orders,\n                'treated': 1 if is_treated else 0,\n                'post': 1 if is_post else 0\n            })\n    \n    df = pd.DataFrame(data_list)\n    df['treat_post'] = df['treated'] * df['post']\n    \n    print(\"=\" * 70)\n    print(\"æ¡ˆä¾‹ 2: æ— æ¥è§¦é…é€åŠŸèƒ½çš„ DID åˆ†æ\")\n    print(\"=\" * 70)\n    \n    # 1. å¯è§†åŒ–è¶‹åŠ¿\n    fig = go.Figure()\n    \n    for city in cities.keys():\n        city_data = df[df['city'] == city]\n        color = COLORS['treatment'] if cities[city]['treated'] else COLORS['control']\n        name = f\"{city}ï¼ˆ{'å¤„ç†ç»„' if cities[city]['treated'] else 'å¯¹ç…§ç»„'}ï¼‰\"\n        \n        fig.add_trace(go.Scatter(\n            x=city_data['month'],\n            y=city_data['orders'],\n            name=name,\n            line=dict(color=color, width=2),\n            mode='lines+markers'\n        ))\n    \n    fig.add_vline(\n        x=months[treatment_month_idx],\n        line_dash=\"dash\",\n        line_color=COLORS['danger'],\n        annotation_text=\"åŠŸèƒ½ä¸Šçº¿\"\n    )\n    \n    fig.update_layout(\n        title='å„åŸå¸‚è®¢å•é‡è¶‹åŠ¿',\n        xaxis_title='æœˆä»½',\n        yaxis_title='æ—¥å‡è®¢å•é‡',\n        template='plotly_white',\n        height=500,\n        hovermode='x unified'\n    )\n    \n    fig.show()\n    \n    # 2. DID ä¼°è®¡\n    model = smf.ols('orders ~ treated + post + treat_post', data=df).fit(\n        cov_type='cluster',\n        cov_kwds={'groups': df['city']}\n    )\n    \n    print(\"\\n[DID å›å½’ç»“æœ]\")\n    print(model.summary().tables[1])\n    \n    did_effect = model.params['treat_post']\n    did_se = model.bse['treat_post']\n    did_pvalue = model.pvalues['treat_post']\n    \n    print(f\"\\nğŸ“Š DID ä¼°è®¡é‡: {did_effect:.2f} å•/å¤©\")\n    print(f\"   æ ‡å‡†è¯¯: {did_se:.2f}\")\n    print(f\"   p-value: {did_pvalue:.4f}\")\n    \n    # 3. å¹³è¡Œè¶‹åŠ¿æ£€éªŒ\n    print(\"\\n[å¹³è¡Œè¶‹åŠ¿æ£€éªŒ]\")\n    df_pre = df[df['post'] == 0].copy()\n    df_pre['month_num'] = (df_pre['month'] - df_pre['month'].min()).dt.days / 30\n    df_pre['treat_month'] = df_pre['treated'] * df_pre['month_num']\n    \n    trend_model = smf.ols('orders ~ treated + month_num + treat_month', data=df_pre).fit()\n    trend_coef = trend_model.params['treat_month']\n    trend_pvalue = trend_model.pvalues['treat_month']\n    \n    print(f\"   å¤„ç†ç»„ä¸å¯¹ç…§ç»„è¶‹åŠ¿å·®å¼‚: {trend_coef:.4f}\")\n    print(f\"   p-value: {trend_pvalue:.4f}\")\n    \n    if trend_pvalue > 0.05:\n        print(\"   âœ… æ”¿ç­–å‰è¶‹åŠ¿æ— æ˜¾è‘—å·®å¼‚ï¼Œæ”¯æŒå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n    else:\n        print(\"   âš ï¸  æ”¿ç­–å‰è¶‹åŠ¿å­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½è¿åå¹³è¡Œè¶‹åŠ¿å‡è®¾\")\n    \n    # 4. ä¸šåŠ¡å»ºè®®\n    print(\"\\n[ä¸šåŠ¡å»ºè®®]\")\n    baseline_orders = df[(df['treated'] == 1) & (df['post'] == 0)]['orders'].mean()\n    pct_increase = (did_effect / baseline_orders) * 100\n    \n    print(f\"  1. æ— æ¥è§¦é…é€ä½¿è®¢å•é‡æå‡ {did_effect:.0f} å•/å¤© ({pct_increase:.1f}%)\")\n    print(f\"  2. å»ºè®®åœ¨æ·±åœ³ã€å¹¿å·å°½å¿«ä¸Šçº¿è¯¥åŠŸèƒ½\")\n    print(f\"  3. é¢„è®¡å¯å¸¦æ¥æœˆå¢é‡: {did_effect * 30:.0f} å•\")\n    \n    print(\"=\" * 70)\n    \n    return df, model\n\n# è¿è¡Œæ¡ˆä¾‹åˆ†æ\ndf_case2, model_case2 = contactless_delivery_case_study()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 7: é¢è¯•é«˜é¢‘é¢˜æ¨¡æ‹Ÿ\n\n### ğŸ¯ é¢è¯•é¢˜ 1ï¼šDID çš„æ ¸å¿ƒå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•éªŒè¯ï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**ï¼š\n\nDID çš„æ ¸å¿ƒå‡è®¾æ˜¯ **å¹³è¡Œè¶‹åŠ¿å‡è®¾** (Parallel Trends Assumption)ï¼š\n\n**å½¢å¼åŒ–å®šä¹‰**ï¼š\n$$\nE[Y_{it}(0) | T=1] - E[Y_{i,t-1}(0) | T=1] = E[Y_{it}(0) | T=0] - E[Y_{i,t-1}(0) | T=0]\n$$\n\n**é€šä¿—è§£é‡Š**ï¼šå¦‚æœæ²¡æœ‰æ”¿ç­–å¹²é¢„ï¼Œå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„è¶‹åŠ¿åº”è¯¥æ˜¯å¹³è¡Œçš„ã€‚\n\n**éªŒè¯æ–¹æ³•**ï¼š\n\n1. **å›¾å½¢åŒ–æ£€éªŒ**ï¼š\n   - ç»˜åˆ¶æ”¿ç­–å‰çš„è¶‹åŠ¿å›¾\n   - è§‚å¯Ÿä¸¤ç»„æ˜¯å¦å¹³è¡Œ\n\n2. **ç»Ÿè®¡æ£€éªŒ - Lead æ£€éªŒ**ï¼š\n   - åœ¨æ”¿ç­–å‰å„æœŸæ·»åŠ è™šæ‹Ÿäº¤äº’é¡¹\n   - æ£€éªŒç³»æ•°æ˜¯å¦æ˜¾è‘—ä¸º0\n   - å…¬å¼ï¼š$Y_{it} = \\alpha + \\sum_{k=-K}^{-2} \\beta_k D_i \\cdot \\mathbb{1}[t=k] + \\gamma_t + \\delta_i + \\epsilon_{it}$\n\n3. **å®‰æ…°å‰‚æ£€éªŒ (Placebo Test)**ï¼š\n   - åœ¨æ”¿ç­–å‰è®¾ç½®å‡çš„æ”¿ç­–æ—¶é—´ç‚¹\n   - ä¼°è®¡\"å‡DID\"ï¼Œåº”è¯¥ä¸æ˜¾è‘—\n\n4. **Event Study**ï¼š\n   - ä¼°è®¡æ¯æœŸçš„å¤„ç†æ•ˆåº”\n   - æ”¿ç­–å‰å„æœŸçš„ç³»æ•°åº”è¯¥æ¥è¿‘0\n\n**é¢è¯•åŠ åˆ†å›ç­”**ï¼š\n- \"å¹³è¡Œè¶‹åŠ¿æ˜¯ä¸å¯æ£€éªŒçš„åäº‹å®å‡è®¾ï¼Œæˆ‘ä»¬åªèƒ½æ£€éªŒæ”¿ç­–å‰çš„è¶‹åŠ¿\"\n- \"å³ä½¿æ”¿ç­–å‰å¹³è¡Œï¼Œä¹Ÿä¸èƒ½ä¿è¯æ”¿ç­–åç»§ç»­å¹³è¡Œ\"\n- \"å¯ä»¥é€šè¿‡å¤šç§æ–¹æ³•ç›¸äº’éªŒè¯ï¼Œå¢å¼ºå¯ä¿¡åº¦\"\n\n---\n\n### ğŸ¯ é¢è¯•é¢˜ 2ï¼šåˆæˆæ§åˆ¶æ³•é€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**ï¼š\n\nåˆæˆæ§åˆ¶æ³•é€‚ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š\n\n1. **å¤„ç†å•ä½æ•°é‡å°‘**ï¼š\n   - é€šå¸¸åªæœ‰1ä¸ªå¤„ç†å•ä½\n   - ä¾‹å¦‚ï¼šæŸä¸ªåŸå¸‚ã€æŸä¸ªå›½å®¶ã€æŸä¸ªå…¬å¸\n\n2. **æ‰¾ä¸åˆ°å®Œç¾çš„å¯¹ç…§ç»„**ï¼š\n   - æ²¡æœ‰ä¸€ä¸ªå•ä½ä¸å¤„ç†å•ä½å®Œå…¨ç›¸ä¼¼\n   - éœ€è¦ç”¨å¤šä¸ªå•ä½çš„åŠ æƒç»„åˆæ¥æ„å»º\"è™šæ‹Ÿ\"å¯¹ç…§\n\n3. **å¤„ç†æ˜¯é›†ä¸­å¼äº‹ä»¶**ï¼š\n   - æŸä¸ªæ³•æ¡ˆé€šè¿‡\n   - æŸä¸ªæ”¿ç­–å®æ–½\n   - æŸä¸ªå¤–éƒ¨å†²å‡»\n\n4. **æœ‰å¤šæœŸè§‚æµ‹æ•°æ®**ï¼š\n   - éœ€è¦è¶³å¤Ÿçš„å‰å¤„ç†æœŸæ•°æ®æ¥ä¼°è®¡æƒé‡\n   - é€šå¸¸è‡³å°‘éœ€è¦10æœŸä»¥ä¸Š\n\n**ä¸DIDçš„å¯¹æ¯”**ï¼š\n\n| ç»´åº¦ | DID | åˆæˆæ§åˆ¶ |\n|------|-----|----------|\n| å¤„ç†å•ä½æ•° | å¤šä¸ª | é€šå¸¸1ä¸ª |\n| å¯¹ç…§ç»„æ„å»º | ç®€å•å¹³å‡ | ä¼˜åŒ–æƒé‡ |\n| å…³é”®å‡è®¾ | å¹³è¡Œè¶‹åŠ¿ | å¯åˆæˆæ€§ |\n| æ¨æ–­æ–¹æ³• | æ ‡å‡†è¯¯ | Placebo Tests |\n\n**é¢è¯•åŠ åˆ†å›ç­”**ï¼š\n- \"åˆæˆæ§åˆ¶æ˜¯DIDçš„æ¨å¹¿ï¼Œå…è®¸ä¸ç­‰æƒé‡\"\n- \"å½“å¤„ç†å•ä½å¾ˆç‰¹æ®Šï¼ˆå¦‚åŠ å·ã€å¾·å›½ç»Ÿä¸€ï¼‰æ—¶ï¼Œåˆæˆæ§åˆ¶æ›´åˆé€‚\"\n- \"ç¼ºç‚¹æ˜¯å¤–éƒ¨æ•ˆåº¦ä½ï¼Œåªèƒ½è¯´æ˜å¯¹è¯¥ç‰¹å®šå•ä½çš„æ•ˆåº”\"\n\n---\n\n### ğŸ¯ é¢è¯•é¢˜ 3ï¼šSharp RDD å’Œ Fuzzy RDD çš„åŒºåˆ«ï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**ï¼š\n\n**Sharp RDD**ï¼š\n- å®šä¹‰ï¼šå¤„ç†çŠ¶æ€å®Œå…¨ç”±é©±åŠ¨å˜é‡å’Œé—¨æ§›å†³å®š\n- æ•°å­¦è¡¨è¾¾ï¼š$D_i = \\mathbb{1}[X_i \\geq c]$\n- ä¾‹å­ï¼šæ³•å®šå¹´é¾„é™åˆ¶ã€è€ƒè¯•åŠæ ¼çº¿\n- ä¼°è®¡ï¼šç›´æ¥ç”¨æ–­ç‚¹å¤„çš„è·³è·ƒ\n\n**Fuzzy RDD**ï¼š\n- å®šä¹‰ï¼šé—¨æ§›å¤„å¤„ç†æ¦‚ç‡è·³è·ƒï¼Œä½†ä¸æ˜¯0â†’1\n- æ•°å­¦è¡¨è¾¾ï¼š$P(D_i=1|X_i=c^+) \\neq P(D_i=1|X_i=c^-)$ï¼Œä½†è·³è·ƒå¹…åº¦<1\n- ä¾‹å­ï¼šå¥–å­¦é‡‘èµ„æ ¼ã€ä¼˜æƒ åˆ¸å¯ç”¨æ€§\n- ä¼°è®¡ï¼šä½¿ç”¨2SLSï¼Œé—¨æ§›ä½œä¸ºå·¥å…·å˜é‡\n\n**å…³é”®åŒºåˆ«**ï¼š\n\n| ç»´åº¦ | Sharp RDD | Fuzzy RDD |\n|------|-----------|-----------|\n| å¤„ç†åˆ†é… | ç¡®å®šæ€§ | æ¦‚ç‡æ€§ |\n| ä¼°è®¡æ–¹æ³• | ç›´æ¥ä¼°è®¡è·³è·ƒ | 2SLS / Waldä¼°è®¡ |\n| ä¼°è®¡å¯¹è±¡ | ATE (é—¨æ§›å¤„) | LATE (Compliers) |\n| ä¾‹å­ | å¹´é¾„æ»¡18å²å¯æŠ•ç¥¨ | æ»¡60å²æœ‰èµ„æ ¼é€€ä¼‘ |\n\n**Fuzzy RDD çš„ Wald ä¼°è®¡é‡**ï¼š\n$$\n\\tau_{Fuzzy} = \\frac{\\lim_{x \\downarrow c} E[Y|X=x] - \\lim_{x \\uparrow c} E[Y|X=x]}{\\lim_{x \\downarrow c} E[D|X=x] - \\lim_{x \\uparrow c} E[D|X=x]}\n$$\n\n**é¢è¯•åŠ åˆ†å›ç­”**ï¼š\n- \"Fuzzy RDDæœ¬è´¨ä¸Šæ˜¯IVè®¾è®¡ï¼Œé—¨æ§›æ˜¯å·¥å…·å˜é‡\"\n- \"Fuzzy RDDä¼°è®¡çš„æ˜¯Local ATTï¼Œå¤–æ¨è¦è°¨æ…\"\n- \"éœ€è¦å•è°ƒæ€§å‡è®¾ï¼šæ²¡æœ‰Defiers\"\n\n---\n\n### ğŸ¯ é¢è¯•é¢˜ 4ï¼šå·¥å…·å˜é‡éœ€è¦æ»¡è¶³ä»€ä¹ˆæ¡ä»¶ï¼Ÿå¦‚ä½•æ£€éªŒï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**ï¼š\n\nå·¥å…·å˜é‡éœ€è¦æ»¡è¶³**ä¸‰ä¸ªæ ¸å¿ƒå‡è®¾**ï¼š\n\n**1. ç›¸å…³æ€§ (Relevance)**ï¼š\n$$\n\\text{Cov}(Z, X) \\neq 0\n$$\n- å«ä¹‰ï¼šå·¥å…·å˜é‡å¿…é¡»ä¸å†…ç”Ÿå˜é‡ç›¸å…³\n- **å¯æ£€éªŒ**ï¼šç¬¬ä¸€é˜¶æ®µFç»Ÿè®¡é‡ > 10\n- æ£€éªŒæ–¹æ³•ï¼šå›å½’ $X \\sim Z$ï¼Œè®¡ç®—Fç»Ÿè®¡é‡\n\n**2. æ’ä»–æ€§ (Exclusion Restriction)**ï¼š\n- å«ä¹‰ï¼šå·¥å…·å˜é‡åªèƒ½é€šè¿‡å¤„ç†å˜é‡å½±å“ç»“æœ\n- **ä¸å¯æ£€éªŒ**ï¼šéœ€è¦ç»æµå­¦é€»è¾‘æ”¯æ’‘\n- å› æœå›¾ï¼š$Z \\to X \\to Y$ï¼Œæ²¡æœ‰ $Z \\to Y$ çš„ç›´æ¥è·¯å¾„\n\n**3. å¤–ç”Ÿæ€§ (Exogeneity)**ï¼š\n$$\n\\text{Cov}(Z, \\epsilon) = 0\n$$\n- å«ä¹‰ï¼šå·¥å…·å˜é‡ä¸è¯¯å·®é¡¹ä¸ç›¸å…³\n- **ä¸å¯æ£€éªŒ**ï¼šéœ€è¦åˆ¶åº¦èƒŒæ™¯æ”¯æ’‘\n- è¦æ±‚ï¼šå·¥å…·å˜é‡æ˜¯\"as-if random\"çš„\n\n**é¢å¤–æ£€éªŒï¼ˆæœ‰å¤šä¸ªIVæ—¶ï¼‰**ï¼š\n\n**4. è¿‡åº¦è¯†åˆ«æ£€éªŒ (Hansen J Test)**ï¼š\n- å‰æï¼šå·¥å…·å˜é‡æ•° > å†…ç”Ÿå˜é‡æ•°\n- åŸå‡è®¾ï¼šæ‰€æœ‰IVéƒ½æœ‰æ•ˆ\n- ç»Ÿè®¡é‡ï¼š$J = n \\cdot R^2 \\sim \\chi^2(m-k)$\n- **å±€é™**ï¼šåªèƒ½æ£€éªŒ\"è‡³å°‘ä¸€ä¸ªIVæ— æ•ˆ\"ï¼Œä¸èƒ½è¯´å“ªä¸ªæ— æ•ˆ\n\n**é¢è¯•åŠ åˆ†å›ç­”**ï¼š\n- \"å¥½çš„IVå¯é‡ä¸å¯æ±‚ï¼Œå¯»æ‰¾IVæ¯”ä¼°è®¡æœ¬èº«æ›´é‡è¦\"\n- \"å¼±IVæ¯”æ²¡æœ‰IVæ›´ç³Ÿç³•ï¼ˆfinite sample biasï¼‰\"\n- \"IVä¼°è®¡çš„æ˜¯LATEï¼Œä¸æ˜¯ATE\"\n\n---\n\n### ğŸ¯ é¢è¯•é¢˜ 5ï¼šå¦‚ä½•é€‰æ‹©å› æœæ¨æ–­æ–¹æ³•ï¼Ÿ\n\n**æ ‡å‡†ç­”æ¡ˆ**ï¼š\n\næ ¹æ®æ•°æ®ç‰¹å¾å’Œç ”ç©¶åœºæ™¯é€‰æ‹©ï¼š\n\n**å†³ç­–æ ‘**ï¼š\n\n```\næ˜¯å¦å¯ä»¥åšéšæœºå®éªŒï¼Ÿ\nâ”œâ”€â”€ æ˜¯ â†’ RCT (é‡‘æ ‡å‡†)\nâ””â”€â”€ å¦ â†’ è§‚å¯Ÿæ€§ç ”ç©¶\n    â”œâ”€â”€ æœ‰æ—¶é—´ç»´åº¦ï¼Ÿ\n    â”‚   â”œâ”€â”€ æœ‰å¤„ç†å‰åå¯¹ç…§ç»„ â†’ DID\n    â”‚   â”‚   â”œâ”€â”€ å¤šä¸ªå¤„ç†å•ä½ â†’ æ ‡å‡†DID\n    â”‚   â”‚   â””â”€â”€ å•ä¸ªå¤„ç†å•ä½ â†’ åˆæˆæ§åˆ¶\n    â”‚   â””â”€â”€ æœ‰å¤šæœŸé¢æ¿æ•°æ® â†’ å›ºå®šæ•ˆåº” / Event Study\n    â”œâ”€â”€ æœ‰è¿ç»­åˆ†é…å˜é‡å’Œé—¨æ§›ï¼Ÿ â†’ RDD\n    â”‚   â”œâ”€â”€ å®Œå…¨å†³å®šå¤„ç† â†’ Sharp RDD\n    â”‚   â””â”€â”€ å½±å“å¤„ç†æ¦‚ç‡ â†’ Fuzzy RDD\n    â”œâ”€â”€ æœ‰å¤–ç”Ÿå·¥å…·å˜é‡ï¼Ÿ â†’ IV / 2SLS\n    â””â”€â”€ æœ‰ä¸°å¯Œåå˜é‡ï¼Ÿ â†’ Matching / IPW\n```\n\n**æ–¹æ³•å¯¹æ¯”è¡¨**ï¼š\n\n| æ–¹æ³• | å…³é”®å‡è®¾ | æ•°æ®è¦æ±‚ | ä¼°è®¡å¯¹è±¡ | å¤–éƒ¨æ•ˆåº¦ |\n|------|----------|----------|----------|----------|\n| RCT | éšæœºåˆ†é… | å®éªŒæ•°æ® | ATE | é«˜ |\n| DID | å¹³è¡Œè¶‹åŠ¿ | é¢æ¿+å¯¹ç…§ç»„ | ATT | ä¸­ |\n| åˆæˆæ§åˆ¶ | å¯åˆæˆæ€§ | é•¿é¢æ¿+å¤šå¯¹ç…§ | ATT | ä½ |\n| RDD | è¿ç»­æ€§ | é—¨æ§›é™„è¿‘æ•°æ® | LATE | ä½ |\n| IV | ä¸‰ä¸ªå‡è®¾ | å¤–ç”Ÿå†²å‡» | LATE | ä½ |\n| Matching | CIA | ä¸°å¯Œåå˜é‡ | ATT | ä¸­ |\n\n**é¢è¯•åŠ åˆ†å›ç­”**ï¼š\n- \"æ²¡æœ‰ä¸€ç§æ–¹æ³•æ˜¯ä¸‡èƒ½çš„ï¼Œè¦æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©\"\n- \"æœ€å¥½ç”¨å¤šç§æ–¹æ³•éªŒè¯ï¼Œrobustness check\"\n- \"å†…éƒ¨æ•ˆåº¦å’Œå¤–éƒ¨æ•ˆåº¦é€šå¸¸æ˜¯trade-off\"\n- \"å®è·µä¸­å¸¸ç»„åˆä½¿ç”¨ï¼Œå¦‚DID + Matching\"\n\n---"
  },
  {
   "cell_type": "code",
   "source": "class MyDIDEstimator:\n    \"\"\"\n    ä»é›¶å®ç°çš„ DID ä¼°è®¡å™¨\n    \n    æ”¯æŒï¼š\n    - åŸºæœ¬ DIDï¼ˆä¸¤æœŸä¸¤ç»„ï¼‰\n    - å¤šæœŸ DID / Event Study\n    - å¹³è¡Œè¶‹åŠ¿æ£€éªŒ\n    - å®‰æ…°å‰‚æ£€éªŒ\n    - èšç±»æ ‡å‡†è¯¯\n    \"\"\"\n    \n    def __init__(self):\n        self.tau = None  # DID ä¼°è®¡é‡\n        self.se = None   # æ ‡å‡†è¯¯\n        self.model = None\n        self.data = None\n        \n    def fit(self, data, outcome, treatment, post, cluster=None):\n        \"\"\"\n        æ‹ŸåˆåŸºæœ¬ DID æ¨¡å‹\n        \n        å‚æ•°:\n            data: DataFrame\n            outcome: ç»“æœå˜é‡å\n            treatment: å¤„ç†ç»„indicatorå\n            post: æ”¿ç­–åindicatorå\n            cluster: èšç±»å˜é‡åï¼ˆç”¨äºèšç±»æ ‡å‡†è¯¯ï¼‰\n        \"\"\"\n        self.data = data.copy()\n        \n        # åˆ›å»ºäº¤äº’é¡¹\n        self.data['treat_post'] = self.data[treatment] * self.data[post]\n        \n        # æ„å»ºè®¾è®¡çŸ©é˜µ\n        X = self.data[[treatment, post, 'treat_post']].values\n        X = np.column_stack([np.ones(len(X)), X])  # æ·»åŠ æˆªè·\n        y = self.data[outcome].values\n        \n        # OLS ä¼°è®¡\n        beta = np.linalg.lstsq(X.T @ X, X.T @ y, rcond=None)[0]\n        \n        # æ®‹å·®\n        y_pred = X @ beta\n        residuals = y - y_pred\n        \n        # DID ä¼°è®¡é‡ï¼ˆäº¤äº’é¡¹ç³»æ•°ï¼‰\n        self.tau = beta[3]\n        \n        # æ ‡å‡†è¯¯\n        if cluster is not None:\n            # èšç±»æ ‡å‡†è¯¯\n            self.se = self._cluster_se(X, residuals, self.data[cluster], coef_idx=3)\n        else:\n            # å¼‚æ–¹å·®ç¨³å¥æ ‡å‡†è¯¯\n            self.se = self._robust_se(X, residuals, coef_idx=3)\n        \n        # ä¿å­˜ç»“æœ\n        self.model = {\n            'beta': beta,\n            'X': X,\n            'y': y,\n            'residuals': residuals,\n            'outcome': outcome,\n            'treatment': treatment,\n            'post': post\n        }\n        \n        return self\n    \n    def _robust_se(self, X, residuals, coef_idx):\n        \"\"\"è®¡ç®—å¼‚æ–¹å·®ç¨³å¥æ ‡å‡†è¯¯ï¼ˆHC1ï¼‰\"\"\"\n        n, k = X.shape\n        \n        # è‚‰ (Meat)\n        meat = X.T @ np.diag(residuals**2) @ X\n        \n        # é¢åŒ… (Bread)\n        bread = np.linalg.inv(X.T @ X)\n        \n        # ä¸‰æ˜æ²»ä¼°è®¡é‡\n        vcov = bread @ meat @ bread\n        \n        # è‡ªç”±åº¦è°ƒæ•´\n        vcov = vcov * (n / (n - k))\n        \n        return np.sqrt(vcov[coef_idx, coef_idx])\n    \n    def _cluster_se(self, X, residuals, clusters, coef_idx):\n        \"\"\"è®¡ç®—èšç±»æ ‡å‡†è¯¯\"\"\"\n        n, k = X.shape\n        clusters_unique = np.unique(clusters)\n        n_clusters = len(clusters_unique)\n        \n        # è‚‰ (Meat) - èšç±»å±‚é¢æ±‚å’Œ\n        meat = np.zeros((k, k))\n        for cluster_id in clusters_unique:\n            mask = (clusters == cluster_id)\n            X_c = X[mask]\n            res_c = residuals[mask]\n            meat += (X_c.T @ res_c).reshape(-1, 1) @ (X_c.T @ res_c).reshape(1, -1)\n        \n        # é¢åŒ… (Bread)\n        bread = np.linalg.inv(X.T @ X)\n        \n        # ä¸‰æ˜æ²»ä¼°è®¡é‡\n        vcov = bread @ meat @ bread\n        \n        # è‡ªç”±åº¦è°ƒæ•´\n        vcov = vcov * (n_clusters / (n_clusters - 1)) * ((n - 1) / (n - k))\n        \n        return np.sqrt(vcov[coef_idx, coef_idx])\n    \n    def summary(self, alpha=0.05):\n        \"\"\"è¾“å‡ºä¼°è®¡ç»“æœ\"\"\"\n        if self.tau is None:\n            raise ValueError(\"è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•\")\n        \n        z = stats.norm.ppf(1 - alpha/2)\n        ci_lower = self.tau - z * self.se\n        ci_upper = self.tau + z * self.se\n        t_stat = self.tau / self.se\n        p_value = 2 * (1 - stats.norm.cdf(np.abs(t_stat)))\n        \n        print(\"=\" * 60)\n        print(\"DID ä¼°è®¡ç»“æœ (MyDIDEstimator)\")\n        print(\"=\" * 60)\n        print(f\"DID ä¼°è®¡é‡ (Ï„):     {self.tau:.4f}\")\n        print(f\"æ ‡å‡†è¯¯ (SE):        {self.se:.4f}\")\n        print(f\"t ç»Ÿè®¡é‡:           {t_stat:.4f}\")\n        print(f\"p å€¼:              {p_value:.4f}\")\n        print(f\"{int((1-alpha)*100)}% ç½®ä¿¡åŒºé—´:      [{ci_lower:.4f}, {ci_upper:.4f}]\")\n        \n        if p_value < alpha:\n            print(f\"\\nâœ… åœ¨ {alpha} æ˜¾è‘—æ€§æ°´å¹³ä¸‹æ‹’ç»åŸå‡è®¾\")\n        else:\n            print(f\"\\nâŒ åœ¨ {alpha} æ˜¾è‘—æ€§æ°´å¹³ä¸‹æ— æ³•æ‹’ç»åŸå‡è®¾\")\n        \n        print(\"=\" * 60)\n        \n        return {\n            'tau': self.tau,\n            'se': self.se,\n            't_stat': t_stat,\n            'p_value': p_value,\n            'ci': (ci_lower, ci_upper)\n        }\n    \n    def event_study(self, data, outcome, treatment, time_var, treatment_time, \n                   cluster=None, omit_period=-1):\n        \"\"\"\n        Event Study ä¼°è®¡\n        \n        å‚æ•°:\n            omit_period: çœç•¥çš„åŸºå‡†æœŸï¼ˆç›¸å¯¹æ—¶é—´ï¼‰\n        \"\"\"\n        df = data.copy()\n        \n        # è®¡ç®—ç›¸å¯¹æ—¶é—´\n        df['rel_time'] = df[time_var] - treatment_time\n        \n        # åˆ›å»ºç›¸å¯¹æ—¶é—´è™šæ‹Ÿå˜é‡ï¼ˆæ’é™¤åŸºå‡†æœŸï¼‰\n        rel_times = sorted(df['rel_time'].unique())\n        rel_times = [t for t in rel_times if t != omit_period]\n        \n        # æ„å»ºè®¾è®¡çŸ©é˜µ\n        X_list = [np.ones(len(df))]  # æˆªè·\n        \n        for t in rel_times:\n            indicator = (df['rel_time'] == t).astype(int).values\n            interaction = df[treatment].values * indicator\n            X_list.append(interaction)\n        \n        X = np.column_stack(X_list)\n        y = df[outcome].values\n        \n        # OLS ä¼°è®¡\n        beta = np.linalg.lstsq(X.T @ X, X.T @ y, rcond=None)[0]\n        residuals = y - X @ beta\n        \n        # æå–ç³»æ•°\n        coeffs = []\n        for i, t in enumerate(rel_times):\n            coef = beta[i + 1]\n            \n            if cluster is not None:\n                se = self._cluster_se(X, residuals, df[cluster], coef_idx=i+1)\n            else:\n                se = self._robust_se(X, residuals, coef_idx=i+1)\n            \n            coeffs.append({\n                'rel_time': t,\n                'coef': coef,\n                'se': se,\n                'ci_lower': coef - 1.96 * se,\n                'ci_upper': coef + 1.96 * se\n            })\n        \n        # æ·»åŠ åŸºå‡†æœŸ\n        coeffs.append({\n            'rel_time': omit_period,\n            'coef': 0,\n            'se': 0,\n            'ci_lower': 0,\n            'ci_upper': 0\n        })\n        \n        return pd.DataFrame(coeffs).sort_values('rel_time')\n    \n    def placebo_test(self, data, outcome, treatment, post, fake_treatment_time, \n                    cluster=None):\n        \"\"\"\n        å®‰æ…°å‰‚æ£€éªŒ\n        \n        å‚æ•°:\n            fake_treatment_time: å‡çš„æ”¿ç­–æ—¶é—´ç‚¹\n        \"\"\"\n        # åªä½¿ç”¨çœŸå®æ”¿ç­–å‰çš„æ•°æ®\n        df_pre = data[data[post] == 0].copy()\n        \n        # åˆ›å»ºå‡çš„ post å˜é‡\n        # è¿™é‡Œéœ€è¦æ—¶é—´å˜é‡ï¼Œç®€åŒ–å¤„ç†\n        df_pre['fake_post'] = 0  # éœ€è¦æ ¹æ®å®é™…æ—¶é—´å˜é‡è®¾ç½®\n        \n        # æ‹Ÿåˆ DID\n        self.fit(df_pre, outcome, treatment, 'fake_post', cluster)\n        \n        print(\"\\nå®‰æ…°å‰‚æ£€éªŒï¼š\")\n        print(f\"  å‡çš„æ”¿ç­–æ—¶é—´ç‚¹æ•ˆåº”: {self.tau:.4f} (p={2*(1-stats.norm.cdf(abs(self.tau/self.se))):.4f})\")\n        \n        return self.tau\n\n# æµ‹è¯• MyDIDEstimator\nprint(\"æµ‹è¯• MyDIDEstimator ç±»ï¼š\\n\")\n\n# ç”Ÿæˆç®€å•æµ‹è¯•æ•°æ®\nnp.random.seed(42)\ntest_data = []\nfor i in range(500):\n    user_id = f\"user_{i}\"\n    treat = 1 if i < 250 else 0\n    baseline = 100 if treat else 80\n    \n    # å‰æœŸ\n    test_data.append({\n        'user_id': user_id,\n        'treat': treat,\n        'post': 0,\n        'outcome': baseline + np.random.normal(0, 5)\n    })\n    \n    # åæœŸ\n    effect = 10 if treat else 0\n    test_data.append({\n        'user_id': user_id,\n        'treat': treat,\n        'post': 1,\n        'outcome': baseline + 40 + effect + np.random.normal(0, 5)\n    })\n\ndf_test = pd.DataFrame(test_data)\n\n# ä½¿ç”¨è‡ªå®šä¹‰ä¼°è®¡å™¨\nmy_did = MyDIDEstimator()\nmy_did.fit(df_test, 'outcome', 'treat', 'post', cluster='user_id')\nresults = my_did.summary()\n\nprint(f\"\\nå¯¹æ¯”å®˜æ–¹åŒ…çš„ç»“æœï¼š\")\nmodel_official = smf.ols('outcome ~ treat + post + treat:post', data=df_test).fit(\n    cov_type='cluster',\n    cov_kwds={'groups': df_test['user_id']}\n)\nprint(f\"å®˜æ–¹ä¼°è®¡é‡: {model_official.params['treat:post']:.4f}\")\nprint(f\"å®˜æ–¹æ ‡å‡†è¯¯: {model_official.bse['treat:post']:.4f}\")\n\nprint(\"\\nâœ… ä»é›¶å®ç°çš„ä¼°è®¡å™¨ä¸å®˜æ–¹åŒ…ç»“æœä¸€è‡´ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 8: ä»é›¶å®ç° DID ä¼°è®¡å™¨\n\nä¸ºäº†æ·±å…¥ç†è§£ DID çš„åŸç†ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªå®Œæ•´çš„ DID ä¼°è®¡å™¨ç±»ã€‚\n\n### MyDIDEstimator ç±»è®¾è®¡\n\n**åŠŸèƒ½éœ€æ±‚**ï¼š\n1. åŸºæœ¬ DID ä¼°è®¡ï¼ˆä¸¤æœŸä¸¤ç»„ï¼‰\n2. å¤šæœŸ DIDï¼ˆEvent Studyï¼‰\n3. å¹³è¡Œè¶‹åŠ¿æ£€éªŒ\n4. å®‰æ…°å‰‚æ£€éªŒ\n5. èšç±»æ ‡å‡†è¯¯\n6. å¯è§†åŒ–\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒè¦ç‚¹\n",
    "\n",
    "1. **DID çš„ç›´è§‰**ï¼š\n",
    "   - ç¬¬ä¸€æ¬¡å·®åˆ†ï¼šå»é™¤æ—¶é—´è¶‹åŠ¿\n",
    "   - ç¬¬äºŒæ¬¡å·®åˆ†ï¼šå»é™¤ç»„é—´å·®å¼‚\n",
    "   - å¯¹ç…§ç»„æä¾›åäº‹å®\n",
    "\n",
    "2. **å¹³è¡Œè¶‹åŠ¿å‡è®¾**ï¼š\n",
    "   - æ ¸å¿ƒè¯†åˆ«å‡è®¾\n",
    "   - æ— æ³•ç›´æ¥éªŒè¯ï¼Œä½†å¯é—´æ¥æ£€éªŒ\n",
    "   - å›¾å½¢åŒ–æ£€éªŒã€Lead æ£€éªŒã€å®‰æ…°å‰‚æ£€éªŒ\n",
    "\n",
    "3. **Event Study**ï¼š\n",
    "   - å¯è§†åŒ–åŠ¨æ€æ•ˆåº”\n",
    "   - æ£€éªŒå¹³è¡Œè¶‹åŠ¿\n",
    "   - å‘ç°æ”¿ç­–çš„æ»åæˆ–é¢„æœŸæ•ˆåº”\n",
    "\n",
    "4. **äº¤é”™ DID**ï¼š\n",
    "   - TWFE å¯èƒ½æœ‰å\n",
    "   - ä½¿ç”¨ç°ä»£ä¼°è®¡æ–¹æ³•ï¼ˆCallaway-Sant'Anna, Sun-Abrahamï¼‰\n",
    "   - é¿å…ç¦å¿Œæ¯”è¾ƒ\n",
    "\n",
    "5. **å®è·µå»ºè®®**ï¼š\n",
    "   - å§‹ç»ˆä½¿ç”¨èšç±»æ ‡å‡†è¯¯\n",
    "   - æŠ¥å‘Šå¹³è¡Œè¶‹åŠ¿æ£€éªŒç»“æœ\n",
    "   - è¿›è¡Œç¨³å¥æ€§æ£€éªŒï¼ˆæ”¹å˜æ ·æœ¬ã€æ—¶é—´çª—å£ç­‰ï¼‰\n",
    "   - è®¨è®ºæ½œåœ¨çš„è¿åå¹³è¡Œè¶‹åŠ¿çš„å› ç´ \n",
    "\n",
    "### DID é€‚ç”¨åœºæ™¯\n",
    "\n",
    "âœ… **é€‚åˆä½¿ç”¨ DID**ï¼š\n",
    "- æ”¿ç­–åœ¨ç‰¹å®šåœ°åŒº/æ—¶é—´å®æ–½\n",
    "- æœ‰åˆé€‚çš„å¯¹ç…§ç»„\n",
    "- æ”¿ç­–å‰è¶‹åŠ¿ç›¸ä¼¼\n",
    "- æœ‰æ”¿ç­–å‰åçš„é¢æ¿æ•°æ®\n",
    "\n",
    "âŒ **ä¸é€‚åˆä½¿ç”¨ DID**ï¼š\n",
    "- æ²¡æœ‰åˆé€‚çš„å¯¹ç…§ç»„\n",
    "- æ”¿ç­–å‰è¶‹åŠ¿æ˜æ˜¾ä¸åŒ\n",
    "- åªæœ‰æˆªé¢æ•°æ®\n",
    "- å­˜åœ¨ä¸¥é‡çš„é¢„æœŸæ•ˆåº”\n",
    "\n",
    "### è¿›ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "**ç»å…¸è®ºæ–‡**ï¼š\n",
    "1. Card & Krueger (1994) - æœ€ä½å·¥èµ„ä¸å°±ä¸š\n",
    "2. Angrist & Pischke (2009) - *Mostly Harmless Econometrics*\n",
    "3. Callaway & Sant'Anna (2021) - äº¤é”™ DID\n",
    "4. Goodman-Bacon (2021) - TWFE çš„åˆ†è§£\n",
    "\n",
    "**R/Python åŒ…**ï¼š\n",
    "- Python: `linearmodels`, `pyfixest`\n",
    "- R: `did`, `fixest`, `bacondecomp`\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œä½ å®Œæˆäº† DID çš„å­¦ä¹ ï¼**\n",
    "\n",
    "DID æ˜¯å› æœæ¨æ–­ä¸­æœ€å¸¸ç”¨çš„æ–¹æ³•ä¹‹ä¸€ï¼ŒæŒæ¡å®ƒå°†å¤§å¤§æå‡ä½ åˆ†ææ”¿ç­–æ•ˆåº”çš„èƒ½åŠ›ã€‚\n",
    "\n",
    "ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦ä¸€ä¸ªå¼ºå¤§çš„å‡†å®éªŒæ–¹æ³•ï¼š**å›å½’æ–­ç‚¹è®¾è®¡ (RDD)**ã€‚\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}