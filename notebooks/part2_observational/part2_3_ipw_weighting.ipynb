{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš–ï¸ ç¬¬äºŒç«  ç»ƒä¹  2: é€†æ¦‚ç‡åŠ æƒ (Inverse Probability Weighting, IPW)\n",
    "\n",
    "---\n",
    "\n",
    "## ä»ã€Œä¸¢å¼ƒæ ·æœ¬ã€åˆ°ã€Œé‡æ–°åŠ æƒã€\n",
    "\n",
    "åœ¨ä¸Šä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº† PSMã€‚å®ƒå¾ˆç›´è§‚ï¼Œä½†æœ‰ä¸€ä¸ªé—®é¢˜ï¼š\n",
    "\n",
    "> å¾ˆå¤šæ§åˆ¶ç»„çš„æ ·æœ¬æ²¡æœ‰è¢«åŒ¹é…ä¸Šï¼Œè¢«ç™½ç™½ä¸¢å¼ƒäº†ï¼\n",
    "\n",
    "æœ‰æ²¡æœ‰åŠæ³•åˆ©ç”¨æ‰€æœ‰æ ·æœ¬å‘¢ï¼Ÿæœ‰çš„ï¼è¿™å°±æ˜¯**é€†æ¦‚ç‡åŠ æƒ (IPW)**ã€‚\n",
    "\n",
    "### ä¸€ä¸ªæŠ•ç¥¨çš„ç±»æ¯” ğŸ—³ï¸\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹é€‰ä¸¾æ°‘è°ƒï¼š\n",
    "\n",
    "ä½ æƒ³çŸ¥é“å…¨å›½äººæ°‘å¯¹æŸå€™é€‰äººçš„æ”¯æŒç‡ï¼Œä½†ä½ åªèƒ½è°ƒæŸ¥åˆ°ä¸€éƒ¨åˆ†äººã€‚é—®é¢˜æ˜¯ï¼š\n",
    "- å¹´è½»äººæ›´æ„¿æ„å‚ä¸è°ƒæŸ¥ï¼ˆå“åº”ç‡é«˜ï¼‰\n",
    "- è€å¹´äººä¸å¤ªæ„¿æ„å‚ä¸ï¼ˆå“åº”ç‡ä½ï¼‰\n",
    "\n",
    "å¦‚æœç›´æ¥ç”¨æ ·æœ¬è®¡ç®—æ”¯æŒç‡ï¼Œä¼šåå‘å¹´è½»äººçš„è§‚ç‚¹ã€‚\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆ**: ç»™æ¯ä¸ªäººä¸€ä¸ªã€Œæƒé‡ã€ï¼\n",
    "- å¹´è½»äººæƒé‡å°ï¼ˆå› ä¸ºé‡‡æ ·æ—¶å·²ç»è¢«è¿‡åº¦ä»£è¡¨äº†ï¼‰\n",
    "- è€å¹´äººæƒé‡å¤§ï¼ˆå› ä¸ºé‡‡æ ·æ—¶è¢«ä½ä¼°äº†ï¼‰\n",
    "\n",
    "è¿™æ ·åŠ æƒå¹³å‡åï¼Œå°±èƒ½è¿˜åŸçœŸå®çš„æ”¯æŒç‡ï¼\n",
    "\n",
    "**IPW çš„æ€æƒ³å®Œå…¨ä¸€æ ·**ï¼šç»™é‚£äº›ã€Œä¸å¤ªå¯èƒ½æ¥å—å¤„ç†ä½†æ¥å—äº†ã€çš„äººæ›´å¤§æƒé‡ï¼Œç»™é‚£äº›ã€Œå¾ˆå¯èƒ½æ¥å—å¤„ç†ä¹Ÿæ¥å—äº†ã€çš„äººæ›´å°æƒé‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£ IPW çš„æ ¸å¿ƒæ€æƒ³ï¼šåˆ›é€ ã€Œä¼ªæ€»ä½“ã€\n",
    "2. å®ç° IPW æƒé‡è®¡ç®—\n",
    "3. ä½¿ç”¨åŠ æƒæ–¹æ³•ä¼°è®¡ ATE\n",
    "4. ç†è§£æç«¯æƒé‡çš„é—®é¢˜å’Œè§£å†³æ–¹æ³•\n",
    "5. è¯Šæ–­æƒé‡åˆ†å¸ƒå’Œæœ‰æ•ˆæ ·æœ¬é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒŸ Part 1: IPW çš„æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦åŠ æƒï¼Ÿ\n",
    "\n",
    "åœ¨è§‚æµ‹æ•°æ®ä¸­ï¼Œå¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„ç‰¹å¾åˆ†å¸ƒä¸åŒã€‚\n",
    "\n",
    "**ä¾‹å­**: ç ”ç©¶ã€Œè¯»ç ”ã€å¯¹ã€Œæ”¶å…¥ã€çš„å½±å“\n",
    "- è¯»ç ”çš„äººå¾€å¾€æœ¬ç§‘æˆç»©æ›´å¥½\n",
    "- æœ¬ç§‘æˆç»©å¥½çš„äººä¸è¯»ç ”æ”¶å…¥ä¹Ÿé«˜\n",
    "\n",
    "å¦‚æœç›´æ¥æ¯”è¾ƒï¼Œä¼šé«˜ä¼°è¯»ç ”çš„æ•ˆæœã€‚\n",
    "\n",
    "### IPW å¦‚ä½•å·¥ä½œï¼Ÿ\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡åŠ æƒï¼Œè®©å¤„ç†ç»„å’Œæ§åˆ¶ç»„ã€Œçœ‹èµ·æ¥ã€åƒæ˜¯éšæœºåˆ†é…çš„ã€‚\n",
    "\n",
    "æƒé‡å…¬å¼ï¼š\n",
    "\n",
    "$$w_i = \\frac{T_i}{e(X_i)} + \\frac{1-T_i}{1-e(X_i)}$$\n",
    "\n",
    "å…¶ä¸­ $e(X_i) = P(T=1|X_i)$ æ˜¯å€¾å‘å¾—åˆ†ã€‚\n",
    "\n",
    "### ç›´è§‰ç†è§£\n",
    "\n",
    "| æƒ…å†µ | æƒé‡ | ä¸ºä»€ä¹ˆï¼Ÿ |\n",
    "|-----|------|----------|\n",
    "| å¾ˆå¯èƒ½è¢«å¤„ç†ï¼Œä¹Ÿè¢«å¤„ç†äº† | å° | è¿™ç±»äººåœ¨å¤„ç†ç»„ä¸­å·²ç»å¾ˆå¤šäº† |\n",
    "| ä¸å¤ªå¯èƒ½è¢«å¤„ç†ï¼Œä½†è¢«å¤„ç†äº† | å¤§ | è¿™ç±»äººåœ¨å¤„ç†ç»„ä¸­å¾ˆç¨€å°‘ï¼Œä»£è¡¨äº†æ›´å¤šäºº |\n",
    "| å¾ˆå¯èƒ½è¢«å¤„ç†ï¼Œä½†æ²¡è¢«å¤„ç† | å¤§ | è¿™ç±»äººåœ¨æ§åˆ¶ç»„ä¸­å¾ˆç¨€å°‘ |\n",
    "| ä¸å¤ªå¯èƒ½è¢«å¤„ç†ï¼Œä¹Ÿæ²¡è¢«å¤„ç† | å° | è¿™ç±»äººåœ¨æ§åˆ¶ç»„ä¸­å·²ç»å¾ˆå¤šäº† |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ§® æ•°å­¦æ¨å¯¼\n\nIPW ä¼°è®¡å™¨çš„æ ¸å¿ƒå…¬å¼ï¼š\n\n$$\\hat{E}[Y(1)] = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{T_i Y_i}{e(X_i)}$$\n\n$$\\hat{E}[Y(0)] = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{(1-T_i) Y_i}{1-e(X_i)}$$\n\n$$\\widehat{ATE} = \\hat{E}[Y(1)] - \\hat{E}[Y(0)]$$\n\nä¸ºä»€ä¹ˆè¿™ä¸ªå…¬å¼æœ‰æ•ˆï¼Ÿ\n\nå› ä¸º $E\\left[\\frac{TY}{e(X)}\\right] = E\\left[E\\left[\\frac{TY}{e(X)}\\Big|X\\right]\\right] = E\\left[\\frac{E[TY|X]}{e(X)}\\right] = E\\left[\\frac{e(X) \\cdot E[Y|X,T=1]}{e(X)}\\right] = E[Y(1)]$\n\n---\n\n## ğŸ“ IPW æ— åæ€§è¯æ˜\n\n**å®šç†**: å¦‚æœæ»¡è¶³æ— æ··æ·†å‡è®¾å’Œæ­£å€¼å‡è®¾ï¼ŒIPW ä¼°è®¡é‡æ˜¯ ATE çš„æ— åä¼°è®¡ã€‚\n\n**è¯æ˜**:\n\næˆ‘ä»¬éœ€è¦è¯æ˜ $E\\left[\\frac{T \\cdot Y}{e(X)}\\right] = E[Y(1)]$\n\n**Step 1**: å¯¹ X æ¡ä»¶åŒ–\n$$E\\left[\\frac{T \\cdot Y}{e(X)}\\right] = E\\left[E\\left[\\frac{T \\cdot Y}{e(X)} \\Big| X\\right]\\right]$$\n\n**Step 2**: åœ¨ç»™å®š X çš„æ¡ä»¶ä¸‹ï¼Œ$e(X)$ æ˜¯å¸¸æ•°ï¼Œå¯ä»¥æå‡ºæ¥\n$$= E\\left[\\frac{1}{e(X)} \\cdot E[T \\cdot Y | X]\\right]$$\n\n**Step 3**: åˆ†è§£ $E[T \\cdot Y | X]$\n$$E[T \\cdot Y | X] = E[T \\cdot Y | X, T=1] \\cdot P(T=1|X) + E[T \\cdot Y | X, T=0] \\cdot P(T=0|X)$$\n\nç”±äº $T=0$ æ—¶ $T \\cdot Y = 0$ï¼Œæ‰€ä»¥ï¼š\n$$= E[Y | X, T=1] \\cdot P(T=1|X) = E[Y | X, T=1] \\cdot e(X)$$\n\n**Step 4**: ä»£å›åŸå¼\n$$E\\left[\\frac{1}{e(X)} \\cdot E[T \\cdot Y | X]\\right] = E\\left[\\frac{1}{e(X)} \\cdot E[Y|X,T=1] \\cdot e(X)\\right] = E[E[Y|X,T=1]]$$\n\n**Step 5**: åº”ç”¨æ— æ··æ·†å‡è®¾ $Y(1) \\perp T | X$\n$$E[Y|X,T=1] = E[Y(1)|X,T=1] = E[Y(1)|X]$$\n\n**Step 6**: ç»“è®º\n$$E\\left[\\frac{T \\cdot Y}{e(X)}\\right] = E[E[Y(1)|X]] = E[Y(1)]$$\n\nåŒç†å¯è¯ $E\\left[\\frac{(1-T) \\cdot Y}{1-e(X)}\\right] = E[Y(0)]$\n\nå› æ­¤ï¼š$$E[\\widehat{ATE}_{IPW}] = E[Y(1)] - E[Y(0)] = ATE$$\n\n**å…³é”®å‡è®¾**:\n1. **æ— æ··æ·†**: $(Y(0), Y(1)) \\perp T | X$\n2. **æ­£å€¼**: $0 < e(X) < 1$ ï¼ˆå¦åˆ™åˆ†æ¯ä¸º 0ï¼‰\n\n---\n\n## ğŸ“Š Horvitz-Thompson ä¼°è®¡é‡çš„è§†è§’\n\nIPW å®é™…ä¸Šæ˜¯ **Horvitz-Thompson ä¼°è®¡é‡**åœ¨å› æœæ¨æ–­ä¸­çš„åº”ç”¨ã€‚\n\nåœ¨æŠ½æ ·ç†è®ºä¸­ï¼Œå¦‚æœä¸ªä½“ i è¢«æŠ½ä¸­çš„æ¦‚ç‡æ˜¯ $\\pi_i$ï¼Œé‚£ä¹ˆæ€»ä½“å‡å€¼çš„æ— åä¼°è®¡æ˜¯ï¼š\n\n$$\\hat{\\mu} = \\frac{1}{N}\\sum_{i \\in \\text{sample}} \\frac{y_i}{\\pi_i}$$\n\nåœ¨å› æœæ¨æ–­ä¸­ï¼š\n- å¤„ç†ç»„çš„\"æŠ½æ ·æ¦‚ç‡\"æ˜¯ $e(X_i)$\n- æ§åˆ¶ç»„çš„\"æŠ½æ ·æ¦‚ç‡\"æ˜¯ $1-e(X_i)$\n\næ‰€ä»¥ IPW å°±æ˜¯åœ¨ç”¨ HT ä¼°è®¡é‡ä¼°è®¡ $E[Y(1)]$ å’Œ $E[Y(0)]$ï¼\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¬ Part 2: å®ç° IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ipw_data(n: int = 2000, confounding_strength: float = 1.5, seed: int = 42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆç”¨äº IPW å®éªŒçš„æ•°æ®\n",
    "    \n",
    "    çœŸå® ATE = 2\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç”Ÿæˆåå˜é‡\n",
    "    X1 = np.random.randn(n)\n",
    "    X2 = np.random.randn(n)\n",
    "    \n",
    "    # TODO: ç”Ÿæˆå€¾å‘å¾—åˆ†\n",
    "    # logit(e) = confounding_strength * (X1 + 0.5*X2)\n",
    "    propensity_logit = confounding_strength * (X1 + 0.5 * X2)\n",
    "    propensity = None  # ğŸ‘ˆ ä½ çš„ä»£ç : 1 / (1 + np.exp(-propensity_logit))\n",
    "    \n",
    "    # TODO: ç”Ÿæˆå¤„ç†\n",
    "    T = None  # ğŸ‘ˆ ä½ çš„ä»£ç : np.random.binomial(1, propensity)\n",
    "    \n",
    "    # TODO: ç”Ÿæˆç»“æœ\n",
    "    # Y = 5 + 2*T + 1.5*X1 + X2 + noise\n",
    "    true_ate = 2.0\n",
    "    Y = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'X1': X1, 'X2': X2, 'T': T, 'Y': Y,\n",
    "        'true_propensity': propensity\n",
    "    })\n",
    "    \n",
    "    return df, true_ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "def generate_ipw_data(n: int = 2000, confounding_strength: float = 1.5, seed: int = 42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆç”¨äº IPW å®éªŒçš„æ•°æ®\n",
    "    \n",
    "    çœŸå® ATE = 2\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ç”Ÿæˆåå˜é‡\n",
    "    X1 = np.random.randn(n)\n",
    "    X2 = np.random.randn(n)\n",
    "    \n",
    "    # ç”Ÿæˆå€¾å‘å¾—åˆ†\n",
    "    propensity_logit = confounding_strength * (X1 + 0.5 * X2)\n",
    "    propensity = 1 / (1 + np.exp(-propensity_logit))\n",
    "    \n",
    "    # ç”Ÿæˆå¤„ç†\n",
    "    T = np.random.binomial(1, propensity)\n",
    "    \n",
    "    # ç”Ÿæˆç»“æœ\n",
    "    # Y = 5 + 2*T + 1.5*X1 + X2 + noise\n",
    "    true_ate = 2.0\n",
    "    Y = 5 + true_ate * T + 1.5 * X1 + X2 + np.random.randn(n) * 0.5\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'X1': X1, 'X2': X2, 'T': T, 'Y': Y,\n",
    "        'true_propensity': propensity\n",
    "    })\n",
    "    \n",
    "    return df, true_ate\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ipw_weights(propensity: np.ndarray, treatment: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è®¡ç®— IPW æƒé‡\n",
    "    \n",
    "    æƒé‡å…¬å¼:\n",
    "    - å¤„ç†ç»„: w = 1 / e(X)\n",
    "    - æ§åˆ¶ç»„: w = 1 / (1 - e(X))\n",
    "    \n",
    "    å®Œæ•´å…¬å¼: w = T/e + (1-T)/(1-e)\n",
    "    \"\"\"\n",
    "    # TODO: è£å‰ªå€¾å‘å¾—åˆ†ä»¥é¿å…æç«¯æƒé‡\n",
    "    # å°†å€¾å‘å¾—åˆ†é™åˆ¶åœ¨ [0.01, 0.99] èŒƒå›´å†…\n",
    "    propensity_clipped = None  # ğŸ‘ˆ ä½ çš„ä»£ç : np.clip(propensity, 0.01, 0.99)\n",
    "    \n",
    "    # TODO: è®¡ç®—æƒé‡\n",
    "    # w = T/e + (1-T)/(1-e)\n",
    "    weights = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "def compute_ipw_weights(propensity: np.ndarray, treatment: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è®¡ç®— IPW æƒé‡\n",
    "    \"\"\"\n",
    "    # è£å‰ªå€¾å‘å¾—åˆ†ä»¥é¿å…æç«¯æƒé‡\n",
    "    propensity_clipped = np.clip(propensity, 0.01, 0.99)\n",
    "    \n",
    "    # è®¡ç®—æƒé‡\n",
    "    weights = (treatment / propensity_clipped + \n",
    "              (1 - treatment) / (1 - propensity_clipped))\n",
    "    \n",
    "    return weights\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ate_ipw(\n",
    "    Y: np.ndarray,\n",
    "    treatment: np.ndarray,\n",
    "    weights: np.ndarray\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ IPW ä¼°è®¡ ATE\n",
    "    \n",
    "    ä½¿ç”¨ Hajek ä¼°è®¡å™¨ï¼ˆå½’ä¸€åŒ–æƒé‡ï¼‰:\n",
    "    E[Y(1)] = Î£(Y * w * T) / Î£(w * T)\n",
    "    E[Y(0)] = Î£(Y * w * (1-T)) / Î£(w * (1-T))\n",
    "    \"\"\"\n",
    "    treated_mask = treatment == 1\n",
    "    control_mask = treatment == 0\n",
    "    \n",
    "    # TODO: è®¡ç®—åŠ æƒçš„ E[Y(1)]\n",
    "    # åˆ†å­: sum(Y * weight) for treated\n",
    "    # åˆ†æ¯: sum(weight) for treated\n",
    "    y1_weighted = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: è®¡ç®—åŠ æƒçš„ E[Y(0)]\n",
    "    y0_weighted = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: è®¡ç®— ATE\n",
    "    ate = None  # ğŸ‘ˆ ä½ çš„ä»£ç : y1_weighted - y0_weighted\n",
    "    \n",
    "    if ate is None:\n",
    "        return None, None\n",
    "    \n",
    "    # è®¡ç®—æ ‡å‡†è¯¯ï¼ˆå½±å“å‡½æ•°æ–¹æ³•ï¼‰\n",
    "    n = len(Y)\n",
    "    influence_1 = np.zeros(n)\n",
    "    influence_1[treated_mask] = (Y[treated_mask] - y1_weighted) * weights[treated_mask]\n",
    "    \n",
    "    influence_0 = np.zeros(n)\n",
    "    influence_0[control_mask] = (Y[control_mask] - y0_weighted) * weights[control_mask]\n",
    "    \n",
    "    influence = influence_1 - influence_0\n",
    "    se = np.sqrt(np.var(influence) / n)\n",
    "    \n",
    "    return ate, se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "def estimate_ate_ipw(\n",
    "    Y: np.ndarray,\n",
    "    treatment: np.ndarray,\n",
    "    weights: np.ndarray\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ IPW ä¼°è®¡ ATE (Hajek ä¼°è®¡å™¨)\n",
    "    \"\"\"\n",
    "    treated_mask = treatment == 1\n",
    "    control_mask = treatment == 0\n",
    "    \n",
    "    # è®¡ç®—åŠ æƒçš„ E[Y(1)]\n",
    "    y1_weighted = (Y[treated_mask] * weights[treated_mask]).sum() / weights[treated_mask].sum()\n",
    "    \n",
    "    # è®¡ç®—åŠ æƒçš„ E[Y(0)]\n",
    "    y0_weighted = (Y[control_mask] * weights[control_mask]).sum() / weights[control_mask].sum()\n",
    "    \n",
    "    # è®¡ç®— ATE\n",
    "    ate = y1_weighted - y0_weighted\n",
    "    \n",
    "    # è®¡ç®—æ ‡å‡†è¯¯ï¼ˆå½±å“å‡½æ•°æ–¹æ³•ï¼‰\n",
    "    n = len(Y)\n",
    "    influence_1 = np.zeros(n)\n",
    "    influence_1[treated_mask] = (Y[treated_mask] - y1_weighted) * weights[treated_mask]\n",
    "    \n",
    "    influence_0 = np.zeros(n)\n",
    "    influence_0[control_mask] = (Y[control_mask] - y0_weighted) * weights[control_mask]\n",
    "    \n",
    "    influence = influence_1 - influence_0\n",
    "    se = np.sqrt(np.var(influence) / n)\n",
    "    \n",
    "    return ate, se\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• IPW\n",
    "df, true_ate = generate_ipw_data(n=2000, confounding_strength=1.5)\n",
    "\n",
    "if df['Y'] is not None:\n",
    "    X = df[['X1', 'X2']].values\n",
    "    T = df['T'].values\n",
    "    Y = df['Y'].values\n",
    "    \n",
    "    # æœ´ç´ ä¼°è®¡\n",
    "    naive_ate = df[df['T']==1]['Y'].mean() - df[df['T']==0]['Y'].mean()\n",
    "    print(f\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"   æ ·æœ¬é‡: {len(df)}\")\n",
    "    print(f\"   å¤„ç†ç»„: {T.sum()} ({T.mean()*100:.1f}%)\")\n",
    "    print(f\"   çœŸå® ATE: {true_ate}\")\n",
    "    print(f\"   æœ´ç´  ATE: {naive_ate:.4f} (åå·®: {naive_ate - true_ate:+.4f})\")\n",
    "    \n",
    "    # ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X, T)\n",
    "    propensity = lr.predict_proba(X)[:, 1]\n",
    "    \n",
    "    print(f\"\\nå€¾å‘å¾—åˆ†èŒƒå›´: [{propensity.min():.4f}, {propensity.max():.4f}]\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ generate_ipw_data å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®— IPW æƒé‡\n",
    "if df['Y'] is not None:\n",
    "    weights = compute_ipw_weights(propensity, T)\n",
    "    \n",
    "    if weights is not None:\n",
    "        print(\"âš–ï¸ IPW æƒé‡ç»Ÿè®¡:\")\n",
    "        print(f\"   èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "        print(f\"   å‡å€¼: {weights.mean():.4f}\")\n",
    "        print(f\"   å¤„ç†ç»„æƒé‡å‡å€¼: {weights[T==1].mean():.4f}\")\n",
    "        print(f\"   æ§åˆ¶ç»„æƒé‡å‡å€¼: {weights[T==0].mean():.4f}\")\n",
    "        \n",
    "        # è®¡ç®— IPW ATE\n",
    "        ipw_ate, ipw_se = estimate_ate_ipw(Y, T, weights)\n",
    "        \n",
    "        if ipw_ate is not None:\n",
    "            print(f\"\\nğŸ“ˆ IPW ATE ä¼°è®¡:\")\n",
    "            print(f\"   IPW ATE: {ipw_ate:.4f} Â± {ipw_se:.4f}\")\n",
    "            print(f\"   95% CI: [{ipw_ate - 1.96*ipw_se:.4f}, {ipw_ate + 1.96*ipw_se:.4f}]\")\n",
    "            print(f\"   åå·®: {ipw_ate - true_ate:+.4f}\")\n",
    "            \n",
    "            print(f\"\\nğŸ”¬ å¯¹æ¯”:\")\n",
    "            print(f\"   æœ´ç´ ä¼°è®¡åå·®: {naive_ate - true_ate:+.4f}\")\n",
    "            print(f\"   IPW ä¼°è®¡åå·®: {ipw_ate - true_ate:+.4f}\")\n",
    "            \n",
    "            if abs(ipw_ate - true_ate) < abs(naive_ate - true_ate):\n",
    "                print(f\"   âœ… IPW å‡å°‘äº† {abs(naive_ate - true_ate) - abs(ipw_ate - true_ate):.2f} çš„åå·®ï¼\")\n",
    "        else:\n",
    "            print(\"âŒ è¯·å®Œæˆ estimate_ate_ipw å‡½æ•°ï¼\")\n",
    "    else:\n",
    "        print(\"âŒ è¯·å®Œæˆ compute_ipw_weights å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Part 3: æƒé‡è¯Šæ–­\n",
    "\n",
    "### æç«¯æƒé‡çš„é—®é¢˜\n",
    "\n",
    "å½“å€¾å‘å¾—åˆ†æ¥è¿‘ 0 æˆ– 1 æ—¶ï¼Œæƒé‡ä¼šå˜å¾—éå¸¸å¤§ï¼Œå¯¼è‡´ï¼š\n",
    "- ä¼°è®¡ä¸ç¨³å®šï¼ˆæ–¹å·®å¤§ï¼‰\n",
    "- å°‘æ•°æ ·æœ¬ä¸»å¯¼ç»“æœ\n",
    "\n",
    "### æœ‰æ•ˆæ ·æœ¬é‡ (ESS)\n",
    "\n",
    "ESS è¡¡é‡ã€Œæœ‰å¤šå°‘æ ·æœ¬çœŸæ­£åœ¨èµ·ä½œç”¨ã€ï¼š\n",
    "\n",
    "$$ESS = \\frac{(\\sum w_i)^2}{\\sum w_i^2}$$\n",
    "\n",
    "- å½“æ‰€æœ‰æƒé‡ç›¸ç­‰æ—¶ï¼ŒESS = n\n",
    "- å½“æƒé‡å·®å¼‚å¾ˆå¤§æ—¶ï¼ŒESS << n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_effective_sample_size(weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    \n",
    "    ESS = (sum(w))^2 / sum(w^2)\n",
    "    \"\"\"\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    ess = None  # (weights.sum())**2 / (weights**2).sum()\n",
    "    return ess\n",
    "\n",
    "\n",
    "def diagnose_weights(weights: np.ndarray, treatment: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    è¯Šæ–­ IPW æƒé‡åˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    treated_weights = weights[treatment == 1]\n",
    "    control_weights = weights[treatment == 0]\n",
    "    \n",
    "    # TODO: è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    ess = compute_effective_sample_size(weights)\n",
    "    ess_fraction = ess / len(weights) if ess else None\n",
    "    \n",
    "    stats = {\n",
    "        'n_samples': len(weights),\n",
    "        'ess': ess,\n",
    "        'ess_fraction': ess_fraction,\n",
    "        'treated_weight_mean': treated_weights.mean(),\n",
    "        'treated_weight_max': treated_weights.max(),\n",
    "        'control_weight_mean': control_weights.mean(),\n",
    "        'control_weight_max': control_weights.max(),\n",
    "        'max_weight': weights.max(),\n",
    "        'weight_cv': weights.std() / weights.mean()  # coefficient of variation\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "def compute_effective_sample_size(weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\n",
    "    \n",
    "    ESS = (sum(w))^2 / sum(w^2)\n",
    "    \"\"\"\n",
    "    ess = (weights.sum())**2 / (weights**2).sum()\n",
    "    return ess\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æƒé‡è¯Šæ–­\n",
    "if weights is not None:\n",
    "    diag = diagnose_weights(weights, T)\n",
    "    \n",
    "    if diag['ess'] is not None:\n",
    "        print(\"ğŸ” æƒé‡è¯Šæ–­æŠ¥å‘Š:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"æ€»æ ·æœ¬é‡: {diag['n_samples']}\")\n",
    "        print(f\"æœ‰æ•ˆæ ·æœ¬é‡ (ESS): {diag['ess']:.1f} ({diag['ess_fraction']*100:.1f}%)\")\n",
    "        print(f\"\\nå¤„ç†ç»„æƒé‡:\")\n",
    "        print(f\"   å‡å€¼: {diag['treated_weight_mean']:.4f}\")\n",
    "        print(f\"   æœ€å¤§: {diag['treated_weight_max']:.4f}\")\n",
    "        print(f\"\\næ§åˆ¶ç»„æƒé‡:\")\n",
    "        print(f\"   å‡å€¼: {diag['control_weight_mean']:.4f}\")\n",
    "        print(f\"   æœ€å¤§: {diag['control_weight_max']:.4f}\")\n",
    "        print(f\"\\næƒé‡å˜å¼‚ç³»æ•° (CV): {diag['weight_cv']:.4f}\")\n",
    "        \n",
    "        if diag['ess_fraction'] < 0.5:\n",
    "            print(\"\\nâš ï¸ è­¦å‘Š: ESS å°äº 50%ï¼Œå¯èƒ½æœ‰æç«¯æƒé‡é—®é¢˜ï¼\")\n",
    "        else:\n",
    "            print(\"\\nâœ… æƒé‡åˆ†å¸ƒè‰¯å¥½\")\n",
    "        \n",
    "        # å¯è§†åŒ–æƒé‡åˆ†å¸ƒ\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        ax1 = axes[0]\n",
    "        ax1.hist(weights[T==1], bins=30, alpha=0.5, label='å¤„ç†ç»„', color='coral')\n",
    "        ax1.hist(weights[T==0], bins=30, alpha=0.5, label='æ§åˆ¶ç»„', color='steelblue')\n",
    "        ax1.set_xlabel('æƒé‡', fontsize=12)\n",
    "        ax1.set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "        ax1.set_title('IPW æƒé‡åˆ†å¸ƒ', fontsize=14)\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2 = axes[1]\n",
    "        ax2.scatter(propensity[T==1], weights[T==1], alpha=0.3, label='å¤„ç†ç»„', c='coral')\n",
    "        ax2.scatter(propensity[T==0], weights[T==0], alpha=0.3, label='æ§åˆ¶ç»„', c='steelblue')\n",
    "        ax2.set_xlabel('å€¾å‘å¾—åˆ†', fontsize=12)\n",
    "        ax2.set_ylabel('æƒé‡', fontsize=12)\n",
    "        ax2.set_title('æƒé‡ vs å€¾å‘å¾—åˆ†', fontsize=14)\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"âŒ è¯·å®Œæˆ compute_effective_sample_size å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ‚ï¸ Part 4: æƒé‡è£å‰ªä¸ç¨³å®šæƒé‡\n",
    "\n",
    "### æ–¹æ³• 1: è£å‰ªæç«¯æƒé‡\n",
    "\n",
    "å°†æƒé‡é™åˆ¶åœ¨æŸä¸ªç™¾åˆ†ä½æ•°ä»¥ä¸‹ï¼š\n",
    "\n",
    "$$w_i^{\\text{clipped}} = \\min(w_i, \\text{percentile}_{99})$$\n",
    "\n",
    "### æ–¹æ³• 2: ç¨³å®šæƒé‡ (Stabilized Weights)\n",
    "\n",
    "åœ¨åˆ†å­ä¸Šä¹˜ä»¥è¾¹é™…å¤„ç†æ¦‚ç‡ï¼š\n",
    "\n",
    "$$w_i^{\\text{stab}} = \\frac{P(T)}{e(X_i)} \\text{ for treated}, \\quad \\frac{1-P(T)}{1-e(X_i)} \\text{ for control}$$\n",
    "\n",
    "ç¨³å®šæƒé‡çš„å‡å€¼æ¥è¿‘ 1ï¼Œæ–¹å·®æ›´å°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_extreme_weights(weights: np.ndarray, percentile: float = 99) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è£å‰ªæç«¯æƒé‡\n",
    "    \"\"\"\n",
    "    # TODO: è®¡ç®—æƒé‡çš„ç™¾åˆ†ä½æ•°\n",
    "    max_weight = None  # ğŸ‘ˆ ä½ çš„ä»£ç : np.percentile(weights, percentile)\n",
    "    \n",
    "    # TODO: è£å‰ªæƒé‡\n",
    "    clipped = None  # ğŸ‘ˆ ä½ çš„ä»£ç : np.clip(weights, None, max_weight)\n",
    "    \n",
    "    return clipped\n",
    "\n",
    "\n",
    "def compute_stabilized_weights(propensity: np.ndarray, treatment: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç¨³å®šæƒé‡\n",
    "    \n",
    "    w_stab = P(T) / e(X) for treated\n",
    "    w_stab = (1-P(T)) / (1-e(X)) for control\n",
    "    \"\"\"\n",
    "    # TODO: è®¡ç®—è¾¹é™…å¤„ç†æ¦‚ç‡\n",
    "    marginal_prob = None  # ğŸ‘ˆ ä½ çš„ä»£ç : treatment.mean()\n",
    "    \n",
    "    # TODO: è£å‰ªå€¾å‘å¾—åˆ†\n",
    "    propensity_clipped = np.clip(propensity, 0.01, 0.99)\n",
    "    \n",
    "    # TODO: è®¡ç®—ç¨³å®šæƒé‡\n",
    "    weights_stab = np.zeros(len(treatment))\n",
    "    \n",
    "    # å¤„ç†ç»„: w = marginal_prob / propensity\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    # æ§åˆ¶ç»„: w = (1 - marginal_prob) / (1 - propensity)\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    return weights_stab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "def clip_extreme_weights(weights: np.ndarray, percentile: float = 99) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è£å‰ªæç«¯æƒé‡\n",
    "    \"\"\"\n",
    "    # è®¡ç®—æƒé‡çš„ç™¾åˆ†ä½æ•°\n",
    "    max_weight = np.percentile(weights, percentile)\n",
    "    \n",
    "    # è£å‰ªæƒé‡\n",
    "    clipped = np.clip(weights, None, max_weight)\n",
    "    \n",
    "    return clipped\n",
    "\n",
    "\n",
    "def compute_stabilized_weights(propensity: np.ndarray, treatment: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç¨³å®šæƒé‡\n",
    "    \"\"\"\n",
    "    # è®¡ç®—è¾¹é™…å¤„ç†æ¦‚ç‡\n",
    "    marginal_prob = treatment.mean()\n",
    "    \n",
    "    # è£å‰ªå€¾å‘å¾—åˆ†\n",
    "    propensity_clipped = np.clip(propensity, 0.01, 0.99)\n",
    "    \n",
    "    # è®¡ç®—ç¨³å®šæƒé‡\n",
    "    weights_stab = np.zeros(len(treatment))\n",
    "    \n",
    "    # å¤„ç†ç»„: w = marginal_prob / propensity\n",
    "    weights_stab[treatment == 1] = marginal_prob / propensity_clipped[treatment == 1]\n",
    "    \n",
    "    # æ§åˆ¶ç»„: w = (1 - marginal_prob) / (1 - propensity)\n",
    "    weights_stab[treatment == 0] = (1 - marginal_prob) / (1 - propensity_clipped[treatment == 0])\n",
    "    \n",
    "    return weights_stab\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¾ƒä¸åŒæƒé‡æ–¹æ³•\n",
    "if weights is not None:\n",
    "    # è£å‰ªæƒé‡\n",
    "    weights_clipped = clip_extreme_weights(weights, 99)\n",
    "    \n",
    "    # ç¨³å®šæƒé‡\n",
    "    weights_stab = compute_stabilized_weights(propensity, T)\n",
    "    \n",
    "    # å¯¹æ¯”\n",
    "    results = []\n",
    "    \n",
    "    for name, w in [('æ ‡å‡† IPW', weights), \n",
    "                    ('è£å‰ªæƒé‡ (99%)', weights_clipped),\n",
    "                    ('ç¨³å®šæƒé‡', weights_stab)]:\n",
    "        if w is not None and w.sum() > 0:\n",
    "            ate, se = estimate_ate_ipw(Y, T, w)\n",
    "            ess = compute_effective_sample_size(w)\n",
    "            \n",
    "            results.append({\n",
    "                'æ–¹æ³•': name,\n",
    "                'ATE': ate,\n",
    "                'SE': se,\n",
    "                'åå·®': ate - true_ate if ate else None,\n",
    "                'ESS': ess,\n",
    "                'Max Weight': w.max()\n",
    "            })\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"ğŸ”¬ ä¸åŒæƒé‡æ–¹æ³•çš„æ¯”è¾ƒ:\")\n",
    "        print(\"=\" * 70)\n",
    "        display(results_df.round(4))\n",
    "        \n",
    "        print(\"\\nğŸ’¡ è§‚å¯Ÿ:\")\n",
    "        print(\"   - ç¨³å®šæƒé‡å’Œè£å‰ªæƒé‡å¯ä»¥å‡å°‘æƒé‡æ–¹å·®\")\n",
    "        print(\"   - ESS è¶Šé«˜ï¼Œä¼°è®¡è¶Šç¨³å®š\")\n",
    "        print(\"   - ä½†è¿‡åº¦è£å‰ªå¯èƒ½å¼•å…¥åå·®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ Part 5: æ··æ·†å¼ºåº¦å®éªŒ\n",
    "\n",
    "è®©æˆ‘ä»¬çœ‹çœ‹åœ¨ä¸åŒæ··æ·†å¼ºåº¦ä¸‹ï¼ŒIPW çš„è¡¨ç°å¦‚ä½•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ipw_experiment(confounding_strengths: list = [0.5, 1.0, 1.5, 2.0, 2.5]):\n",
    "    \"\"\"\n",
    "    åœ¨ä¸åŒæ··æ·†å¼ºåº¦ä¸‹æµ‹è¯• IPW\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for strength in confounding_strengths:\n",
    "        df, true_ate = generate_ipw_data(n=2000, confounding_strength=strength)\n",
    "        \n",
    "        if df['Y'] is None:\n",
    "            continue\n",
    "            \n",
    "        X = df[['X1', 'X2']].values\n",
    "        T = df['T'].values\n",
    "        Y = df['Y'].values\n",
    "        \n",
    "        # æœ´ç´ ä¼°è®¡\n",
    "        naive_ate = df[df['T']==1]['Y'].mean() - df[df['T']==0]['Y'].mean()\n",
    "        \n",
    "        # ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(X, T)\n",
    "        propensity = lr.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # IPW ä¼°è®¡\n",
    "        weights = compute_ipw_weights(propensity, T)\n",
    "        if weights is not None:\n",
    "            ipw_ate, ipw_se = estimate_ate_ipw(Y, T, weights)\n",
    "            ess = compute_effective_sample_size(weights)\n",
    "        else:\n",
    "            ipw_ate, ipw_se, ess = None, None, None\n",
    "        \n",
    "        results.append({\n",
    "            'æ··æ·†å¼ºåº¦': strength,\n",
    "            'çœŸå® ATE': true_ate,\n",
    "            'æœ´ç´  ATE': naive_ate,\n",
    "            'æœ´ç´ åå·®': naive_ate - true_ate,\n",
    "            'IPW ATE': ipw_ate,\n",
    "            'IPW åå·®': ipw_ate - true_ate if ipw_ate else None,\n",
    "            'ESS': ess\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œå®éªŒ\n",
    "exp_results = run_ipw_experiment()\n",
    "\n",
    "if not exp_results.empty and exp_results['IPW ATE'].notna().any():\n",
    "    print(\"ğŸ“Š æ··æ·†å¼ºåº¦å®éªŒç»“æœ:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(exp_results.round(4))\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(exp_results['æ··æ·†å¼ºåº¦'], exp_results['æœ´ç´  ATE'], \n",
    "             'o-', label='æœ´ç´ ä¼°è®¡', color='red', markersize=8)\n",
    "    ax1.plot(exp_results['æ··æ·†å¼ºåº¦'], exp_results['IPW ATE'], \n",
    "             's-', label='IPW ä¼°è®¡', color='green', markersize=8)\n",
    "    ax1.axhline(2.0, color='blue', linestyle='--', label='çœŸå® ATE', linewidth=2)\n",
    "    ax1.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax1.set_ylabel('ATE ä¼°è®¡', fontsize=12)\n",
    "    ax1.set_title('ATE ä¼°è®¡ vs æ··æ·†å¼ºåº¦', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(exp_results['æ··æ·†å¼ºåº¦'] - 0.1, np.abs(exp_results['æœ´ç´ åå·®']), \n",
    "            width=0.2, label='æœ´ç´ ä¼°è®¡', color='red', alpha=0.7)\n",
    "    ax2.bar(exp_results['æ··æ·†å¼ºåº¦'] + 0.1, np.abs(exp_results['IPW åå·®']), \n",
    "            width=0.2, label='IPW ä¼°è®¡', color='green', alpha=0.7)\n",
    "    ax2.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax2.set_ylabel('|åå·®|', fontsize=12)\n",
    "    ax2.set_title('åå·®ç»å¯¹å€¼ vs æ··æ·†å¼ºåº¦', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "    print(\"   1. IPW åœ¨å„ç§æ··æ·†å¼ºåº¦ä¸‹éƒ½èƒ½æœ‰æ•ˆå‡å°‘åå·®\")\n",
    "    print(\"   2. æ··æ·†å¼ºåº¦è¶Šå¤§ï¼Œæœ´ç´ ä¼°è®¡åå·®è¶Šå¤§\")\n",
    "    print(\"   3. ä½†æ··æ·†å¼ºåº¦å¤§æ—¶ï¼ŒESS ä¼šä¸‹é™ï¼ˆæç«¯æƒé‡å¢å¤šï¼‰\")\n",
    "else:\n",
    "    print(\"è¯·å…ˆå®Œæˆå‰é¢çš„ TODO éƒ¨åˆ†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Part 6: æ€è€ƒé¢˜\n",
    "\n",
    "### é—®é¢˜ 1: IPW çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆé‡åŠ æƒå¯ä»¥å»é™¤æ··æ·†åå·®ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 2: IPW æƒé‡çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå¤„ç†ç»„ç”¨ 1/e(X)ï¼Œæ§åˆ¶ç»„ç”¨ 1/(1-e(X))ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 3: ä»€ä¹ˆæƒ…å†µä¸‹ä¼šå‡ºç°æç«¯æƒé‡ï¼Ÿæç«¯æƒé‡æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 4: æœ‰æ•ˆæ ·æœ¬é‡ (ESS) çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼ŸESS å°æ„å‘³ç€ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 5: IPW ç›¸æ¯” PSM æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### é—®é¢˜ 6: å¦‚æœå€¾å‘å¾—åˆ†æ¨¡å‹è¯¯è®¾å®šï¼ŒIPW ä¼°è®¡ä¼šæ€æ ·ï¼Ÿ\n\n**æç¤º:** è¿™å¼•å‡ºäº†ä¸‹ä¸€èŠ‚çš„ä¸»é¢˜â€”â€”åŒé‡ç¨³å¥ä¼°è®¡\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n\n---\n\n## ğŸ’» ä»é›¶å®ç° IPW\n\nè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå®Œæ•´çš„ IPW ä¼°è®¡å™¨ç±»ï¼"
  },
  {
   "cell_type": "code",
   "source": "class MyIPWEstimator:\n    \"\"\"\n    ä»é›¶å®ç°çš„ IPW ä¼°è®¡å™¨\n    \n    åŠŸèƒ½:\n    - ä¼°è®¡å€¾å‘å¾—åˆ†\n    - è®¡ç®— IPW æƒé‡ï¼ˆæ ‡å‡†/ç¨³å®š/è£å‰ªï¼‰\n    - ä¼°è®¡ ATE å’Œæ ‡å‡†è¯¯\n    - è¯Šæ–­æƒé‡åˆ†å¸ƒ\n    \"\"\"\n    \n    def __init__(self, trim_propensity: Tuple[float, float] = (0.01, 0.99),\n                 stabilize: bool = False,\n                 clip_weights_percentile: Optional[float] = None):\n        \"\"\"\n        Args:\n            trim_propensity: å€¾å‘å¾—åˆ†è£å‰ªèŒƒå›´\n            stabilize: æ˜¯å¦ä½¿ç”¨ç¨³å®šæƒé‡\n            clip_weights_percentile: æƒé‡è£å‰ªç™¾åˆ†ä½æ•°ï¼ˆå¦‚ 99ï¼‰\n        \"\"\"\n        self.trim_propensity = trim_propensity\n        self.stabilize = stabilize\n        self.clip_weights_percentile = clip_weights_percentile\n        self.propensity_ = None\n        self.weights_ = None\n        \n    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray):\n        \"\"\"æ‹Ÿåˆ IPW æ¨¡å‹\"\"\"\n        self.X_ = X\n        self.T_ = T\n        self.Y_ = Y\n        \n        # ä¼°è®¡å€¾å‘å¾—åˆ†\n        self.propensity_model_ = LogisticRegression(max_iter=1000)\n        self.propensity_model_.fit(X, T)\n        self.propensity_ = self.propensity_model_.predict_proba(X)[:, 1]\n        \n        # è£å‰ªå€¾å‘å¾—åˆ†\n        self.propensity_ = np.clip(self.propensity_, \n                                   self.trim_propensity[0], \n                                   self.trim_propensity[1])\n        \n        # è®¡ç®—æƒé‡\n        if self.stabilize:\n            self.weights_ = self._compute_stabilized_weights()\n        else:\n            self.weights_ = self._compute_weights()\n        \n        # è£å‰ªæƒé‡\n        if self.clip_weights_percentile is not None:\n            max_w = np.percentile(self.weights_, self.clip_weights_percentile)\n            self.weights_ = np.clip(self.weights_, None, max_w)\n        \n        return self\n    \n    def _compute_weights(self):\n        \"\"\"è®¡ç®—æ ‡å‡† IPW æƒé‡\"\"\"\n        weights = (self.T_ / self.propensity_ + \n                  (1 - self.T_) / (1 - self.propensity_))\n        return weights\n    \n    def _compute_stabilized_weights(self):\n        \"\"\"è®¡ç®—ç¨³å®šæƒé‡\"\"\"\n        marginal_prob = self.T_.mean()\n        weights = np.zeros(len(self.T_))\n        weights[self.T_ == 1] = marginal_prob / self.propensity_[self.T_ == 1]\n        weights[self.T_ == 0] = (1 - marginal_prob) / (1 - self.propensity_[self.T_ == 0])\n        return weights\n    \n    def estimate_ate(self) -> Tuple[float, float]:\n        \"\"\"ä¼°è®¡ ATEï¼ˆHajek ä¼°è®¡å™¨ï¼‰\"\"\"\n        if self.weights_ is None:\n            raise ValueError(\"Must fit before estimating ATE\")\n        \n        treated_mask = self.T_ == 1\n        control_mask = self.T_ == 0\n        \n        # Hajek ä¼°è®¡å™¨ï¼ˆå½’ä¸€åŒ–æƒé‡ï¼‰\n        y1_hat = (self.Y_[treated_mask] * self.weights_[treated_mask]).sum() / \\\n                 self.weights_[treated_mask].sum()\n        \n        y0_hat = (self.Y_[control_mask] * self.weights_[control_mask]).sum() / \\\n                 self.weights_[control_mask].sum()\n        \n        ate = y1_hat - y0_hat\n        \n        # è®¡ç®—æ ‡å‡†è¯¯ï¼ˆåŸºäºå½±å“å‡½æ•°ï¼‰\n        n = len(self.Y_)\n        influence_1 = np.zeros(n)\n        influence_1[treated_mask] = (self.Y_[treated_mask] - y1_hat) * \\\n                                    self.weights_[treated_mask] / self.weights_[treated_mask].sum()\n        \n        influence_0 = np.zeros(n)\n        influence_0[control_mask] = (self.Y_[control_mask] - y0_hat) * \\\n                                    self.weights_[control_mask] / self.weights_[control_mask].sum()\n        \n        influence = influence_1 - influence_0\n        se = np.sqrt((influence ** 2).sum()) / np.sqrt(n)\n        \n        return ate, se\n    \n    def compute_ess(self) -> float:\n        \"\"\"è®¡ç®—æœ‰æ•ˆæ ·æœ¬é‡\"\"\"\n        if self.weights_ is None:\n            raise ValueError(\"Must fit before computing ESS\")\n        \n        return (self.weights_.sum() ** 2) / (self.weights_ ** 2).sum()\n    \n    def diagnose(self) -> Dict:\n        \"\"\"è¯Šæ–­æƒé‡åˆ†å¸ƒ\"\"\"\n        if self.weights_ is None:\n            raise ValueError(\"Must fit before diagnosing\")\n        \n        treated_weights = self.weights_[self.T_ == 1]\n        control_weights = self.weights_[self.T_ == 0]\n        \n        return {\n            'n_samples': len(self.Y_),\n            'ess': self.compute_ess(),\n            'ess_fraction': self.compute_ess() / len(self.Y_),\n            'weight_mean': self.weights_.mean(),\n            'weight_std': self.weights_.std(),\n            'weight_min': self.weights_.min(),\n            'weight_max': self.weights_.max(),\n            'weight_cv': self.weights_.std() / self.weights_.mean(),\n            'treated_weight_mean': treated_weights.mean(),\n            'treated_weight_max': treated_weights.max(),\n            'control_weight_mean': control_weights.mean(),\n            'control_weight_max': control_weights.max(),\n        }\n    \n    def summary(self):\n        \"\"\"æ‰“å°ä¼°è®¡ç»“æœæ‘˜è¦\"\"\"\n        ate, se = self.estimate_ate()\n        diag = self.diagnose()\n        \n        print(\"=\" * 60)\n        print(\"IPW ä¼°è®¡ç»“æœ\")\n        print(\"=\" * 60)\n        print(f\"ATE ä¼°è®¡: {ate:.4f} Â± {se:.4f}\")\n        print(f\"95% CI: [{ate - 1.96*se:.4f}, {ate + 1.96*se:.4f}]\")\n        print()\n        print(f\"æ ·æœ¬é‡: {diag['n_samples']}\")\n        print(f\"æœ‰æ•ˆæ ·æœ¬é‡ (ESS): {diag['ess']:.1f} ({diag['ess_fraction']*100:.1f}%)\")\n        print()\n        print(f\"æƒé‡ç»Ÿè®¡:\")\n        print(f\"  èŒƒå›´: [{diag['weight_min']:.4f}, {diag['weight_max']:.4f}]\")\n        print(f\"  å‡å€¼: {diag['weight_mean']:.4f}\")\n        print(f\"  æ ‡å‡†å·®: {diag['weight_std']:.4f}\")\n        print(f\"  å˜å¼‚ç³»æ•°: {diag['weight_cv']:.4f}\")\n        print(\"=\" * 60)\n\n\n# æµ‹è¯•æˆ‘ä»¬çš„å®ç°\nprint(\"ğŸ§ª æµ‹è¯•è‡ªå·±å®ç°çš„ IPW ä¼°è®¡å™¨\\n\")\n\n# ç”Ÿæˆæ•°æ®\nnp.random.seed(42)\nn = 1500\nX_test = np.random.randn(n, 3)\npropensity_logit = 1.5 * (X_test[:, 0] + 0.5 * X_test[:, 1])\npropensity_true = 1 / (1 + np.exp(-propensity_logit))\nT_test = np.random.binomial(1, propensity_true)\nY_test = 2.0 * T_test + 1.5 * X_test[:, 0] + X_test[:, 1] + np.random.randn(n) * 0.5\n\nprint(f\"çœŸå® ATE = 2.0\\n\")\n\n# æµ‹è¯•ä¸‰ç§é…ç½®\nconfigs = [\n    {\"name\": \"æ ‡å‡† IPW\", \"params\": {}},\n    {\"name\": \"ç¨³å®šæƒé‡ IPW\", \"params\": {\"stabilize\": True}},\n    {\"name\": \"è£å‰ªæƒé‡ IPW (99%)\", \"params\": {\"clip_weights_percentile\": 99}}\n]\n\nfor config in configs:\n    print(f\"\\n{'='*60}\")\n    print(f\"é…ç½®: {config['name']}\")\n    print(f\"{'='*60}\")\n    ipw = MyIPWEstimator(**config['params'])\n    ipw.fit(X_test, T_test, Y_test)\n    ipw.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ¤ é«˜é¢‘é¢è¯•é¢˜\n\n### é¢è¯•é¢˜ 1: IPW çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆé‡åŠ æƒå¯ä»¥å»é™¤æ··æ·†åå·®ï¼Ÿ\n\n**å‚è€ƒå›ç­”**:\n\nIPW çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**é‡æ–°åŠ æƒ**ï¼Œåˆ›é€ ä¸€ä¸ª\"ä¼ªæ€»ä½“\"ï¼Œåœ¨è¿™ä¸ªä¼ªæ€»ä½“ä¸­å¤„ç†æ˜¯éšæœºåˆ†é…çš„ã€‚\n\n**ç›´è§‚è§£é‡Š**:\n- åœ¨è§‚æµ‹æ•°æ®ä¸­ï¼ŒæŸäº›ç±»å‹çš„äººæ›´å¯èƒ½æ¥å—å¤„ç†ï¼ˆæ¯”å¦‚å¹´è½»äººæ›´å¯èƒ½ç”¨æ–°è¯ï¼‰\n- è¿™å¯¼è‡´å¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„äººç¾¤ä¸å¯æ¯”\n- IPW ç»™æ¯ä¸ªäººèµ‹äºˆæƒé‡ï¼š$w_i = \\frac{T_i}{e(X_i)} + \\frac{1-T_i}{1-e(X_i)}$\n- è¿™ä¸ªæƒé‡è®©\"ä¸å¤ªå¯èƒ½æ¥å—å¤„ç†ä½†æ¥å—äº†\"çš„äººè´¡çŒ®æ›´å¤§\n- åŠ æƒåçš„æ•°æ®å°±åƒéšæœºå®éªŒä¸€æ ·ï¼ŒX ä¸ T ç‹¬ç«‹äº†ï¼\n\n**æ•°å­¦æœ¬è´¨**: IPW æ˜¯ Horvitz-Thompson ä¼°è®¡é‡ï¼Œé€šè¿‡é€†æ¦‚ç‡åŠ æƒæ¥çº æ­£é€‰æ‹©åå·®ã€‚\n\n---\n\n### é¢è¯•é¢˜ 2: IPW çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ\n\n**å‚è€ƒå›ç­”**:\n\n**ä¸»è¦å±€é™æ€§**:\n\n1. **æç«¯æƒé‡é—®é¢˜**\n   - å½“ $e(X) \\approx 0$ æˆ– $e(X) \\approx 1$ æ—¶ï¼Œæƒé‡ $\\frac{1}{e(X)}$ æˆ– $\\frac{1}{1-e(X)}$ ä¼šéå¸¸å¤§\n   - å°‘æ•°å‡ ä¸ªæ ·æœ¬ä¸»å¯¼ç»“æœï¼Œä¼°è®¡ä¸ç¨³å®šï¼Œæ–¹å·®å·¨å¤§\n   \n2. **ä¾èµ–å€¾å‘å¾—åˆ†æ¨¡å‹**\n   - å¦‚æœå€¾å‘å¾—åˆ†æ¨¡å‹è¯¯è®¾å®šï¼ŒIPW ä¼°è®¡æœ‰å\n   - æ²¡æœ‰\"åŒé‡ç¨³å¥\"æ€§è´¨\n\n3. **æ•ˆç‡æŸå¤±**\n   - ç›¸æ¯”ç»“æœæ¨¡å‹æ–¹æ³•ï¼Œæ–¹å·®å¯èƒ½æ›´å¤§\n\n**è§£å†³æ–¹æ³•**:\n\n1. **æƒé‡è£å‰ª (Weight Trimming)**\n   ```python\n   max_weight = np.percentile(weights, 99)\n   weights_clipped = np.clip(weights, None, max_weight)\n   ```\n\n2. **ç¨³å®šæƒé‡ (Stabilized Weights)**\n   $$w_i^{stab} = \\frac{P(T=T_i)}{P(T=T_i|X_i)}$$\n   ç¨³å®šæƒé‡çš„å‡å€¼æ¥è¿‘ 1ï¼Œæ–¹å·®æ›´å°\n\n3. **ä¿®å‰ªå€¾å‘å¾—åˆ† (Propensity Score Trimming)**\n   - ä¸¢å¼ƒå€¾å‘å¾—åˆ†è¿‡äºæç«¯çš„æ ·æœ¬ï¼ˆå¦‚ <0.1 æˆ– >0.9ï¼‰\n\n4. **ä½¿ç”¨åŒé‡ç¨³å¥æ–¹æ³• (AIPW)**\n   - ç»“åˆ IPW å’Œç»“æœæ¨¡å‹ï¼Œæ›´ç¨³å¥\n\n---\n\n### é¢è¯•é¢˜ 3: ä»€ä¹ˆæ˜¯æœ‰æ•ˆæ ·æœ¬é‡(ESS)ï¼Ÿå®ƒçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\n\n**å‚è€ƒå›ç­”**:\n\n**å®šä¹‰**:\n$$ESS = \\frac{(\\sum w_i)^2}{\\sum w_i^2}$$\n\n**æ„ä¹‰**:\n- ESS è¡¡é‡\"æœ‰å¤šå°‘æ ·æœ¬åœ¨çœŸæ­£èµ·ä½œç”¨\"\n- å½“æ‰€æœ‰æƒé‡ç›¸ç­‰æ—¶ï¼ŒESS = nï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½æœ‰æ•ˆï¼‰\n- å½“æƒé‡å·®å¼‚å¾ˆå¤§æ—¶ï¼ŒESS << nï¼ˆå°‘æ•°æ ·æœ¬ä¸»å¯¼ï¼‰\n\n**ä¾‹å­**:\n- å¦‚æœ n=1000ï¼Œä½† ESS=100ï¼Œè¯´æ˜å®é™…ä¸Šåªæœ‰ 100 ä¸ªæ ·æœ¬çš„ä¿¡æ¯é‡\n- æ ‡å‡†è¯¯åº”è¯¥ç”¨ ESS è€Œä¸æ˜¯ n æ¥è®¡ç®—\n\n**ç»éªŒæ³•åˆ™**:\n- ESS / n > 0.5: è‰¯å¥½\n- ESS / n < 0.3: è­¦å‘Šï¼Œå¯èƒ½éœ€è¦ä¿®å‰ªæç«¯æƒé‡\n\n---\n\n### é¢è¯•é¢˜ 4: IPW ç›¸æ¯” PSM æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\n\n**å‚è€ƒå›ç­”**:\n\n| æ–¹é¢ | PSM | IPW |\n|------|-----|-----|\n| **æ ·æœ¬åˆ©ç”¨** | ä¸¢å¼ƒæœªåŒ¹é…æ ·æœ¬ | ä½¿ç”¨æ‰€æœ‰æ ·æœ¬ âœ… |\n| **ä¼°è®¡é‡** | ATT | ATE âœ… |\n| **æç«¯å€¼** | è‡ªåŠ¨ä¸¢å¼ƒ âœ… | éœ€è¦å¤„ç†æç«¯æƒé‡ âŒ |\n| **ç›´è§‚æ€§** | å¾ˆç›´è§‚ âœ… | ç¨æŠ½è±¡ âŒ |\n| **æ–¹å·®** | é€šå¸¸è¾ƒå° âœ… | å¯èƒ½è¾ƒå¤§ âŒ |\n| **å¹³è¡¡æ£€æŸ¥** | æ˜“äºæ£€æŸ¥ âœ… | éœ€è¦åŠ æƒå¹³è¡¡æ£€æŸ¥ âŒ |\n\n**é€‰æ‹©å»ºè®®**:\n- å°æ•°æ®ã€éœ€è¦å¯è§£é‡Šæ€§ â†’ PSM\n- å¤§æ•°æ®ã€éœ€è¦ ATEã€æ‹…å¿ƒæ ·æœ¬æŸå¤± â†’ IPW\n- é«˜ç»´æ•°æ®ã€æ‹…å¿ƒæ¨¡å‹è¯¯è®¾å®š â†’ AIPWï¼ˆåŒé‡ç¨³å¥ï¼‰\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "| æ¦‚å¿µ | å…¬å¼ | ä½œç”¨ |\n",
    "|-----|------|------|\n",
    "| IPW æƒé‡ | $w = T/e + (1-T)/(1-e)$ | åˆ›é€ ä¼ªæ€»ä½“ |\n",
    "| æœ‰æ•ˆæ ·æœ¬é‡ | $ESS = (\\sum w)^2 / \\sum w^2$ | è¡¡é‡æƒé‡è´¨é‡ |\n",
    "| ç¨³å®šæƒé‡ | $w^{stab} = P(T)/e$ | å‡å°‘æƒé‡æ–¹å·® |\n",
    "\n",
    "### IPW vs PSM\n",
    "\n",
    "| æ–¹é¢ | PSM | IPW |\n",
    "|-----|-----|-----|\n",
    "| æ ·æœ¬åˆ©ç”¨ | ä¸¢å¼ƒæœªåŒ¹é…æ ·æœ¬ | ä½¿ç”¨æ‰€æœ‰æ ·æœ¬ |\n",
    "| ä¼°è®¡é‡ | ATT | ATE |\n",
    "| æç«¯å€¼å¤„ç† | è‡ªåŠ¨ä¸¢å¼ƒ | éœ€è¦è£å‰ªæƒé‡ |\n",
    "| ç›´è§‚æ€§ | å¾ˆç›´è§‚ | ç•¥æŠ½è±¡ |\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "IPW ä¾èµ–äºå€¾å‘å¾—åˆ†æ¨¡å‹çš„æ­£ç¡®æ€§ã€‚å¦‚æœæ¨¡å‹é”™äº†æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ **åŒé‡ç¨³å¥ä¼°è®¡ (AIPW)**â€”â€”ä¸€ç§åªè¦å€¾å‘å¾—åˆ†æ¨¡å‹æˆ–ç»“æœæ¨¡å‹ä¹‹ä¸€æ­£ç¡®å°±èƒ½å¾—åˆ°æ— åä¼°è®¡çš„æ–¹æ³•ï¼\n",
    "\n",
    "---\n",
    "\n",
    "**ã€Œç»™æ¯ä¸ªäººæ°å½“çš„æƒé‡ï¼Œè®©æ•°æ®å‘Šè¯‰æˆ‘ä»¬çœŸç›¸ã€‚ã€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}