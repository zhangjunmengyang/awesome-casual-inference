{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Chapter 6 练习 3: 用户定向干预 - 因果推断的终极应用\n",
    "\n",
    "## 把所有技术串起来!\n",
    "\n",
    "恭喜你来到最后一个练习！这里我们将综合运用前面学到的所有技术：\n",
    "\n",
    "- **CATE 估计**: T-Learner, X-Learner\n",
    "- **用户分群**: 识别高价值用户\n",
    "- **策略优化**: 成本-收益权衡\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 学习目标\n",
    "\n",
    "1. 掌握 T-Learner 和 X-Learner 方法\n",
    "2. 学习最优干预策略 (Policy Learning)\n",
    "3. 理解成本-收益权衡\n",
    "4. 进行敏感性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 场景: 网约车司机激励\n",
    "\n",
    "你是一家网约车平台的运营经理，需要决定：\n",
    "\n",
    "> **给哪些司机发奖励？每个奖励花费 ¥100，希望司机多在线，每多在线 1 小时平台能赚 ¥30**\n",
    "\n",
    "### 关键洞察\n",
    "\n",
    "不同司机对激励的反应不同：\n",
    "\n",
    "| 司机类型 | 激励效果 | 原因 |\n",
    "|----------|----------|------|\n",
    "| 兼职司机 | +2 小时 | 对激励敏感 |\n",
    "| 全职司机 | +0.5 小时 | 已经很活跃，边际效应小 |\n",
    "| 新手司机 | +2.5 小时 | 需要激励来建立习惯 |\n",
    "| 老司机 | +1 小时 | 已经有稳定习惯 |\n",
    "\n",
    "**关键**: 要找到那些「激励效果好」的司机，而不是「本来就活跃」的司机！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📐 核心公式\n\n### 最优决策规则\n\n$$\\text{干预} \\Leftrightarrow \\text{CATE}(x) \\times \\text{Value} > \\text{Cost}$$\n\n即：只有当预期收益超过成本时才干预。\n\n### X-Learner 公式\n\n**Stage 1**: 训练两个结果模型\n$$\\hat{\\mu}_0(x) = \\mathbb{E}[Y|X=x, T=0]$$\n$$\\hat{\\mu}_1(x) = \\mathbb{E}[Y|X=x, T=1]$$\n\n**Stage 2**: 计算伪处理效应\n$$D_i^1 = Y_i - \\hat{\\mu}_0(X_i) \\quad \\text{(处理组)}$$\n$$D_i^0 = \\hat{\\mu}_1(X_i) - Y_i \\quad \\text{(控制组)}$$\n\n然后训练模型:\n$$\\hat{\\tau}_1(x) = \\mathbb{E}[D^1|X=x]$$\n$$\\hat{\\tau}_0(x) = \\mathbb{E}[D^0|X=x]$$\n\n**Stage 3**: 加权组合\n$$\\hat{\\tau}(x) = g(x) \\cdot \\hat{\\tau}_0(x) + (1-g(x)) \\cdot \\hat{\\tau}_1(x)$$\n\n其中 $g(x) = P(T=1|X=x)$ 是倾向得分。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 环境设置\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom IPython.display import display\n\n# 设置随机种子\nnp.random.seed(42)\n\n# 绘图设置\nplt.rcParams['figure.figsize'] = [12, 6]\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use('default')\n\nprint(\"环境配置完成! 🎯\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习 3.1: 生成司机激励数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_driver_data(\n    n_samples: int = 2000,\n    seed: int = 42\n) -> pd.DataFrame:\n    \"\"\"\n    生成网约车司机激励数据\n    \n    场景: 平台给司机发放奖励，激励其增加在线时长\n    \n    异质性效应:\n    - 兼职司机: +2 小时 (对激励敏感)\n    - 全职司机: +0.5 小时 (已经很活跃)\n    - 新手司机 (订单<100): +2.5 小时 (需要激励建立习惯)\n    - 老司机 (订单≥300): +1 小时 (习惯稳定)\n    \"\"\"\n    np.random.seed(seed)\n    \n    # 生成司机特征\n    # rating: 评分 4.0-5.0\n    rating = np.random.beta(8, 1, n_samples) * 1.0 + 4.0\n    \n    # order_history: 历史完单数 (50-400)\n    order_history = np.random.poisson(200, n_samples)\n    order_history = np.clip(order_history, 50, 400)\n    \n    # is_fulltime: 是否全职 (30% 概率)\n    is_fulltime = np.random.binomial(1, 0.3, n_samples)\n    \n    # 随机分配处理\n    T = np.random.binomial(1, 0.5, n_samples)\n    \n    # 生成在线时长\n    online_hours = []\n    for i in range(n_samples):\n        # 基线时长\n        if is_fulltime[i] == 1:\n            base = 6 + np.random.randn() * 1.5\n        else:\n            base = 3 + np.random.randn() * 1.0\n        \n        # 激励效应 (异质性!)\n        if T[i] == 1:\n            # 全职 vs 兼职效应\n            if is_fulltime[i] == 1:\n                fulltime_effect = 0.5\n            else:\n                fulltime_effect = 2.0\n            \n            # 新手 vs 老司机效应\n            if order_history[i] < 100:\n                experience_effect = 2.5  # 新手\n            elif order_history[i] >= 300:\n                experience_effect = 1.0  # 老司机\n            else:\n                experience_effect = 1.75  # 中等经验\n            \n            # 综合效应: 取平均\n            effect = (fulltime_effect + experience_effect) / 2\n        else:\n            effect = 0\n        \n        online_hours.append(max(0, base + effect))\n    \n    return pd.DataFrame({\n        'rating': rating,\n        'order_history': order_history,\n        'is_fulltime': is_fulltime,\n        'T': T,\n        'online_hours': online_hours\n    })"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "df = generate_driver_data(n_samples=2000)\n",
    "\n",
    "if df is not None and df['rating'].iloc[0] is not None:\n",
    "    print(\"司机数据生成成功! 🚗\")\n",
    "    print(f\"\\n样本量: {len(df)}\")\n",
    "    print(f\"全职司机占比: {df['is_fulltime'].mean():.2%}\")\n",
    "    print(f\"平均在线时长: {df['online_hours'].mean():.1f} 小时\")\n",
    "    print(f\"激励组占比: {df['T'].mean():.2%}\")\n",
    "    \n",
    "    # 按组查看\n",
    "    print(f\"\\n=== 各组平均在线时长 ===\")\n",
    "    for is_ft in [0, 1]:\n",
    "        for t in [0, 1]:\n",
    "            mask = (df['is_fulltime']==is_ft) & (df['T']==t)\n",
    "            ft_str = '全职' if is_ft == 1 else '兼职'\n",
    "            t_str = '激励' if t == 1 else '无激励'\n",
    "            print(f\"  {ft_str} + {t_str}: {df.loc[mask, 'online_hours'].mean():.2f} 小时\")\n",
    "else:\n",
    "    print(\"[TODO] 请完成 generate_driver_data 函数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习 3.2: T-Learner CATE 估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TLearner:\n    \"\"\"\n    T-Learner: 分别训练处理组和控制组的模型\n    \"\"\"\n    \n    def __init__(self):\n        self.model_control = GradientBoostingRegressor(\n            n_estimators=50, max_depth=4, random_state=42\n        )\n        self.model_treatment = GradientBoostingRegressor(\n            n_estimators=50, max_depth=4, random_state=43\n        )\n    \n    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray):\n        \"\"\"训练 T-Learner\"\"\"\n        # 分离控制组和处理组\n        mask_control = (T == 0)\n        mask_treatment = (T == 1)\n        \n        # 训练两个模型\n        self.model_control.fit(X[mask_control], Y[mask_control])\n        self.model_treatment.fit(X[mask_treatment], Y[mask_treatment])\n        \n        return self\n    \n    def predict_cate(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"预测 CATE\"\"\"\n        # 分别预测\n        mu1 = self.model_treatment.predict(X)\n        mu0 = self.model_control.predict(X)\n        \n        # CATE = mu1 - mu0\n        cate = mu1 - mu0\n        \n        return cate"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 T-Learner\n",
    "if df is not None and df['rating'].iloc[0] is not None:\n",
    "    X = df[['rating', 'order_history', 'is_fulltime']].values\n",
    "    T = df['T'].values\n",
    "    Y = df['online_hours'].values\n",
    "    \n",
    "    try:\n",
    "        t_learner = TLearner()\n",
    "        t_learner.fit(X, T, Y)\n",
    "        cate_t = t_learner.predict_cate(X)\n",
    "        \n",
    "        if cate_t is not None:\n",
    "            print(\"T-Learner 训练成功! 🎉\")\n",
    "            print(f\"\\nCATE 统计:\")\n",
    "            print(f\"  范围: [{cate_t.min():.2f}, {cate_t.max():.2f}] 小时\")\n",
    "            print(f\"  均值: {cate_t.mean():.2f} 小时\")\n",
    "            print(f\"  标准差: {cate_t.std():.2f} 小时\")\n",
    "            \n",
    "            # 按司机类型查看\n",
    "            print(f\"\\n各类型司机的预测 CATE:\")\n",
    "            print(f\"  全职: {cate_t[df['is_fulltime']==1].mean():.2f} 小时\")\n",
    "            print(f\"  兼职: {cate_t[df['is_fulltime']==0].mean():.2f} 小时\")\n",
    "        else:\n",
    "            print(\"[TODO] 请完成 TLearner.predict_cate 方法\")\n",
    "    except Exception as e:\n",
    "        print(f\"[TODO] TLearner 实现有误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习 3.3: X-Learner CATE 估计\n",
    "\n",
    "X-Learner 是更高级的 Meta-Learner，通常比 T-Learner 更准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class XLearner:\n    \"\"\"\n    X-Learner: 额外学习伪处理效应\n    \"\"\"\n    \n    def __init__(self):\n        self.model_control = GradientBoostingRegressor(n_estimators=50, max_depth=4, random_state=42)\n        self.model_treatment = GradientBoostingRegressor(n_estimators=50, max_depth=4, random_state=43)\n        self.tau_control = GradientBoostingRegressor(n_estimators=30, max_depth=3, random_state=44)\n        self.tau_treatment = GradientBoostingRegressor(n_estimators=30, max_depth=3, random_state=45)\n        self.propensity_model = LogisticRegression(random_state=46)\n    \n    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray):\n        \"\"\"训练 X-Learner (三阶段)\"\"\"\n        mask_control = (T == 0)\n        mask_treatment = (T == 1)\n        \n        # Stage 1 - 训练 mu_0 和 mu_1\n        self.model_control.fit(X[mask_control], Y[mask_control])\n        self.model_treatment.fit(X[mask_treatment], Y[mask_treatment])\n        \n        # Stage 2 - 计算伪处理效应\n        # D_treatment = Y_treatment - mu_0(X_treatment)  # 处理组: 实际 - 反事实\n        # D_control = mu_1(X_control) - Y_control        # 控制组: 反事实 - 实际\n        D_treatment = Y[mask_treatment] - self.model_control.predict(X[mask_treatment])\n        D_control = self.model_treatment.predict(X[mask_control]) - Y[mask_control]\n        \n        # 训练 tau 模型\n        self.tau_treatment.fit(X[mask_treatment], D_treatment)\n        self.tau_control.fit(X[mask_control], D_control)\n        \n        # Stage 3 - 估计倾向得分\n        self.propensity_model.fit(X, T)\n        \n        return self\n    \n    def predict_cate(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"预测 CATE (使用倾向得分加权)\"\"\"\n        # 预测两个 tau\n        tau0 = self.tau_control.predict(X)\n        tau1 = self.tau_treatment.predict(X)\n        \n        # 估计倾向得分\n        propensity = self.propensity_model.predict_proba(X)[:, 1]\n        \n        # 加权组合\n        # cate = propensity * tau0 + (1 - propensity) * tau1\n        cate = propensity * tau0 + (1 - propensity) * tau1\n        \n        return cate"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 X-Learner\n",
    "if df is not None and df['rating'].iloc[0] is not None:\n",
    "    try:\n",
    "        x_learner = XLearner()\n",
    "        x_learner.fit(X, T, Y)\n",
    "        cate_x = x_learner.predict_cate(X)\n",
    "        \n",
    "        if cate_x is not None:\n",
    "            print(\"X-Learner 训练成功! 🎉\")\n",
    "            print(f\"\\nCATE 统计:\")\n",
    "            print(f\"  范围: [{cate_x.min():.2f}, {cate_x.max():.2f}] 小时\")\n",
    "            print(f\"  均值: {cate_x.mean():.2f} 小时\")\n",
    "            \n",
    "            if cate_t is not None:\n",
    "                corr = np.corrcoef(cate_t, cate_x)[0, 1]\n",
    "                print(f\"\\n与 T-Learner 的相关性: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"[TODO] 请完成 XLearner.predict_cate 方法\")\n",
    "    except Exception as e:\n",
    "        print(f\"[TODO] XLearner 实现有误: {e}\")\n",
    "        # 使用 T-Learner 的结果继续\n",
    "        if 'cate_t' in dir() and cate_t is not None:\n",
    "            cate_x = cate_t\n",
    "            print(\"使用 T-Learner 的结果继续...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习 3.4: 最优策略学习\n",
    "\n",
    "现在我们来学习最优的激励策略！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def learn_optimal_policy(\n    cate: np.ndarray,\n    cost_per_treatment: float = 100,\n    value_per_hour: float = 30\n) -> Tuple[np.ndarray, Dict]:\n    \"\"\"\n    学习最优干预策略\n    \n    决策规则: 当 CATE * value > cost 时进行干预\n    \n    Args:\n        cate: 估计的 CATE (在线时长增量)\n        cost_per_treatment: 每次激励的成本 (元)\n        value_per_hour: 每小时的价值 (元)\n    \"\"\"\n    # 计算阈值\n    # 多少小时增量才划算? cost / value\n    threshold = cost_per_treatment / value_per_hour\n    \n    # 最优策略\n    optimal_policy = (cate > threshold).astype(int)\n    \n    # 计算指标\n    n_treated = optimal_policy.sum()\n    expected_hours = cate[optimal_policy == 1].sum() if n_treated > 0 else 0\n    total_cost = n_treated * cost_per_treatment\n    total_value = expected_hours * value_per_hour\n    net_benefit = total_value - total_cost\n    roi = net_benefit / total_cost if total_cost > 0 else 0\n    \n    metrics = {\n        'n_treated': n_treated,\n        'treatment_rate': n_treated / len(cate),\n        'expected_hours': expected_hours,\n        'total_cost': total_cost,\n        'total_value': total_value,\n        'net_benefit': net_benefit,\n        'roi': roi,\n        'threshold': threshold\n    }\n    \n    return optimal_policy, metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习最优策略\n",
    "if 'cate_x' in dir() and cate_x is not None:\n",
    "    cate_to_use = cate_x if cate_x is not None else cate_t\n",
    "    \n",
    "    optimal_policy, metrics = learn_optimal_policy(\n",
    "        cate_to_use, \n",
    "        cost_per_treatment=100, \n",
    "        value_per_hour=30\n",
    "    )\n",
    "    \n",
    "    if optimal_policy is not None and metrics.get('total_cost') is not None:\n",
    "        print(\"最优策略学习成功! 🎯\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"\\n激励成本: ¥100/人\")\n",
    "        print(f\"每小时价值: ¥30\")\n",
    "        print(f\"阈值: {metrics['threshold']:.2f} 小时 (CATE 需要超过这个值才划算)\")\n",
    "        print(f\"\\n=== 最优策略结果 ===\")\n",
    "        print(f\"干预人数: {metrics['n_treated']}\")\n",
    "        print(f\"干预比例: {metrics['treatment_rate']:.2%}\")\n",
    "        print(f\"预期时长增量: {metrics['expected_hours']:.0f} 小时\")\n",
    "        print(f\"总成本: ¥{metrics['total_cost']:,.0f}\")\n",
    "        print(f\"总价值: ¥{metrics['total_value']:,.0f}\")\n",
    "        print(f\"净收益: ¥{metrics['net_benefit']:,.0f}\")\n",
    "        print(f\"ROI: {metrics['roi']:.2f}\")\n",
    "    else:\n",
    "        print(\"[TODO] 请完成 learn_optimal_policy 函数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习 3.5: 策略对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_targeting_strategies(\n    df: pd.DataFrame,\n    cate: np.ndarray,\n    cost: float = 100,\n    value: float = 30\n) -> pd.DataFrame:\n    \"\"\"\n    对比不同干预策略\n    \"\"\"\n    results = []\n    \n    # 策略 1 - No Treatment\n    results.append({\n        'strategy': 'No Treatment',\n        'n_treated': 0,\n        'cost': 0,\n        'value': 0,\n        'net_benefit': 0,\n        'roi': 0\n    })\n    \n    # 策略 2 - Treat All\n    n_all = len(df)\n    expected_hours_all = cate.sum()\n    cost_all = n_all * cost\n    value_all = expected_hours_all * value\n    results.append({\n        'strategy': 'Treat All',\n        'n_treated': n_all,\n        'cost': cost_all,\n        'value': value_all,\n        'net_benefit': value_all - cost_all,\n        'roi': (value_all - cost_all) / cost_all if cost_all > 0 else 0\n    })\n    \n    # 策略 3 - Treat Part-time Only (传统规则)\n    parttime_mask = df['is_fulltime'] == 0\n    n_parttime = parttime_mask.sum()\n    expected_hours_parttime = cate[parttime_mask].sum()\n    cost_parttime = n_parttime * cost\n    value_parttime = expected_hours_parttime * value\n    results.append({\n        'strategy': 'Treat Part-time',\n        'n_treated': n_parttime,\n        'cost': cost_parttime,\n        'value': value_parttime,\n        'net_benefit': value_parttime - cost_parttime,\n        'roi': (value_parttime - cost_parttime) / cost_parttime if cost_parttime > 0 else 0\n    })\n    \n    # 策略 4 - Optimal Policy\n    optimal_policy, optimal_metrics = learn_optimal_policy(cate, cost, value)\n    if optimal_policy is not None and optimal_metrics.get('total_cost') is not None:\n        results.append({\n            'strategy': 'Optimal Policy',\n            'n_treated': optimal_metrics['n_treated'],\n            'cost': optimal_metrics['total_cost'],\n            'value': optimal_metrics['total_value'],\n            'net_benefit': optimal_metrics['net_benefit'],\n            'roi': optimal_metrics['roi']\n        })\n    \n    return pd.DataFrame(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 策略对比\n",
    "if 'cate_to_use' in dir() and cate_to_use is not None:\n",
    "    comparison = compare_targeting_strategies(df, cate_to_use, cost=100, value=30)\n",
    "    \n",
    "    if comparison is not None and len(comparison) > 0:\n",
    "        print(\"策略对比:\")\n",
    "        print(\"=\"*70)\n",
    "        display(comparison.round(0))\n",
    "        \n",
    "        # 找最佳策略\n",
    "        best_idx = comparison['net_benefit'].idxmax()\n",
    "        best_strategy = comparison.loc[best_idx, 'strategy']\n",
    "        best_benefit = comparison.loc[best_idx, 'net_benefit']\n",
    "        \n",
    "        print(f\"\\n最佳策略: {best_strategy}\")\n",
    "        print(f\"净收益: ¥{best_benefit:,.0f}\")\n",
    "        \n",
    "        # 可视化\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        colors = ['#95a5a6', '#e74c3c', '#3498db', '#2ecc71']\n",
    "        \n",
    "        # 净收益对比\n",
    "        bars = axes[0].bar(comparison['strategy'], comparison['net_benefit'], color=colors)\n",
    "        axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[0].set_ylabel('净收益 (¥)')\n",
    "        axes[0].set_title('不同策略的净收益对比')\n",
    "        axes[0].tick_params(axis='x', rotation=15)\n",
    "        \n",
    "        # ROI 对比\n",
    "        bars = axes[1].bar(comparison['strategy'], comparison['roi'], color=colors)\n",
    "        axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[1].set_ylabel('ROI')\n",
    "        axes[1].set_title('不同策略的 ROI 对比')\n",
    "        axes[1].tick_params(axis='x', rotation=15)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习 3.6: 用户分层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def segment_by_cate(\n    df: pd.DataFrame,\n    cate: np.ndarray\n) -> pd.DataFrame:\n    \"\"\"\n    根据 CATE 将用户分层\n    \"\"\"\n    df = df.copy()\n    df['cate'] = cate\n    \n    # 计算分位数\n    p75 = np.percentile(cate, 75)\n    p25 = np.percentile(cate, 25)\n    \n    # 分层\n    segments = []\n    for c in cate:\n        if c >= p75:\n            segments.append('High Impact')\n        elif c >= p25:\n            segments.append('Medium Impact')\n        elif c > 0:\n            segments.append('Low Impact')\n        else:\n            segments.append('Negative Impact')\n    \n    df['segment'] = segments\n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户分层\n",
    "if 'cate_to_use' in dir() and cate_to_use is not None:\n",
    "    df_segmented = segment_by_cate(df, cate_to_use)\n",
    "    \n",
    "    print(\"用户分层结果:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n分层分布:\")\n",
    "    print(df_segmented['segment'].value_counts())\n",
    "    \n",
    "    print(f\"\\n各层统计:\")\n",
    "    for segment in ['High Impact', 'Medium Impact', 'Low Impact', 'Negative Impact']:\n",
    "        mask = df_segmented['segment'] == segment\n",
    "        if mask.sum() > 0:\n",
    "            avg_cate = df_segmented.loc[mask, 'cate'].mean()\n",
    "            avg_fulltime = df_segmented.loc[mask, 'is_fulltime'].mean()\n",
    "            avg_orders = df_segmented.loc[mask, 'order_history'].mean()\n",
    "            print(f\"\\n  {segment}:\")\n",
    "            print(f\"    平均 CATE: {avg_cate:.2f} 小时\")\n",
    "            print(f\"    全职比例: {avg_fulltime:.1%}\")\n",
    "            print(f\"    平均历史订单: {avg_orders:.0f}\")\n",
    "    \n",
    "    # 可视化\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    segment_order = ['High Impact', 'Medium Impact', 'Low Impact', 'Negative Impact']\n",
    "    colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
    "    \n",
    "    segment_cates = [df_segmented.loc[df_segmented['segment']==s, 'cate'].mean() \n",
    "                    for s in segment_order]\n",
    "    \n",
    "    bars = ax.bar(segment_order, segment_cates, color=colors)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_ylabel('平均 CATE (小时)')\n",
    "    ax.set_title('各用户分层的平均激励效果')\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 思考题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 思考题 1: T-Learner 和 X-Learner 的主要区别是什么？X-Learner 在什么情况下更优？\n\nanswer_1 = \"\"\"\n你的答案:\n\n\n\n\"\"\"\nprint(answer_1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 思考题 2: 最优策略 \"CATE × value > cost\" 的经济学直觉是什么？\n",
    "\n",
    "answer_2 = \"\"\"\n",
    "你的答案:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 思考题 3: 为什么要使用倾向得分来加权 X-Learner 的预测？\n",
    "\n",
    "answer_3 = \"\"\"\n",
    "你的答案:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 思考题 4: 在真实业务中，如何处理 CATE 估计的不确定性？\n",
    "\n",
    "answer_4 = \"\"\"\n",
    "你的答案:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(answer_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 📝 参考答案\n\n<details>\n<summary>思考题 1: T-Learner 和 X-Learner 的主要区别</summary>\n\n**主要区别:**\n\n1. **T-Learner (Two-Learner)**:\n   - 分别训练处理组和控制组的两个独立模型\n   - CATE = μ₁(x) - μ₀(x)\n   - 简单直观，但两个模型不共享信息\n\n2. **X-Learner**:\n   - 三个阶段：(1) 训练结果模型 (2) 学习伪处理效应 (3) 倾向得分加权\n   - 利用反事实预测来构造伪标签\n   - 使用倾向得分加权来平衡两个估计\n\n**X-Learner 更优的情况:**\n\n- **样本不平衡**: 处理组和控制组大小差异很大时\n- **小样本**: 需要更充分利用数据信息\n- **强混杂**: 协变量分布差异较大时\n\n**原因**: X-Learner 通过伪标签和倾向得分加权更好地借用了样本间的信息，在不平衡数据上表现更稳健。\n\n</details>\n\n<details>\n<summary>思考题 2: 最优策略经济学直觉</summary>\n\n**决策规则**: CATE × value > cost\n\n**经济学直觉:**\n\n1. **CATE × value** = 预期收益\n   - CATE: 干预带来的增量效果（如增加 2 小时在线时长）\n   - value: 每单位效果的价值（如每小时 ¥30）\n   - 预期收益 = 2 × 30 = ¥60\n\n2. **cost** = 干预成本\n   - 每次干预的直接成本（如 ¥100）\n\n3. **决策逻辑:**\n   - 如果 ¥60 > ¥100 → 不划算，不干预\n   - 如果 ¥150 > ¥100 → 划算，干预\n\n**核心原则**: **边际收益 > 边际成本** 时才行动，这是经济学的基本原则。\n\n**实践意义**: 不是对所有人干预，而是对那些「干预效果好且成本可覆盖」的人干预，最大化 ROI。\n\n</details>\n\n<details>\n<summary>思考题 3: 倾向得分加权的作用</summary>\n\n**为什么用倾向得分加权?**\n\n在 X-Learner 中，我们有两个 CATE 估计:\n- τ₀(x): 基于控制组训练的模型\n- τ₁(x): 基于处理组训练的模型\n\n**倾向得分 g(x) = P(T=1|X=x) 反映了:**\n- 样本在处理组的概率\n- 哪个组的样本更可靠\n\n**加权逻辑:**\n```\nτ̂(x) = g(x) · τ₀(x) + (1-g(x)) · τ₁(x)\n```\n\n- 如果 g(x) 接近 1（很可能被分到处理组）:\n  - 控制组样本稀缺，τ₀ 不太可靠\n  - 更依赖 τ₀（基于控制组样本估计的效应）\n  \n- 如果 g(x) 接近 0（很可能在控制组）:\n  - 处理组样本稀缺，τ₁ 不太可靠  \n  - 更依赖 τ₁（基于处理组样本估计的效应）\n\n**本质**: 倾向得分加权是一种**自适应融合**，根据样本分布自动调整两个估计的权重，提高估计的稳健性。\n\n</details>\n\n<details>\n<summary>思考题 4: 处理 CATE 估计的不确定性</summary>\n\n**不确定性来源:**\n\n1. **模型不确定性**: 模型选择、超参数\n2. **数据不确定性**: 样本量、噪声\n3. **异质性不确定性**: CATE 本身的变异\n\n**实践方法:**\n\n1. **Bootstrap 置信区间**:\n   ```python\n   # 重复采样 100 次\n   cate_samples = []\n   for b in range(100):\n       X_boot, T_boot, Y_boot = resample(X, T, Y)\n       model.fit(X_boot, T_boot, Y_boot)\n       cate_samples.append(model.predict_cate(X_test))\n   \n   # 计算 95% 置信区间\n   cate_lower = np.percentile(cate_samples, 2.5, axis=0)\n   cate_upper = np.percentile(cate_samples, 97.5, axis=0)\n   ```\n\n2. **保守决策阈值**:\n   ```python\n   # 使用置信下界做决策\n   optimal_policy = (cate_lower * value > cost)\n   ```\n\n3. **A/B 测试验证**:\n   - 在小规模用户上测试策略\n   - 观察实际效果是否与预测一致\n   - 逐步扩大规模\n\n4. **敏感性分析**:\n   - 调整成本/价值参数，观察策略稳定性\n   - 使用不同模型（T/X/S-Learner），对比一致性\n\n5. **分层验证**:\n   - 在高 CATE 和低 CATE 群体分别验证\n   - 确保效应真实存在\n\n**推荐流程**: Bootstrap CI → 保守阈值 → 小规模 A/B → 逐步扩量\n\n</details>\n\n<details>\n<summary>思考题 5: 激励疲劳的策略调整</summary>\n\n**激励疲劳现象:**\n- 第 1 次激励: +2 小时\n- 第 2 次激励: +1.5 小时  \n- 第 3 次激励: +1 小时\n- ...\n\n**策略调整方法:**\n\n1. **时间衰减模型**:\n   ```python\n   # CATE 随干预次数衰减\n   def get_dynamic_cate(x, n_interventions):\n       base_cate = model.predict_cate(x)\n       decay_factor = 0.8 ** n_interventions  # 每次衰减 20%\n       return base_cate * decay_factor\n   ```\n\n2. **动态阈值**:\n   ```python\n   # 阈值随干预次数提高\n   threshold = cost / (value * decay_factor)\n   # 第 1 次: 100/30 = 3.33 小时\n   # 第 2 次: 100/24 = 4.17 小时 (更严格)\n   ```\n\n3. **冷却期 (Cooldown)**:\n   ```python\n   # 记录上次干预时间\n   last_intervention_days = get_days_since_last_intervention(user)\n   \n   # 只对冷却期结束的用户干预\n   if last_intervention_days >= 30:  # 30 天冷却\n       if cate * value > cost:\n           intervene(user)\n   ```\n\n4. **干预频率上限**:\n   ```python\n   # 每用户每季度最多 3 次激励\n   intervention_count = get_quarterly_count(user)\n   if intervention_count < 3 and cate * value > cost:\n       intervene(user)\n   ```\n\n5. **轮换用户池**:\n   - 不总是对同一批高 CATE 用户干预\n   - 周期性轮换，给用户「休息」时间\n\n6. **效应监控**:\n   ```python\n   # 跟踪每次干预的实际效应\n   for user in users:\n       actual_effects = get_intervention_history(user)\n       if np.mean(actual_effects[-3:]) < threshold:\n           # 最近 3 次效应太低，暂停干预\n           blacklist.add(user)\n   ```\n\n**最优实践**: 冷却期 + 动态阈值 + 效应监控，平衡短期收益和长期用户健康。\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 总结\n",
    "\n",
    "### Meta-Learner 对比\n",
    "\n",
    "| 方法 | 复杂度 | 优点 | 缺点 |\n",
    "|------|--------|------|------|\n",
    "| S-Learner | 低 | 简单 | 可能无法捕获异质性 |\n",
    "| T-Learner | 中 | 直观 | 处理组和控制组不共享信息 |\n",
    "| X-Learner | 高 | 更准确，适合不平衡数据 | 更复杂 |\n",
    "\n",
    "### 最优策略公式\n",
    "\n",
    "$$\\pi^*(x) = \\mathbb{1}[\\hat{\\tau}(x) \\cdot \\text{value} > \\text{cost}]$$\n",
    "\n",
    "### 业务应用框架\n",
    "\n",
    "1. **数据准备**: 收集历史实验数据\n",
    "2. **CATE 估计**: 使用 T/X-Learner\n",
    "3. **用户分层**: 识别高价值用户\n",
    "4. **策略优化**: 成本-收益权衡\n",
    "5. **持续迭代**: A/B 测试验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 恭喜完成用户定向干预练习!\")\n",
    "print(\"\\n你已经学会了:\")\n",
    "print(\"  ✓ T-Learner 和 X-Learner CATE 估计\")\n",
    "print(\"  ✓ 最优干预策略学习\")\n",
    "print(\"  ✓ 成本-收益权衡\")\n",
    "print(\"  ✓ 用户分层分析\")\n",
    "print(\"\\n🎉 恭喜完成所有章节!\")\n",
    "print(\"\\n你现在已经掌握了:\")\n",
    "print(\"  ✓ 因果推断基础概念\")\n",
    "print(\"  ✓ 处理效应估计方法\")\n",
    "print(\"  ✓ Uplift 建模\")\n",
    "print(\"  ✓ 深度因果模型\")\n",
    "print(\"  ✓ 异质效应分析\")\n",
    "print(\"  ✓ 实际应用场景\")\n",
    "print(\"\\n继续探索，继续学习! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}