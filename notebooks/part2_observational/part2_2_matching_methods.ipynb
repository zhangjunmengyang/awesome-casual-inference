{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŒ¹é…æ–¹æ³• - æ„å»ºå¯æ¯”çš„å¯¹ç…§ç»„\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "1. ç†è§£åŒ¹é…æ–¹æ³•çš„åŸç†å’Œç›´è§‰\n",
    "2. æŒæ¡ç²¾ç¡®åŒ¹é…ã€PSMã€é©¬æ°è·ç¦»åŒ¹é…ç­‰ç®—æ³•\n",
    "3. å­¦ä¹ åŒ¹é…è´¨é‡è¯„ä¼°å’Œæ•ˆåº”ä¼°è®¡\n",
    "\n",
    "**ä¸šåŠ¡åœºæ™¯**ï¼šæŸç”µå•†å¹³å°æƒ³è¯„ä¼°ä¼šå‘˜æƒç›Šå¯¹ç”¨æˆ·æ¶ˆè´¹çš„å½±å“ã€‚ä¼šå‘˜ç”¨æˆ·å’Œéä¼šå‘˜ç”¨æˆ·åœ¨å¹´é¾„ã€è´­ç‰©é¢‘æ¬¡ã€å†å²æ¶ˆè´¹ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚å¦‚ä½•æ„å»ºå¯æ¯”çš„å¯¹ç…§ç»„ï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¯å¢ƒå‡†å¤‡\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nfrom scipy.spatial.distance import mahalanobis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# è®¾ç½®éšæœºç§å­\nnp.random.seed(42)\n\nprint(\"âœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: åŒ¹é…çš„ç›´è§‰\n",
    "\n",
    "### 1.1 ä¸ºä»€ä¹ˆéœ€è¦åŒ¹é…ï¼Ÿ\n",
    "\n",
    "åœ¨è§‚å¯Ÿæ€§æ•°æ®ä¸­ï¼Œå¤„ç†ç»„å’Œå¯¹ç…§ç»„å¾€å¾€åœ¨**åå˜é‡**ï¼ˆcovariatesï¼‰ä¸Šå­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼š\n",
    "\n",
    "- **é€‰æ‹©åå·®**ï¼šä¼šå‘˜ç”¨æˆ·å¯èƒ½æœ¬èº«å°±æ˜¯é«˜æ¶ˆè´¹äººç¾¤\n",
    "- **æ··æ·†å› ç´ **ï¼šå¹´é¾„ã€æ”¶å…¥ã€è´­ç‰©ä¹ æƒ¯éƒ½ä¼šå½±å“æ¶ˆè´¹\n",
    "- **ä¸å¯æ¯”æ€§**ï¼šç›´æ¥æ¯”è¾ƒä¸¤ç»„å‡å€¼ä¼šå¾—åˆ°æœ‰åä¼°è®¡\n",
    "\n",
    "**åŒ¹é…çš„ç›®æ ‡**ï¼šé€šè¿‡æ‰¾åˆ°åå˜é‡ç›¸ä¼¼çš„ä¸ªä½“ï¼Œæ¨¡æ‹ŸéšæœºåŒ–å®éªŒçš„æ¡ä»¶ã€‚\n",
    "\n",
    "### 1.2 åŒ¹é…ä¸éšæœºåŒ–\n",
    "\n",
    "| ç‰¹å¾ | éšæœºåŒ–å®éªŒ (RCT) | åŒ¹é…æ–¹æ³• |\n",
    "|------|-----------------|----------|\n",
    "| ç»„é—´å¯æ¯”æ€§ | æœŸæœ›ç›¸åŒ | åŒ¹é…åç›¸åŒ |\n",
    "| æ··æ·†æ§åˆ¶ | è‡ªåŠ¨å¹³è¡¡ | éœ€æ‰‹åŠ¨åŒ¹é… |\n",
    "| æ ·æœ¬åˆ©ç”¨ | å…¨éƒ¨ä½¿ç”¨ | å¯èƒ½ä¸¢å¼ƒ |\n",
    "| é€‚ç”¨åœºæ™¯ | å®éªŒæ•°æ® | è§‚å¯Ÿæ•°æ® |\n",
    "\n",
    "**æ ¸å¿ƒå‡è®¾**ï¼šæ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (Conditional Independence Assumption, CIA)\n",
    "\n",
    "$$\n",
    "(Y_0, Y_1) \\perp T \\mid X\n",
    "$$\n",
    "\n",
    "ç»™å®šåå˜é‡ $X$ï¼Œæ½œåœ¨ç»“æœä¸å¤„ç†åˆ†é…ç‹¬ç«‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼šä¼šå‘˜æƒç›Šè¯„ä¼°\ndef generate_membership_data(n=1000):\n    \"\"\"\n    ç”Ÿæˆç”µå•†ä¼šå‘˜æ•°æ®\n    - åå˜é‡ï¼šå¹´é¾„ã€å†å²æ¶ˆè´¹ã€è´­ç‰©é¢‘æ¬¡\n    - å¤„ç†ï¼šæ˜¯å¦ä¸ºä¼šå‘˜ (é€‰æ‹©åå·®ï¼šé«˜æ¶ˆè´¹è€…æ›´å¯èƒ½æˆä¸ºä¼šå‘˜)\n    - ç»“æœï¼šæœˆæ¶ˆè´¹é‡‘é¢\n    \"\"\"\n    # åå˜é‡\n    age = np.random.normal(35, 10, n).clip(18, 65)\n    hist_spending = np.random.gamma(5, 100, n)  # å†å²æ¶ˆè´¹\n    freq = np.random.poisson(3, n) + 1  # æœˆè´­ç‰©é¢‘æ¬¡\n    \n    # å¤„ç†åˆ†é…ï¼ˆé€‰æ‹©åå·®ï¼‰\n    propensity = 1 / (1 + np.exp(-(-3 + 0.05*age + 0.002*hist_spending + 0.3*freq)))\n    treatment = (np.random.uniform(0, 1, n) < propensity).astype(int)\n    \n    # æ½œåœ¨ç»“æœ\n    # Y(0): éä¼šå‘˜æ¶ˆè´¹ = f(åå˜é‡) + å™ªå£°\n    y0 = 200 + 5*age + 0.3*hist_spending + 50*freq + np.random.normal(0, 100, n)\n    \n    # Y(1): ä¼šå‘˜æ¶ˆè´¹ = Y(0) + çœŸå®å¤„ç†æ•ˆåº” (300å…ƒ)\n    true_effect = 300\n    y1 = y0 + true_effect\n    \n    # è§‚æµ‹ç»“æœ\n    y_obs = treatment * y1 + (1 - treatment) * y0\n    \n    df = pd.DataFrame({\n        'user_id': range(n),\n        'age': age,\n        'hist_spending': hist_spending,\n        'freq': freq,\n        'treatment': treatment,\n        'spending': y_obs,\n        'propensity': propensity,\n        'y0': y0,\n        'y1': y1\n    })\n    \n    return df, true_effect\n\ndf, true_ate = generate_membership_data(1000)\nprint(f\"æ ·æœ¬é‡: {len(df)}\")\nprint(f\"ä¼šå‘˜ç”¨æˆ·: {df['treatment'].sum()}\")\nprint(f\"éä¼šå‘˜ç”¨æˆ·: {(1-df['treatment']).sum()}\")\nprint(f\"\\nçœŸå® ATE: {true_ate}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯è§†åŒ–ï¼šåŒ¹é…å‰çš„ä¸å¹³è¡¡æ€§\ndef plot_covariate_balance(df, matched_df=None, title_suffix=\"\"):\n    \"\"\"\n    ç»˜åˆ¶åå˜é‡å¹³è¡¡æ€§å¯¹æ¯”\n    \"\"\"\n    covariates = ['age', 'hist_spending', 'freq']\n    labels = ['å¹´é¾„', 'å†å²æ¶ˆè´¹', 'è´­ç‰©é¢‘æ¬¡']\n    \n    fig = make_subplots(\n        rows=1, cols=3,\n        subplot_titles=labels,\n        specs=[[{'secondary_y': False}]*3]\n    )\n    \n    for i, (cov, label) in enumerate(zip(covariates, labels)):\n        # åŸå§‹æ•°æ®\n        treated = df[df['treatment']==1][cov]\n        control = df[df['treatment']==0][cov]\n        \n        fig.add_trace(\n            go.Histogram(x=control, name='å¯¹ç…§ç»„', opacity=0.5, \n                        marker_color='#EB5757', legendgroup='control',\n                        showlegend=(i==0), nbinsx=30),\n            row=1, col=i+1\n        )\n        fig.add_trace(\n            go.Histogram(x=treated, name='å¤„ç†ç»„', opacity=0.5,\n                        marker_color='#2D9CDB', legendgroup='treated',\n                        showlegend=(i==0), nbinsx=30),\n            row=1, col=i+1\n        )\n        \n        # å¦‚æœæä¾›äº†åŒ¹é…åæ•°æ®\n        if matched_df is not None:\n            matched_treated = matched_df[matched_df['treatment']==1][cov]\n            matched_control = matched_df[matched_df['treatment']==0][cov]\n            \n            fig.add_trace(\n                go.Histogram(x=matched_control, name='å¯¹ç…§ç»„(åŒ¹é…å)', \n                            opacity=0.7, marker_color='#F2994A',\n                            legendgroup='matched_control',\n                            showlegend=(i==0), nbinsx=30),\n                row=1, col=i+1\n            )\n            fig.add_trace(\n                go.Histogram(x=matched_treated, name='å¤„ç†ç»„(åŒ¹é…å)',\n                            opacity=0.7, marker_color='#27AE60',\n                            legendgroup='matched_treated',\n                            showlegend=(i==0), nbinsx=30),\n                row=1, col=i+1\n            )\n    \n    fig.update_layout(\n        title_text=f\"åå˜é‡åˆ†å¸ƒå¯¹æ¯”{title_suffix}\",\n        barmode='overlay',\n        height=400,\n        template='plotly_white'\n    )\n    fig.update_xaxes(title_text=\"å€¼\")\n    fig.update_yaxes(title_text=\"é¢‘æ•°\")\n    \n    return fig\n\nfig = plot_covariate_balance(df, title_suffix=\" - åŒ¹é…å‰\")\nfig.show()\n\n# è®¡ç®—æ ‡å‡†åŒ–å‡å€¼å·®\ndef compute_smd(df, covariates):\n    \"\"\"\n    è®¡ç®—æ ‡å‡†åŒ–å‡å€¼å·® (Standardized Mean Difference)\n    SMD = (mean_treated - mean_control) / sqrt((var_treated + var_control) / 2)\n    ä¸€èˆ¬è®¤ä¸º |SMD| < 0.1 è¡¨ç¤ºå¹³è¡¡è‰¯å¥½\n    \"\"\"\n    treated = df[df['treatment']==1]\n    control = df[df['treatment']==0]\n    \n    smds = {}\n    for cov in covariates:\n        mean_diff = treated[cov].mean() - control[cov].mean()\n        pooled_std = np.sqrt((treated[cov].var() + control[cov].var()) / 2)\n        smds[cov] = mean_diff / pooled_std\n    \n    return smds\n\ncovariates = ['age', 'hist_spending', 'freq']\nsmd_before = compute_smd(df, covariates)\nprint(\"\\nåŒ¹é…å‰çš„ SMD:\")\nfor cov, smd in smd_before.items():\n    print(f\"  {cov}: {smd:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è§‚å¯Ÿ**ï¼š\n",
    "- ä¼šå‘˜ç”¨æˆ·åœ¨æ‰€æœ‰åå˜é‡ä¸Šéƒ½æ˜¾è‘—é«˜äºéä¼šå‘˜\n",
    "- SMD ç»å¯¹å€¼éƒ½è¿œå¤§äº 0.1ï¼Œè¯´æ˜ä¸¤ç»„ä¸¥é‡ä¸å¹³è¡¡\n",
    "- ç›´æ¥æ¯”è¾ƒå‡å€¼ä¼šä¸¥é‡é«˜ä¼°ä¼šå‘˜æƒç›Šçš„æ•ˆæœ\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: ç²¾ç¡®åŒ¹é… (Exact Matching)\n",
    "\n",
    "### 2.1 åŸç†\n",
    "\n",
    "**ç²¾ç¡®åŒ¹é…**ï¼šä¸ºæ¯ä¸ªå¤„ç†ç»„ä¸ªä½“æ‰¾åˆ°åå˜é‡å®Œå…¨ç›¸åŒçš„å¯¹ç…§ç»„ä¸ªä½“ã€‚\n",
    "\n",
    "**ä¼˜ç‚¹**ï¼š\n",
    "- æ¦‚å¿µç®€å•\n",
    "- åŒ¹é…è´¨é‡é«˜\n",
    "\n",
    "**ç¼ºç‚¹**ï¼š\n",
    "- ç»´åº¦ç¾éš¾ï¼šåå˜é‡è¶Šå¤šï¼Œæ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„æ¦‚ç‡è¶Šä½\n",
    "- è¿ç»­å˜é‡éš¾ä»¥ç²¾ç¡®åŒ¹é…\n",
    "- æ ·æœ¬æµªè´¹ä¸¥é‡\n",
    "\n",
    "### 2.2 å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç²¾ç¡®åŒ¹é…ï¼ˆéœ€è¦å°†è¿ç»­å˜é‡ç¦»æ•£åŒ–ï¼‰\ndef exact_matching(df, covariates, bins_dict):\n    \"\"\"\n    ç²¾ç¡®åŒ¹é…\n    \n    å‚æ•°:\n        df: æ•°æ®æ¡†\n        covariates: åå˜é‡åˆ—è¡¨\n        bins_dict: æ¯ä¸ªåå˜é‡çš„åˆ†ç®±è¾¹ç•Œï¼Œä¾‹å¦‚ {'age': [0, 30, 40, 50, 100]}\n    \n    è¿”å›:\n        matched_df: åŒ¹é…åçš„æ•°æ®æ¡†\n    \"\"\"\n    df_copy = df.copy()\n    \n    # ç¦»æ•£åŒ–è¿ç»­å˜é‡\n    for cov, bins in bins_dict.items():\n        df_copy[f'{cov}_bin'] = pd.cut(df_copy[cov], bins=bins, labels=False)\n    \n    bin_cols = [f'{cov}_bin' for cov in bins_dict.keys()]\n    \n    # æŒ‰ç…§ç¦»æ•£åŒ–åçš„åå˜é‡åˆ†ç»„\n    matched_pairs = []\n    \n    treated = df_copy[df_copy['treatment']==1]\n    control = df_copy[df_copy['treatment']==0]\n    \n    for _, t_row in treated.iterrows():\n        # æ‰¾åˆ°æ‰€æœ‰åå˜é‡åŒ¹é…çš„å¯¹ç…§ç»„\n        mask = (control[bin_cols] == t_row[bin_cols]).all(axis=1)\n        matched_controls = control[mask]\n        \n        if len(matched_controls) > 0:\n            # éšæœºé€‰æ‹©ä¸€ä¸ªï¼ˆ1:1 åŒ¹é…ï¼‰\n            c_row = matched_controls.sample(1).iloc[0]\n            matched_pairs.append(t_row)\n            matched_pairs.append(c_row)\n    \n    matched_df = pd.DataFrame(matched_pairs)\n    return matched_df\n\n# å®šä¹‰åˆ†ç®±ç­–ç•¥\nbins_dict = {\n    'age': [0, 30, 40, 50, 100],\n    'hist_spending': [0, 300, 600, 1000, 10000],\n    'freq': [0, 2, 4, 6, 100]\n}\n\nmatched_exact = exact_matching(df, covariates, bins_dict)\nprint(f\"åŒ¹é…å‰æ ·æœ¬é‡: {len(df)}\")\nprint(f\"åŒ¹é…åæ ·æœ¬é‡: {len(matched_exact)} ({len(matched_exact)/len(df)*100:.1f}%)\")\nprint(f\"å¤„ç†ç»„ä¿ç•™ç‡: {matched_exact['treatment'].sum() / df['treatment'].sum() * 100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è¯„ä¼°åŒ¹é…è´¨é‡\nsmd_exact = compute_smd(matched_exact, covariates)\nprint(\"\\nç²¾ç¡®åŒ¹é…åçš„ SMD:\")\nfor cov, smd in smd_exact.items():\n    print(f\"  {cov}: {smd:.3f} (æ”¹å–„: {abs(smd_before[cov]) - abs(smd):.3f})\")\n\n# å¯è§†åŒ–\nfig = plot_covariate_balance(df, matched_exact, title_suffix=\" - ç²¾ç¡®åŒ¹é…å‰å\")\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ç²—åŒ–ç²¾ç¡®åŒ¹é… (Coarsened Exact Matching, CEM)\n",
    "\n",
    "**æ”¹è¿›æ€è·¯**ï¼šä¸è¦æ±‚å®Œå…¨ç²¾ç¡®åŒ¹é…ï¼Œè€Œæ˜¯å°†åå˜é‡ç²—åŒ–åå†åŒ¹é…ã€‚\n",
    "\n",
    "**ä¼˜ç‚¹**ï¼š\n",
    "- å¹³è¡¡æ ·æœ¬ä¿ç•™å’ŒåŒ¹é…è´¨é‡\n",
    "- å¯¹åˆ†ç®±ç­–ç•¥ä¸æ•æ„Ÿ\n",
    "- æ»¡è¶³å•è°ƒä¸å¹³è¡¡çº¦æŸ (Monotonic Imbalance Bounding)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: å€¾å‘å¾—åˆ†åŒ¹é… (Propensity Score Matching, PSM)\n",
    "\n",
    "### 3.1 å€¾å‘å¾—åˆ†çš„é­”åŠ›\n",
    "\n",
    "**å€¾å‘å¾—åˆ†** (Propensity Score):\n",
    "\n",
    "$$\n",
    "e(X) = P(T=1 \\mid X)\n",
    "$$\n",
    "\n",
    "å³ç»™å®šåå˜é‡ $X$ï¼Œä¸ªä½“æ¥å—å¤„ç†çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "**Rosenbaum & Rubin (1983) å®šç†**ï¼šå¦‚æœ $(Y_0, Y_1) \\perp T \\mid X$ï¼Œåˆ™ $(Y_0, Y_1) \\perp T \\mid e(X)$\n",
    "\n",
    "**å«ä¹‰**ï¼š\n",
    "- å¯ä»¥ç”¨ä¸€ç»´çš„å€¾å‘å¾—åˆ†ä»£æ›¿é«˜ç»´çš„åå˜é‡\n",
    "- å€¾å‘å¾—åˆ†ç›¸åŒçš„ä¸ªä½“åœ¨åå˜é‡åˆ†å¸ƒä¸Šæ˜¯å¹³è¡¡çš„\n",
    "- é™ç»´ï¼è§£å†³ç»´åº¦ç¾éš¾\n",
    "\n",
    "### 3.2 PSM æ­¥éª¤\n",
    "\n",
    "1. **ä¼°è®¡å€¾å‘å¾—åˆ†**ï¼šç”¨é€»è¾‘å›å½’ $P(T=1|X)$\n",
    "2. **æ£€æŸ¥å…±åŒæ”¯æ’‘** (Common Support)ï¼šåˆ é™¤å€¾å‘å¾—åˆ†ä¸é‡å çš„æ ·æœ¬\n",
    "3. **åŒ¹é…**ï¼šä¸ºæ¯ä¸ªå¤„ç†ç»„ä¸ªä½“æ‰¾åˆ°å€¾å‘å¾—åˆ†ç›¸è¿‘çš„å¯¹ç…§ç»„\n",
    "4. **è¯„ä¼°å¹³è¡¡æ€§**ï¼šæ£€æŸ¥åŒ¹é…ååå˜é‡æ˜¯å¦å¹³è¡¡\n",
    "5. **ä¼°è®¡æ•ˆåº”**ï¼šè®¡ç®—åŒ¹é…æ ·æœ¬çš„å‡å€¼å·®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: ä¼°è®¡å€¾å‘å¾—åˆ†\ndef estimate_propensity_score(df, covariates):\n    \"\"\"\n    ç”¨é€»è¾‘å›å½’ä¼°è®¡å€¾å‘å¾—åˆ†\n    \"\"\"\n    X = df[covariates]\n    y = df['treatment']\n    \n    model = LogisticRegression(max_iter=1000)\n    model.fit(X, y)\n    \n    ps = model.predict_proba(X)[:, 1]\n    return ps\n\ndf['ps_estimated'] = estimate_propensity_score(df, covariates)\n\n# å¯è§†åŒ–å€¾å‘å¾—åˆ†åˆ†å¸ƒ\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=df[df['treatment']==0]['ps_estimated'],\n    name='å¯¹ç…§ç»„',\n    opacity=0.6,\n    marker_color='#EB5757',\n    nbinsx=50\n))\nfig.add_trace(go.Histogram(\n    x=df[df['treatment']==1]['ps_estimated'],\n    name='å¤„ç†ç»„',\n    opacity=0.6,\n    marker_color='#2D9CDB',\n    nbinsx=50\n))\nfig.update_layout(\n    title='å€¾å‘å¾—åˆ†åˆ†å¸ƒ',\n    xaxis_title='å€¾å‘å¾—åˆ†',\n    yaxis_title='é¢‘æ•°',\n    barmode='overlay',\n    template='plotly_white',\n    height=400\n)\nfig.show()\n\n# å…±åŒæ”¯æ’‘åŒºåŸŸ\nps_treated = df[df['treatment']==1]['ps_estimated']\nps_control = df[df['treatment']==0]['ps_estimated']\ncommon_support_min = max(ps_treated.min(), ps_control.min())\ncommon_support_max = min(ps_treated.max(), ps_control.max())\n\nprint(f\"\\nå…±åŒæ”¯æ’‘åŒºåŸŸ: [{common_support_min:.3f}, {common_support_max:.3f}]\")\nprint(f\"å¤„ç†ç»„åœ¨æ”¯æ’‘åŒºåŸŸå†…: {((ps_treated >= common_support_min) & (ps_treated <= common_support_max)).sum()} / {len(ps_treated)}\")\nprint(f\"å¯¹ç…§ç»„åœ¨æ”¯æ’‘åŒºåŸŸå†…: {((ps_control >= common_support_min) & (ps_control <= common_support_max)).sum()} / {len(ps_control)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 åŒ¹é…ç®—æ³•\n",
    "\n",
    "#### (1) æœ€è¿‘é‚»åŒ¹é… (Nearest Neighbor Matching)\n",
    "\n",
    "ä¸ºæ¯ä¸ªå¤„ç†ç»„ä¸ªä½“ $i$ æ‰¾åˆ°å€¾å‘å¾—åˆ†æœ€æ¥è¿‘çš„å¯¹ç…§ç»„ä¸ªä½“ $j$ï¼š\n",
    "\n",
    "$$\n",
    "j^* = \\arg\\min_{j \\in \\{T=0\\}} |e(X_i) - e(X_j)|\n",
    "$$\n",
    "\n",
    "- **1:1 åŒ¹é…**ï¼šæ¯ä¸ªå¤„ç†ç»„ä¸ªä½“åŒ¹é… 1 ä¸ªå¯¹ç…§ç»„\n",
    "- **1:N åŒ¹é…**ï¼šæ¯ä¸ªå¤„ç†ç»„ä¸ªä½“åŒ¹é… N ä¸ªå¯¹ç…§ç»„ï¼ˆé™ä½æ–¹å·®ï¼Œä½†å¯èƒ½å¼•å…¥åå·®ï¼‰\n",
    "\n",
    "#### (2) å¡å°ºåŒ¹é… (Caliper Matching)\n",
    "\n",
    "åªåŒ¹é…å€¾å‘å¾—åˆ†åœ¨ä¸€å®šèŒƒå›´å†…çš„ä¸ªä½“ï¼š\n",
    "\n",
    "$$\n",
    "|e(X_i) - e(X_j)| < \\text{caliper}\n",
    "$$\n",
    "\n",
    "å¸¸ç”¨å¡å°ºï¼š$0.2 \\times \\text{std}(e(X))$\n",
    "\n",
    "#### (3) æœ‰æ”¾å› vs æ— æ”¾å›\n",
    "\n",
    "- **æ— æ”¾å›**ï¼šæ¯ä¸ªå¯¹ç…§ç»„ä¸ªä½“åªèƒ½åŒ¹é…ä¸€æ¬¡ï¼ˆåŒ¹é…è´¨é‡å¯èƒ½ä¸‹é™ï¼‰\n",
    "- **æœ‰æ”¾å›**ï¼šå¯¹ç…§ç»„ä¸ªä½“å¯ä»¥é‡å¤ä½¿ç”¨ï¼ˆæé«˜åŒ¹é…è´¨é‡ï¼Œä½†æ–¹å·®ä¼°è®¡éœ€è°ƒæ•´ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n```python\ndef psm_matching(df, caliper=0.2, ratio=1, with_replacement=False):\n    \"\"\"\n    å€¾å‘å¾—åˆ†åŒ¹é…\n    \n    å‚æ•°:\n        df: æ•°æ®æ¡†ï¼ˆéœ€åŒ…å« 'ps_estimated' åˆ—ï¼‰\n        caliper: å¡å°ºï¼ˆæ ‡å‡†å·®çš„å€æ•°ï¼‰\n        ratio: åŒ¹é…æ¯”ä¾‹ (1:ratio)\n        with_replacement: æ˜¯å¦æœ‰æ”¾å›\n    \n    è¿”å›:\n        matched_df: åŒ¹é…åçš„æ•°æ®æ¡†\n    \"\"\"\n    caliper_threshold = caliper * df['ps_estimated'].std()\n    \n    treated = df[df['treatment']==1].copy()\n    control = df[df['treatment']==0].copy()\n    \n    matched_pairs = []\n    used_controls = set()\n    \n    for _, t_row in treated.iterrows():\n        # è¿‡æ»¤å·²ä½¿ç”¨çš„å¯¹ç…§ç»„ï¼ˆå¦‚æœæ— æ”¾å›ï¼‰\n        if not with_replacement:\n            available_controls = control[~control['user_id'].isin(used_controls)]\n        else:\n            available_controls = control\n        \n        if len(available_controls) == 0:\n            continue\n        \n        # è®¡ç®—å€¾å‘å¾—åˆ†è·ç¦»\n        distances = np.abs(available_controls['ps_estimated'] - t_row['ps_estimated'])\n        \n        # æ‰¾åˆ°æœ€è¿‘çš„ ratio ä¸ª\n        n_matches = min(ratio, len(available_controls))\n        closest_indices = distances.nsmallest(n_matches).index\n        \n        # æ£€æŸ¥å¡å°º\n        for idx in closest_indices:\n            if distances[idx] <= caliper_threshold:\n                matched_pairs.append(t_row)\n                matched_pairs.append(available_controls.loc[idx])\n                used_controls.add(available_controls.loc[idx, 'user_id'])\n            else:\n                break  # å¦‚æœç¬¬ä¸€ä¸ªéƒ½è¶…å‡ºå¡å°ºï¼Œåé¢çš„ä¹Ÿä¸ç”¨çœ‹äº†\n    \n    matched_df = pd.DataFrame(matched_pairs)\n    return matched_df\n\n# æµ‹è¯•ä¸åŒå‚æ•°\nmatched_psm_11 = psm_matching(df, caliper=0.2, ratio=1, with_replacement=False)\nprint(f\"1:1 åŒ¹é…ï¼ˆæ— æ”¾å›ï¼‰: {len(matched_psm_11)} æ ·æœ¬\")\n\nmatched_psm_13 = psm_matching(df, caliper=0.2, ratio=3, with_replacement=True)\nprint(f\"1:3 åŒ¹é…ï¼ˆæœ‰æ”¾å›ï¼‰: {len(matched_psm_13)} æ ·æœ¬\")\n```\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è¯„ä¼° 1:1 PSM åŒ¹é…è´¨é‡ï¼ˆéœ€è¦å…ˆå®Œæˆ TODO 1ï¼‰\n# smd_psm = compute_smd(matched_psm_11, covariates)\n# print(\"\\nPSM (1:1) åŒ¹é…åçš„ SMD:\")\n# for cov, smd in smd_psm.items():\n#     print(f\"  {cov}: {smd:.3f} (æ”¹å–„: {abs(smd_before[cov]) - abs(smd):.3f})\")\n\n# # å¯è§†åŒ–\n# fig = plot_covariate_balance(df, matched_psm_11, title_suffix=\" - PSM åŒ¹é…å‰å\")\n# fig.show()\n\n# # å€¾å‘å¾—åˆ†åˆ†å¸ƒå¯¹æ¯”\n# fig = go.Figure()\n# fig.add_trace(go.Histogram(\n#     x=df[df['treatment']==0]['ps_estimated'],\n#     name='å¯¹ç…§ç»„ï¼ˆåŸå§‹ï¼‰',\n#     opacity=0.4,\n#     marker_color='#EB5757',\n#     nbinsx=50\n# ))\n# fig.add_trace(go.Histogram(\n#     x=df[df['treatment']==1]['ps_estimated'],\n#     name='å¤„ç†ç»„ï¼ˆåŸå§‹ï¼‰',\n#     opacity=0.4,\n#     marker_color='#2D9CDB',\n#     nbinsx=50\n# ))\n# fig.add_trace(go.Histogram(\n#     x=matched_psm_11[matched_psm_11['treatment']==0]['ps_estimated'],\n#     name='å¯¹ç…§ç»„ï¼ˆåŒ¹é…åï¼‰',\n#     opacity=0.7,\n#     marker_color='#F2994A',\n#     nbinsx=50\n# ))\n# fig.add_trace(go.Histogram(\n#     x=matched_psm_11[matched_psm_11['treatment']==1]['ps_estimated'],\n#     name='å¤„ç†ç»„ï¼ˆåŒ¹é…åï¼‰',\n#     opacity=0.7,\n#     marker_color='#27AE60',\n#     nbinsx=50\n# ))\n# fig.update_layout(\n#     title='PSM åŒ¹é…å‰åçš„å€¾å‘å¾—åˆ†åˆ†å¸ƒ',\n#     xaxis_title='å€¾å‘å¾—åˆ†',\n#     yaxis_title='é¢‘æ•°',\n#     barmode='overlay',\n#     template='plotly_white',\n#     height=400\n# )\n# fig.show()\n\nprint(\"å®Œæˆ TODO 1 åå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹ç»“æœ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: é©¬æ°è·ç¦»åŒ¹é… (Mahalanobis Distance Matching)\n",
    "\n",
    "### 4.1 ä¸ºä»€ä¹ˆéœ€è¦é©¬æ°è·ç¦»ï¼Ÿ\n",
    "\n",
    "PSM åªè€ƒè™‘å€¾å‘å¾—åˆ†è¿™ä¸€ä¸ªç»´åº¦ï¼Œå¯èƒ½å¿½ç•¥åå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚\n",
    "\n",
    "**é©¬æ°è·ç¦»** (Mahalanobis Distance):\n",
    "\n",
    "$$\n",
    "d_M(i, j) = \\sqrt{(X_i - X_j)^T \\Sigma^{-1} (X_i - X_j)}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\Sigma$ æ˜¯åå˜é‡çš„åæ–¹å·®çŸ©é˜µã€‚\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- è€ƒè™‘å˜é‡çš„å°ºåº¦å·®å¼‚å’Œç›¸å…³æ€§\n",
    "- å¯¹å¤šç»´æ•°æ®æ›´é²æ£’\n",
    "- è®¡ç®—æˆæœ¬é«˜\n",
    "\n",
    "### 4.2 å®ç°"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n```python\ndef mahalanobis_matching(df, covariates, caliper=None, ratio=1):\n    \"\"\"\n    é©¬æ°è·ç¦»åŒ¹é…\n    \n    å‚æ•°:\n        df: æ•°æ®æ¡†\n        covariates: åå˜é‡åˆ—è¡¨\n        caliper: å¡å°ºï¼ˆå¯é€‰ï¼‰\n        ratio: åŒ¹é…æ¯”ä¾‹\n    \n    è¿”å›:\n        matched_df: åŒ¹é…åçš„æ•°æ®æ¡†\n    \"\"\"\n    treated = df[df['treatment']==1].copy()\n    control = df[df['treatment']==0].copy()\n    \n    # è®¡ç®—åæ–¹å·®çŸ©é˜µåŠå…¶é€†\n    X = df[covariates]\n    cov_matrix = np.cov(X.T)\n    cov_inv = np.linalg.inv(cov_matrix)\n    \n    matched_pairs = []\n    \n    for _, t_row in treated.iterrows():\n        t_cov = t_row[covariates].values\n        \n        # è®¡ç®—ä¸æ‰€æœ‰å¯¹ç…§ç»„çš„é©¬æ°è·ç¦»\n        distances = []\n        for _, c_row in control.iterrows():\n            c_cov = c_row[covariates].values\n            dist = mahalanobis(t_cov, c_cov, cov_inv)\n            distances.append((c_row, dist))\n        \n        # æŒ‰è·ç¦»æ’åº\n        distances.sort(key=lambda x: x[1])\n        \n        # é€‰æ‹©æœ€è¿‘çš„ ratio ä¸ª\n        for c_row, dist in distances[:ratio]:\n            if caliper is None or dist <= caliper:\n                matched_pairs.append(t_row)\n                matched_pairs.append(c_row)\n    \n    matched_df = pd.DataFrame(matched_pairs)\n    return matched_df\n\nmatched_maha = mahalanobis_matching(df, covariates, ratio=1)\nprint(f\"é©¬æ°è·ç¦»åŒ¹é…: {len(matched_maha)} æ ·æœ¬\")\n\n# è¯„ä¼°åŒ¹é…è´¨é‡\nsmd_maha = compute_smd(matched_maha, covariates)\nprint(\"\\né©¬æ°è·ç¦»åŒ¹é…åçš„ SMD:\")\nfor cov, smd in smd_maha.items():\n    print(f\"  {cov}: {smd:.3f} (æ”¹å–„: {abs(smd_before[cov]) - abs(smd):.3f})\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PSM vs é©¬æ°è·ç¦»\n",
    "\n",
    "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| PSM | é™ç»´ï¼›ç†è®ºåŸºç¡€å¼º | å¯èƒ½ä¸¢å¤±ä¿¡æ¯ | åå˜é‡å¤šï¼›å¤„ç†åˆ†é…æœºåˆ¶æ¸…æ™° |\n",
    "| é©¬æ°è·ç¦» | ä¿ç•™å¤šç»´ä¿¡æ¯ | è®¡ç®—å¤æ‚ï¼›æ ·æœ¬éœ€æ±‚å¤§ | åå˜é‡ç›¸å…³æ€§å¼º |\n",
    "\n",
    "**å®è·µå»ºè®®**ï¼šå¯ä»¥ç»“åˆä¸¤è€…ï¼Œå…ˆç”¨ PSM ç²—ç­›ï¼Œå†ç”¨é©¬æ°è·ç¦»ç²¾åŒ¹é…ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: åŒ¹é…è´¨é‡è¯„ä¼°\n",
    "\n",
    "### 5.1 å¹³è¡¡æ€§æ£€éªŒ\n",
    "\n",
    "#### (1) æ ‡å‡†åŒ–å‡å€¼å·® (SMD)\n",
    "\n",
    "$$\n",
    "\\text{SMD} = \\frac{\\bar{X}_{\\text{treated}} - \\bar{X}_{\\text{control}}}{\\sqrt{(s^2_{\\text{treated}} + s^2_{\\text{control}}) / 2}}\n",
    "$$\n",
    "\n",
    "**åˆ¤æ–­æ ‡å‡†**ï¼š\n",
    "- $|\\text{SMD}| < 0.1$ï¼šå¹³è¡¡è‰¯å¥½\n",
    "- $0.1 \\leq |\\text{SMD}| < 0.25$ï¼šå¯æ¥å—\n",
    "- $|\\text{SMD}| \\geq 0.25$ï¼šä¸å¹³è¡¡\n",
    "\n",
    "#### (2) æ–¹å·®æ¯” (Variance Ratio)\n",
    "\n",
    "$$\n",
    "\\text{VR} = \\frac{s^2_{\\text{treated}}}{s^2_{\\text{control}}}\n",
    "$$\n",
    "\n",
    "**åˆ¤æ–­æ ‡å‡†**ï¼š$0.5 < \\text{VR} < 2$\n",
    "\n",
    "#### (3) å‡è®¾æ£€éªŒ\n",
    "\n",
    "- t æ£€éªŒï¼šæ£€éªŒå‡å€¼å·®å¼‚\n",
    "- KS æ£€éªŒï¼šæ£€éªŒåˆ†å¸ƒå·®å¼‚\n",
    "\n",
    "**æ³¨æ„**ï¼šåŒ¹é…åæ ·æœ¬ä¸ç‹¬ç«‹ï¼Œp å€¼ä»…ä¾›å‚è€ƒï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å®Œæ•´çš„å¹³è¡¡æ€§æ£€éªŒï¼ˆéœ€è¦å…ˆå®Œæˆ TODO 1 å’Œ TODO 2ï¼‰\ndef balance_assessment(df_before, df_after, covariates):\n    \"\"\"\n    å…¨é¢è¯„ä¼°åŒ¹é…è´¨é‡\n    \"\"\"\n    results = []\n    \n    for cov in covariates:\n        # åŒ¹é…å‰\n        before_t = df_before[df_before['treatment']==1][cov]\n        before_c = df_before[df_before['treatment']==0][cov]\n        \n        mean_diff_before = before_t.mean() - before_c.mean()\n        pooled_std_before = np.sqrt((before_t.var() + before_c.var()) / 2)\n        smd_before = mean_diff_before / pooled_std_before\n        vr_before = before_t.var() / before_c.var()\n        \n        # åŒ¹é…å\n        after_t = df_after[df_after['treatment']==1][cov]\n        after_c = df_after[df_after['treatment']==0][cov]\n        \n        mean_diff_after = after_t.mean() - after_c.mean()\n        pooled_std_after = np.sqrt((after_t.var() + after_c.var()) / 2)\n        smd_after = mean_diff_after / pooled_std_after\n        vr_after = after_t.var() / after_c.var()\n        \n        # KS æ£€éªŒ\n        ks_before = stats.ks_2samp(before_t, before_c)\n        ks_after = stats.ks_2samp(after_t, after_c)\n        \n        results.append({\n            'åå˜é‡': cov,\n            'SMD (å‰)': f\"{smd_before:.3f}\",\n            'SMD (å)': f\"{smd_after:.3f}\",\n            'VR (å‰)': f\"{vr_before:.3f}\",\n            'VR (å)': f\"{vr_after:.3f}\",\n            'KS på€¼ (å)': f\"{ks_after.pvalue:.3f}\"\n        })\n    \n    return pd.DataFrame(results)\n\n# balance_table = balance_assessment(df, matched_psm_11, covariates)\n# print(\"\\nåŒ¹é…è´¨é‡è¯„ä¼° (PSM 1:1):\")\n# print(balance_table.to_string(index=False))\n\nprint(\"å®Œæˆ TODO 1 åå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹ç»“æœ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯è§†åŒ– SMD æ”¹å–„ï¼ˆéœ€è¦å…ˆå®Œæˆ TODO 1ï¼‰\ndef plot_smd_comparison(df_before, df_after, covariates, labels=None):\n    \"\"\"\n    Love Plot: å¯è§†åŒ–åŒ¹é…å‰åçš„ SMD\n    \"\"\"\n    if labels is None:\n        labels = covariates\n    \n    smd_before = compute_smd(df_before, covariates)\n    smd_after = compute_smd(df_after, covariates)\n    \n    fig = go.Figure()\n    \n    # åŒ¹é…å‰\n    fig.add_trace(go.Scatter(\n        x=[abs(smd_before[cov]) for cov in covariates],\n        y=labels,\n        mode='markers',\n        marker=dict(size=12, color='#EB5757', symbol='circle'),\n        name='åŒ¹é…å‰'\n    ))\n    \n    # åŒ¹é…å\n    fig.add_trace(go.Scatter(\n        x=[abs(smd_after[cov]) for cov in covariates],\n        y=labels,\n        mode='markers',\n        marker=dict(size=12, color='#27AE60', symbol='diamond'),\n        name='åŒ¹é…å'\n    ))\n    \n    # é˜ˆå€¼çº¿\n    fig.add_vline(x=0.1, line_dash=\"dash\", line_color=\"gray\",\n                  annotation_text=\"é˜ˆå€¼ (0.1)\", annotation_position=\"top right\")\n    \n    fig.update_layout(\n        title='Love Plot: åŒ¹é…å‰åçš„æ ‡å‡†åŒ–å‡å€¼å·®',\n        xaxis_title='|SMD|',\n        yaxis_title='åå˜é‡',\n        template='plotly_white',\n        height=400\n    )\n    \n    return fig\n\n# fig = plot_smd_comparison(df, matched_psm_11, covariates, \n#                           labels=['å¹´é¾„', 'å†å²æ¶ˆè´¹', 'è´­ç‰©é¢‘æ¬¡'])\n# fig.show()\n\nprint(\"å®Œæˆ TODO 1 åå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹ç»“æœ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: æ•ˆåº”ä¼°è®¡\n",
    "\n",
    "### 6.1 ATT vs ATE\n",
    "\n",
    "åŒ¹é…æ–¹æ³•é€šå¸¸ä¼°è®¡çš„æ˜¯ **å¤„ç†ç»„å¹³å‡å¤„ç†æ•ˆåº” (ATT)**ï¼š\n",
    "\n",
    "$$\n",
    "\\text{ATT} = E[Y_1 - Y_0 \\mid T=1]\n",
    "$$\n",
    "\n",
    "è€Œéæ€»ä½“å¹³å‡å¤„ç†æ•ˆåº” (ATE)ï¼š\n",
    "\n",
    "$$\n",
    "\\text{ATE} = E[Y_1 - Y_0]\n",
    "$$\n",
    "\n",
    "**åŸå› **ï¼šåŒ¹é…æ˜¯ä»¥å¤„ç†ç»„ä¸ºåŸºå‡†ï¼Œä¸ºå…¶å¯»æ‰¾å¯¹ç…§ç»„ã€‚\n",
    "\n",
    "**ä¼°è®¡**ï¼š\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{ATT}} = \\frac{1}{N_T} \\sum_{i: T_i=1} \\left( Y_i - \\frac{1}{M_i} \\sum_{j \\in \\mathcal{M}(i)} Y_j \\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\mathcal{M}(i)$ æ˜¯ä¸ªä½“ $i$ çš„åŒ¹é…é›†ï¼Œ$M_i$ æ˜¯åŒ¹é…ä¸ªæ•°ã€‚\n",
    "\n",
    "### 6.2 æ–¹å·®ä¼°è®¡\n",
    "\n",
    "**æŒ‘æˆ˜**ï¼š\n",
    "- åŒ¹é…æ ·æœ¬ä¸ç‹¬ç«‹ï¼ˆå°¤å…¶æ˜¯æœ‰æ”¾å›åŒ¹é…ï¼‰\n",
    "- å€¾å‘å¾—åˆ†æ˜¯ä¼°è®¡å‡ºæ¥çš„ï¼ˆä¼°è®¡è¯¯å·®ï¼‰\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆ**ï¼š\n",
    "1. **Abadie-Imbens æ–¹å·®ä¼°è®¡**ï¼ˆè€ƒè™‘åŒ¹é…ä¸ç¡®å®šæ€§ï¼‰\n",
    "2. **Bootstrap**ï¼ˆé‡æŠ½æ ·ï¼‰\n",
    "3. **ç¨³å¥æ ‡å‡†è¯¯**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n```python\ndef estimate_att(matched_df):\n    \"\"\"\n    ä¼°è®¡ ATT\n    \"\"\"\n    treated = matched_df[matched_df['treatment']==1]\n    control = matched_df[matched_df['treatment']==0]\n    \n    att = treated['spending'].mean() - control['spending'].mean()\n    return att\n\ndef bootstrap_att(matched_df, n_bootstrap=1000):\n    \"\"\"\n    Bootstrap ä¼°è®¡ ATT çš„æ ‡å‡†è¯¯å’Œç½®ä¿¡åŒºé—´\n    \n    å‚æ•°:\n        matched_df: åŒ¹é…åçš„æ•°æ®æ¡†\n        n_bootstrap: Bootstrap æ¬¡æ•°\n    \n    è¿”å›:\n        att, se, ci_lower, ci_upper\n    \"\"\"\n    att_point = estimate_att(matched_df)\n    \n    # è¯†åˆ«åŒ¹é…å¯¹ï¼ˆå‡è®¾æŒ‰é¡ºåºæ’åˆ—ï¼šå¤„ç†-å¯¹ç…§-å¤„ç†-å¯¹ç…§...ï¼‰\n    treated_indices = matched_df[matched_df['treatment']==1].index\n    control_indices = matched_df[matched_df['treatment']==0].index\n    \n    # ç¡®ä¿é…å¯¹\n    n_pairs = min(len(treated_indices), len(control_indices))\n    \n    att_bootstrap = []\n    for _ in range(n_bootstrap):\n        # æœ‰æ”¾å›æŠ½æ ·é…å¯¹\n        sampled_pair_indices = np.random.choice(n_pairs, n_pairs, replace=True)\n        \n        sampled_treated = matched_df.loc[treated_indices[sampled_pair_indices]]\n        sampled_control = matched_df.loc[control_indices[sampled_pair_indices]]\n        \n        att_b = sampled_treated['spending'].mean() - sampled_control['spending'].mean()\n        att_bootstrap.append(att_b)\n    \n    att_bootstrap = np.array(att_bootstrap)\n    se = att_bootstrap.std()\n    ci_lower = np.percentile(att_bootstrap, 2.5)\n    ci_upper = np.percentile(att_bootstrap, 97.5)\n    \n    return att_point, se, ci_lower, ci_upper\n\n# ä¼°è®¡ä¸åŒåŒ¹é…æ–¹æ³•çš„ ATT\nmethods = {\n    'ç²¾ç¡®åŒ¹é…': matched_exact,\n    'PSM (1:1)': matched_psm_11,\n    'PSM (1:3)': matched_psm_13,\n    'é©¬æ°è·ç¦»': matched_maha\n}\n\nprint(f\"çœŸå® ATE: {true_ate}\\n\")\nprint(\"ATT ä¼°è®¡ç»“æœ:\")\nprint(\"-\" * 70)\n\nfor name, matched_df in methods.items():\n    if len(matched_df) > 0:\n        att, se, ci_lower, ci_upper = bootstrap_att(matched_df, n_bootstrap=500)\n        bias = att - true_ate\n        print(f\"{name:15} ATT={att:7.2f}  SE={se:6.2f}  95% CI=[{ci_lower:7.2f}, {ci_upper:7.2f}]  Bias={bias:7.2f}\")\n    else:\n        print(f\"{name:15} æ— æ³•åŒ¹é…\")\n```\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯è§†åŒ–æ•ˆåº”ä¼°è®¡ï¼ˆéœ€è¦å…ˆå®Œæˆ TODO 3ï¼‰\ndef plot_treatment_effect_estimates(methods_dict, true_ate):\n    \"\"\"\n    å¯è§†åŒ–ä¸åŒæ–¹æ³•çš„æ•ˆåº”ä¼°è®¡\n    \"\"\"\n    results = []\n    \n    for name, matched_df in methods_dict.items():\n        if len(matched_df) > 0:\n            att, se, ci_lower, ci_upper = bootstrap_att(matched_df, n_bootstrap=500)\n            results.append({\n                'method': name,\n                'att': att,\n                'se': se,\n                'ci_lower': ci_lower,\n                'ci_upper': ci_upper\n            })\n    \n    results_df = pd.DataFrame(results)\n    \n    fig = go.Figure()\n    \n    # ç‚¹ä¼°è®¡\n    fig.add_trace(go.Scatter(\n        x=results_df['att'],\n        y=results_df['method'],\n        mode='markers',\n        marker=dict(size=12, color='#2D9CDB'),\n        error_x=dict(\n            type='data',\n            symmetric=False,\n            array=results_df['ci_upper'] - results_df['att'],\n            arrayminus=results_df['att'] - results_df['ci_lower']\n        ),\n        name='ATT ä¼°è®¡'\n    ))\n    \n    # çœŸå®å€¼\n    fig.add_vline(x=true_ate, line_dash=\"dash\", line_color=\"#27AE60\",\n                  annotation_text=f\"çœŸå® ATE = {true_ate}\",\n                  annotation_position=\"top right\")\n    \n    fig.update_layout(\n        title='ä¸åŒåŒ¹é…æ–¹æ³•çš„ ATT ä¼°è®¡åŠ 95% ç½®ä¿¡åŒºé—´',\n        xaxis_title='ATT',\n        yaxis_title='åŒ¹é…æ–¹æ³•',\n        template='plotly_white',\n        height=400\n    )\n    \n    return fig\n\n# fig = plot_treatment_effect_estimates(methods, true_ate)\n# fig.show()\n\nprint(\"å®Œæˆ TODO 3 åå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹ç»“æœ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 æ•æ„Ÿæ€§åˆ†æ\n",
    "\n",
    "**é—®é¢˜**ï¼šå¦‚æœå­˜åœ¨æœªè§‚æµ‹çš„æ··æ·†å› ç´ æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "**Rosenbaum è¾¹ç•Œ (Rosenbaum Bounds)**ï¼šè¯„ä¼°æœªè§‚æµ‹æ··æ·†å¯¹ç»“æœçš„å½±å“ã€‚\n",
    "\n",
    "**æ€è·¯**ï¼šå‡è®¾å­˜åœ¨ä¸€ä¸ªæœªè§‚æµ‹å˜é‡ $U$ï¼Œä½¿å¾—ä¸¤ä¸ªåå˜é‡ç›¸åŒçš„ä¸ªä½“åœ¨å¤„ç†æ¦‚ç‡ä¸Šç›¸å·® $\\Gamma$ å€ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\Gamma} \\leq \\frac{P(T=1 \\mid X, U)}{P(T=1 \\mid X, U')} \\leq \\Gamma\n",
    "$$\n",
    "\n",
    "è®¡ç®—åœ¨ä¸åŒ $\\Gamma$ ä¸‹ï¼Œç»“è®ºæ˜¯å¦ä¼šæ”¹å˜ï¼ˆp å€¼è¾¹ç•Œï¼‰ã€‚\n",
    "\n",
    "**è§£è¯»**ï¼š\n",
    "- $\\Gamma = 1$ï¼šæ— æœªè§‚æµ‹æ··æ·†\n",
    "- $\\Gamma = 2$ï¼šæœªè§‚æµ‹æ··æ·†ä½¿å¤„ç†æ¦‚ç‡ç›¸å·® 2 å€\n",
    "- å¦‚æœåœ¨ $\\Gamma = 2$ ä¸‹ç»“è®ºä»æ˜¾è‘—ï¼Œè¯´æ˜ç»“æœå¯¹æœªè§‚æµ‹æ··æ·†è¾ƒç¨³å¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç®€åŒ–ç‰ˆ Rosenbaum æ•æ„Ÿæ€§åˆ†æï¼ˆéœ€è¦å…ˆå®Œæˆ TODO 1ï¼‰\ndef rosenbaum_bounds_simulation(matched_df, gamma_range=np.arange(1, 3.1, 0.2)):\n    \"\"\"\n    æ¨¡æ‹Ÿ Rosenbaum è¾¹ç•Œ\n    \n    æ€è·¯ï¼š\n    - å¯¹äºæ¯ä¸ª gammaï¼Œæ¨¡æ‹Ÿæœªè§‚æµ‹æ··æ·†å¯¼è‡´çš„å¤„ç†åˆ†é…åå·®\n    - é‡æ–°åˆ†é…å¤„ç†ï¼Œè®¡ç®—æ•ˆåº”ä¼°è®¡\n    - çœ‹æ•ˆåº”æ˜¯å¦ä»æ˜¾è‘—\n    \"\"\"\n    treated = matched_df[matched_df['treatment']==1]\n    control = matched_df[matched_df['treatment']==0]\n    \n    observed_effect = treated['spending'].mean() - control['spending'].mean()\n    \n    results = []\n    \n    for gamma in gamma_range:\n        # æ¨¡æ‹Ÿ 1000 æ¬¡\n        effects = []\n        for _ in range(1000):\n            # ä¸ºæ¯å¯¹åŒ¹é…ä¸ªä½“ï¼Œä»¥æ¦‚ç‡ p äº’æ¢å¤„ç†çŠ¶æ€\n            # p çš„èŒƒå›´ç”± gamma å†³å®š\n            p = gamma / (1 + gamma)\n            \n            # éšæœºç¿»è½¬ä¸€éƒ¨åˆ†é…å¯¹\n            n_pairs = min(len(treated), len(control))\n            flip = np.random.binomial(1, p, n_pairs)\n            \n            # é‡æ–°è®¡ç®—æ•ˆåº”ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰\n            perturbed_effect = observed_effect * (1 - 0.1 * (gamma - 1) * np.random.uniform(0.5, 1.5))\n            effects.append(perturbed_effect)\n        \n        effects = np.array(effects)\n        p_value = (effects <= 0).mean()  # å•ä¾§æ£€éªŒ\n        \n        results.append({\n            'gamma': gamma,\n            'p_value': p_value,\n            'effect_lower': np.percentile(effects, 5),\n            'effect_upper': np.percentile(effects, 95)\n        })\n    \n    return pd.DataFrame(results), observed_effect\n\n# sensitivity_df, obs_effect = rosenbaum_bounds_simulation(matched_psm_11)\n\n# # å¯è§†åŒ–\n# fig = make_subplots(rows=1, cols=2, \n#                     subplot_titles=['P-value vs Gamma', 'Effect Bounds vs Gamma'])\n\n# # P-value\n# fig.add_trace(\n#     go.Scatter(x=sensitivity_df['gamma'], y=sensitivity_df['p_value'],\n#                mode='lines+markers', name='P-value',\n#                line=dict(color='#2D9CDB')),\n#     row=1, col=1\n# )\n# fig.add_hline(y=0.05, line_dash=\"dash\", line_color=\"red\",\n#               annotation_text=\"Î±=0.05\", row=1, col=1)\n\n# # Effect bounds\n# fig.add_trace(\n#     go.Scatter(x=sensitivity_df['gamma'], y=sensitivity_df['effect_upper'],\n#                mode='lines', name='Upper bound',\n#                line=dict(color='#27AE60', dash='dash')),\n#     row=1, col=2\n# )\n# fig.add_trace(\n#     go.Scatter(x=sensitivity_df['gamma'], y=sensitivity_df['effect_lower'],\n#                mode='lines', name='Lower bound',\n#                line=dict(color='#EB5757', dash='dash'),\n#                fill='tonexty', fillcolor='rgba(45, 156, 219, 0.2)'),\n#     row=1, col=2\n# )\n# fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=2)\n# fig.add_hline(y=obs_effect, line_dash=\"dot\", line_color=\"#2D9CDB\",\n#               annotation_text=f\"Observed = {obs_effect:.1f}\", row=1, col=2)\n\n# fig.update_xaxes(title_text=\"Î“ (æœªè§‚æµ‹æ··æ·†å¼ºåº¦)\", row=1, col=1)\n# fig.update_xaxes(title_text=\"Î“ (æœªè§‚æµ‹æ··æ·†å¼ºåº¦)\", row=1, col=2)\n# fig.update_yaxes(title_text=\"P-value\", row=1, col=1)\n# fig.update_yaxes(title_text=\"ATT\", row=1, col=2)\n\n# fig.update_layout(\n#     title_text='Rosenbaum æ•æ„Ÿæ€§åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰',\n#     template='plotly_white',\n#     height=400,\n#     showlegend=True\n# )\n\n# fig.show()\n\n# print(\"\\næ•æ„Ÿæ€§åˆ†æç»“æœ:\")\n# print(sensitivity_df.head(10).to_string(index=False))\n\nprint(\"å®Œæˆ TODO 1 åå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹ç»“æœ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: ä¸šåŠ¡æ¡ˆä¾‹\n",
    "\n",
    "### æ¡ˆä¾‹ 1: è¯„ä¼°ä¼šå‘˜æƒç›Šæ•ˆæœ\n",
    "\n",
    "**èƒŒæ™¯**ï¼šç”µå•†å¹³å°æ¨å‡ºä»˜è´¹ä¼šå‘˜æœåŠ¡ï¼Œéœ€è¦è¯„ä¼°ä¼šå‘˜æƒç›Šå¯¹ç”¨æˆ·æ¶ˆè´¹çš„çœŸå®å½±å“ã€‚\n",
    "\n",
    "**æŒ‘æˆ˜**ï¼š\n",
    "- ä¼šå‘˜ç”¨æˆ·æœ¬èº«å°±æ˜¯é«˜ä»·å€¼ç”¨æˆ·ï¼ˆé€‰æ‹©åå·®ï¼‰\n",
    "- æ— æ³•è¿›è¡ŒéšæœºåŒ–å®éªŒï¼ˆå•†ä¸šè€ƒè™‘ï¼‰\n",
    "\n",
    "**æ–¹æ¡ˆ**ï¼šä½¿ç”¨ PSM åŒ¹é…\n",
    "\n",
    "**æ­¥éª¤**ï¼š\n",
    "1. æ”¶é›†åå˜é‡ï¼šå¹´é¾„ã€å†å²æ¶ˆè´¹ã€è´­ç‰©é¢‘æ¬¡ã€æ³¨å†Œæ—¶é•¿ç­‰\n",
    "2. ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "3. 1:3 åŒ¹é…ï¼ˆæé«˜ç»Ÿè®¡æ•ˆç‡ï¼‰\n",
    "4. æ£€æŸ¥å¹³è¡¡æ€§\n",
    "5. ä¼°è®¡ ATT\n",
    "6. æ•æ„Ÿæ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å®Œæ•´çš„ä¼šå‘˜æƒç›Šè¯„ä¼°æµç¨‹ï¼ˆéœ€è¦å…ˆå®Œæˆ TODO 1 å’Œ TODO 3ï¼‰\n# print(\"=\" * 70)\n# print(\"ä¼šå‘˜æƒç›Šæ•ˆæœè¯„ä¼°æŠ¥å‘Š\")\n# print(\"=\" * 70)\n\n# print(\"\\n1. æ•°æ®æ¦‚è§ˆ\")\n# print(f\"   æ€»æ ·æœ¬é‡: {len(df)}\")\n# print(f\"   ä¼šå‘˜ç”¨æˆ·: {df['treatment'].sum()} ({df['treatment'].mean()*100:.1f}%)\")\n# print(f\"   éä¼šå‘˜ç”¨æˆ·: {(1-df['treatment']).sum()} ({(1-df['treatment']).mean()*100:.1f}%)\")\n\n# print(\"\\n2. åå˜é‡å¹³è¡¡æ€§ï¼ˆåŒ¹é…å‰ï¼‰\")\n# for cov in covariates:\n#     t_mean = df[df['treatment']==1][cov].mean()\n#     c_mean = df[df['treatment']==0][cov].mean()\n#     print(f\"   {cov:15} å¤„ç†ç»„={t_mean:8.2f}  å¯¹ç…§ç»„={c_mean:8.2f}  å·®å¼‚={t_mean-c_mean:8.2f}\")\n\n# print(\"\\n3. PSM 1:3 åŒ¹é…\")\n# print(f\"   åŒ¹é…åæ ·æœ¬é‡: {len(matched_psm_13)}\")\n# print(f\"   å¤„ç†ç»„ä¿ç•™ç‡: {matched_psm_13['treatment'].sum() / df['treatment'].sum() * 100:.1f}%\")\n\n# print(\"\\n4. åå˜é‡å¹³è¡¡æ€§ï¼ˆåŒ¹é…åï¼‰\")\n# for cov in covariates:\n#     t_mean = matched_psm_13[matched_psm_13['treatment']==1][cov].mean()\n#     c_mean = matched_psm_13[matched_psm_13['treatment']==0][cov].mean()\n#     smd = abs(compute_smd(matched_psm_13, [cov])[cov])\n#     status = \"âœ“\" if smd < 0.1 else (\"âš \" if smd < 0.25 else \"âœ—\")\n#     print(f\"   {cov:15} å¤„ç†ç»„={t_mean:8.2f}  å¯¹ç…§ç»„={c_mean:8.2f}  SMD={smd:.3f} {status}\")\n\n# att, se, ci_lower, ci_upper = bootstrap_att(matched_psm_13, n_bootstrap=1000)\n# print(\"\\n5. æ•ˆåº”ä¼°è®¡\")\n# print(f\"   ATT: {att:.2f} å…ƒ/æœˆ\")\n# print(f\"   æ ‡å‡†è¯¯: {se:.2f}\")\n# print(f\"   95% ç½®ä¿¡åŒºé—´: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n# print(f\"   æ˜¯å¦æ˜¾è‘—: {'æ˜¯ (p<0.05)' if ci_lower > 0 else 'å¦'}\")\n\n# print(\"\\n6. ä¸šåŠ¡è§£è¯»\")\n# print(f\"   - ä¼šå‘˜æƒç›Šä½¿ç”¨æˆ·æœˆæ¶ˆè´¹å¹³å‡å¢åŠ  {att:.0f} å…ƒ\")\n# print(f\"   - å¦‚æœä¼šå‘˜å¹´è´¹ä¸º {12*50} å…ƒï¼ŒROI = {att*12 / (12*50):.2f}\")\n# print(f\"   - æ¨è: {'å€¼å¾—æ¨å¹¿' if att > 200 else 'éœ€è¦ä¼˜åŒ–æƒç›Š'}\")\n\n# print(\"\\n\" + \"=\" * 70)\n\nprint(\"å®Œæˆ TODO 1 å’Œ TODO 3 åå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹ç»“æœ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¡ˆä¾‹ 2: å¹¿å‘ŠæŠ•æ”¾æ•ˆæœè¯„ä¼°\n",
    "\n",
    "**èƒŒæ™¯**ï¼šæŸ App åœ¨éƒ¨åˆ†åŸå¸‚æŠ•æ”¾äº†æˆ·å¤–å¹¿å‘Šï¼Œæƒ³è¯„ä¼°å¹¿å‘Šå¯¹æ–°ç”¨æˆ·æ³¨å†Œçš„å½±å“ã€‚\n",
    "\n",
    "**æ•°æ®**ï¼š\n",
    "- å¤„ç†ç»„ï¼šæŠ•æ”¾å¹¿å‘Šçš„åŸå¸‚\n",
    "- å¯¹ç…§ç»„ï¼šæœªæŠ•æ”¾å¹¿å‘Šçš„åŸå¸‚\n",
    "- åå˜é‡ï¼šåŸå¸‚äººå£ã€GDPã€äº’è”ç½‘æ¸—é€ç‡ã€ç«å“æ•°é‡ç­‰\n",
    "\n",
    "**æŒ‘æˆ˜**ï¼š\n",
    "- æ ·æœ¬é‡å°ï¼ˆåŸå¸‚ç»´åº¦ï¼‰\n",
    "- åå˜é‡ç»´åº¦é«˜\n",
    "- åœ°ç†ç›¸å…³æ€§\n",
    "\n",
    "**æ–¹æ¡ˆ**ï¼šé©¬æ°è·ç¦»åŒ¹é… + åœ°ç†åŠ æƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n```python\ndef generate_advertising_data(n_cities=100):\n    \"\"\"\n    ç”Ÿæˆå¹¿å‘ŠæŠ•æ”¾æ•°æ®ï¼ˆåŸå¸‚ç»´åº¦ï¼‰\n    \"\"\"\n    # åå˜é‡\n    population = np.random.gamma(5, 100, n_cities)  # äººå£ï¼ˆä¸‡ï¼‰\n    gdp = np.random.gamma(10, 500, n_cities)  # GDPï¼ˆäº¿ï¼‰\n    internet_penetration = np.random.beta(8, 2, n_cities)  # äº’è”ç½‘æ¸—é€ç‡\n    competitors = np.random.poisson(3, n_cities)  # ç«å“æ•°é‡\n    \n    # å¤„ç†åˆ†é…ï¼ˆå€¾å‘äºäººå£å¤šã€GDPé«˜çš„åŸå¸‚ï¼‰\n    propensity = 1 / (1 + np.exp(-(-2 + 0.01*population + 0.001*gdp)))\n    treatment = (np.random.uniform(0, 1, n_cities) < propensity).astype(int)\n    \n    # æ½œåœ¨ç»“æœï¼ˆæ–°ç”¨æˆ·æ³¨å†Œæ•°ï¼‰\n    y0 = (10*population + 0.5*gdp + 100*internet_penetration - 20*competitors + \n          np.random.normal(0, 50, n_cities))\n    true_effect = 100  # å¹¿å‘Šå¸¦æ¥ 100 ä¸ªæ–°ç”¨æˆ·\n    y1 = y0 + true_effect\n    \n    y_obs = treatment * y1 + (1 - treatment) * y0\n    \n    df = pd.DataFrame({\n        'city_id': range(n_cities),\n        'population': population,\n        'gdp': gdp,\n        'internet_penetration': internet_penetration,\n        'competitors': competitors,\n        'treatment': treatment,\n        'new_users': y_obs,\n        'y0': y0,\n        'y1': y1\n    })\n    \n    return df, true_effect\n\ndf_ad, true_effect_ad = generate_advertising_data(100)\n\n# ä½¿ç”¨é©¬æ°è·ç¦»åŒ¹é…\nad_covariates = ['population', 'gdp', 'internet_penetration', 'competitors']\nmatched_ad = mahalanobis_matching(df_ad, ad_covariates, ratio=2)\n\n# è¯„ä¼°\nprint(\"å¹¿å‘ŠæŠ•æ”¾æ•ˆæœè¯„ä¼°\")\nprint(f\"çœŸå®æ•ˆæœ: {true_effect_ad} æ–°ç”¨æˆ·\")\nprint(f\"\\nåŒ¹é…æ ·æœ¬é‡: {len(matched_ad)} (åŸå¸‚å¯¹)\")\n\n# å¹³è¡¡æ€§\nbalance_ad = balance_assessment(df_ad, matched_ad, ad_covariates)\nprint(\"\\nå¹³è¡¡æ€§æ£€éªŒ:\")\nprint(balance_ad.to_string(index=False))\n\n# æ•ˆåº”ä¼°è®¡\natt_ad, se_ad, ci_lower_ad, ci_upper_ad = bootstrap_att(matched_ad, n_bootstrap=500)\nprint(f\"\\nATT: {att_ad:.2f} æ–°ç”¨æˆ·\")\nprint(f\"95% CI: [{ci_lower_ad:.2f}, {ci_upper_ad:.2f}]\")\nprint(f\"åå·®: {att_ad - true_effect_ad:.2f}\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: ç»ƒä¹ ä¸æ€è€ƒé¢˜\n",
    "\n",
    "### ç»ƒä¹  1: å®Œå–„å¡å°ºé€‰æ‹©\n",
    "\n",
    "å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¡å°ºï¼ˆé€šè¿‡äº¤å‰éªŒè¯å¹³è¡¡æ€§å’Œæ ·æœ¬é‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<details><summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n```python\ndef optimal_caliper_selection(df, caliper_range=np.arange(0.05, 0.5, 0.05)):\n    \"\"\"\n    é€‰æ‹©æœ€ä¼˜å¡å°º\n    \n    ç›®æ ‡ï¼šå¹³è¡¡åŒ¹é…è´¨é‡ï¼ˆSMDï¼‰å’Œæ ·æœ¬ä¿ç•™ç‡\n    \n    è¿”å›:\n        optimal_caliper, results_df\n    \"\"\"\n    results = []\n    \n    for caliper in caliper_range:\n        # åŒ¹é…\n        matched_df = psm_matching(df, caliper=caliper, ratio=1, with_replacement=False)\n        \n        if len(matched_df) == 0:\n            continue\n        \n        # è®¡ç®—æŒ‡æ ‡\n        sample_retention = len(matched_df) / len(df)\n        smd_dict = compute_smd(matched_df, covariates)\n        avg_smd = np.mean([abs(v) for v in smd_dict.values()])\n        \n        # ç»¼åˆæŒ‡æ ‡ï¼ˆÎ»=1ï¼‰\n        score = sample_retention - 1.0 * avg_smd\n        \n        results.append({\n            'caliper': caliper,\n            'sample_retention': sample_retention,\n            'avg_smd': avg_smd,\n            'score': score\n        })\n    \n    results_df = pd.DataFrame(results)\n    optimal_idx = results_df['score'].idxmax()\n    optimal_caliper = results_df.loc[optimal_idx, 'caliper']\n    \n    return optimal_caliper, results_df\n\n# æµ‹è¯•\noptimal_caliper, results = optimal_caliper_selection(df)\nprint(f\"æœ€ä¼˜å¡å°º: {optimal_caliper}\")\nprint(\"\\nç»“æœè¯¦æƒ…:\")\nprint(results.to_string(index=False))\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ€è€ƒé¢˜\n",
    "\n",
    "1. **åŒ¹é… vs å›å½’**ï¼šä»€ä¹ˆæ—¶å€™ç”¨åŒ¹é…ï¼Œä»€ä¹ˆæ—¶å€™ç”¨å›å½’ï¼Ÿèƒ½å¦ç»“åˆä¸¤è€…ï¼Ÿ\n",
    "\n",
    "2. **å¤šå€¼å¤„ç†**ï¼šå¦‚æœå¤„ç†ä¸æ˜¯äºŒå…ƒçš„ï¼ˆå¦‚ï¼šå¹¿å‘ŠæŠ•æ”¾å¼ºåº¦ 0/50%/100%ï¼‰ï¼Œå¦‚ä½•åŒ¹é…ï¼Ÿ\n",
    "\n",
    "3. **æ—¶é—´ç»´åº¦**ï¼šå¦‚æœéœ€è¦è¯„ä¼°å¤„ç†çš„åŠ¨æ€æ•ˆåº”ï¼ˆå¦‚ï¼šä¼šå‘˜ç¬¬ 1 ä¸ªæœˆ vs ç¬¬ 6 ä¸ªæœˆçš„æ•ˆæœï¼‰ï¼ŒåŒ¹é…æ–¹æ³•å¦‚ä½•è°ƒæ•´ï¼Ÿ\n",
    "\n",
    "4. **ç½‘ç»œæ•ˆåº”**ï¼šå¦‚æœä¸ªä½“ä¹‹é—´å­˜åœ¨ç½‘ç»œæ•ˆåº”ï¼ˆå¦‚ï¼šç”¨æˆ·çš„æœ‹å‹ä¹Ÿæ˜¯ä¼šå‘˜ï¼‰ï¼ŒåŒ¹é…å‡è®¾æ˜¯å¦æˆç«‹ï¼Ÿ\n",
    "\n",
    "5. **å¤–éƒ¨æœ‰æ•ˆæ€§**ï¼šåŒ¹é…æ–¹æ³•ä¼°è®¡çš„ ATT èƒ½å¦æ¨å¹¿åˆ°æœªåŒ¹é…çš„æ€»ä½“ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒè¦ç‚¹\n",
    "\n",
    "1. **åŒ¹é…çš„æœ¬è´¨**ï¼šé€šè¿‡æ„å»ºå¯æ¯”çš„å¯¹ç…§ç»„ï¼Œæ¨¡æ‹ŸéšæœºåŒ–\n",
    "\n",
    "2. **å…³é”®å‡è®¾**ï¼šæ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (CIA) + å…±åŒæ”¯æ’‘\n",
    "\n",
    "3. **æ–¹æ³•é€‰æ‹©**ï¼š\n",
    "   - ç²¾ç¡®åŒ¹é…ï¼šåå˜é‡å°‘ä¸”ç¦»æ•£\n",
    "   - PSMï¼šåå˜é‡å¤šï¼Œå¤„ç†åˆ†é…æœºåˆ¶æ¸…æ™°\n",
    "   - é©¬æ°è·ç¦»ï¼šåå˜é‡ç›¸å…³æ€§å¼º\n",
    "\n",
    "4. **åŒ¹é…è´¨é‡**ï¼šå¿…é¡»æ£€æŸ¥å¹³è¡¡æ€§ï¼ˆSMD < 0.1ï¼‰\n",
    "\n",
    "5. **æ•æ„Ÿæ€§åˆ†æ**ï¼šè¯„ä¼°æœªè§‚æµ‹æ··æ·†çš„å½±å“\n",
    "\n",
    "### æœ€ä½³å®è·µ\n",
    "\n",
    "1. **é¢„åˆ†æ**ï¼šå…ˆå¯è§†åŒ–åå˜é‡åˆ†å¸ƒï¼Œäº†è§£ä¸å¹³è¡¡ç¨‹åº¦\n",
    "2. **å¤šç§æ–¹æ³•**ï¼šå°è¯•ä¸åŒåŒ¹é…ç®—æ³•ï¼Œæ¯”è¾ƒç¨³å¥æ€§\n",
    "3. **ä¿ç•™è¯Šæ–­**ï¼šæŠ¥å‘ŠåŒ¹é…å‰åçš„å¹³è¡¡æ€§ç»Ÿè®¡\n",
    "4. **é€æ˜æŠ¥å‘Š**ï¼šè¯´æ˜æ ·æœ¬æŸå¤±ã€åŒ¹é…å‚æ•°é€‰æ‹©\n",
    "5. **ç»“åˆæ–¹æ³•**ï¼šåŒ¹é…åå¯ä»¥å†åšå›å½’è°ƒæ•´ï¼ˆdoubly robustï¼‰\n",
    "\n",
    "### å»¶ä¼¸é˜…è¯»\n",
    "\n",
    "- Rosenbaum & Rubin (1983): \"The Central Role of the Propensity Score\"\n",
    "- Stuart (2010): \"Matching Methods for Causal Inference: A Review\"\n",
    "- Imbens & Rubin (2015): *Causal Inference for Statistics, Social, and Biomedical Sciences*\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€è®²é¢„å‘Š**ï¼šå·¥å…·å˜é‡æ³• - å½“å­˜åœ¨æœªè§‚æµ‹æ··æ·†æ—¶å¦‚ä½•è¯†åˆ«å› æœæ•ˆåº”ï¼Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}