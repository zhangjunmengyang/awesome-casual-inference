{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ­ ç¬¬ä¸€ç«  ç»ƒä¹  3: æ··æ·†åå·® (Confounding Bias)\n\n---\n\n## è®©åå·®ã€Œç°å½¢ã€\n\nåœ¨å‰ä¸¤ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“äº†ï¼š\n- æ½œåœ¨ç»“æœæ¡†æ¶å‘Šè¯‰æˆ‘ä»¬ã€Œä»€ä¹ˆæ˜¯å› æœæ•ˆåº”ã€\n- å› æœå›¾å‘Šè¯‰æˆ‘ä»¬ã€Œä¸ºä»€ä¹ˆä¼šæœ‰åå·®ã€\n\nä½†ä¸€ä¸ªå…³é”®é—®é¢˜è¿˜æ²¡å›ç­”ï¼š**åå·®åˆ°åº•æœ‰å¤šå¤§ï¼Ÿèƒ½é‡åŒ–å—ï¼Ÿ**\n\n### ä¸€ä¸ªè®©ä½ å´©æºƒçš„åœºæ™¯ ğŸ˜±\n\nä½ æ˜¯ä¸€ååŒ»å­¦ç ”ç©¶è€…ï¼Œå‘ç°äº†ä¸€ä¸ªæƒŠäººçš„ç»“è®ºï¼š\n\n> \"å–çº¢é…’çš„äººæ¯”ä¸å–é…’çš„äººå¿ƒè„ç—…å‘ç—…ç‡ä½ 30%ï¼\"\n\næ–°é—»æ ‡é¢˜å·²ç»æƒ³å¥½äº†ï¼š*ã€Šç§‘å­¦è¯æ˜ï¼šçº¢é…’æ˜¯å¿ƒè„çš„ä¿æŠ¤ç¥ã€‹*\n\nä½†ç­‰ç­‰...ä½ çš„åŒäº‹é—®äº†å‡ ä¸ªé—®é¢˜ï¼š\n\n- å–çº¢é…’çš„äººæ˜¯ä¸æ˜¯æ”¶å…¥æ›´é«˜ï¼Ÿï¼ˆèƒ½ä¹°å¾—èµ·çº¢é…’ï¼‰\n- æ”¶å…¥é«˜çš„äººæ˜¯ä¸æ˜¯åŒ»ç–—æ¡ä»¶æ›´å¥½ï¼Ÿ\n- é‚£è¿™ 30% çš„å·®å¼‚æœ‰å¤šå°‘æ˜¯çº¢é…’çš„åŠŸåŠ³ï¼Œå¤šå°‘æ˜¯æ”¶å…¥çš„åŠŸåŠ³ï¼Ÿ\n\nè¿™å°±æ˜¯**æ··æ·†åå·®**ï¼ˆåœ¨è®¡é‡ç»æµå­¦ä¸­ä¹Ÿç§°ä¸ºé—æ¼å˜é‡åå·®ï¼ŒOmitted Variable Bias, OVBï¼‰çš„å¯æ€•ä¹‹å¤„â€”â€”å®ƒä¼šè®©æ— æ•ˆçš„æ²»ç–—çœ‹èµ·æ¥æœ‰æ•ˆï¼Œä¹Ÿä¼šè®©æœ‰æ•ˆçš„æ²»ç–—çœ‹èµ·æ¥æ— æ•ˆï¼\n\n---\n\n## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n\n1. ç†è§£æ··æ·†åå·®çš„æ•°å­¦å…¬å¼\n2. å­¦ä¼šé‡åŒ–åå·®çš„å¤§å°\n3. æ·±å…¥ç†è§£ Simpson's Paradox\n4. æŒæ¡æ•æ„Ÿæ€§åˆ†ææ–¹æ³•"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯¼å…¥å¿…è¦çš„åº“\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom IPython.display import display\n\n# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¸¦å…¼å®¹æ€§å¤„ç†ï¼‰\ntry:\n    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n    plt.rcParams['axes.unicode_minus'] = False\nexcept Exception as e:\n    print(f\"âš ï¸ å­—ä½“è®¾ç½®è­¦å‘Š: {e}\")\n    print(\"   å¦‚æœå›¾è¡¨ä¸­æ–‡æ˜¾ç¤ºå¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿå­—ä½“\")\n\nsns.set_style('whitegrid')\n\nprint(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæ¯•ï¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ğŸ§® Part 1: æ··æ·†åå·®å…¬å¼ï¼ˆé—æ¼å˜é‡åå·®ï¼ŒOmitted Variable Bias, OVBï¼‰\n\n### æ ¸å¿ƒå…¬å¼\n\nå½“æˆ‘ä»¬é—æ¼äº†ä¸€ä¸ªæ··æ·†å˜é‡ Xï¼Œæœ´ç´ ä¼°è®¡å’ŒçœŸå®æ•ˆåº”ä¹‹é—´çš„åå·®ä¸ºï¼š\n\n$$\\text{Bias} = \\gamma \\times \\delta$$\n\nå…¶ä¸­ï¼š\n- $\\gamma$ï¼šæ··æ·†å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n- $\\delta$ï¼šæ··æ·†å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n\n### ç›´è§‰ç†è§£\n\nåå·® = (X å¯¹ Y çš„å½±å“) Ã— (X ä¸ T çš„å…³è”)\n\n**ä¸¤ä¸ªä¹˜æ•°éƒ½ä¸ä¸ºé›¶æ—¶ï¼Œæ‰ä¼šæœ‰åå·®ï¼**\n\n| æƒ…å†µ | Î³ (Xâ†’Y) | Î´ (X-T) | åå·® | ä¾‹å­ |\n|-----|---------|---------|------|------|\n| æ— åå·® | 0 | ä»»æ„ | 0 | X ä¸å½±å“ Y |\n| æ— åå·® | ä»»æ„ | 0 | 0 | X ä¸ T æ— å…³ |\n| æ­£å‘åå·® | + | + | + | é«˜ä¼°æ•ˆåº” |\n| æ­£å‘åå·® | - | - | + | é«˜ä¼°æ•ˆåº” |\n| è´Ÿå‘åå·® | + | - | - | ä½ä¼°æ•ˆåº” |\n| è´Ÿå‘åå·® | - | + | - | ä½ä¼°æ•ˆåº” |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¬ åŠ¨æ‰‹éªŒè¯åå·®å…¬å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“– ä¸šåŠ¡åœºæ™¯ï¼šç ”ç©¶åŸ¹è®­å¯¹å·¥èµ„çš„å½±å“\n# \n# å˜é‡è¯´æ˜ï¼š\n# - X: èƒ½åŠ›/ç»éªŒï¼ˆæ ‡å‡†åŒ–ï¼‰ â†’ å½±å“æ˜¯å¦å‚åŠ åŸ¹è®­ & å½±å“å·¥èµ„\n# - T: æ˜¯å¦å‚åŠ åŸ¹è®­ï¼ˆäºŒå…ƒå˜é‡ï¼‰\n# - Y: å·¥èµ„ï¼ˆåŸºå‡† 5 åƒå…ƒï¼‰\n#\n# å› æœç»“æ„ï¼šX â†’ T, X â†’ Y, T â†’ Y ï¼ˆX æ˜¯æ··æ·†å˜é‡ï¼‰\n\n# é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿæ•°æ®\nnp.random.seed(42)\nn = 2000\n\n# DAG: X â†’ T, X â†’ Y, T â†’ Y\n# X æ˜¯æ··æ·†å˜é‡\n\n# ç”Ÿæˆæ··æ·†å˜é‡ X\nX = np.random.randn(n)\n\n# ç”Ÿæˆå¤„ç† Tï¼ˆå— X å½±å“ï¼‰\n# delta = 1.5ï¼ˆX å¯¹ T çš„å½±å“ç³»æ•°ï¼‰\nT = (np.random.randn(n) + 1.5 * X > 0).astype(int)\n\n# ç”Ÿæˆç»“æœ Yï¼ˆå— T å’Œ X å½±å“ï¼‰\n# çœŸå® ATE = 2, gamma = 1.5\ntrue_ate = 2.0\ngamma_true = 1.5\nY = 5 + true_ate * T + gamma_true * X + np.random.randn(n) * 0.5\n\ndf = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n\nprint(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\nprint(f\"   æ ·æœ¬é‡: {n}\")\nprint(f\"   çœŸå® ATE: {true_ate}\")\nprint(f\"   X å¯¹ Y çš„çœŸå®æ•ˆåº” (Î³): {gamma_true}\")\nprint(f\"   å¤„ç†ç»„æ¯”ä¾‹: {T.mean():.2%}\")\ndisplay(df.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_confounding_bias(df: pd.DataFrame, confound_var: str = 'X') -> dict:\n    \"\"\"\n    è®¡ç®—æ··æ·†åå·®çš„å„ä¸ªç»„æˆéƒ¨åˆ†\n    \n    Omitted Variable Bias å…¬å¼: bias = Î³ Ã— Î´\n    \n    - Î³ (gamma, å³ Î²_X): æ··æ·†å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”(æ§åˆ¶ T å)\n    - Î´ (delta): æ··æ·†å˜é‡ X ä¸å¤„ç† T çš„å…³è”(T å¯¹ X å›å½’çš„ç³»æ•°)\n    \"\"\"\n    # æœ´ç´ ä¼°è®¡(ä¸æ§åˆ¶ X)\n    model_naive = LinearRegression()\n    model_naive.fit(df[['T']], df['Y'])\n    naive_estimate = model_naive.coef_[0]\n    \n    # è°ƒæ•´ä¼°è®¡(æ§åˆ¶ X)\n    model_adjusted = LinearRegression()\n    model_adjusted.fit(df[['T', confound_var]], df['Y'])\n    adjusted_estimate = model_adjusted.coef_[0]\n    \n    # TODO: è®¡ç®— gamma(X å¯¹ Y çš„æ•ˆåº”,æ§åˆ¶ T)\n    # æç¤º: è¿™æ˜¯ model_adjusted ä¸­ X çš„ç³»æ•°\n    gamma = None  # ğŸ‘ˆ ä½ çš„ä»£ç : model_adjusted.coef_[1]\n    \n    # TODO: è®¡ç®— delta(X ä¸ T çš„å…³è”)\n    # å›å½’ T ~ X,å– X çš„ç³»æ•°\n    model_delta = LinearRegression()\n    model_delta.fit(df[[confound_var]], df['T'])\n    delta = None  # ğŸ‘ˆ ä½ çš„ä»£ç : model_delta.coef_[0]\n    \n    # TODO: è®¡ç®—ç†è®ºåå·®\n    theoretical_bias = None  # ğŸ‘ˆ ä½ çš„ä»£ç : gamma * delta\n    \n    # å®é™…åå·®(ä»ä¼°è®¡å€¼ç›´æ¥ç®—)\n    actual_bias = naive_estimate - adjusted_estimate\n    \n    return {\n        'gamma': gamma,\n        'delta': delta,\n        'theoretical_bias': theoretical_bias,\n        'actual_bias': actual_bias,\n        'naive_estimate': naive_estimate,\n        'adjusted_estimate': adjusted_estimate\n    }"
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ(ç‚¹å‡»å±•å¼€)</summary>\n\n```python\ndef calculate_confounding_bias(df: pd.DataFrame, confound_var: str = 'X') -> dict:\n    \"\"\"è®¡ç®—æ··æ·†åå·®çš„å„ä¸ªç»„æˆéƒ¨åˆ†\"\"\"\n    from sklearn.linear_model import LinearRegression\n    \n    # æœ´ç´ ä¼°è®¡(ä¸æ§åˆ¶ X)\n    model_naive = LinearRegression()\n    model_naive.fit(df[['T']], df['Y'])\n    naive_estimate = model_naive.coef_[0]\n    \n    # è°ƒæ•´ä¼°è®¡(æ§åˆ¶ X)\n    model_adjusted = LinearRegression()\n    model_adjusted.fit(df[['T', confound_var]], df['Y'])\n    adjusted_estimate = model_adjusted.coef_[0]\n    \n    # gamma(X å¯¹ Y çš„æ•ˆåº”,æ§åˆ¶ T)\n    gamma = model_adjusted.coef_[1]\n    \n    # delta(X ä¸ T çš„å…³è”)\n    model_delta = LinearRegression()\n    model_delta.fit(df[[confound_var]], df['T'])\n    delta = model_delta.coef_[0]\n    \n    # ç†è®ºåå·® = gamma Ã— delta\n    theoretical_bias = gamma * delta\n    \n    # å®é™…åå·®\n    actual_bias = naive_estimate - adjusted_estimate\n    \n    return {\n        'gamma': gamma,\n        'delta': delta,\n        'theoretical_bias': theoretical_bias,\n        'actual_bias': actual_bias,\n        'naive_estimate': naive_estimate,\n        'adjusted_estimate': adjusted_estimate\n    }\n```\n\n**è§£é‡Š**:\n- **Î³ (gamma, å³ Î²_X)**: X å¯¹ Y çš„åå›å½’ç³»æ•°,è¡¡é‡æ§åˆ¶ T å X å¯¹ Y çš„å½±å“\n- **Î´ (delta)**: T å¯¹ X å›å½’çš„ç³»æ•°,è¡¡é‡ X å’Œ T çš„å…³è”å¼ºåº¦  \n- **åå·®å…¬å¼**: Bias = Î³ Ã— Î´(é—æ¼å˜é‡åå·®çš„ç»å…¸å…¬å¼)\n- **éªŒè¯**: theoretical_bias åº”è¯¥éå¸¸æ¥è¿‘ actual_bias\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä½ çš„ä»£ç \n",
    "result = calculate_confounding_bias(df)\n",
    "\n",
    "if result['gamma'] is not None:\n",
    "    print(\"ğŸ”¬ æ··æ·†åå·®åˆ†è§£:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nğŸ“ åå·®å…¬å¼ç»„æˆéƒ¨åˆ†:\")\n",
    "    print(f\"   Î³ (X å¯¹ Y çš„æ•ˆåº”): {result['gamma']:.4f}\")\n",
    "    print(f\"   Î´ (X ä¸ T çš„å…³è”): {result['delta']:.4f}\")\n",
    "    print(f\"\\nğŸ“Š åå·®è®¡ç®—:\")\n",
    "    print(f\"   ç†è®ºåå·® (Î³ Ã— Î´): {result['theoretical_bias']:.4f}\")\n",
    "    print(f\"   å®é™…åå·®: {result['actual_bias']:.4f}\")\n",
    "    print(f\"\\nğŸ“ˆ æ•ˆåº”ä¼°è®¡:\")\n",
    "    print(f\"   æœ´ç´ ä¼°è®¡: {result['naive_estimate']:.4f}\")\n",
    "    print(f\"   è°ƒæ•´ä¼°è®¡: {result['adjusted_estimate']:.4f}\")\n",
    "    print(f\"   çœŸå®æ•ˆåº”: {true_ate:.4f}\")\n",
    "    \n",
    "    # éªŒè¯å…¬å¼\n",
    "    if abs(result['theoretical_bias'] - result['actual_bias']) < 0.1:\n",
    "        print(f\"\\nâœ… å…¬å¼éªŒè¯æˆåŠŸï¼ç†è®ºåå·® â‰ˆ å®é™…åå·®\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ calculate_confounding_bias å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ å…³é”®æ´å¯Ÿ\n",
    "\n",
    "ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹åˆ°ï¼š\n",
    "\n",
    "1. **åå·®çš„æ–¹å‘**ï¼šÎ³ å’Œ Î´ åŒå·æ—¶ï¼Œåå·®ä¸ºæ­£ï¼ˆé«˜ä¼°æ•ˆåº”ï¼‰\n",
    "2. **åå·®çš„å¤§å°**ï¼šå–å†³äº Î³ å’Œ Î´ çš„ä¹˜ç§¯\n",
    "3. **å…¬å¼çš„ç²¾ç¡®æ€§**ï¼šç†è®ºåå·®å’Œå®é™…åå·®å‡ ä¹å®Œå…¨ä¸€è‡´ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ Part 2: æ··æ·†å¼ºåº¦å®éªŒ\n",
    "\n",
    "è®©æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶ï¼šæ··æ·†å¼ºåº¦å¦‚ä½•å½±å“ä¼°è®¡åå·®ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef experiment_confounding_strength(n: int = 2000, true_ate: float = 2.0,\n                                   confounding_strengths: list = None, seed: int = 42):\n    \"\"\"å®éªŒä¸åŒæ··æ·†å¼ºåº¦å¯¹ä¼°è®¡çš„å½±å“\"\"\"\n    if confounding_strengths is None:\n        confounding_strengths = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n    \n    np.random.seed(seed)\n    results = []\n    \n    for strength in confounding_strengths:\n        X = np.random.randn(n)\n        \n        # ç”Ÿæˆå¤„ç† Tï¼ˆå— X å½±å“ï¼Œå¼ºåº¦ç”± strength æ§åˆ¶ï¼‰\n        propensity = 1 / (1 + np.exp(-strength * X))\n        T = np.random.binomial(1, propensity)\n        \n        # ç”Ÿæˆç»“æœ Yï¼ˆå— T å’Œ X å½±å“ï¼‰\n        Y = 5 + true_ate * T + strength * X + np.random.randn(n) * 0.5\n        \n        df_temp = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n        \n        # æœ´ç´ ä¼°è®¡\n        naive_est = df_temp[df_temp['T']==1]['Y'].mean() - df_temp[df_temp['T']==0]['Y'].mean()\n        \n        # è°ƒæ•´ä¼°è®¡\n        from sklearn.linear_model import LinearRegression\n        model = LinearRegression()\n        model.fit(df_temp[['T', 'X']], df_temp['Y'])\n        adjusted_est = model.coef_[0]\n        \n        results.append({\n            'confounding_strength': strength,\n            'naive_estimate': naive_est,\n            'adjusted_estimate': adjusted_est,\n            'naive_bias': naive_est - true_ate,\n            'adjusted_bias': adjusted_est - true_ate\n        })\n    \n    return pd.DataFrame(results)\n```\n\n**è§£é‡Š**ï¼š\n- **æ··æ·†å¼ºåº¦å‚æ•°åŒ–**: strength åŒæ—¶æ§åˆ¶ Xâ†’T å’Œ Xâ†’Y çš„å¼ºåº¦\n- **è§‚å¯Ÿ**: æ··æ·†å¼ºåº¦è¶Šå¤§ï¼Œæœ´ç´ ä¼°è®¡åå·®è¶Šå¤§\n- **ç¨³å¥æ€§**: è°ƒæ•´ä¼°è®¡åœ¨å„ç§å¼ºåº¦ä¸‹éƒ½æ¥è¿‘çœŸå®å€¼\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def experiment_confounding_strength(\n    n: int = 2000,\n    true_ate: float = 2.0,\n    confounding_strengths: list = None,\n    seed: int = 42\n) -> pd.DataFrame:\n    \"\"\"\n    å®éªŒä¸åŒæ··æ·†å¼ºåº¦å¯¹ä¼°è®¡çš„å½±å“\n    \n    æ··æ·†å¼ºåº¦åŒæ—¶å½±å“:\n    - P(T=1|X): å€¾å‘å¾—åˆ†\n    - Y ä¸­ X çš„ç³»æ•°\n    \"\"\"\n    if confounding_strengths is None:\n        confounding_strengths = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n    \n    np.random.seed(seed)\n    results = []\n    \n    for strength in confounding_strengths:\n        # ç”Ÿæˆæ··æ·†æ•°æ®\n        X = np.random.randn(n)\n        \n        # TODO: ç”Ÿæˆå¤„ç† T\n        # æç¤º: P(T=1|X) = sigmoid(strength * X)\n        propensity = 1 / (1 + np.exp(-strength * X))\n        T = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n        \n        # TODO: ç”Ÿæˆç»“æœ Y\n        # æç¤º: Y = 5 + true_ate * T + strength * X + å™ªå£°\n        Y = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n        \n        if T is not None and Y is not None:\n            df_temp = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n            \n            # è®¡ç®—æœ´ç´ ä¼°è®¡\n            naive_est = df_temp[df_temp['T']==1]['Y'].mean() - df_temp[df_temp['T']==0]['Y'].mean()\n            \n            # è®¡ç®—è°ƒæ•´ä¼°è®¡\n            model = LinearRegression()\n            model.fit(df_temp[['T', 'X']], df_temp['Y'])\n            adjusted_est = model.coef_[0]\n            \n            results.append({\n                'confounding_strength': strength,\n                'naive_estimate': naive_est,\n                'adjusted_estimate': adjusted_est,\n                'naive_bias': naive_est - true_ate,\n                'adjusted_bias': adjusted_est - true_ate\n            })\n    \n    return pd.DataFrame(results) if results else None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œå®éªŒ\n",
    "exp_results = experiment_confounding_strength(n=3000)\n",
    "\n",
    "if exp_results is not None and not exp_results.empty:\n",
    "    print(\"ğŸ“Š æ··æ·†å¼ºåº¦å®éªŒç»“æœ:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(exp_results.round(3).to_string(index=False))\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å›¾1: ä¼°è®¡å€¼ vs æ··æ·†å¼ºåº¦\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(exp_results['confounding_strength'], exp_results['naive_estimate'], \n",
    "             'o-', label='æœ´ç´ ä¼°è®¡', color='red', markersize=8)\n",
    "    ax1.plot(exp_results['confounding_strength'], exp_results['adjusted_estimate'], \n",
    "             's-', label='è°ƒæ•´ä¼°è®¡', color='green', markersize=8)\n",
    "    ax1.axhline(2.0, color='blue', linestyle='--', label='çœŸå® ATE = 2', linewidth=2)\n",
    "    ax1.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax1.set_ylabel('ATE ä¼°è®¡', fontsize=12)\n",
    "    ax1.set_title('ä¼°è®¡å€¼éšæ··æ·†å¼ºåº¦çš„å˜åŒ–', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å›¾2: åå·® vs æ··æ·†å¼ºåº¦\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(exp_results['confounding_strength'] - 0.1, exp_results['naive_bias'], \n",
    "            width=0.2, label='æœ´ç´ ä¼°è®¡åå·®', color='red', alpha=0.7)\n",
    "    ax2.bar(exp_results['confounding_strength'] + 0.1, exp_results['adjusted_bias'], \n",
    "            width=0.2, label='è°ƒæ•´ä¼°è®¡åå·®', color='green', alpha=0.7)\n",
    "    ax2.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "    ax2.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax2.set_ylabel('åå·®', fontsize=12)\n",
    "    ax2.set_title('åå·®éšæ··æ·†å¼ºåº¦çš„å˜åŒ–', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "    print(\"   1. æ··æ·†å¼ºåº¦ä¸º 0 æ—¶ï¼Œæœ´ç´ ä¼°è®¡ä¹Ÿæ˜¯æ— åçš„\")\n",
    "    print(\"   2. æ··æ·†å¼ºåº¦è¶Šå¤§ï¼Œæœ´ç´ ä¼°è®¡åå·®è¶Šå¤§\")\n",
    "    print(\"   3. è°ƒæ•´ä¼°è®¡å§‹ç»ˆæ¥è¿‘çœŸå®å€¼ï¼\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ experiment_confounding_strength å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸª Part 3: Simpson's Paradoxï¼ˆè¾›æ™®æ£®æ‚–è®ºï¼‰\n",
    "\n",
    "### ä¸€ä¸ªçœŸå®çš„æ•…äº‹\n",
    "\n",
    "1973 å¹´ï¼Œä¼¯å…‹åˆ©å¤§å­¦è¢«æŒ‡æ§æ€§åˆ«æ­§è§†ï¼š\n",
    "\n",
    "| | ç”³è¯·äºº | å½•å–ç‡ |\n",
    "|--|--------|--------|\n",
    "| ç”·æ€§ | 8442 | 44% |\n",
    "| å¥³æ€§ | 4321 | 35% |\n",
    "\n",
    "çœ‹èµ·æ¥ç¡®å®æ­§è§†å¥³æ€§ï¼\n",
    "\n",
    "ä½†ä»”ç»†åˆ†ææ¯ä¸ªç³»çš„æ•°æ®åï¼Œå‘ç°**å¤§å¤šæ•°ç³»çš„å¥³æ€§å½•å–ç‡åè€Œæ›´é«˜**ï¼\n",
    "\n",
    "æ€ä¹ˆå›äº‹ï¼ŸğŸ¤¯\n",
    "\n",
    "åŸæ¥ï¼š\n",
    "- å¥³æ€§å€¾å‘äºç”³è¯·å½•å–ç‡ä½çš„ã€Œçƒ­é—¨ç³»ã€ï¼ˆå¦‚å¿ƒç†å­¦ï¼‰\n",
    "- ç”·æ€§å€¾å‘äºç”³è¯·å½•å–ç‡é«˜çš„ã€Œå†·é—¨ç³»ã€ï¼ˆå¦‚å·¥ç¨‹å­¦ï¼‰\n",
    "\n",
    "ã€Œé™¢ç³»ã€æ˜¯æ··æ·†å˜é‡ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_simpson_paradox_data(n_per_group: int = 500, seed: int = 42) -> pd.DataFrame:\n    \"\"\"\n    åˆ›å»ºå±•ç¤º Simpson's Paradox çš„æ•°æ®\n    \n    åœºæ™¯: ç ”ç©¶æŸè¯ç‰©å¯¹åº·å¤ç‡çš„å½±å“\n    - æœ‰ä¸¤ä¸ªåŒ»é™¢ (A å’Œ B)\n    - åŒ»é™¢ A æ¥æ”¶é‡ç—‡æ‚£è€…å¤šï¼Œè¯ç‰©ä½¿ç”¨ç‡é«˜\n    - åŒ»é™¢ B æ¥æ”¶è½»ç—‡æ‚£è€…å¤šï¼Œè¯ç‰©ä½¿ç”¨ç‡ä½\n    - è¯ç‰©å®é™…ä¸Šæœ‰æ­£æ•ˆåº”ï¼\n    \n    è®¾è®¡æ•°æ®ä½¿å¾—:\n    - æ•´ä½“: ç”¨è¯ç»„åº·å¤ç‡ < æœªç”¨è¯ç»„åº·å¤ç‡ï¼ˆçœ‹èµ·æ¥è¯ç‰©æœ‰å®³!ï¼‰\n    - åˆ†åŒ»é™¢: ç”¨è¯ç»„åº·å¤ç‡ > æœªç”¨è¯ç»„åº·å¤ç‡ï¼ˆè¯ç‰©å®é™…æœ‰ç›Šï¼‰\n    \"\"\"\n    np.random.seed(seed)\n    data = []\n    \n    # TODO: åŒ»é™¢ Aï¼ˆé‡ç—‡å¤šï¼Œç”¨è¯å¤šï¼‰\n    # é‡ç—‡åŸºç¡€åº·å¤ç‡ä½ï¼ˆ30%ï¼‰ï¼Œç”¨è¯æé«˜åˆ° 50%\n    # å¤§éƒ¨åˆ†é‡ç—‡æ‚£è€…åœ¨è¿™é‡Œï¼Œå¤§éƒ¨åˆ†æ¥å—æ²»ç–—\n    \n    # åŒ»é™¢ A - ç”¨è¯ç»„\n    n_A_treated = int(n_per_group * 1.5)  # ç”¨è¯äººæ•°å¤š\n    # æç¤º: ç”¨è¯ååº·å¤æ¦‚ç‡ä¸º 50%\n    recovery_A_treated = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    for i in range(n_A_treated):\n        if recovery_A_treated is not None:\n            data.append({\n                'åŒ»é™¢': 'A (é‡ç—‡)',\n                'ç”¨è¯': 1,\n                'åº·å¤': recovery_A_treated[i],\n                'ç—…æƒ…': 'é‡ç—‡'\n            })\n    \n    # åŒ»é™¢ A - æœªç”¨è¯ç»„\n    n_A_control = int(n_per_group * 0.3)  # æœªç”¨è¯äººæ•°å°‘\n    # æç¤º: æœªç”¨è¯åº·å¤æ¦‚ç‡ä¸º 30%\n    recovery_A_control = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    for i in range(n_A_control):\n        if recovery_A_control is not None:\n            data.append({\n                'åŒ»é™¢': 'A (é‡ç—‡)',\n                'ç”¨è¯': 0,\n                'åº·å¤': recovery_A_control[i],\n                'ç—…æƒ…': 'é‡ç—‡'\n            })\n    \n    # TODO: åŒ»é™¢ Bï¼ˆè½»ç—‡å¤šï¼Œç”¨è¯å°‘ï¼‰\n    # è½»ç—‡åŸºç¡€åº·å¤ç‡é«˜ï¼ˆ70%ï¼‰ï¼Œç”¨è¯æé«˜åˆ° 90%\n    \n    # åŒ»é™¢ B - ç”¨è¯ç»„\n    n_B_treated = int(n_per_group * 0.3)  # ç”¨è¯äººæ•°å°‘\n    # æç¤º: ç”¨è¯ååº·å¤æ¦‚ç‡ä¸º 90%\n    recovery_B_treated = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    for i in range(n_B_treated):\n        if recovery_B_treated is not None:\n            data.append({\n                'åŒ»é™¢': 'B (è½»ç—‡)',\n                'ç”¨è¯': 1,\n                'åº·å¤': recovery_B_treated[i],\n                'ç—…æƒ…': 'è½»ç—‡'\n            })\n    \n    # åŒ»é™¢ B - æœªç”¨è¯ç»„\n    n_B_control = int(n_per_group * 1.5)  # æœªç”¨è¯äººæ•°å¤š\n    # æç¤º: æœªç”¨è¯åº·å¤æ¦‚ç‡ä¸º 70%\n    recovery_B_control = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    for i in range(n_B_control):\n        if recovery_B_control is not None:\n            data.append({\n                'åŒ»é™¢': 'B (è½»ç—‡)',\n                'ç”¨è¯': 0,\n                'åº·å¤': recovery_B_control[i],\n                'ç—…æƒ…': 'è½»ç—‡'\n            })\n    \n    return pd.DataFrame(data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_simpson_paradox(df: pd.DataFrame) -> dict:\n    \"\"\"\n    åˆ†æ Simpson's Paradox\n    \"\"\"\n    results = {}\n    \n    # TODO: æ•´ä½“æ•ˆåº”\n    # æç¤º: åˆ†åˆ«è®¡ç®—ç”¨è¯ç»„å’Œæœªç”¨è¯ç»„çš„åº·å¤ç‡\n    overall_treated = df[df['ç”¨è¯'] == 1]['åº·å¤'].mean()\n    overall_control = df[df['ç”¨è¯'] == 0]['åº·å¤'].mean()\n    results['æ•´ä½“-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    results['æ•´ä½“-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    results['æ•´ä½“-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    # TODO: åŒ»é™¢ A æ•ˆåº”\n    # æç¤º: å…ˆç­›é€‰å‡ºåŒ»é™¢ A çš„æ•°æ®ï¼Œå†åˆ†åˆ«è®¡ç®—ç”¨è¯ç»„å’Œæœªç”¨è¯ç»„çš„åº·å¤ç‡\n    df_A = df[df['åŒ»é™¢'] == 'A (é‡ç—‡)']\n    results['åŒ»é™¢A-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    results['åŒ»é™¢A-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    results['åŒ»é™¢A-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    # TODO: åŒ»é™¢ B æ•ˆåº”\n    # æç¤º: å…ˆç­›é€‰å‡ºåŒ»é™¢ B çš„æ•°æ®ï¼Œå†åˆ†åˆ«è®¡ç®—ç”¨è¯ç»„å’Œæœªç”¨è¯ç»„çš„åº·å¤ç‡\n    df_B = df[df['åŒ»é™¢'] == 'B (è½»ç—‡)']\n    results['åŒ»é™¢B-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    results['åŒ»é™¢B-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    results['åŒ»é™¢B-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¹¶åˆ†æ Simpson's Paradox æ•°æ®\n",
    "simpson_df = create_simpson_paradox_data(n_per_group=500)\n",
    "\n",
    "if simpson_df is not None and not simpson_df.empty:\n",
    "    print(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"   æ€»æ ·æœ¬é‡: {len(simpson_df)}\")\n",
    "    print(f\"\\nå„ç»„äººæ•°:\")\n",
    "    print(simpson_df.groupby(['åŒ»é™¢', 'ç”¨è¯']).size().unstack())\n",
    "    \n",
    "    analysis = analyze_simpson_paradox(simpson_df)\n",
    "    \n",
    "    if analysis.get('æ•´ä½“-æ•ˆåº”') is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸª Simpson's Paradox åˆ†æç»“æœ:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ æ•´ä½“åˆ†æï¼ˆä¸åˆ†åŒ»é™¢ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['æ•´ä½“-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['æ•´ä½“-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['æ•´ä½“-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['æ•´ä½“-æ•ˆåº”'] < 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯ä¼¼ä¹æœ‰å®³ï¼âŒ\")\n",
    "        \n",
    "        print(f\"\\nğŸ¥ åŒ»é™¢ Aï¼ˆé‡ç—‡æ‚£è€…ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢A-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢A-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['åŒ»é™¢A-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['åŒ»é™¢A-æ•ˆåº”'] > 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯æœ‰æ•ˆï¼âœ…\")\n",
    "        \n",
    "        print(f\"\\nğŸ¥ åŒ»é™¢ Bï¼ˆè½»ç—‡æ‚£è€…ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢B-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢B-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['åŒ»é™¢B-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['åŒ»é™¢B-æ•ˆåº”'] > 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯æœ‰æ•ˆï¼âœ…\")\n",
    "        \n",
    "        print(f\"\\nğŸ­ æ‚–è®ºè§£é‡Š:\")\n",
    "        print(f\"   æ•´ä½“çœ‹èµ·æ¥ç”¨è¯æœ‰å®³ï¼Œä½†åˆ†å±‚åæ¯ä¸ªåŒ»é™¢ç”¨è¯éƒ½æœ‰æ•ˆï¼\")\n",
    "        print(f\"   åŸå› : é‡ç—‡æ‚£è€…ç”¨è¯å¤šï¼Œè½»ç—‡æ‚£è€…ä¸ç”¨è¯å¤š\")\n",
    "        print(f\"   'ç—…æƒ…ä¸¥é‡ç¨‹åº¦'æ˜¯æ··æ·†å˜é‡ï¼\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ create_simpson_paradox_data å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯è§†åŒ– Simpson's Paradox\nif simpson_df is not None and not simpson_df.empty:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # å›¾1: æ•´ä½“å¯¹æ¯”\n    ax1 = axes[0]\n    overall_data = simpson_df.groupby('ç”¨è¯')['åº·å¤'].mean()\n    colors = ['steelblue', 'coral']\n    bars = ax1.bar(['æœªç”¨è¯', 'ç”¨è¯'], [overall_data[0], overall_data[1]], color=colors)\n    ax1.set_ylabel('åº·å¤ç‡', fontsize=12)\n    ax1.set_title('æ•´ä½“åº·å¤ç‡\\nï¼ˆçœ‹èµ·æ¥ç”¨è¯æœ‰å®³ï¼ï¼‰', fontsize=14)\n    ax1.set_ylim(0, 1)\n    for bar, val in zip(bars, [overall_data[0], overall_data[1]]):\n        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n                f'{val:.1%}', ha='center', fontsize=12)\n    \n    # å›¾2: åˆ†å±‚å¯¹æ¯”\n    ax2 = axes[1]\n    stratified_data = simpson_df.groupby(['åŒ»é™¢', 'ç”¨è¯'])['åº·å¤'].mean().unstack()\n    x = np.arange(2)\n    width = 0.35\n    bars1 = ax2.bar(x - width/2, stratified_data[0], width, label='æœªç”¨è¯', color='steelblue')\n    bars2 = ax2.bar(x + width/2, stratified_data[1], width, label='ç”¨è¯', color='coral')\n    ax2.set_xticks(x)\n    ax2.set_xticklabels(['åŒ»é™¢A (é‡ç—‡)', 'åŒ»é™¢B (è½»ç—‡)'])\n    ax2.set_ylabel('åº·å¤ç‡', fontsize=12)\n    ax2.set_title('åˆ†å±‚åº·å¤ç‡\\nï¼ˆä¸¤ä¸ªåŒ»é™¢ç”¨è¯éƒ½æœ‰æ•ˆï¼ï¼‰', fontsize=14)\n    ax2.legend()\n    ax2.set_ylim(0, 1)\n    \n    for bars, col in [(bars1, 0), (bars2, 1)]:\n        for bar, hospital in zip(bars, stratified_data.index):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                    f'{stratified_data.loc[hospital, col]:.1%}', ha='center', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Simpson's Paradox çš„æœ¬è´¨\n",
    "\n",
    "Simpson's Paradox ä¸æ˜¯çœŸæ­£çš„ã€Œæ‚–è®ºã€ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ï¼š\n",
    "\n",
    "1. **æ•´ä½“è¶‹åŠ¿ â‰  åˆ†å±‚è¶‹åŠ¿**: å½“å­˜åœ¨æ··æ·†å˜é‡æ—¶ï¼Œæ•´ä½“æ•°æ®å¯èƒ½ç»™å‡ºè¯¯å¯¼æ€§çš„ç»“è®º\n",
    "2. **å› æœæ–¹å‘å¾ˆé‡è¦**: åº”è¯¥æ§åˆ¶æ··æ·†å˜é‡ï¼ˆç—…æƒ…ï¼‰ï¼Œè€Œä¸æ˜¯è¢«å®ƒè¯¯å¯¼\n",
    "3. **æ•°æ®ä¼šè¯´è°**: æ²¡æœ‰æ­£ç¡®çš„å› æœåˆ†æï¼Œæ•°æ®å¯èƒ½å‘Šè¯‰ä½ å®Œå…¨ç›¸åçš„ç»“è®ºï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”® Part 4: æ•æ„Ÿæ€§åˆ†æ\n",
    "\n",
    "### å½“æ··æ·†å˜é‡ä¸å¯è§‚æµ‹æ—¶æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "åœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸æ— æ³•è§‚æµ‹åˆ°æ‰€æœ‰æ··æ·†å˜é‡ã€‚æ¯”å¦‚ï¼š\n",
    "\n",
    "- ç ”ç©¶å¸çƒŸå¯¹è‚ºç™Œçš„å½±å“ï¼Œä½†æ— æ³•è§‚æµ‹ã€Œé—ä¼ å› ç´ ã€\n",
    "- ç ”ç©¶æ•™è‚²å¯¹æ”¶å…¥çš„å½±å“ï¼Œä½†æ— æ³•è§‚æµ‹ã€Œå¤©èµ‹ã€\n",
    "\n",
    "è¿™æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦é—®ï¼š**å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·†ï¼Œæˆ‘ä»¬çš„ç»“è®ºè¿˜å¯é å—ï¼Ÿ**\n",
    "\n",
    "æ•æ„Ÿæ€§åˆ†æå°±æ˜¯ç”¨æ¥å›ç­”è¿™ä¸ªé—®é¢˜çš„ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_to_unmeasured_confounding(\n",
    "    df: pd.DataFrame,\n",
    "    gamma_range: np.ndarray = None,\n",
    "    delta_range: np.ndarray = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æ•æ„Ÿæ€§åˆ†æ: å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·†ï¼Œä¼°è®¡ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ\n",
    "    \n",
    "    å‡è®¾å­˜åœ¨æœªè§‚æµ‹æ··æ·†å˜é‡ U:\n",
    "    - U å¯¹ Y çš„æ•ˆåº”ä¸º gamma_u\n",
    "    - U ä¸ T çš„å…³è”ä¸º delta_u\n",
    "    - åå·® = gamma_u * delta_u\n",
    "    \n",
    "    å¯¹ä¸åŒçš„ (gamma_u, delta_u) ç»„åˆï¼Œè®¡ç®—å¯èƒ½çš„çœŸå®æ•ˆåº”\n",
    "    \"\"\"\n",
    "    if gamma_range is None:\n",
    "        gamma_range = np.linspace(-2, 2, 9)\n",
    "    if delta_range is None:\n",
    "        delta_range = np.linspace(-1, 1, 9)\n",
    "    \n",
    "    # å½“å‰ä¼°è®¡ï¼ˆå‡è®¾å·²æ§åˆ¶è§‚æµ‹åˆ°çš„æ··æ·†ï¼‰\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[['T', 'X']], df['Y'])\n",
    "    current_estimate = model.coef_[0]\n",
    "    \n",
    "    results = []\n",
    "    for gamma_u in gamma_range:\n",
    "        for delta_u in delta_range:\n",
    "            # å¯èƒ½çš„åå·®ï¼ˆå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼‰\n",
    "            possible_bias = gamma_u * delta_u\n",
    "            # å¯èƒ½çš„çœŸå®æ•ˆåº”\n",
    "            possible_true_effect = current_estimate - possible_bias\n",
    "            \n",
    "            results.append({\n",
    "                'gamma_u': gamma_u,\n",
    "                'delta_u': delta_u,\n",
    "                'possible_bias': possible_bias,\n",
    "                'possible_true_effect': possible_true_effect\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ•æ„Ÿæ€§åˆ†æ\n",
    "sensitivity = sensitivity_to_unmeasured_confounding(df)\n",
    "\n",
    "print(\"ğŸ”® æ•æ„Ÿæ€§åˆ†æç»“æœ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å½“å‰ä¼°è®¡\n",
    "model = LinearRegression()\n",
    "model.fit(df[['T', 'X']], df['Y'])\n",
    "current_est = model.coef_[0]\n",
    "\n",
    "print(f\"å½“å‰ä¼°è®¡ï¼ˆæ§åˆ¶ X åï¼‰: {current_est:.4f}\")\n",
    "print(f\"\\nå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† U:\")\n",
    "print(f\"   å¯èƒ½çš„çœŸå®æ•ˆåº”èŒƒå›´: [{sensitivity['possible_true_effect'].min():.2f}, {sensitivity['possible_true_effect'].max():.2f}]\")\n",
    "\n",
    "# æ‰¾å‡ºä½¿æ•ˆåº”å˜ä¸º 0 æˆ–è´Ÿæ•°çš„æ¡ä»¶\n",
    "zero_effect = sensitivity[sensitivity['possible_true_effect'] < 0]\n",
    "if len(zero_effect) > 0:\n",
    "    print(f\"\\nâš ï¸ ä»¥ä¸‹æƒ…å†µä¼šä½¿æ•ˆåº”å˜ä¸ºè´Ÿæ•°:\")\n",
    "    print(f\"   éœ€è¦ Î³_u Ã— Î´_u > {current_est:.2f}\")\n",
    "    print(f\"   ä¾‹å¦‚: Î³_u = 2, Î´_u = 1.5 (å¼ºæ··æ·†)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ•æ„Ÿæ€§åˆ†æ\n",
    "pivot = sensitivity.pivot_table(\n",
    "    index='gamma_u', \n",
    "    columns='delta_u', \n",
    "    values='possible_true_effect'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    pivot, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap='RdYlGn',\n",
    "    center=0,\n",
    "    vmin=-2,\n",
    "    vmax=4\n",
    ")\n",
    "plt.xlabel('Î´_u (U ä¸ T çš„å…³è”)', fontsize=12)\n",
    "plt.ylabel('Î³_u (U å¯¹ Y çš„æ•ˆåº”)', fontsize=12)\n",
    "plt.title('æ•æ„Ÿæ€§åˆ†æ: å¯èƒ½çš„çœŸå®æ•ˆåº”\\n(çº¢è‰² = è´Ÿæ•ˆåº”, ç»¿è‰² = æ­£æ•ˆåº”)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š çƒ­åŠ›å›¾è§£è¯»:\")\n",
    "print(\"   - ä¸­å¿ƒ (0, 0): æ— æœªè§‚æµ‹æ··æ·†ï¼Œæ•ˆåº” = å½“å‰ä¼°è®¡\")\n",
    "print(\"   - ç»¿è‰²åŒºåŸŸ: å³ä½¿æœ‰æœªè§‚æµ‹æ··æ·†ï¼Œæ•ˆåº”ä»ä¸ºæ­£\")\n",
    "print(\"   - çº¢è‰²åŒºåŸŸ: æœªè§‚æµ‹æ··æ·†å¯èƒ½ä½¿æ•ˆåº”ä¸ºè´Ÿ\")\n",
    "print(\"   - æ•ˆåº”ä¸º 0 çš„è¾¹ç•Œ: Î³_u Ã— Î´_u = å½“å‰ä¼°è®¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ğŸ“ Part 5: æ€è€ƒé¢˜\n\n### é—®é¢˜ 1: ä¸ºä»€ä¹ˆ Simpson's Paradox ä¸æ˜¯çœŸæ­£çš„æ‚–è®ºï¼Ÿ\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n\n<details>\n<summary>ğŸ’¡ å‚è€ƒæ€è·¯</summary>\n\nSimpson's Paradox ä¸æ˜¯çœŸæ­£çš„æ‚–è®ºï¼Œå› ä¸ºå®ƒä¸æ˜¯é€»è¾‘çŸ›ç›¾ï¼Œè€Œæ˜¯æ··æ·†å˜é‡å¯¼è‡´çš„ç»Ÿè®¡ç°è±¡ï¼š\n\n1. **æœ¬è´¨æ˜¯æ··æ·†åå·®**: æ•´ä½“æ•°æ®ä¸­å­˜åœ¨æœªåˆ†å±‚çš„æ··æ·†å˜é‡ï¼ˆå¦‚åŒ»é™¢ã€ç—…æƒ…ä¸¥é‡ç¨‹åº¦ï¼‰ï¼Œå¯¼è‡´æ•´ä½“è¶‹åŠ¿è¢«è¯¯å¯¼\n\n2. **å› æœç»“æ„æ¸…æ™°**: ä¸€æ—¦è¯†åˆ«å› æœç»“æ„ï¼ˆé€šè¿‡å› æœå›¾ï¼‰ï¼Œå°±èƒ½æ˜ç¡®åº”è¯¥åˆ†å±‚è¿˜æ˜¯èšåˆï¼š\n   - å¦‚æœå˜é‡æ˜¯æ··æ·†å˜é‡ â†’ åº”è¯¥åˆ†å±‚ï¼ˆæ§åˆ¶è¯¥å˜é‡ï¼‰\n   - å¦‚æœå˜é‡æ˜¯ä¸­ä»‹æˆ–ç¢°æ’å˜é‡ â†’ ä¸åº”è¯¥åˆ†å±‚\n\n3. **ä¸åŒçš„ç ”ç©¶é—®é¢˜ï¼Œä¸åŒçš„ç­”æ¡ˆ**:\n   - é—®\"åœ¨ç›¸åŒæ¡ä»¶ä¸‹ç”¨è¯æ˜¯å¦æœ‰æ•ˆï¼Ÿ\" â†’ åº”è¯¥åˆ†å±‚ï¼ˆæ§åˆ¶ç—…æƒ…ï¼‰\n   - é—®\"ç”¨è¯ç»„æ•´ä½“åº·å¤ç‡å¦‚ä½•ï¼Ÿ\" â†’ å¯ä»¥çœ‹æ•´ä½“ï¼ˆä½†è¦ç†è§£æ··æ·†ï¼‰\n\n4. **æ•°æ®ä¸ä¼šè¯´è°ï¼Œä½†ä¼šè¯¯å¯¼**: æ•°æ®æœ¬èº«æ²¡æœ‰é—®é¢˜ï¼Œé—®é¢˜åœ¨äºæ²¡æœ‰æ­£ç¡®çš„å› æœåˆ†ææ¡†æ¶\n\n**å…³é”®æ´å¯Ÿ**: ç»Ÿè®¡å…³è”ä¸ç­‰äºå› æœå…³ç³»ã€‚æ­£ç¡®çš„å› æœæ¨æ–­éœ€è¦é¢†åŸŸçŸ¥è¯† + å› æœå›¾ + åé—¨å‡†åˆ™ç­‰å·¥å…·ã€‚\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### é—®é¢˜ 2: åå·®å…¬å¼ Bias = Î³ Ã— Î´ ä¸­ï¼Œä»€ä¹ˆæƒ…å†µä¸‹åå·®ä¸º 0ï¼Ÿ\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n\n<details>\n<summary>ğŸ’¡ å‚è€ƒæ€è·¯</summary>\n\nåå·®ä¸º 0 çš„ä¸¤ç§æƒ…å†µï¼š\n\n**æƒ…å†µ 1: Î³ = 0ï¼ˆæ··æ·†å˜é‡ X å¯¹ç»“æœ Y æ²¡æœ‰ç›´æ¥å½±å“ï¼‰**\n- å³ä½¿ X ä¸ T ç›¸å…³ï¼ˆÎ´ â‰  0ï¼‰ï¼Œé—æ¼ X ä¹Ÿä¸ä¼šé€ æˆåå·®\n- ä¾‹å­ï¼šX åªå½±å“å¤„ç†åˆ†é…ï¼ˆæ˜¯å¦å‚åŠ åŸ¹è®­ï¼‰ï¼Œä½†ä¸ç›´æ¥å½±å“ç»“æœï¼ˆå·¥èµ„ï¼‰\n- å› æœå›¾ï¼šX â†’ T â†’ Yï¼ˆX ä¸ç›´æ¥æŒ‡å‘ Yï¼‰\n\n**æƒ…å†µ 2: Î´ = 0ï¼ˆæ··æ·†å˜é‡ X ä¸å¤„ç† T ä¸ç›¸å…³ï¼‰**\n- å³ä½¿ X å½±å“ Yï¼ˆÎ³ â‰  0ï¼‰ï¼Œé—æ¼ X ä¹Ÿä¸ä¼šé€ æˆåå·®\n- ä¾‹å­ï¼šåœ¨éšæœºå®éªŒä¸­ï¼ŒT ç‹¬ç«‹äºæ‰€æœ‰åå˜é‡ X\n- å› æœå›¾ï¼šX â†’ Yï¼ŒT â†’ Yï¼ˆX å’Œ T ä¹‹é—´æ— è¿æ¥ï¼‰\n\n**å…³é”®æ´å¯Ÿ**:\n1. **åªæœ‰æ··æ·†å˜é‡æ‰é€ æˆåå·®**: å¿…é¡»åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š\n   - X å½±å“ Yï¼ˆÎ³ â‰  0ï¼‰\n   - X ä¸ T ç›¸å…³ï¼ˆÎ´ â‰  0ï¼‰\n\n2. **ä¸æ˜¯æ‰€æœ‰åå˜é‡éƒ½éœ€è¦æ§åˆ¶**: \n   - åªå½±å“ Y ä¸å½±å“ T çš„å˜é‡ï¼ˆé¢„æµ‹å˜é‡ï¼‰â†’ ä¸éœ€è¦æ§åˆ¶ï¼ˆä½†æ§åˆ¶å¯ä»¥æé«˜ç²¾åº¦ï¼‰\n   - åªå½±å“ T ä¸å½±å“ Y çš„å˜é‡ï¼ˆå·¥å…·å˜é‡ï¼‰â†’ ä¸éœ€è¦æ§åˆ¶\n   - åŒæ—¶å½±å“ T å’Œ Y çš„å˜é‡ï¼ˆæ··æ·†å˜é‡ï¼‰â†’ å¿…é¡»æ§åˆ¶\n\n3. **éšæœºå®éªŒçš„ä¼˜åŠ¿**: éšæœºåŒ–ç¡®ä¿ Î´ = 0 å¯¹æ‰€æœ‰å˜é‡æˆç«‹ï¼Œå› æ­¤æ— åå·®\n\n**å®è·µæ„ä¹‰**: è¿™æ˜¯åé—¨å‡†åˆ™çš„ç†è®ºåŸºç¡€ - åªéœ€æ§åˆ¶åŒæ—¶å½±å“å¤„ç†å’Œç»“æœçš„æ··æ·†å˜é‡ã€‚\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### é—®é¢˜ 3: åœ¨å®è·µä¸­ï¼Œå¦‚ä½•åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ··æ·†ï¼Ÿ\n\n**æç¤º:** æƒ³æƒ³é¢†åŸŸçŸ¥è¯†ã€æ•æ„Ÿæ€§åˆ†æã€é˜´æ€§å¯¹ç…§...\n\n**ä½ çš„ç­”æ¡ˆ:**\n\n*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n\n<details>\n<summary>ğŸ’¡ å‚è€ƒæ€è·¯</summary>\n\nåˆ¤æ–­æ˜¯å¦å­˜åœ¨æ··æ·†çš„å¤šç§æ–¹æ³•ï¼š\n\n**1. é¢†åŸŸçŸ¥è¯†å’Œå› æœå›¾**\n- åŸºäºä¸“ä¸šçŸ¥è¯†åˆ—å‡ºå¯èƒ½çš„æ··æ·†å˜é‡\n- ç»˜åˆ¶å› æœå›¾ï¼Œè¯†åˆ«åé—¨è·¯å¾„\n- å’¨è¯¢é¢†åŸŸä¸“å®¶ï¼Œäº†è§£å½±å“å¤„ç†åˆ†é…å’Œç»“æœçš„å› ç´ \n- ä¾‹å­ï¼šç ”ç©¶å¸çƒŸå¯¹è‚ºç™Œçš„å½±å“ï¼Œè€ƒè™‘é—ä¼ ã€èŒä¸šã€ç”Ÿæ´»ä¹ æƒ¯ç­‰\n\n**2. æ•æ„Ÿæ€§åˆ†æï¼ˆSensitivity Analysisï¼‰**\n- è¯„ä¼°ä¼°è®¡å¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§\n- è®¡ç®—éœ€è¦å¤šå¼ºçš„æœªè§‚æµ‹æ··æ·†æ‰èƒ½æ¨ç¿»ç»“è®º\n- Rosenbaum bounds, E-value ç­‰æ–¹æ³•\n- å¦‚æœéœ€è¦éå¸¸å¼ºçš„æ··æ·†æ‰èƒ½æ”¹å˜ç»“è®º â†’ ç»“æœè¾ƒç¨³å¥\n\n**3. é˜´æ€§å¯¹ç…§ï¼ˆNegative Controlï¼‰**\n- **é˜´æ€§å¯¹ç…§ç»“æœ**: å¤„ç†ç†è®ºä¸Šä¸åº”è¯¥å½±å“çš„ç»“æœ\n  - å¦‚æœè¿™äº›ç»“æœä¹Ÿæ˜¾ç¤º\"æ•ˆåº”\" â†’ å¯èƒ½å­˜åœ¨æ··æ·†\n  - ä¾‹å­ï¼šå¸çƒŸå¯¹è‚ºç™Œæœ‰å½±å“ï¼Œä½†å¯¹éª¨æŠ˜ä¸åº”æœ‰å½±å“\n  \n- **é˜´æ€§å¯¹ç…§æš´éœ²**: ä¸çœŸå®å¤„ç†ç±»ä¼¼ä½†ä¸åº”æœ‰å› æœæ•ˆåº”çš„\"å‡å¤„ç†\"\n  - å¦‚æœ\"å‡å¤„ç†\"ä¹Ÿæ˜¾ç¤ºæ•ˆåº” â†’ å¯èƒ½å­˜åœ¨æ··æ·†\n  - ä¾‹å­ï¼šç ”ç©¶è¿åŠ¨å¯¹å¥åº·çš„å½±å“ï¼Œç”¨\"çœ‹ç”µè§†\"ä½œä¸ºé˜´æ€§å¯¹ç…§\n\n**4. æ£€æŸ¥åå˜é‡å¹³è¡¡ï¼ˆBalance Checkï¼‰**\n- æ¯”è¾ƒå¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„åå˜é‡åˆ†å¸ƒ\n- å¦‚æœå­˜åœ¨æ˜¾è‘—å·®å¼‚ â†’ å¯èƒ½å­˜åœ¨é€‰æ‹©åå·®/æ··æ·†\n- å¸¸ç”¨æ–¹æ³•ï¼šæ ‡å‡†åŒ–å‡å€¼å·®ï¼ˆSMDï¼‰ã€å€¾å‘å¾—åˆ†åˆ†å¸ƒå¯¹æ¯”\n\n**5. å®‰æ…°å‰‚æ£€éªŒï¼ˆPlacebo Testï¼‰**\n- åœ¨ä¸åº”è¯¥æœ‰æ•ˆåº”çš„æ—¶é—´/åœ°ç‚¹/äººç¾¤ä¸Šæµ‹è¯•\n- å¦‚æœä¹Ÿå‘ç°\"æ•ˆåº”\" â†’ å¯èƒ½æ˜¯æ··æ·†è€Œéå› æœ\n- ä¾‹å­ï¼šæ”¿ç­–åœ¨ 2020 å¹´å®æ–½ï¼Œæ£€éªŒ 2018 å¹´æ˜¯å¦æœ‰\"æ•ˆåº”\"\n\n**6. éšæœºåŒ–æ¨æ–­çš„å¯å‘**\n- å¦‚æœæŸå˜é‡åœ¨éšæœºå®éªŒä¸­ä¸éœ€è¦æ§åˆ¶ï¼Œåœ¨è§‚æµ‹ç ”ç©¶ä¸­æ§åˆ¶åä¼°è®¡ä»æ˜¾è‘—å˜åŒ– â†’ è¯¥å˜é‡æ˜¯æ··æ·†\n- å¯¹æ¯”ç®€å•å›å½’å’Œå¤šå…ƒå›å½’çš„ç³»æ•°å˜åŒ–\n\n**7. æ•°æ®é©±åŠ¨çš„æ–¹æ³•**\n- åŒé‡æœºå™¨å­¦ä¹ ï¼ˆDouble MLï¼‰ï¼šåŒæ—¶å»ºæ¨¡å¤„ç†å’Œç»“æœ\n- å› æœæ£®æ—ï¼ˆCausal Forestï¼‰ï¼šè¯†åˆ«å¼‚è´¨æ€§æ•ˆåº”\n- å¦‚æœä¸åŒæ–¹æ³•ä¼°è®¡å·®å¼‚å¾ˆå¤§ â†’ å¯èƒ½å­˜åœ¨æ··æ·†\n\n**å®è·µå»ºè®®**:\n- **å¤šç®¡é½ä¸‹**: ç»“åˆå¤šç§æ–¹æ³•ï¼Œè€Œéä¾èµ–å•ä¸€æ£€éªŒ\n- **ä¿æŒè°¨æ…**: æ°¸è¿œæ— æ³•100%ç¡®å®šæ²¡æœ‰æœªè§‚æµ‹æ··æ·†\n- **é€æ˜æŠ¥å‘Š**: åœ¨ç ”ç©¶ä¸­æ˜ç¡®è¯´æ˜å¯èƒ½çš„æ··æ·†å’Œå±€é™æ€§\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ¯ é¢è¯•é¢˜æ¨¡æ‹Ÿ\n\n### æ¦‚å¿µé¢˜\n\n**Q1: è§£é‡Šé—æ¼å˜é‡åå·®ï¼ˆOVBï¼‰å…¬å¼ Bias = Î³ Ã— Î´ï¼Œå¹¶è¯´æ˜ä»€ä¹ˆæƒ…å†µä¸‹åå·®ä¸ºé›¶ã€‚**\n\n<details>\n<summary>ç­”æ¡ˆ</summary>\n\n**OVB å…¬å¼**: Bias = Î³ Ã— Î´\n\n- **Î³ (gamma)**: é—æ¼å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n- **Î´ (delta)**: é—æ¼å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n\n**åå·®ä¸ºé›¶çš„æƒ…å†µ**:\n\n1. **Î³ = 0**: X å¯¹ Y æ²¡æœ‰ç›´æ¥å½±å“\n   - å³ä½¿ X ä¸ T ç›¸å…³ï¼Œé—æ¼ X ä¹Ÿä¸ä¼šé€ æˆåå·®\n   - ä¾‹å­ï¼šX åªå½±å“ Tï¼Œä¸å½±å“ Y\n\n2. **Î´ = 0**: X ä¸ T ä¸ç›¸å…³ï¼ˆç‹¬ç«‹ï¼‰\n   - å³ä½¿ X å½±å“ Yï¼Œé—æ¼ X ä¹Ÿä¸ä¼šé€ æˆåå·®\n   - ä¾‹å­ï¼šéšæœºå®éªŒä¸­ï¼ŒT ç‹¬ç«‹äºæ‰€æœ‰åå˜é‡\n\n**å…³é”®æ´å¯Ÿ**: \n- åªæœ‰å½“ X åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶æ—¶æ‰ä¼šäº§ç”Ÿåå·®ï¼š\n  1. X å½±å“ Yï¼ˆÎ³ â‰  0ï¼‰\n  2. X ä¸ T ç›¸å…³ï¼ˆÎ´ â‰  0ï¼‰\n- è¿™æ­£æ˜¯æ··æ·†å˜é‡çš„å®šä¹‰ï¼\n\n**å®è·µæ„ä¹‰**:\n- ä¸æ˜¯æ‰€æœ‰é—æ¼å˜é‡éƒ½ä¼šé€ æˆåå·®\n- åªéœ€æ§åˆ¶åŒæ—¶å½±å“ T å’Œ Y çš„å˜é‡\n- è¿™æ˜¯åé—¨å‡†åˆ™çš„ç†è®ºåŸºç¡€\n\n</details>\n\n---\n\n**Q2: Simpson's Paradox åœ¨å› æœæ¨æ–­ä¸­çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ**\n\n<details>\n<summary>ç­”æ¡ˆ</summary>\n\n**Simpson's Paradox**: æ•´ä½“è¶‹åŠ¿å’Œåˆ†å±‚è¶‹åŠ¿å¯èƒ½ç›¸åçš„ç°è±¡\n\n**æ ¸å¿ƒæ•™è®­**:\n\n1. **èšåˆæ•°æ®ä¼šè¯¯å¯¼**: ä¸åˆ†å±‚çš„æ•´ä½“æ•°æ®å¯èƒ½ç»™å‡ºé”™è¯¯ç»“è®º\n2. **æ··æ·†æ˜¯ç½ªé­ç¥¸é¦–**: æ‚–è®ºçš„æ ¹æºæ˜¯å­˜åœ¨æ··æ·†å˜é‡\n3. **å› æœæ–¹å‘å¾ˆé‡è¦**: éœ€è¦çŸ¥é“å˜é‡ä¹‹é—´çš„å› æœå…³ç³»æ‰èƒ½æ­£ç¡®åˆ†æ\n\n**ç»å…¸ä¾‹å­**: \n\nä¼¯å…‹åˆ©æ€§åˆ«æ­§è§†æ¡ˆï¼ˆ1973ï¼‰:\n- æ•´ä½“ï¼šç”·æ€§å½•å–ç‡ 44%ï¼Œå¥³æ€§ 35% â†’ çœ‹èµ·æ¥æ­§è§†å¥³æ€§\n- åˆ†é™¢ç³»ï¼šå¤§å¤šæ•°é™¢ç³»å¥³æ€§å½•å–ç‡æ›´é«˜æˆ–ç›¸åŒ\n- åŸå› ï¼šå¥³æ€§å€¾å‘ç”³è¯·ç«äº‰æ›´æ¿€çƒˆçš„é™¢ç³»\n\n**åˆ¤æ–­æ ‡å‡†**: åº”è¯¥åˆ†å±‚è¿˜æ˜¯èšåˆï¼Ÿ\n\nå…³é”®åœ¨äºå› æœç»“æ„ï¼š\n- å¦‚æœé™¢ç³»æ˜¯**æ··æ·†å˜é‡**ï¼ˆå½±å“æ€§åˆ«åˆ†å¸ƒå’Œå½•å–ç‡ï¼‰â†’ åº”è¯¥åˆ†å±‚\n- å¦‚æœé™¢ç³»æ˜¯**ç¢°æ’å˜é‡**æˆ–**ä¸­ä»‹** â†’ ä¸åº”è¯¥åˆ†å±‚\n\n**ä¸€èˆ¬åŸåˆ™**:\n1. å…ˆç”»å› æœå›¾\n2. åº”ç”¨åé—¨å‡†åˆ™\n3. æ§åˆ¶æ··æ·†å˜é‡ï¼Œä¸æ§åˆ¶ä¸­ä»‹/ç¢°æ’å˜é‡\n\n**å®è·µè­¦ç¤º**:\n- ä¸è¦ç›²ç›®ç›¸ä¿¡èšåˆç»Ÿè®¡\n- ä¹Ÿä¸è¦ç›²ç›®åˆ†å±‚ï¼ˆå¯èƒ½å¼•å…¥ç¢°æ’åå·®ï¼‰\n- éœ€è¦é¢†åŸŸçŸ¥è¯†å’Œå› æœæ€ç»´\n\n</details>\n\n---\n\n**Q3: ä»€ä¹ˆæ˜¯æ•æ„Ÿæ€§åˆ†æï¼Ÿä¸ºä»€ä¹ˆåœ¨å› æœæ¨æ–­ä¸­å¾ˆé‡è¦ï¼Ÿ**\n\n<details>\n<summary>ç­”æ¡ˆ</summary>\n\n**æ•æ„Ÿæ€§åˆ†æ**: è¯„ä¼°ä¼°è®¡ç»“æœå¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§\n\n**æ ¸å¿ƒæ€æƒ³**: \n- æˆ‘ä»¬æ°¸è¿œæ— æ³•ç¡®å®šæ˜¯å¦è§‚æµ‹åˆ°äº†æ‰€æœ‰æ··æ·†å˜é‡\n- æ•æ„Ÿæ€§åˆ†æé—®ï¼š\"å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼Œç»“è®ºä¼šæ”¹å˜å—ï¼Ÿ\"\n\n**æ–¹æ³•**:\n\n1. **å‚æ•°åŒ–æœªè§‚æµ‹æ··æ·†**:\n   - Î³_U: U å¯¹ Y çš„å½±å“\n   - Î´_U: U ä¸ T çš„å…³è”\n\n2. **è®¡ç®—å¯èƒ½çš„åå·®èŒƒå›´**:\n   - çœŸå®æ•ˆåº” âˆˆ [ä¼°è®¡å€¼ - Î³_UÃ—Î´_U, ä¼°è®¡å€¼ + Î³_UÃ—Î´_U]\n\n3. **è¯„ä¼°ç¨³å¥æ€§**:\n   - éœ€è¦å¤šå¼ºçš„æ··æ·†æ‰èƒ½æ¨ç¿»ç»“è®ºï¼Ÿ\n   - è¿™æ ·çš„æ··æ·†åˆç†å—ï¼Ÿ\n\n**Rosenbaum Bounds ä¾‹å­**:\n\nå‡è®¾å½“å‰ä¼°è®¡ ATE = 5ï¼Œæ˜¾è‘—æ€§ p < 0.05\n\n| Î“å€¼ | å«ä¹‰ | på€¼èŒƒå›´ | ç»“è®º |\n|-----|------|---------|------|\n| 1.0 | æ— æœªè§‚æµ‹æ··æ·† | [0.01, 0.01] | æ˜¾è‘— |\n| 1.5 | å€¾å‘å¾—åˆ†å¯ç›¸å·®50% | [0.01, 0.08] | ä»æ˜¾è‘— |\n| 2.0 | å€¾å‘å¾—åˆ†å¯ç›¸å·®100% | [0.01, 0.15] | ä¸ç¨³å¥ |\n\nè§£è¯»ï¼š\n- Î“ = 1.5 æ—¶ç»“è®ºä»æ˜¾è‘— â†’ ç»“æœç›¸å¯¹ç¨³å¥\n- Î“ = 2.0 æ—¶ç»“è®ºå¯èƒ½ä¸æ˜¾è‘— â†’ å­˜åœ¨é£é™©\n\n**å®è·µä»·å€¼**:\n\n1. **è¯šå®æ€§**: æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œä¸è¿‡åº¦è‡ªä¿¡\n2. **è¯´æœåŠ›**: å±•ç¤ºç»“æœå¯¹å‡è®¾çš„ä¾èµ–ç¨‹åº¦\n3. **æŒ‡å¯¼æ”¶é›†æ•°æ®**: æŒ‡å‡ºå“ªäº›æœªè§‚æµ‹å˜é‡æœ€å…³é”®\n\n**ä½•æ—¶ä½¿ç”¨**:\n- è§‚æµ‹ç ”ç©¶ï¼ˆééšæœºï¼‰\n- å·²æ§åˆ¶å·²çŸ¥æ··æ·†ï¼Œä½†æ‹…å¿ƒé—æ¼\n- éœ€è¦å‘åˆ©ç›Šç›¸å…³è€…è¯æ˜ç»“è®ºçš„ç¨³å¥æ€§\n\n</details>\n\n---\n\n### ç¼–ç¨‹é¢˜\n\n**Q4: å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ¨¡æ‹Ÿ Simpson's Paradoxï¼Œå±•ç¤ºæ•´ä½“å’Œåˆ†å±‚çš„æ•ˆåº”ä¼°è®¡ã€‚**\n\n<details>\n<summary>å‚è€ƒä»£ç </summary>\n\n```python\ndef create_simpsons_paradox_example(n_per_group=500, seed=42):\n    \"\"\"\n    åˆ›å»º Simpson's Paradox ç¤ºä¾‹\n    \n    åœºæ™¯ï¼šè¯„ä¼°æ–°è¯æ•ˆæœ\n    - ä¸¤ä¸ªåŒ»é™¢ A å’Œ B\n    - åŒ»é™¢ A ä¸»è¦æ”¶æ²»é‡ç—‡ï¼ˆç”¨è¯å¤šï¼‰\n    - åŒ»é™¢ B ä¸»è¦æ”¶æ²»è½»ç—‡ï¼ˆç”¨è¯å°‘ï¼‰\n    - æ¯ä¸ªåŒ»é™¢å†…ç”¨è¯éƒ½æœ‰æ­£æ•ˆåº”\n    - ä½†æ•´ä½“çœ‹èµ·æ¥ç”¨è¯æ•ˆæœä¸ºè´Ÿï¼\n    \"\"\"\n    np.random.seed(seed)\n    data = []\n    \n    # åŒ»é™¢ Aï¼ˆé‡ç—‡ä¸ºä¸»ï¼‰\n    # åŸºç¡€åº·å¤ç‡ä½ï¼Œä½†ç”¨è¯æé«˜åº·å¤ç‡\n    n_A_treated = int(n_per_group * 1.6)  # åŒ»é™¢Aç”¨è¯å¤š\n    n_A_control = int(n_per_group * 0.4)\n    \n    recovery_A_treated = np.random.binomial(1, 0.50, n_A_treated)  # ç”¨è¯åº·å¤ç‡50%\n    recovery_A_control = np.random.binomial(1, 0.30, n_A_control)  # ä¸ç”¨è¯30%\n    \n    for r in recovery_A_treated:\n        data.append({'åŒ»é™¢': 'A(é‡ç—‡)', 'ç”¨è¯': 1, 'åº·å¤': r})\n    for r in recovery_A_control:\n        data.append({'åŒ»é™¢': 'A(é‡ç—‡)', 'ç”¨è¯': 0, 'åº·å¤': r})\n    \n    # åŒ»é™¢ Bï¼ˆè½»ç—‡ä¸ºä¸»ï¼‰\n    # åŸºç¡€åº·å¤ç‡é«˜ï¼Œç”¨è¯è¿›ä¸€æ­¥æé«˜\n    n_B_treated = int(n_per_group * 0.4)  # åŒ»é™¢Bç”¨è¯å°‘\n    n_B_control = int(n_per_group * 1.6)\n    \n    recovery_B_treated = np.random.binomial(1, 0.90, n_B_treated)  # ç”¨è¯åº·å¤ç‡90%\n    recovery_B_control = np.random.binomial(1, 0.70, n_B_control)  # ä¸ç”¨è¯70%\n    \n    for r in recovery_B_treated:\n        data.append({'åŒ»é™¢': 'B(è½»ç—‡)', 'ç”¨è¯': 1, 'åº·å¤': r})\n    for r in recovery_B_control:\n        data.append({'åŒ»é™¢': 'B(è½»ç—‡)', 'ç”¨è¯': 0, 'åº·å¤': r})\n    \n    df = pd.DataFrame(data)\n    \n    # è®¡ç®—æ•ˆåº”\n    # æ•´ä½“æ•ˆåº”ï¼ˆé”™è¯¯çš„ï¼ï¼‰\n    overall_treated = df[df['ç”¨è¯']==1]['åº·å¤'].mean()\n    overall_control = df[df['ç”¨è¯']==0]['åº·å¤'].mean()\n    overall_effect = overall_treated - overall_control\n    \n    # åˆ†å±‚æ•ˆåº”ï¼ˆæ­£ç¡®çš„ï¼‰\n    df_A = df[df['åŒ»é™¢']=='A(é‡ç—‡)']\n    effect_A = df_A[df_A['ç”¨è¯']==1]['åº·å¤'].mean() - df_A[df_A['ç”¨è¯']==0]['åº·å¤'].mean()\n    \n    df_B = df[df['åŒ»é™¢']=='B(è½»ç—‡)']\n    effect_B = df_B[df_B['ç”¨è¯']==1]['åº·å¤'].mean() - df_B[df_B['ç”¨è¯']==0]['åº·å¤'].mean()\n    \n    # æ ‡å‡†åŒ–æ•ˆåº”ï¼ˆæŒ‰æ€»ä½“åŒ»é™¢åˆ†å¸ƒåŠ æƒï¼‰\n    weight_A = (len(df_A) / len(df))\n    weight_B = (len(df_B) / len(df))\n    standardized_effect = effect_A * weight_A + effect_B * weight_B\n    \n    print(f\"ğŸ“Š Simpson's Paradox ç¤ºä¾‹åˆ†æ\")\n    print(f\"=\" * 60)\n    print(f\"\\næ•´ä½“æ•ˆåº”ï¼ˆä¸åˆ†å±‚ï¼‰: {overall_effect:+.1%}\")\n    print(f\"  ç”¨è¯ç»„åº·å¤ç‡: {overall_treated:.1%}\")\n    print(f\"  å¯¹ç…§ç»„åº·å¤ç‡: {overall_control:.1%}\")\n    if overall_effect < 0:\n        print(f\"  âŒ é”™è¯¯ç»“è®ºï¼šç”¨è¯æœ‰å®³ï¼\")\n    \n    print(f\"\\nåˆ†å±‚æ•ˆåº”:\")\n    print(f\"  åŒ»é™¢Aï¼ˆé‡ç—‡ï¼‰: {effect_A:+.1%} âœ…\")\n    print(f\"  åŒ»é™¢Bï¼ˆè½»ç—‡ï¼‰: {effect_B:+.1%} âœ…\")\n    print(f\"  âœ… æ­£ç¡®ç»“è®ºï¼šæ¯ä¸ªåŒ»é™¢å†…ç”¨è¯éƒ½æœ‰ç›Šï¼\")\n    \n    print(f\"\\næ ‡å‡†åŒ–æ•ˆåº”ï¼ˆæ­£ç¡®åŠ æƒï¼‰: {standardized_effect:+.1%}\")\n    print(f\"\\nğŸ” æ‚–è®ºåŸå› :\")\n    print(f\"  - åŒ»é™¢Aï¼ˆé‡ç—‡ï¼‰ç”¨è¯æ¯”ä¾‹é«˜: {df_A['ç”¨è¯'].mean():.1%}\")\n    print(f\"  - åŒ»é™¢Bï¼ˆè½»ç—‡ï¼‰ç”¨è¯æ¯”ä¾‹ä½: {df_B['ç”¨è¯'].mean():.1%}\")\n    print(f\"  - ç”¨è¯ç»„æ··å…¥äº†æ›´å¤šé‡ç—‡æ‚£è€…ï¼Œæ‹‰ä½äº†æ•´ä½“åº·å¤ç‡\")\n    \n    return df\n\n# è¿è¡Œç¤ºä¾‹\ndf_simpson = create_simpsons_paradox_example()\n```\n\n**å…³é”®ç‚¹**:\n- æ•´ä½“æ•ˆåº”è¢«æ··æ·†å˜é‡ï¼ˆåŒ»é™¢/ç—…æƒ…ï¼‰è¯¯å¯¼\n- æ¯ä¸ªå±‚å†…æ•ˆåº”éƒ½æ˜¯æ­£çš„ï¼Œä½†æ•´ä½“å¯èƒ½æ˜¯è´Ÿçš„\n- å¿…é¡»æŒ‰æ­£ç¡®æƒé‡æ ‡å‡†åŒ–\n\n</details>\n\n---\n\n**Q5: å®ç°æ•æ„Ÿæ€§åˆ†æå‡½æ•°ï¼Œè¯„ä¼°ä¼°è®¡å¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§ã€‚**\n\n<details>\n<summary>å‚è€ƒä»£ç ï¼ˆå·²åœ¨notebookä¸­å®ç°ï¼‰</summary>\n\nè§ notebook ä¸­çš„ `sensitivity_to_unmeasured_confounding` å‡½æ•°ã€‚\n\n**ä½¿ç”¨ç¤ºä¾‹**:\n\n```python\n# å‡è®¾å½“å‰ä¼°è®¡ ATE = 5ï¼ˆæ§åˆ¶äº†è§‚æµ‹åˆ°çš„ Xï¼‰\n# è¯„ä¼°ï¼šå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼Œæ•ˆåº”ä¼šåœ¨ä»€ä¹ˆèŒƒå›´ï¼Ÿ\n\ngamma_range = np.linspace(-3, 3, 13)  # U å¯¹ Y çš„å¯èƒ½å½±å“\ndelta_range = np.linspace(-1, 1, 11)  # U ä¸ T çš„å¯èƒ½å…³è”\n\nsensitivity = sensitivity_to_unmeasured_confounding(\n    df, gamma_range=gamma_range, delta_range=delta_range\n)\n\n# æŸ¥çœ‹åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æ•ˆåº”ä¼šå˜å·\nprint(\"ä½¿æ•ˆåº”å˜ä¸ºè´Ÿæ•°çš„æƒ…å†µ:\")\nnegative_cases = sensitivity[sensitivity['possible_true_effect'] < 0]\nprint(negative_cases[['gamma_u', 'delta_u', 'possible_true_effect']])\n\n# åˆ¤æ–­ç¨³å¥æ€§\nif len(negative_cases) > 0:\n    min_gamma = negative_cases['gamma_u'].abs().min()\n    min_delta = negative_cases['delta_u'].abs().min()\n    print(f\"\\néœ€è¦ |Î³_U| â‰¥ {min_gamma:.2f} ä¸” |Î´_U| â‰¥ {min_delta:.2f} æ‰èƒ½æ¨ç¿»ç»“è®º\")\n    print(f\"åˆ¤æ–­ï¼šè¿™æ ·çš„æœªè§‚æµ‹æ··æ·†æ˜¯å¦åˆç†ï¼Ÿ\")\nelse:\n    print(f\"\\nåœ¨ç»™å®šå‚æ•°èŒƒå›´å†…ï¼Œæ•ˆåº”å§‹ç»ˆä¸ºæ­£\")\n    print(f\"ç»“è®ºè¾ƒç¨³å¥\")\n```\n\n**å®è·µæŠ€å·§**:\n1. æ ¹æ®é¢†åŸŸçŸ¥è¯†è®¾å®šåˆç†çš„ Î³ å’Œ Î´ èŒƒå›´\n2. å¯è§†åŒ–æ•æ„Ÿæ€§çƒ­å›¾ï¼ˆå¦‚notebookä¸­æ‰€ç¤ºï¼‰\n3. å…³æ³¨ä½¿ç»“è®ºå‘ç”Ÿè´¨å˜çš„ä¸´ç•Œå€¼\n4. ä¸å·²è§‚æµ‹æ··æ·†çš„å½±å“å¯¹æ¯”ï¼Œè¯„ä¼°åˆç†æ€§\n\n</details>\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ“ æ•°å­¦æ¨å¯¼\n\n### 1. é—æ¼å˜é‡åå·®å…¬å¼(OVB)\n\n**åœºæ™¯**: çœŸå®æ¨¡å‹ä¸º $Y = \\beta_0 + \\beta_T T + \\beta_X X + \\varepsilon$,ä½†æˆ‘ä»¬é—æ¼äº† X,åªä¼°è®¡ $Y = \\alpha_0 + \\alpha_T T + u$\n\n**å®šç†**: é—æ¼ X å¯¼è‡´çš„åå·®ä¸º:\n\n$$\\text{Bias}(\\hat{\\alpha}_T) = E[\\hat{\\alpha}_T] - \\beta_T = \\beta_X \\cdot \\delta$$\n\nå…¶ä¸­:\n- $\\beta_X$ (è®°ä¸º $\\gamma$):X å¯¹ Y çš„çœŸå®æ•ˆåº”(æ§åˆ¶ T å)\n- $\\delta$:T å¯¹ X å›å½’çš„ç³»æ•°,å³ $T = \\gamma_0 + \\delta X + v$\n\n**è¯æ˜**:\n\n1. çœŸå®æ¨¡å‹:\n   $$Y = \\beta_0 + \\beta_T T + \\beta_X X + \\varepsilon$$\n\n2. é—æ¼ X å,ä¼°è®¡çš„æ¨¡å‹:\n   $$Y = \\alpha_0 + \\alpha_T T + u$$\n\n3. å¯ä»¥è¯æ˜(é€šè¿‡æœ€å°äºŒä¹˜çš„æ­£è§„æ–¹ç¨‹):\n   $$E[\\hat{\\alpha}_T] = \\beta_T + \\beta_X \\cdot \\frac{\\text{Cov}(T, X)}{\\text{Var}(T)}$$\n\n4. è€Œ $\\delta = \\frac{\\text{Cov}(T, X)}{\\text{Var}(X)}$ æ˜¯ T å¯¹ X å›å½’çš„ç³»æ•°,å› æ­¤:\n   $$\\frac{\\text{Cov}(T, X)}{\\text{Var}(T)} = \\delta \\cdot \\frac{\\text{Var}(X)}{\\text{Var}(T)}$$\n\n5. åœ¨å®è·µä¸­,æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¼°è®¡ Î´:\n   - å›å½’ $T \\sim X$,å¾—åˆ° X çš„ç³»æ•°\n   - è¿™ä¸ªç³»æ•°åæ˜ äº† X ä¸ T çš„å…³è”å¼ºåº¦\n\n**ç¬¦å·è¯´æ˜**:\n- $\\beta_X$ (gamma, $\\gamma$): X å¯¹ Y çš„çœŸå®æ•ˆåº”(æ§åˆ¶ T å)\n- $\\delta$: X ä¸ T çš„å…³è”(é€šè¿‡ T ~ X å›å½’å¾—åˆ°)\n\n**åå·®æ–¹å‘**:\n| $\\beta_X$ (Î³) | $\\delta$ | åå·® | è§£é‡Š |\n|----------|---------|------|------|\n| + | + | + | é«˜ä¼° |\n| + | - | - | ä½ä¼° |\n| - | + | - | ä½ä¼° |\n| - | - | + | é«˜ä¼° |\n\n**å…³é”®ç†è§£**:\n- Î³ (æ§åˆ¶ T å X å¯¹ Y çš„æ•ˆåº”)å¯ä»¥ä» Y ~ T + X çš„å›å½’ä¸­å¾—åˆ°\n- Î´ (X ä¸ T çš„å…³è”)å¯ä»¥ä» T ~ X çš„å›å½’ä¸­å¾—åˆ°\n- åå·® = Î³ Ã— Î´,åªæœ‰ä¸¤è€…éƒ½ä¸ä¸ºé›¶æ—¶æ‰æœ‰åå·®\n\n---\n\n### 2. é€‰æ‹©åå·®çš„æ•°å­¦è¡¨è¾¾\n\n**å®šä¹‰**: å½“å¤„ç†åˆ†é…ä¸æ½œåœ¨ç»“æœç›¸å…³æ—¶äº§ç”Ÿçš„åå·®ã€‚\n\n$$\\begin{align}\nE[Y|T=1] - E[Y|T=0] &= E[Y(1)|T=1] - E[Y(0)|T=0] \\\\\n&= \\underbrace{E[Y(1) - Y(0)|T=1]}_{\\text{ATT(çœŸå®æ•ˆåº”)}} \\\\\n&\\quad + \\underbrace{E[Y(0)|T=1] - E[Y(0)|T=0]}_{\\text{é€‰æ‹©åå·®}}\n\\end{align}$$\n\n**é€‰æ‹©åå·®é¡¹**: $E[Y(0)|T=1] - E[Y(0)|T=0]$\n\n**è§£é‡Š**: å³ä½¿ä¸æ¥å—å¤„ç†,å¤„ç†ç»„å’Œå¯¹ç…§ç»„çš„ç»“æœä¹Ÿä¸åŒã€‚\n\n**ä¾‹å­**:\n- T = ä¸Šå¤§å­¦,Y = æ”¶å…¥\n- é€‰æ‹©åå·® = \"ä¸Šå¤§å­¦çš„äººå³ä½¿ä¸ä¸Šå¤§å­¦æ”¶å…¥ä¹Ÿä¼šæ›´é«˜\"\n- åŸå› :ä¸Šå¤§å­¦çš„äººå¯èƒ½æœ¬æ¥å°±æ›´èªæ˜ã€æ›´åŠªåŠ›\n\n**æ¶ˆé™¤æ–¹æ³•**:\n1. éšæœºåŒ–:ä½¿ $E[Y(0)|T=1] = E[Y(0)|T=0]$\n2. æ¡ä»¶ç‹¬ç«‹å‡è®¾:æ§åˆ¶æ‰€æœ‰å½±å“é€‰æ‹©çš„åå˜é‡ X\n3. å·¥å…·å˜é‡ã€DID ç­‰å‡†å®éªŒæ–¹æ³•\n\n---\n\n### 3. Simpson's Paradox çš„æ¦‚ç‡è®ºè§£é‡Š\n\n**ç°è±¡**: æ•´ä½“è¶‹åŠ¿å’Œåˆ†å±‚è¶‹åŠ¿ç›¸å\n\n**æ•°å­¦æ¡ä»¶**: å­˜åœ¨å˜é‡ Z ä½¿å¾—:\n\n$$\\text{sign}\\left(\\frac{P(Y=1|T=1)}{P(Y=1|T=0)}\\right) \\neq \\text{sign}\\left(\\frac{P(Y=1|T=1, Z=z)}{P(Y=1|T=0, Z=z)}\\right)$$\n\nå¯¹æ‰€æœ‰ z æˆç«‹ã€‚\n\n**ä¾‹å­**: åŒ»é™¢æ•°æ®\n\n|  | ç”¨è¯åº·å¤ç‡ | æœªç”¨è¯åº·å¤ç‡ | ç”¨è¯æ›´å¥½? |\n|--|-----------|-------------|-----------|\n| æ•´ä½“ | 40% | 50% | âœ— |\n| åŒ»é™¢A(é‡ç—‡) | 50% | 30% | âœ“ |\n| åŒ»é™¢B(è½»ç—‡) | 90% | 70% | âœ“ |\n\n**æ•°å­¦éªŒè¯**:\n\nè®¾:\n- $n_{A,1} = 200$ (åŒ»é™¢Aç”¨è¯), $n_{A,0} = 100$ (åŒ»é™¢Aä¸ç”¨è¯)\n- $n_{B,1} = 100$ (åŒ»é™¢Bç”¨è¯), $n_{B,0} = 200$ (åŒ»é™¢Bä¸ç”¨è¯)\n\næ•´ä½“åº·å¤ç‡:\n$$P(Y=1|T=1) = \\frac{200 \\times 0.5 + 100 \\times 0.9}{300} = \\frac{190}{300} \\approx 0.63$$\n\nä½†è¿™**é”™è¯¯åœ°**æ··åˆäº†ä¸åŒä¸¥é‡ç¨‹åº¦çš„æ‚£è€…!\n\næ­£ç¡®æ–¹æ³•(æ ‡å‡†åŒ–):\n$$P(Y(1)=1) = P(Y=1|T=1, Z=A) \\cdot P(Z=A) + P(Y=1|T=1, Z=B) \\cdot P(Z=B)$$\n\nä½¿ç”¨**æ€»ä½“**çš„åŒ»é™¢åˆ†å¸ƒä½œä¸ºæƒé‡,è€Œéç”¨è¯ç»„çš„åˆ†å¸ƒã€‚\n\n---\n\n### 4. æ•æ„Ÿæ€§åˆ†æçš„æ•°å­¦æ¡†æ¶\n\n**é—®é¢˜**: å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† U,ä¼°è®¡ä¼šå¦‚ä½•å˜åŒ–?\n\n**æ¨¡å‹**:\n\n$$Y = \\beta_0 + \\beta_T T + \\beta_X X + \\beta_U U + \\varepsilon$$\n\nå…¶ä¸­ U æœªè§‚æµ‹ã€‚\n\n**å½“å‰ä¼°è®¡**(æ§åˆ¶ X å):\n\n$$\\hat{\\beta}_T^{obs} = \\beta_T + \\beta_U \\delta_U$$\n\nå…¶ä¸­ $\\delta_U$ æ˜¯ U å¯¹ T å›å½’æ—¶çš„ç³»æ•°(æ§åˆ¶ X å)ã€‚\n\n**æ•æ„Ÿæ€§å‚æ•°**:\n- $\\Gamma$ = æœ€å¤§å€¾å‘å¾—åˆ†æ¯”å€¼:$\\frac{P(T=1|X,U)/(1-P(T=1|X,U))}{P(T=1|X)/(1-P(T=1|X))}$\n- $\\Lambda$ = U å¯¹ Y çš„æœ€å¤§å½±å“\n\n**Rosenbaum bounds**: \n$$\\beta_T \\in [\\hat{\\beta}_T^{obs} - \\beta_U \\delta_U, \\hat{\\beta}_T^{obs} + \\beta_U \\delta_U]$$\n\n**å®è·µåº”ç”¨**:\n- å¦‚æœ $\\Gamma = 1.5$(å€¾å‘å¾—åˆ†æœ€å¤šç›¸å·® 50%),ç»“è®ºä»ç„¶æ˜¾è‘— â†’ ç»“æœç¨³å¥\n- å¦‚æœ $\\Gamma = 2$ å°±ä¼šæ”¹å˜ç»“è®º â†’ ç»“æœä¸ç¨³å¥\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### é—®é¢˜ 4: å®é™…æ¡ˆä¾‹åˆ†æ\n\n**åœºæ™¯**: è¯„ä¼°ä¿ƒé”€é‚®ä»¶å¯¹è´­ä¹°çš„å½±å“\n\n- T: æ˜¯å¦æ”¶åˆ°ä¿ƒé”€é‚®ä»¶\n- Y: æ˜¯å¦è´­ä¹°\n\nè¯·åˆ†æ:\n1. åˆ—å‡ºå¯èƒ½çš„æ··æ·†å˜é‡\n2. æ¯ä¸ªæ··æ·†å˜é‡å¦‚ä½•åŒæ—¶å½±å“ T å’Œ Yï¼Ÿ\n3. æœ´ç´ ä¼°è®¡ä¼šé«˜ä¼°è¿˜æ˜¯ä½ä¼°çœŸå®æ•ˆåº”ï¼Ÿ\n4. å¦‚ä½•æ§åˆ¶è¿™äº›æ··æ·†ï¼Ÿ\n\n**ä½ çš„åˆ†æ:**\n\n*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n\n<details>\n<summary>ğŸ’¡ å‚è€ƒæ€è·¯</summary>\n\n**1. å¯èƒ½çš„æ··æ·†å˜é‡**\n\n| æ··æ·†å˜é‡ | è¯´æ˜ |\n|---------|------|\n| å†å²è´­ä¹°é¢‘æ¬¡ | è€å®¢æˆ·æ›´å¯èƒ½æ”¶åˆ°é‚®ä»¶ï¼Œä¹Ÿæ›´å¯èƒ½è´­ä¹° |\n| ç”¨æˆ·æ´»è·ƒåº¦ | æ´»è·ƒç”¨æˆ·æ›´å¯èƒ½æ”¶åˆ°é‚®ä»¶ï¼Œä¹Ÿæ›´å¯èƒ½è´­ä¹° |\n| è´­ä¹°èƒ½åŠ› | é«˜æ”¶å…¥ç”¨æˆ·æ›´å¯èƒ½è¢«å®šå‘æŠ•æ”¾ï¼Œä¹Ÿæ›´å¯èƒ½è´­ä¹° |\n| å“ç±»åå¥½ | å¯¹ä¿ƒé”€å“ç±»æ„Ÿå…´è¶£çš„ç”¨æˆ·æ›´å¯èƒ½æ”¶åˆ°é‚®ä»¶ï¼Œä¹Ÿæ›´å¯èƒ½è´­ä¹° |\n| ä¼šå‘˜ç­‰çº§ | é«˜çº§ä¼šå‘˜æ›´å¯èƒ½æ”¶åˆ°ä¸“å±é‚®ä»¶ï¼Œä¹Ÿæ›´å¯èƒ½è´­ä¹° |\n| åœ°ç†ä½ç½® | æŸäº›åœ°åŒºé‚®ä»¶æŠ•æ”¾ç‡é«˜ï¼Œè´­ä¹°åŠ›ä¹Ÿå¯èƒ½ä¸åŒ |\n\n**2. æ¯ä¸ªæ··æ·†å˜é‡å¦‚ä½•å½±å“ T å’Œ Y**\n\nä»¥\"å†å²è´­ä¹°é¢‘æ¬¡\"ä¸ºä¾‹ï¼š\n\n```\nå†å²è´­ä¹°é¢‘æ¬¡ (X)\n    â†™          â†˜\næ”¶åˆ°é‚®ä»¶ (T)  â†’  è´­ä¹° (Y)\n```\n\n- **X â†’ T**: è¥é”€ç³»ç»Ÿå€¾å‘äºç»™è€å®¢æˆ·å‘é‚®ä»¶ï¼ˆÎ´ > 0ï¼‰\n- **X â†’ Y**: è€å®¢æˆ·æœ¬æ¥å°±æ›´å¯èƒ½è´­ä¹°ï¼ˆÎ³ > 0ï¼‰\n- **åå·®**: Î³ Ã— Î´ > 0ï¼ˆæ­£å‘åå·®ï¼Œé«˜ä¼°é‚®ä»¶æ•ˆåº”ï¼‰\n\nå…¶ä»–å˜é‡ç±»ä¼¼ï¼Œéƒ½ä¼šå¯¼è‡´ï¼š\n- æ”¶åˆ°é‚®ä»¶çš„äººæœ¬æ¥å°±æ›´å¯èƒ½è´­ä¹°\n- æœ´ç´ ä¼°è®¡ä¼šé«˜ä¼°é‚®ä»¶çš„çœŸå®å› æœæ•ˆåº”\n\n**3. æœ´ç´ ä¼°è®¡ä¼šé«˜ä¼°è¿˜æ˜¯ä½ä¼°ï¼Ÿ**\n\n**ä¼šé«˜ä¼°ï¼** åŸå› ï¼š\n\n- å¤§éƒ¨åˆ†æ··æ·†å˜é‡éƒ½æ˜¯\"æ­£å‘é€‰æ‹©\"ï¼š\n  - æ›´æœ‰è´­ä¹°å€¾å‘çš„ç”¨æˆ· â†’ æ›´å¯èƒ½æ”¶åˆ°é‚®ä»¶\n  - Î³ > 0, Î´ > 0 â†’ åå·® > 0\n  \n- æœ´ç´ ä¼°è®¡ï¼š\n  ```\n  E[Y|T=1] - E[Y|T=0] = ATE + æ­£å‘é€‰æ‹©åå·®\n  ```\n\n- ä¾‹å­ï¼š\n  - æ”¶åˆ°é‚®ä»¶ç»„è´­ä¹°ç‡ 20%\n  - æœªæ”¶åˆ°é‚®ä»¶ç»„è´­ä¹°ç‡ 5%\n  - æœ´ç´ ä¼°è®¡ï¼š20% - 5% = 15%\n  - ä½†çœŸå®å› æœæ•ˆåº”å¯èƒ½åªæœ‰ 5%\n  - å‰©ä½™ 10% æ˜¯é€‰æ‹©åå·®\n\n**4. å¦‚ä½•æ§åˆ¶è¿™äº›æ··æ·†ï¼Ÿ**\n\n**æ–¹æ³• 1: å®éªŒè®¾è®¡ï¼ˆæœ€ä¼˜ï¼‰**\n- A/B æµ‹è¯•ï¼šéšæœºåˆ†é…ç”¨æˆ·åˆ°é‚®ä»¶ç»„å’Œå¯¹ç…§ç»„\n- ç¡®ä¿ Î´ = 0ï¼ˆæ··æ·†å˜é‡ä¸å¤„ç†ç‹¬ç«‹ï¼‰\n\n**æ–¹æ³• 2: å›å½’è°ƒæ•´**\n```python\n# æ§åˆ¶æ··æ·†å˜é‡\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(df[['T', 'history_purchase', 'activity', 'income']], df['Y'])\nate_adjusted = model.coef_[0]  # T çš„ç³»æ•°\n```\n\n**æ–¹æ³• 3: å€¾å‘å¾—åˆ†åŒ¹é…ï¼ˆPSMï¼‰**\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\n\n# 1. ä¼°è®¡å€¾å‘å¾—åˆ†\nps_model = LogisticRegression()\nps_model.fit(X_confounders, df['T'])\nps = ps_model.predict_proba(X_confounders)[:, 1]\n\n# 2. åŒ¹é…\n# ä¸ºæ¯ä¸ªå¤„ç†ç»„ä¸ªä½“æ‰¾åˆ°å€¾å‘å¾—åˆ†ç›¸ä¼¼çš„å¯¹ç…§ç»„ä¸ªä½“\n\n# 3. åœ¨åŒ¹é…æ ·æœ¬ä¸Šä¼°è®¡ ATE\n```\n\n**æ–¹æ³• 4: åŒé‡å·®åˆ†ï¼ˆå¦‚æœæœ‰æ—¶é—´ç»´åº¦ï¼‰**\n- å¯¹æ¯”æ”¶åˆ°é‚®ä»¶å‰åçš„è´­ä¹°å˜åŒ–\n- æ§åˆ¶æ—¶é—´ä¸å˜çš„ç”¨æˆ·ç‰¹å¾\n\n**æ–¹æ³• 5: å·¥å…·å˜é‡ï¼ˆå¦‚æœæœ‰åˆé€‚çš„å·¥å…·ï¼‰**\n- æ‰¾ä¸€ä¸ªåªå½±å“æ˜¯å¦æ”¶åˆ°é‚®ä»¶ï¼Œä¸ç›´æ¥å½±å“è´­ä¹°çš„å˜é‡\n- ä¾‹å¦‚ï¼šé‚®ä»¶ç³»ç»Ÿçš„éšæœºå´©æºƒå¯¼è‡´éƒ¨åˆ†ç”¨æˆ·æœªæ”¶åˆ°é‚®ä»¶\n\n**æ–¹æ³• 6: æ–­ç‚¹å›å½’ï¼ˆRDDï¼‰**\n- å¦‚æœé‚®ä»¶å‘æ”¾æœ‰æ˜ç¡®çš„æˆªæ–­ç‚¹\n- ä¾‹å¦‚ï¼šåªç»™è´­ä¹°é‡‘é¢ > 100 å…ƒçš„ç”¨æˆ·å‘é‚®ä»¶\n- å¯¹æ¯”æˆªæ–­ç‚¹é™„è¿‘çš„ç”¨æˆ·\n\n**å®è·µå»ºè®®**:\n1. **é¦–é€‰ A/B æµ‹è¯•**: å¦‚æœå¯èƒ½ï¼ŒéšæœºåŒ–æ˜¯é‡‘æ ‡å‡†\n2. **å¤šæ–¹æ³•éªŒè¯**: ç”¨ä¸åŒæ–¹æ³•ä¼°è®¡ï¼Œæ£€æŸ¥ç¨³å¥æ€§\n3. **æ•æ„Ÿæ€§åˆ†æ**: è¯„ä¼°æœªè§‚æµ‹æ··æ·†çš„å½±å“\n4. **é¢†åŸŸçŸ¥è¯†**: ç¡®ä¿æ§åˆ¶äº†æ‰€æœ‰é‡è¦çš„æ··æ·†å˜é‡\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "$$\\text{åå·®} = \\gamma \\times \\delta = \\text{(Xå¯¹Yçš„æ•ˆåº”)} \\times \\text{(Xä¸Tçš„å…³è”)}$$\n",
    "\n",
    "### å…³é”®çŸ¥è¯†ç‚¹\n",
    "\n",
    "| æ¦‚å¿µ | å®šä¹‰ | å¯ç¤º |\n",
    "|-----|------|------|\n",
    "| æ··æ·†åå·® | é—æ¼æ··æ·†å˜é‡å¯¼è‡´çš„ä¼°è®¡åå·® | å¯ä»¥ç²¾ç¡®è®¡ç®—ï¼ |\n",
    "| Simpson's Paradox | æ•´ä½“è¶‹åŠ¿ä¸åˆ†å±‚è¶‹åŠ¿ç›¸å | ä¸æ˜¯æ‚–è®ºï¼Œæ˜¯æ··æ·† |\n",
    "| æ•æ„Ÿæ€§åˆ†æ | è¯„ä¼°æœªè§‚æµ‹æ··æ·†çš„å½±å“ | å³ä½¿æœ‰é—æ¼ï¼Œä¹Ÿèƒ½è¯„ä¼° |\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "1. **ç”»å› æœå›¾**: åœ¨åˆ†æå‰å…ˆç”»å‡ºå‡è®¾çš„å› æœç»“æ„\n",
    "2. **è¯†åˆ«æ··æ·†**: æ‰¾å‡ºåŒæ—¶å½±å“å¤„ç†å’Œç»“æœçš„å˜é‡\n",
    "3. **åšæ•æ„Ÿæ€§åˆ†æ**: è¯„ä¼°ç»“è®ºå¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§\n",
    "4. **è°¨æ…ä¸‹ç»“è®º**: é™¤éæœ‰å¼ºæœ‰åŠ›çš„è¯æ®ï¼Œå¦åˆ™ä¿æŒè°¨æ…\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬æ·±åˆ»ç†è§£äº†æ··æ·†åå·®ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ å…·ä½“çš„**å¤„ç†æ–¹æ³•**â€”â€”å€¾å‘å¾—åˆ†åŒ¹é… (PSM)ï¼\n",
    "\n",
    "---\n",
    "\n",
    "**ã€ŒçŸ¥é“åå·®æœ‰å¤šå¤§ï¼Œæ¯”ä¸çŸ¥é“åå·®å­˜åœ¨è¦å¥½å¾—å¤šã€‚ã€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}