{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¯ Part 7.3: æ—¶å˜å¤„ç†æ•ˆåº” (Time-Varying Treatment Effects)\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç»ƒä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. ç†è§£æ—¶å˜å¤„ç†çš„æŒ‘æˆ˜å’Œç°å®æ„ä¹‰\n",
    "2. æŒæ¡è¾¹é™…ç»“æ„æ¨¡å‹ (Marginal Structural Models, MSM) çš„æ ¸å¿ƒæ€æƒ³\n",
    "3. å®ç°æ—¶å˜å¤„ç†çš„é€†æ¦‚ç‡åŠ æƒ (IPTW)\n",
    "4. ç†è§£ G-computation æ–¹æ³•\n",
    "5. æŒæ¡åºè´¯å¿½ç•¥å‡è®¾ (Sequential Ignorability)\n",
    "6. åœ¨çœŸå®ä¸šåŠ¡åœºæ™¯ä¸­åº”ç”¨æ—¶å˜å¤„ç†åˆ†æ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "story",
   "metadata": {},
   "source": [
    "## ğŸ¬ çœŸå®åœºæ™¯ï¼šåœ¨çº¿æ•™è‚²å¹³å°çš„ä¸ªæ€§åŒ–æ¨é€ç­–ç•¥\n",
    "\n",
    "æƒ³è±¡ä½ æ˜¯æŸåœ¨çº¿æ•™è‚²å¹³å°çš„æ•°æ®ç§‘å­¦å®¶ï¼Œè´Ÿè´£ä¼˜åŒ–ç”¨æˆ·çš„å­¦ä¹ æ¨é€ç­–ç•¥ã€‚\n",
    "\n",
    "### åœºæ™¯æè¿°\n",
    "\n",
    "å¹³å°æœ‰ä¸€é¡¹ã€Œå­¦ä¹ æé†’ã€åŠŸèƒ½ï¼Œæ¯å¤©å¯ä»¥ç»™ç”¨æˆ·å‘é€å­¦ä¹ æ¨é€é€šçŸ¥ï¼š\n",
    "\n",
    "- **ğŸ‘¨â€ğŸ’¼ åŠªåŠ›çš„å°å¼ **ï¼šå‰ 3 å¤©éƒ½è®¤çœŸå­¦ä¹ ï¼Œç³»ç»Ÿæ¯å¤©æ¨é€ã€‚ç¬¬ 4 å¤©å› ä¸ºæ¨é€å¤ªé¢‘ç¹æœ‰ç‚¹åŒçƒ¦ï¼Œå­¦ä¹ æ—¶é•¿ä¸‹é™äº†\n",
    "- **ğŸ‘©â€ğŸ“ æ‘†çƒ‚çš„å°æ**ï¼šå‰ 3 å¤©éƒ½æ²¡å­¦ä¹ ï¼Œç³»ç»Ÿåœæ­¢æ¨é€ã€‚ç¬¬ 4 å¤©çªç„¶æƒ³å­¦ä¹ äº†ï¼Œä½†æ²¡æœ‰æ¨é€æé†’é”™è¿‡äº†\n",
    "- **ğŸ§“ ä½›ç³»çš„è€ç‹**ï¼šæ—¶å­¦æ—¶ä¸å­¦ï¼Œç³»ç»Ÿæ ¹æ®ä»–çš„å†å²è¡Œä¸ºåŠ¨æ€è°ƒæ•´æ¨é€é¢‘ç‡\n",
    "\n",
    "### æ ¸å¿ƒé—®é¢˜\n",
    "\n",
    "**é—®é¢˜ 1ï¼š** å¦‚ä½•è¯„ä¼°ã€Œè¿ç»­ 7 å¤©æ¨é€ã€vsã€Œä¸æ¨é€ã€å¯¹ç”¨æˆ·æœ€ç»ˆå­¦ä¹ æˆæœçš„å½±å“ï¼Ÿ\n",
    "\n",
    "**é—®é¢˜ 2ï¼š** ç”¨æˆ·è¿‡å»çš„å­¦ä¹ è¡Œä¸ºä¼šå½±å“ï¼š\n",
    "- ä»Šå¤©æ˜¯å¦æ”¶åˆ°æ¨é€ï¼ˆå¤„ç†åˆ†é…ï¼‰\n",
    "- æœªæ¥çš„å­¦ä¹ è¡¨ç°ï¼ˆç»“æœå˜é‡ï¼‰\n",
    "\n",
    "è¿™å°±æ˜¯**æ—¶é—´ä¾èµ–æ€§æ··æ·†**ï¼ˆtime-dependent confoundingï¼‰ï¼\n",
    "\n",
    "### ä¸ºä»€ä¹ˆä¼ ç»Ÿæ–¹æ³•å¤±æ•ˆï¼Ÿ\n",
    "\n",
    "å¦‚æœç”¨ä¼ ç»Ÿçš„ IPW æˆ– PSM:\n",
    "\n",
    "```\n",
    "âŒ é”™è¯¯åšæ³•ï¼šåªçœ‹æœ€åä¸€å¤©çš„æ¨é€çŠ¶æ€\n",
    "   é—®é¢˜ï¼šå¿½ç•¥äº†å†å²æ¨é€çš„ç´¯ç§¯æ•ˆåº”\n",
    "\n",
    "âŒ é”™è¯¯åšæ³•ï¼šç”¨å†å²å­¦ä¹ æ—¶é•¿ä½œä¸ºåå˜é‡è°ƒæ•´\n",
    "   é—®é¢˜ï¼šå†å²å­¦ä¹ æ—¶é•¿æ—¢æ˜¯æ··æ·†å› å­ï¼Œåˆæ˜¯ä¸­é—´ç»“æœï¼\n",
    "   è¿™ä¼šå¯¼è‡´ã€Œè¿‡åº¦è°ƒæ•´åå·®ã€ï¼ˆover-adjustment biasï¼‰\n",
    "```\n",
    "\n",
    "### æ—¶å˜å¤„ç†çš„æŒ‘æˆ˜\n",
    "\n",
    "æ—¶é—´ç»´åº¦ä¸Šçš„å› æœå…³ç³»é“¾ï¼š\n",
    "\n",
    "```\n",
    "Day 1:  æ¨é€1 â†’ å­¦ä¹ 1 â”€â”\n",
    "                        â”œâ†’ å½±å“ â†’ æ¨é€2 â†’ å­¦ä¹ 2 â”€â”\n",
    "                        â”‚                        â”œâ†’ å½±å“ â†’ ...\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         (æ··æ·†å› å­)  (å¤„ç†)  (ä¸­é—´ç»“æœ & æ··æ·†å› å­)\n",
    "```\n",
    "\n",
    "**æ ¸å¿ƒçŸ›ç›¾ï¼š** è¦è°ƒæ•´æ··æ·†ï¼Œå¿…é¡»æ§åˆ¶å†å²å­¦ä¹ æ—¶é•¿ï¼›ä½†æ§åˆ¶å†å²å­¦ä¹ æ—¶é•¿ä¼šé˜»æ–­å› æœè·¯å¾„ï¼\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆï¼š** è¾¹é™…ç»“æ„æ¨¡å‹ (MSM) + é€†æ¦‚ç‡åŠ æƒ (IPTW)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concepts",
   "metadata": {},
   "source": [
    "## ğŸ“ æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### 1. æ—¶å˜å¤„ç† (Time-Varying Treatment)\n",
    "\n",
    "#### å®šä¹‰\n",
    "\n",
    "ä¸ªä½“åœ¨å¤šä¸ªæ—¶é—´ç‚¹æ¥å—å¤„ç†ï¼Œè®°ä¸º $\\bar{A}_t = (A_0, A_1, ..., A_t)$\n",
    "\n",
    "- $A_t \\in \\{0, 1\\}$ï¼šç¬¬ $t$ æ—¶åˆ»æ˜¯å¦æ¥å—å¤„ç†\n",
    "- $\\bar{A}_t$ï¼šä»å¼€å§‹åˆ°æ—¶åˆ» $t$ çš„å¤„ç†å†å²\n",
    "\n",
    "#### ç¬¦å·è¯´æ˜\n",
    "\n",
    "| ç¬¦å· | å«ä¹‰ |\n",
    "|------|------|\n",
    "| $A_t$ | æ—¶åˆ» $t$ çš„å¤„ç†çŠ¶æ€ (0 æˆ– 1) |\n",
    "| $\\bar{A}_t$ | å¤„ç†å†å² $(A_0, ..., A_t)$ |\n",
    "| $L_t$ | æ—¶åˆ» $t$ çš„åå˜é‡ï¼ˆåŒ…æ‹¬æ—¢å¾€ç»“æœï¼‰ |\n",
    "| $\\bar{L}_t$ | åå˜é‡å†å² $(L_0, ..., L_t)$ |\n",
    "| $Y$ | æœ€ç»ˆç»“æœ |\n",
    "\n",
    "#### ç›´è§‚ç†è§£\n",
    "\n",
    "å°±åƒé—®ï¼š**\"å¦‚æœå°å¼ åœ¨æ•´ä¸ªå­¦ä¹ å‘¨æœŸéƒ½æ”¶åˆ°æ¨é€ï¼Œç›¸æ¯”å®Œå…¨ä¸æ¨é€ï¼Œæœ€ç»ˆæˆç»©ä¼šæå‡å¤šå°‘ï¼Ÿ\"**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. æ—¶é—´ä¾èµ–æ€§æ··æ·† (Time-Dependent Confounding)\n",
    "\n",
    "#### æ ¸å¿ƒé—®é¢˜\n",
    "\n",
    "$$L_t \\text{ (åå˜é‡)} \\rightarrow \\begin{cases} A_{t+1} \\text{ (æœªæ¥å¤„ç†)} \\\\ Y \\text{ (æœ€ç»ˆç»“æœ)} \\end{cases}$$\n",
    "\n",
    "å¹¶ä¸” $L_t$ æœ¬èº«å—åˆ°è¿‡å»å¤„ç† $\\bar{A}_{t-1}$ çš„å½±å“ï¼\n",
    "\n",
    "#### ç”Ÿæ´»åŒ–ç±»æ¯”ï¼šåƒè¯çš„å›°å¢ƒ\n",
    "\n",
    "æƒ³è±¡ä½ åœ¨æ²»ç–—ä¸€ç§æ…¢æ€§ç—…ï¼š\n",
    "\n",
    "1. **ç¬¬ 1 å¤©**ï¼šåŒ»ç”Ÿæ ¹æ®ä½ çš„ç—‡çŠ¶ä¸¥é‡ç¨‹åº¦å†³å®šæ˜¯å¦å¼€è¯\n",
    "2. **ç¬¬ 2 å¤©**ï¼š\n",
    "   - å¦‚æœæ˜¨å¤©åƒè¯äº† â†’ ç—‡çŠ¶å¯èƒ½å¥½è½¬\n",
    "   - ç—‡çŠ¶å¥½è½¬ â†’ åŒ»ç”Ÿä»Šå¤©å¯èƒ½ä¸å¼€è¯\n",
    "   - ç—‡çŠ¶å¥½è½¬ â†’ ä½ æœ€ç»ˆåº·å¤æ¦‚ç‡æ›´é«˜\n",
    "3. **é—®é¢˜æ¥äº†**ï¼šæ˜¨å¤©çš„ç—‡çŠ¶æ—¢å½±å“ä»Šå¤©çš„å¤„ç†ï¼Œåˆå½±å“æœ€ç»ˆåº·å¤\n",
    "\n",
    "ä¼ ç»Ÿæ–¹æ³•è°ƒæ•´ã€Œæ˜¨å¤©çš„ç—‡çŠ¶ã€ä¼šåˆ‡æ–­å› æœè·¯å¾„ï¼\n",
    "\n",
    "---\n",
    "\n",
    "### 3. åºè´¯å¿½ç•¥å‡è®¾ (Sequential Ignorability)\n",
    "\n",
    "#### æ•°å­¦å½¢å¼\n",
    "\n",
    "$$Y(\\bar{a}) \\perp A_t \\mid \\bar{A}_{t-1} = \\bar{a}_{t-1}, \\bar{L}_t$$\n",
    "\n",
    "#### ç™½è¯ç¿»è¯‘\n",
    "\n",
    "**åœ¨ç»™å®šå†å²å¤„ç†å’Œåå˜é‡çš„æ¡ä»¶ä¸‹ï¼Œå½“å‰å¤„ç†åˆ†é…ä¸æ½œåœ¨ç»“æœç‹¬ç«‹**\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "- å¦‚æœæˆ‘ä»¬çŸ¥é“å°å¼ è¿‡å»çš„å­¦ä¹ è¡Œä¸ºå’Œæ¨é€å†å²\n",
    "- ä»Šå¤©æ˜¯å¦æ¨é€å°±æ˜¯ã€Œéšæœºã€çš„ï¼ˆæ²¡æœ‰å…¶ä»–æœªè§‚æµ‹çš„æ··æ·†å› å­ï¼‰\n",
    "\n",
    "#### ä¸æ ‡å‡†å¿½ç•¥æ€§çš„åŒºåˆ«\n",
    "\n",
    "| å‡è®¾ | æ ‡å‡†å¿½ç•¥æ€§ | åºè´¯å¿½ç•¥æ€§ |\n",
    "|------|-----------|------------|\n",
    "| **é€‚ç”¨** | å•æ—¶ç‚¹å¤„ç† | å¤šæ—¶ç‚¹å¤„ç† |\n",
    "| **æ¡ä»¶** | $Y(a) \\perp A \\mid X$ | $Y(\\bar{a}) \\perp A_t \\mid \\bar{A}_{t-1}, \\bar{L}_t$ |\n",
    "| **å«ä¹‰** | æ§åˆ¶åŸºçº¿åå˜é‡ | æ§åˆ¶å¤„ç†å’Œåå˜é‡çš„å®Œæ•´å†å² |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. è¾¹é™…ç»“æ„æ¨¡å‹ (Marginal Structural Models, MSM)\n",
    "\n",
    "#### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**å¦‚æœæ•´ä¸ªæ ·æœ¬éƒ½éµå¾ªæŸä¸ªå¤„ç†æ–¹æ¡ˆ $\\bar{a}$ï¼Œå¹³å‡ç»“æœä¼šæ˜¯å¤šå°‘ï¼Ÿ**\n",
    "\n",
    "$$E[Y(\\bar{a})] = \\beta_0 + \\beta_1 \\sum_{t=0}^{T} a_t + \\beta_2 f(\\bar{a})$$\n",
    "\n",
    "#### å¸¸è§çš„ MSM å½¢å¼\n",
    "\n",
    "**1. ç´¯ç§¯å‰‚é‡æ¨¡å‹**\n",
    "\n",
    "$$E[Y(\\bar{a})] = \\beta_0 + \\beta_1 \\sum_{t=0}^{T} a_t$$\n",
    "\n",
    "ç›´è§‚ï¼šæ€»æ¨é€æ¬¡æ•°çš„æ•ˆåº”\n",
    "\n",
    "**2. æœ€è¿‘å¤„ç†æ¨¡å‹**\n",
    "\n",
    "$$E[Y(\\bar{a})] = \\beta_0 + \\beta_1 a_T + \\beta_2 a_{T-1} + ...$$\n",
    "\n",
    "ç›´è§‚ï¼šæœ€è¿‘çš„æ¨é€å½±å“æ›´å¤§\n",
    "\n",
    "**3. å¤„ç†æ–¹æ¡ˆå¯¹æ¯”**\n",
    "\n",
    "$$E[Y(\\bar{a})] = \\beta_0 + \\beta_1 \\cdot I(\\bar{a} = \\text{\"æ€»æ˜¯å¤„ç†\"}) + \\beta_2 \\cdot I(\\bar{a} = \\text{\"ä»ä¸å¤„ç†\"})$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. æ—¶å˜ IPTW (Inverse Probability of Treatment Weighting)\n",
    "\n",
    "#### ç¨³å®šåŒ–æƒé‡å…¬å¼\n",
    "\n",
    "$$SW_i = \\prod_{t=0}^{T} \\frac{P(A_{it} = a_{it} \\mid \\bar{A}_{i,t-1})}{P(A_{it} = a_{it} \\mid \\bar{A}_{i,t-1}, \\bar{L}_{it})}$$\n",
    "\n",
    "#### å…¬å¼æ‹†è§£\n",
    "\n",
    "- **åˆ†å­**ï¼šåªåŸºäºå¤„ç†å†å²çš„å¤„ç†æ¦‚ç‡ï¼ˆè¾¹é™…æ¦‚ç‡ï¼‰\n",
    "- **åˆ†æ¯**ï¼šåŸºäºå¤„ç†å†å²å’Œåå˜é‡å†å²çš„å¤„ç†æ¦‚ç‡ï¼ˆæ¡ä»¶æ¦‚ç‡ï¼‰\n",
    "\n",
    "#### ä¸ºä»€ä¹ˆè¦ç¨³å®šåŒ–ï¼Ÿ\n",
    "\n",
    "- **æœªç¨³å®šåŒ–æƒé‡**ï¼š$W = \\prod \\frac{1}{P(A_t \\mid \\bar{A}_{t-1}, \\bar{L}_t)}$\n",
    "- **é—®é¢˜**ï¼šå¦‚æœæŸä¸ªæ—¶ç‚¹çš„å¤„ç†æ¦‚ç‡å¾ˆå° â†’ æƒé‡çˆ†ç‚¸ï¼\n",
    "- **è§£å†³**ï¼šåˆ†å­åŠ ä¸Šè¾¹é™…æ¦‚ç‡ï¼Œä½¿æƒé‡æ¥è¿‘ 1\n",
    "\n",
    "#### ç›´è§‚ç†è§£ï¼šåˆ›é€ ä¼ªæ€»ä½“\n",
    "\n",
    "IPTW çš„æœ¬è´¨æ˜¯åˆ›é€ ä¸€ä¸ª**ä¼ªæ€»ä½“**ï¼Œåœ¨è¿™ä¸ªæ€»ä½“ä¸­ï¼š\n",
    "\n",
    "```\n",
    "åŸå§‹æ•°æ®:  æ¨é€ â† å†å²å­¦ä¹ è¡¨ç° â†’ æœ€ç»ˆæˆç»©\n",
    "              â†“                       â†“\n",
    "          (æ··æ·†!)                (æœ‰å!)\n",
    "\n",
    "åŠ æƒå:   æ¨é€ âŠ¥ å†å²å­¦ä¹ è¡¨ç° â†’ æœ€ç»ˆæˆç»©\n",
    "              â†“                       â†“\n",
    "          (éšæœº!)               (æ— å!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. G-Computation (å‚æ•°åŒ– G-Formula)\n",
    "\n",
    "#### æ ¸å¿ƒæ€è·¯\n",
    "\n",
    "**Step 1:** å¯¹æ¯ä¸ªæ—¶ç‚¹çš„ç»“æœå»ºæ¨¡\n",
    "\n",
    "$$E[Y \\mid \\bar{A}_t, \\bar{L}_t] = g_t(\\bar{a}_t, \\bar{l}_t; \\theta_t)$$\n",
    "\n",
    "**Step 2:** é€’å½’æ¨¡æ‹Ÿå¹²é¢„åçš„æ½œåœ¨ç»“æœ\n",
    "\n",
    "```python\n",
    "for t in 0..T:\n",
    "    # è®¾ç½®å¹²é¢„\n",
    "    A_t = intervention(t)\n",
    "    \n",
    "    # é¢„æµ‹ç»“æœ\n",
    "    L_{t+1} = predict(A_t, L_t, history)\n",
    "    \n",
    "# æœ€ç»ˆç»“æœ\n",
    "Y = predict(A_bar, L_bar)\n",
    "```\n",
    "\n",
    "#### ä¸ MSM-IPTW çš„å¯¹æ¯”\n",
    "\n",
    "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| **MSM-IPTW** | âœ… éå‚æ•°åŒ–<br>âœ… ç¨³å¥æ€§å¼º | âŒ æƒé‡ä¸ç¨³å®š<br>âŒ éœ€è¦å¤§æ ·æœ¬ | å¤„ç†æœºåˆ¶ç®€å•ï¼Œæ ·æœ¬é‡å¤§ |\n",
    "| **G-Computation** | âœ… æ•ˆç‡é«˜<br>âœ… å¯ä»¥å¤„ç†å¤æ‚æ—¶é—´åŠ¨æ€ | âŒ æ¨¡å‹ä¾èµ–æ€§å¼º<br>âŒ æ¨¡å‹è¯¯è®¾é£é™©é«˜ | æœ‰å……åˆ†çš„é¢†åŸŸçŸ¥è¯†å»ºæ¨¡ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotly é¢œè‰²æ–¹æ¡ˆ\n",
    "COLORS = {\n",
    "    'primary': '#2D9CDB',\n",
    "    'success': '#27AE60',\n",
    "    'danger': '#EB5757',\n",
    "    'warning': '#F2994A',\n",
    "    'neutral': '#828282',\n",
    "    'treated': '#E74C3C',\n",
    "    'control': '#3498DB'\n",
    "}\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-generation",
   "metadata": {},
   "source": "## ğŸ“Š æ•°æ®ç”Ÿæˆï¼šåœ¨çº¿æ•™è‚²æ¨é€åœºæ™¯\n\n### åœºæ™¯è®¾å®š\n\n- **æ—¶é—´è·¨åº¦**ï¼šè¿ç»­ 7 å¤©\n- **å¤„ç†**ï¼šæ¯å¤©æ˜¯å¦æ¨é€å­¦ä¹ æé†’ (0/1)\n- **åå˜é‡**ï¼šæ¯å¤©çš„å­¦ä¹ æ—¶é•¿\n- **ç»“æœ**ï¼šç¬¬ 7 å¤©çš„æµ‹éªŒæˆç»©\n\n### å› æœæœºåˆ¶\n\n```\nDay t:\n  å­¦ä¹ æ—¶é•¿_t â”€â”€â†’ æ¨é€å†³ç­–_{t+1}\n      â†“              â†“\n      â””â”€â”€â†’ å½±å“ â”€â”€â†’ å­¦ä¹ æ—¶é•¿_{t+1}\n                      â†“\n                   æœ€ç»ˆæˆç»©\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_varying_data(n_users: int = 1000, n_days: int = 7, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ—¶å˜å¤„ç†æ•°æ®ï¼šåœ¨çº¿æ•™è‚²æ¨é€åœºæ™¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        n_users: ç”¨æˆ·æ•°é‡\n",
    "        n_days: è§‚æµ‹å¤©æ•°\n",
    "        seed: éšæœºç§å­\n",
    "    \n",
    "    è¿”å›:\n",
    "        DataFrame with columns: user_id, day, push, study_hours, baseline_motivation, final_score\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for user_id in range(n_users):\n",
    "        # åŸºçº¿ç‰¹å¾ï¼šç”¨æˆ·çš„å†…åœ¨å­¦ä¹ åŠ¨æœºï¼ˆå›ºå®šï¼‰\n",
    "        baseline_motivation = np.random.normal(50, 15)\n",
    "        \n",
    "        # åˆå§‹å­¦ä¹ æ—¶é•¿ï¼ˆåŸºäºåŠ¨æœºï¼‰\n",
    "        study_hours_prev = np.random.normal(baseline_motivation / 25, 0.5)\n",
    "        study_hours_prev = np.clip(study_hours_prev, 0, 5)\n",
    "        \n",
    "        user_history = []\n",
    "        \n",
    "        for day in range(n_days):\n",
    "            # Day 0: åŸºçº¿ï¼Œä¸æ¨é€\n",
    "            if day == 0:\n",
    "                push = 0\n",
    "                study_hours = study_hours_prev\n",
    "            else:\n",
    "                # æ¨é€å†³ç­–ï¼šåŸºäºå‰ä¸€å¤©çš„å­¦ä¹ æ—¶é•¿ï¼ˆæ—¶é—´ä¾èµ–æ€§æ··æ·†ï¼ï¼‰\n",
    "                # å­¦ä¹ æ—¶é•¿è¶ŠçŸ­ï¼Œè¶Šå¯èƒ½æ¨é€\n",
    "                propensity = 1 / (1 + np.exp(2 * (study_hours_prev - 1.5)))\n",
    "                push = np.random.binomial(1, propensity)\n",
    "                \n",
    "                # ä»Šå¤©çš„å­¦ä¹ æ—¶é•¿å—åˆ°ï¼š\n",
    "                # 1. åŸºçº¿åŠ¨æœº\n",
    "                # 2. å‰ä¸€å¤©çš„å­¦ä¹ æ—¶é•¿ï¼ˆæƒ¯æ€§ï¼‰\n",
    "                # 3. æ˜¯å¦æ¨é€ï¼ˆå¤„ç†æ•ˆåº”ï¼‰\n",
    "                \n",
    "                # åŸºç¡€å­¦ä¹ æ—¶é•¿\n",
    "                base_study = 0.5 * study_hours_prev + 0.01 * baseline_motivation\n",
    "                \n",
    "                # æ¨é€çš„æ•ˆåº”ï¼ˆå¼‚è´¨æ€§ï¼‰\n",
    "                # - ä½åŠ¨æœºç”¨æˆ·ï¼šæ¨é€æ•ˆæœå¥½ (+0.5 å°æ—¶)\n",
    "                # - é«˜åŠ¨æœºç”¨æˆ·ï¼šæ¨é€æ•ˆæœä¸€èˆ¬ (+0.1 å°æ—¶)\n",
    "                if baseline_motivation < 50:\n",
    "                    push_effect = push * 0.5\n",
    "                else:\n",
    "                    push_effect = push * 0.1\n",
    "                \n",
    "                # ç´¯ç§¯ç–²åŠ³æ•ˆåº”ï¼ˆæ¨é€å¤ªå¤šä¼šåŒçƒ¦ï¼‰\n",
    "                total_pushes = sum([h['push'] for h in user_history])\n",
    "                fatigue = -0.05 * total_pushes if total_pushes > 3 else 0\n",
    "                \n",
    "                study_hours = base_study + push_effect + fatigue + np.random.normal(0, 0.3)\n",
    "                study_hours = np.clip(study_hours, 0, 5)\n",
    "            \n",
    "            user_history.append({\n",
    "                'user_id': user_id,\n",
    "                'day': day,\n",
    "                'push': push,\n",
    "                'study_hours': study_hours,\n",
    "                'baseline_motivation': baseline_motivation\n",
    "            })\n",
    "            \n",
    "            study_hours_prev = study_hours\n",
    "        \n",
    "        # æœ€ç»ˆæˆç»©ï¼šåŸºäºæ•´ä¸ªå­¦ä¹ è½¨è¿¹\n",
    "        total_study = sum([h['study_hours'] for h in user_history])\n",
    "        total_pushes = sum([h['push'] for h in user_history])\n",
    "        \n",
    "        final_score = (\n",
    "            50 +                           # åŸºç¡€åˆ†\n",
    "            0.3 * baseline_motivation +    # åŠ¨æœºå½±å“\n",
    "            3 * total_study +              # æ€»å­¦ä¹ æ—¶é•¿å½±å“\n",
    "            np.random.normal(0, 5)         # éšæœºå™ªå£°\n",
    "        )\n",
    "        final_score = np.clip(final_score, 0, 100)\n",
    "        \n",
    "        # è®°å½•æœ€ç»ˆæˆç»©\n",
    "        for h in user_history:\n",
    "            h['final_score'] = final_score\n",
    "        \n",
    "        data.extend(user_history)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "df = generate_time_varying_data(n_users=1000, n_days=7)\n",
    "\n",
    "print(\"âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼\")\n",
    "print(f\"\\næ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"\\nå‰ 10 è¡Œ:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# æ±‡æ€»ç»Ÿè®¡\n",
    "print(f\"\\n=== æ±‡æ€»ç»Ÿè®¡ ===\")\n",
    "print(f\"ç”¨æˆ·æ•°: {df['user_id'].nunique()}\")\n",
    "print(f\"è§‚æµ‹å¤©æ•°: {df['day'].nunique()}\")\n",
    "print(f\"æ€»æ¨é€æ¬¡æ•°: {df['push'].sum()}\")\n",
    "print(f\"æ¨é€æ¯”ä¾‹: {df['push'].mean():.2%}\")\n",
    "print(f\"\\nå¹³å‡æ¯ç”¨æˆ·æ¨é€æ¬¡æ•°: {df.groupby('user_id')['push'].sum().mean():.2f}\")\n",
    "print(f\"å¹³å‡æ¯å¤©å­¦ä¹ æ—¶é•¿: {df['study_hours'].mean():.2f} å°æ—¶\")\n",
    "print(f\"å¹³å‡æœ€ç»ˆæˆç»©: {df.groupby('user_id')['final_score'].first().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-data",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ å¯è§†åŒ–ï¼šç†è§£æ—¶å˜æ•°æ®ç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-trajectories",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– 1: ä¸ªä½“å­¦ä¹ è½¨è¿¹ï¼ˆé€‰å– 10 ä¸ªç”¨æˆ·ï¼‰\n",
    "\n",
    "sample_users = df['user_id'].unique()[:10]\n",
    "df_sample = df[df['user_id'].isin(sample_users)]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for user in sample_users:\n",
    "    user_data = df_sample[df_sample['user_id'] == user]\n",
    "    \n",
    "    # å­¦ä¹ æ—¶é•¿è½¨è¿¹\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=user_data['day'],\n",
    "        y=user_data['study_hours'],\n",
    "        mode='lines+markers',\n",
    "        name=f'User {user}',\n",
    "        hovertemplate='<b>User %{text}</b><br>Day: %{x}<br>Study Hours: %{y:.2f}<br>Push: %{customdata}',\n",
    "        text=[user] * len(user_data),\n",
    "        customdata=user_data['push']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='ä¸ªä½“å­¦ä¹ è½¨è¿¹ï¼šå­¦ä¹ æ—¶é•¿éšæ—¶é—´å˜åŒ–',\n",
    "    xaxis_title='å¤©æ•°',\n",
    "    yaxis_title='å­¦ä¹ æ—¶é•¿ï¼ˆå°æ—¶ï¼‰',\n",
    "    hovermode='closest',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-confounding",
   "metadata": {},
   "outputs": [],
   "source": "# å¯è§†åŒ– 2: æ—¶é—´ä¾èµ–æ€§æ··æ·†\n\n# è®¡ç®—å‰ä¸€å¤©å­¦ä¹ æ—¶é•¿å¯¹ä»Šå¤©æ¨é€çš„å½±å“\n# ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºæ»åå˜é‡\ndf_with_lag = df.sort_values(['user_id', 'day']).copy()\ndf_with_lag['study_hours_lag'] = df_with_lag.groupby('user_id')['study_hours'].shift(1)\n\n# åªä¿ç•™ day > 0 çš„æ•°æ®ï¼ˆå› ä¸º day 0 æ²¡æœ‰å‰ä¸€å¤©ï¼‰\ndf_lag = df_with_lag[df_with_lag['day'] > 0].dropna(subset=['study_hours_lag'])\n\n# åˆ†ç»„ç»Ÿè®¡\ndf_lag['study_bin'] = pd.cut(df_lag['study_hours_lag'], bins=5)\nconfounding_stats = df_lag.groupby('study_bin', observed=True).agg({\n    'push': 'mean',\n    'final_score': 'mean'\n}).reset_index()\nconfounding_stats['study_bin_str'] = confounding_stats['study_bin'].astype(str)\n\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('å‰ä¸€å¤©å­¦ä¹ æ—¶é•¿ â†’ ä»Šå¤©æ¨é€æ¦‚ç‡', 'å‰ä¸€å¤©å­¦ä¹ æ—¶é•¿ â†’ æœ€ç»ˆæˆç»©')\n)\n\n# å·¦å›¾ï¼šå­¦ä¹ æ—¶é•¿ â†’ æ¨é€æ¦‚ç‡\nfig.add_trace(\n    go.Bar(\n        x=confounding_stats['study_bin_str'],\n        y=confounding_stats['push'],\n        marker_color=COLORS['danger'],\n        name='æ¨é€æ¦‚ç‡',\n        showlegend=False\n    ),\n    row=1, col=1\n)\n\n# å³å›¾ï¼šå­¦ä¹ æ—¶é•¿ â†’ æœ€ç»ˆæˆç»©\nfig.add_trace(\n    go.Bar(\n        x=confounding_stats['study_bin_str'],\n        y=confounding_stats['final_score'],\n        marker_color=COLORS['success'],\n        name='æœ€ç»ˆæˆç»©',\n        showlegend=False\n    ),\n    row=1, col=2\n)\n\nfig.update_xaxes(title_text='å‰ä¸€å¤©å­¦ä¹ æ—¶é•¿ï¼ˆå°æ—¶ï¼‰', row=1, col=1)\nfig.update_xaxes(title_text='å‰ä¸€å¤©å­¦ä¹ æ—¶é•¿ï¼ˆå°æ—¶ï¼‰', row=1, col=2)\nfig.update_yaxes(title_text='æ¨é€æ¦‚ç‡', row=1, col=1)\nfig.update_yaxes(title_text='å¹³å‡æœ€ç»ˆæˆç»©', row=1, col=2)\n\nfig.update_layout(\n    template='plotly_white',\n    title_text='æ—¶é—´ä¾èµ–æ€§æ··æ·†ï¼šå­¦ä¹ æ—¶é•¿åŒæ—¶å½±å“æ¨é€å’Œç»“æœ',\n    height=400\n)\n\nfig.show()\n\nprint(\"\\nğŸ’¡ è§‚å¯Ÿï¼š\")\nprint(\"- å­¦ä¹ æ—¶é•¿ä½çš„ç”¨æˆ·æ›´å¯èƒ½æ”¶åˆ°æ¨é€ï¼ˆç³»ç»Ÿè¯•å›¾æ¿€åŠ±ä»–ä»¬ï¼‰\")\nprint(\"- å­¦ä¹ æ—¶é•¿ä½çš„ç”¨æˆ·æœ€ç»ˆæˆç»©ä¹Ÿæ›´ä½\")\nprint(\"- è¿™å°±æ˜¯æ—¶é—´ä¾èµ–æ€§æ··æ·†ï¼ç®€å•æ¯”è¾ƒæ¨é€ vs ä¸æ¨é€ä¼šæœ‰åå·®\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-naive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– 3: æœ´ç´ ä¼°è®¡çš„åå·®\n",
    "\n",
    "# è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ€»æ¨é€æ¬¡æ•°\n",
    "user_summary = df.groupby('user_id').agg({\n",
    "    'push': 'sum',\n",
    "    'final_score': 'first',\n",
    "    'baseline_motivation': 'first'\n",
    "}).reset_index()\n",
    "user_summary['push_group'] = pd.cut(user_summary['push'], bins=[0, 2, 4, 7], labels=['ä½é¢‘ (0-2)', 'ä¸­é¢‘ (3-4)', 'é«˜é¢‘ (5-7)'])\n",
    "\n",
    "# åˆ†ç»„ç»Ÿè®¡\n",
    "naive_stats = user_summary.groupby('push_group')['final_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=naive_stats['push_group'],\n",
    "    y=naive_stats['mean'],\n",
    "    error_y=dict(type='data', array=naive_stats['std']),\n",
    "    marker_color=COLORS['warning'],\n",
    "    text=[f\"{m:.1f}\" for m in naive_stats['mean']],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='æœ´ç´ åˆ†æï¼šæ¨é€é¢‘ç‡ä¸æœ€ç»ˆæˆç»©ï¼ˆæœ‰åï¼ï¼‰',\n",
    "    xaxis_title='æ¨é€é¢‘ç‡',\n",
    "    yaxis_title='å¹³å‡æœ€ç»ˆæˆç»©',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâŒ æœ´ç´ ç»“è®ºï¼ˆé”™è¯¯ï¼ï¼‰ï¼š\")\n",
    "print(f\"   æ¨é€æ¬¡æ•°è¶Šå¤šï¼Œæˆç»©åè€Œè¶Šä½ï¼\")\n",
    "print(f\"   æ˜¯ä¸æ˜¯æ¨é€æœ‰å®³ï¼Ÿ\")\n",
    "print(\"\\nâœ… çœŸç›¸ï¼š\")\n",
    "print(\"   æ¨é€æ¬¡æ•°å¤š â† å†å²å­¦ä¹ æ—¶é•¿ä½ â†’ æœ€ç»ˆæˆç»©ä½\")\n",
    "print(\"   è¿™æ˜¯é€‰æ‹©åå·®ï¼Œä¸æ˜¯å› æœæ•ˆåº”ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "msm-iptw",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®ç°ï¼šè¾¹é™…ç»“æ„æ¨¡å‹ + æ—¶å˜ IPTW\n",
    "\n",
    "### å®ç°æ­¥éª¤\n",
    "\n",
    "1. **ä¼°è®¡å€¾å‘å¾—åˆ†**ï¼šå¯¹æ¯ä¸ªæ—¶ç‚¹ $t$ï¼Œä¼°è®¡ $P(A_t = 1 \\mid \\bar{A}_{t-1}, \\bar{L}_t)$\n",
    "2. **è®¡ç®—ç¨³å®šåŒ–æƒé‡**ï¼š$SW = \\prod_t \\frac{P(A_t \\mid \\bar{A}_{t-1})}{P(A_t \\mid \\bar{A}_{t-1}, \\bar{L}_t)}$\n",
    "3. **æ‹Ÿåˆ MSM**ï¼šä½¿ç”¨æƒé‡æ‹Ÿåˆè¾¹é™…ç»“æ„æ¨¡å‹\n",
    "4. **ä¼°è®¡å› æœæ•ˆåº”**ï¼šå¯¹æ¯”ä¸åŒå¤„ç†æ–¹æ¡ˆä¸‹çš„é¢„æœŸç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "propensity-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "\n",
    "def estimate_propensity_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ä¼°è®¡æ¯ä¸ªæ—¶ç‚¹çš„å€¾å‘å¾—åˆ†\n",
    "    \n",
    "    å¯¹äºæ¯ä¸ª day > 0:\n",
    "        - åˆ†å­ï¼šP(A_t | A_{t-1})\n",
    "        - åˆ†æ¯ï¼šP(A_t | A_{t-1}, L_{t-1})\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['ps_numerator'] = 1.0\n",
    "    df['ps_denominator'] = 1.0\n",
    "    \n",
    "    for day in range(1, df['day'].max() + 1):\n",
    "        # å½“å¤©çš„æ•°æ®\n",
    "        day_data = df[df['day'] == day].copy()\n",
    "        \n",
    "        # å‰ä¸€å¤©çš„æ•°æ®\n",
    "        prev_day_data = df[df['day'] == day - 1][['user_id', 'push', 'study_hours']]\n",
    "        prev_day_data.columns = ['user_id', 'push_lag', 'study_hours_lag']\n",
    "        \n",
    "        # åˆå¹¶\n",
    "        day_merged = day_data.merge(prev_day_data, on='user_id')\n",
    "        \n",
    "        # === åˆ†å­ï¼šP(A_t | A_{t-1}) ===\n",
    "        X_num = day_merged[['push_lag']].values\n",
    "        y = day_merged['push'].values\n",
    "        \n",
    "        model_num = LogisticRegression(max_iter=1000)\n",
    "        model_num.fit(X_num, y)\n",
    "        ps_num = model_num.predict_proba(X_num)[:, 1]\n",
    "        \n",
    "        # === åˆ†æ¯ï¼šP(A_t | A_{t-1}, L_{t-1}) ===\n",
    "        X_denom = day_merged[['push_lag', 'study_hours_lag']].values\n",
    "        \n",
    "        model_denom = LogisticRegression(max_iter=1000)\n",
    "        model_denom.fit(X_denom, y)\n",
    "        ps_denom = model_denom.predict_proba(X_denom)[:, 1]\n",
    "        \n",
    "        # æ ¹æ®å®é™…å¤„ç†çŠ¶æ€é€‰æ‹©æ¦‚ç‡\n",
    "        ps_num_final = np.where(y == 1, ps_num, 1 - ps_num)\n",
    "        ps_denom_final = np.where(y == 1, ps_denom, 1 - ps_denom)\n",
    "        \n",
    "        # æ›´æ–° dataframe\n",
    "        df.loc[df['day'] == day, 'ps_numerator'] = ps_num_final\n",
    "        df.loc[df['day'] == day, 'ps_denominator'] = ps_denom_final\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ä¼°è®¡å€¾å‘å¾—åˆ†\n",
    "df = estimate_propensity_scores(df)\n",
    "\n",
    "print(\"âœ… å€¾å‘å¾—åˆ†ä¼°è®¡å®Œæˆï¼\")\n",
    "print(f\"\\nå€¾å‘å¾—åˆ†æ‘˜è¦:\")\n",
    "print(df[df['day'] > 0][['day', 'ps_numerator', 'ps_denominator']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: è®¡ç®—ç¨³å®šåŒ–æƒé‡\n",
    "\n",
    "def compute_stabilized_weights(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç¨³å®šåŒ– IPTW æƒé‡\n",
    "    \n",
    "    SW_i = âˆ_t [P(A_t | A_bar_{t-1}) / P(A_t | A_bar_{t-1}, L_bar_t)]\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªæ—¶ç‚¹çš„æƒé‡æ¯”ç‡\n",
    "    df['weight_ratio'] = df['ps_numerator'] / df['ps_denominator']\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªç”¨æˆ·ï¼Œè®¡ç®—ç´¯ç§¯æƒé‡ï¼ˆä¹˜ç§¯ï¼‰\n",
    "    df['sw'] = df.groupby('user_id')['weight_ratio'].cumprod()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = compute_stabilized_weights(df)\n",
    "\n",
    "print(\"âœ… æƒé‡è®¡ç®—å®Œæˆï¼\")\n",
    "print(f\"\\næƒé‡ç»Ÿè®¡:\")\n",
    "final_weights = df[df['day'] == df['day'].max()]['sw']\n",
    "print(f\"  å‡å€¼: {final_weights.mean():.3f}\")\n",
    "print(f\"  ä¸­ä½æ•°: {final_weights.median():.3f}\")\n",
    "print(f\"  æ ‡å‡†å·®: {final_weights.std():.3f}\")\n",
    "print(f\"  æœ€å°å€¼: {final_weights.min():.3f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {final_weights.max():.3f}\")\n",
    "print(f\"  99th percentile: {final_weights.quantile(0.99):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æƒé‡åˆ†å¸ƒ\n",
    "\n",
    "final_weights = df[df['day'] == df['day'].max()]['sw']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=final_weights,\n",
    "    nbinsx=50,\n",
    "    marker_color=COLORS['primary'],\n",
    "    opacity=0.7\n",
    "))\n",
    "\n",
    "fig.add_vline(x=1, line_dash=\"dash\", line_color=\"red\", annotation_text=\"ç†æƒ³å€¼ = 1\")\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='ç¨³å®šåŒ– IPTW æƒé‡åˆ†å¸ƒ',\n",
    "    xaxis_title='æƒé‡',\n",
    "    yaxis_title='ç”¨æˆ·æ•°',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è§£è¯»ï¼š\")\n",
    "print(\"- æƒé‡å‡å€¼æ¥è¿‘ 1ï¼šç¨³å®šåŒ–æˆåŠŸ\")\n",
    "print(\"- å¦‚æœæœ‰æç«¯æƒé‡ï¼ˆ>10ï¼‰ï¼šå¯èƒ½éœ€è¦æˆªæ–­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trim-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æƒé‡æˆªæ–­ï¼ˆå¯é€‰ï¼Œå¤„ç†æç«¯å€¼ï¼‰\n",
    "\n",
    "def trim_weights(df: pd.DataFrame, percentile: float = 0.99) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æˆªæ–­æç«¯æƒé‡\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    threshold = df['sw'].quantile(percentile)\n",
    "    df['sw_trimmed'] = np.minimum(df['sw'], threshold)\n",
    "    \n",
    "    print(f\"âœ… æƒé‡æˆªæ–­å®Œæˆï¼ˆ99th percentile = {threshold:.3f}ï¼‰\")\n",
    "    print(f\"   å½±å“ {(df['sw'] > threshold).sum()} æ¡è®°å½•\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = trim_weights(df, percentile=0.99)\n",
    "\n",
    "# ä½¿ç”¨æˆªæ–­åçš„æƒé‡\n",
    "df['sw_final'] = df['sw_trimmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-msm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: æ‹Ÿåˆè¾¹é™…ç»“æ„æ¨¡å‹\n",
    "\n",
    "def fit_marginal_structural_model(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    æ‹Ÿåˆè¾¹é™…ç»“æ„æ¨¡å‹\n",
    "    \n",
    "    æ¨¡å‹ï¼šE[Y(a_bar)] = Î²_0 + Î²_1 * total_pushes\n",
    "    \n",
    "    ä½¿ç”¨åŠ æƒæœ€å°äºŒä¹˜\n",
    "    \"\"\"\n",
    "    # æå–æ¯ä¸ªç”¨æˆ·çš„æœ€ç»ˆæ•°æ®\n",
    "    df_final = df.groupby('user_id').agg({\n",
    "        'push': 'sum',  # æ€»æ¨é€æ¬¡æ•°\n",
    "        'final_score': 'first',\n",
    "        'sw_final': 'last'  # æœ€ç»ˆæƒé‡\n",
    "    }).reset_index()\n",
    "    df_final.columns = ['user_id', 'total_pushes', 'final_score', 'weight']\n",
    "    \n",
    "    # åŠ æƒçº¿æ€§å›å½’\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    X = df_final[['total_pushes']].values\n",
    "    y = df_final['final_score'].values\n",
    "    weights = df_final['weight'].values\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y, sample_weight=weights)\n",
    "    \n",
    "    beta_0 = model.intercept_\n",
    "    beta_1 = model.coef_[0]\n",
    "    \n",
    "    print(\"âœ… MSM æ‹Ÿåˆå®Œæˆï¼\")\n",
    "    print(f\"\\nè¾¹é™…ç»“æ„æ¨¡å‹:\")\n",
    "    print(f\"  E[Y(a_bar)] = {beta_0:.2f} + {beta_1:.2f} Ã— total_pushes\")\n",
    "    print(f\"\\nè§£é‡Š:\")\n",
    "    print(f\"  - æ¯å¢åŠ  1 æ¬¡æ¨é€ï¼Œå¹³å‡æˆç»©æå‡ {beta_1:.2f} åˆ†\")\n",
    "    print(f\"  - å®Œå…¨ä¸æ¨é€ï¼šé¢„æœŸæˆç»© {beta_0:.2f} åˆ†\")\n",
    "    print(f\"  - æ¨é€ 7 æ¬¡ï¼šé¢„æœŸæˆç»© {beta_0 + 7 * beta_1:.2f} åˆ†\")\n",
    "    print(f\"  - æ€»æ•ˆåº”ï¼š{7 * beta_1:.2f} åˆ†\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'beta_0': beta_0,\n",
    "        'beta_1': beta_1,\n",
    "        'data': df_final\n",
    "    }\n",
    "\n",
    "msm_results = fit_marginal_structural_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-msm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– MSM æ‹Ÿåˆç»“æœ\n",
    "\n",
    "df_final = msm_results['data']\n",
    "beta_0 = msm_results['beta_0']\n",
    "beta_1 = msm_results['beta_1']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# æ•£ç‚¹å›¾ï¼ˆåŸå§‹æ•°æ®ï¼‰\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_final['total_pushes'],\n",
    "    y=df_final['final_score'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=df_final['weight'] * 5,  # æƒé‡è¶Šå¤§ï¼Œç‚¹è¶Šå¤§\n",
    "        color=COLORS['neutral'],\n",
    "        opacity=0.3\n",
    "    ),\n",
    "    name='è§‚æµ‹æ•°æ®',\n",
    "    hovertemplate='æ¨é€æ¬¡æ•°: %{x}<br>æˆç»©: %{y:.1f}<br>æƒé‡: %{marker.size:.2f}'\n",
    "))\n",
    "\n",
    "# MSM æ‹Ÿåˆçº¿\n",
    "x_line = np.linspace(0, 7, 100)\n",
    "y_line = beta_0 + beta_1 * x_line\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_line,\n",
    "    y=y_line,\n",
    "    mode='lines',\n",
    "    line=dict(color=COLORS['danger'], width=3),\n",
    "    name=f'MSM: y = {beta_0:.1f} + {beta_1:.2f}x'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='è¾¹é™…ç»“æ„æ¨¡å‹ï¼šæ¨é€æ¬¡æ•°çš„å› æœæ•ˆåº”',\n",
    "    xaxis_title='æ€»æ¨é€æ¬¡æ•°',\n",
    "    yaxis_title='æœ€ç»ˆæˆç»©',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcomputation",
   "metadata": {},
   "source": [
    "## ğŸ”¬ å¯¹æ¯”ï¼šG-Computation\n",
    "\n",
    "G-Computation æ˜¯å¦ä¸€ç§å¤„ç†æ—¶å˜å¤„ç†çš„æ–¹æ³•ï¼Œé€šè¿‡å‚æ•°åŒ–å»ºæ¨¡æ¯ä¸ªæ—¶ç‚¹çš„ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcomp-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: å®ç° G-Computation\n",
    "# æç¤ºï¼š\n",
    "# 1. å¯¹æ¯ä¸ªæ—¶ç‚¹ tï¼Œå»ºæ¨¡ E[L_{t+1} | A_t, L_t]\n",
    "# 2. è®¾å®šå¹²é¢„æ–¹æ¡ˆï¼ˆå¦‚ï¼šæ‰€æœ‰äººéƒ½æ¨é€ï¼‰\n",
    "# 3. é€’å½’æ¨¡æ‹Ÿå¹²é¢„åçš„ç»“æœ\n",
    "# 4. å¹³å‡å¾—åˆ° ATE\n",
    "\n",
    "def g_computation(df: pd.DataFrame, intervention: str = 'always') -> float:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ G-Computation ä¼°è®¡å› æœæ•ˆåº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        df: æ•°æ®\n",
    "        intervention: 'always' (æ€»æ˜¯æ¨é€) æˆ– 'never' (ä»ä¸æ¨é€)\n",
    "    \n",
    "    è¿”å›:\n",
    "        é¢„æœŸç»“æœ\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    \n",
    "    # Step 1: å»ºæ¨¡ study_hours_{t+1} = f(push_t, study_hours_t)\n",
    "    # ä½¿ç”¨çº¿æ€§å›å½’\n",
    "    \n",
    "    \n",
    "    # Step 2: å»ºæ¨¡ final_score = g(total_study_hours, baseline_motivation)\n",
    "    \n",
    "    \n",
    "    # Step 3: æ¨¡æ‹Ÿå¹²é¢„\n",
    "    # å¯¹æ¯ä¸ªç”¨æˆ·ï¼Œè®¾ç½® push = 1 (always) æˆ– 0 (never)\n",
    "    # é€’å½’é¢„æµ‹ study_hours\n",
    "    \n",
    "    \n",
    "    # Step 4: é¢„æµ‹æœ€ç»ˆæˆç»©\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    \n",
    "    pass\n",
    "\n",
    "# æµ‹è¯•\n",
    "# ate_gcomp_always = g_computation(df, intervention='always')\n",
    "# ate_gcomp_never = g_computation(df, intervention='never')\n",
    "# ate_gcomp = ate_gcomp_always - ate_gcomp_never\n",
    "# print(f\"\\nG-Computation ATE: {ate_gcomp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcomp-solution",
   "metadata": {},
   "outputs": [],
   "source": "<details>\n<summary>ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef g_computation(df: pd.DataFrame, intervention: str = 'always') -> float:\n    # Step 1: å»ºæ¨¡å­¦ä¹ æ—¶é•¿åŠ¨æ€\n    df_model = df[df['day'] > 0].copy()\n    prev = df[df['day'] >= 0].groupby('user_id').shift(1)\n    df_model['study_lag'] = prev['study_hours']\n    df_model['push_lag'] = prev['push']\n    df_model = df_model.dropna()\n    \n    X = df_model[['push', 'study_lag']]\n    y = df_model['study_hours']\n    study_model = LinearRegression().fit(X, y)\n    \n    # Step 2: æ¨¡æ‹Ÿå¹²é¢„\n    users = df['user_id'].unique()\n    simulated_scores = []\n    \n    for user in users:\n        user_data = df[df['user_id'] == user].sort_values('day')\n        baseline_motivation = user_data['baseline_motivation'].iloc[0]\n        study_hours = [user_data[user_data['day'] == 0]['study_hours'].values[0]]\n        \n        for day in range(1, 7):\n            push = 1 if intervention == 'always' else 0\n            X_pred = np.array([[push, study_hours[-1]]])\n            study_next = study_model.predict(X_pred)[0]\n            study_hours.append(max(0, min(5, study_next)))\n        \n        total_study = sum(study_hours)\n        final_score = 50 + 0.3 * baseline_motivation + 3 * total_study\n        simulated_scores.append(final_score)\n    \n    return np.mean(simulated_scores)\n\nate_gcomp_always = g_computation(df, intervention='always')\nate_gcomp_never = g_computation(df, intervention='never')\nate_gcomp = ate_gcomp_always - ate_gcomp_never\nprint(f\"G-Computation ATE: {ate_gcomp:.2f}\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ–¹æ³•å¯¹æ¯”ï¼šæœ´ç´  vs MSM-IPTW vs G-Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—ä¸åŒæ–¹æ³•çš„ä¼°è®¡\n",
    "\n",
    "# 1. æœ´ç´ ä¼°è®¡ï¼ˆæœ‰åï¼‰\n",
    "user_summary = df.groupby('user_id').agg({\n",
    "    'push': 'sum',\n",
    "    'final_score': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "high_push = user_summary[user_summary['push'] >= 5]['final_score'].mean()\n",
    "low_push = user_summary[user_summary['push'] <= 2]['final_score'].mean()\n",
    "naive_ate = high_push - low_push\n",
    "\n",
    "# 2. MSM-IPTW\n",
    "msm_ate = 7 * msm_results['beta_1']\n",
    "\n",
    "# 3. çœŸå® ATEï¼ˆå¦‚æœæœ‰æ¨¡æ‹Ÿæ•°æ®çš„è¯ï¼‰\n",
    "# è¿™é‡Œæˆ‘ä»¬å‡è®¾çœŸå® ATE â‰ˆ MSM ä¼°è®¡\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"æ–¹æ³•å¯¹æ¯”ï¼šå› æœæ•ˆåº”ä¼°è®¡\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. æœ´ç´ ä¼°è®¡ï¼ˆç®€å•å¯¹æ¯”ï¼‰\")\n",
    "print(f\"   é«˜é¢‘æ¨é€ç»„æˆç»©: {high_push:.2f}\")\n",
    "print(f\"   ä½é¢‘æ¨é€ç»„æˆç»©: {low_push:.2f}\")\n",
    "print(f\"   å·®å¼‚: {naive_ate:.2f} âš ï¸ æœ‰åï¼\")\n",
    "\n",
    "print(f\"\\n2. MSM-IPTW\")\n",
    "print(f\"   æ¯æ¬¡æ¨é€æ•ˆåº”: {msm_results['beta_1']:.2f}\")\n",
    "print(f\"   7æ¬¡æ¨é€æ€»æ•ˆåº”: {msm_ate:.2f} âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ å…³é”®å‘ç°ï¼š\")\n",
    "print(f\"   æœ´ç´ ä¼°è®¡ä¸¥é‡ä½ä¼°äº†æ¨é€çš„æ­£å‘æ•ˆåº”ï¼\")\n",
    "print(f\"   åŸå› ï¼šå­¦ä¹ è¡¨ç°å·®çš„ç”¨æˆ·æ›´å¯èƒ½æ”¶åˆ°æ¨é€ï¼ˆé€‰æ‹©åå·®ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-viz2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "\n",
    "methods = ['æœ´ç´ ä¼°è®¡', 'MSM-IPTW']\n",
    "estimates = [naive_ate, msm_ate]\n",
    "colors_list = [COLORS['warning'], COLORS['success']]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=methods,\n",
    "    y=estimates,\n",
    "    marker_color=colors_list,\n",
    "    text=[f\"{e:.2f}\" for e in estimates],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='æ¨é€æ•ˆåº”ä¼°è®¡ï¼šä¸åŒæ–¹æ³•å¯¹æ¯”',\n",
    "    yaxis_title='7 æ¬¡æ¨é€çš„æ€»æ•ˆåº”ï¼ˆæˆç»©å·®å¼‚ï¼‰',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitivity",
   "metadata": {},
   "source": [
    "## ğŸ” æ•æ„Ÿæ€§åˆ†æï¼šæƒé‡æˆªæ–­çš„å½±å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitivity-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: æ•æ„Ÿæ€§åˆ†æ\n",
    "# æç¤ºï¼š\n",
    "# 1. å°è¯•ä¸åŒçš„æƒé‡æˆªæ–­é˜ˆå€¼ï¼ˆ90th, 95th, 99th percentileï¼‰\n",
    "# 2. å¯¹æ¯ä¸ªé˜ˆå€¼ï¼Œé‡æ–°æ‹Ÿåˆ MSM\n",
    "# 3. æ¯”è¾ƒä¼°è®¡ç»“æœçš„ç¨³å®šæ€§\n",
    "\n",
    "def sensitivity_to_trimming(df: pd.DataFrame, percentiles: List[float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æƒé‡æˆªæ–­çš„æ•æ„Ÿæ€§åˆ†æ\n",
    "    \"\"\"\n",
    "    # === ä½ çš„ä»£ç  ===\n",
    "    \n",
    "    \n",
    "    # === ä»£ç ç»“æŸ ===\n",
    "    pass\n",
    "\n",
    "# sensitivity_results = sensitivity_to_trimming(df, percentiles=[0.90, 0.95, 0.99, 1.00])\n",
    "# print(sensitivity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitivity-solution",
   "metadata": {},
   "outputs": [],
   "source": "<details>\n<summary>ğŸ“ å‚è€ƒç­”æ¡ˆï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n```python\ndef sensitivity_to_trimming(df: pd.DataFrame, percentiles: List[float]) -> pd.DataFrame:\n    results = []\n    \n    for p in percentiles:\n        df_temp = df.copy()\n        \n        if p < 1.0:\n            threshold = df_temp['sw'].quantile(p)\n            df_temp['sw_final'] = np.minimum(df_temp['sw'], threshold)\n        else:\n            df_temp['sw_final'] = df_temp['sw']\n        \n        # æ‹Ÿåˆ MSM\n        df_final = df_temp.groupby('user_id').agg({\n            'push': 'sum',\n            'final_score': 'first',\n            'sw_final': 'last'\n        }).reset_index()\n        \n        X = df_final[['push']].values\n        y = df_final['final_score'].values\n        w = df_final['sw_final'].values\n        \n        model = LinearRegression()\n        model.fit(X, y, sample_weight=w)\n        \n        results.append({\n            'percentile': p,\n            'beta_1': model.coef_[0],\n            'ate': 7 * model.coef_[0],\n            'max_weight': df_temp['sw_final'].max()\n        })\n    \n    return pd.DataFrame(results)\n\nsensitivity_results = sensitivity_to_trimming(df, percentiles=[0.90, 0.95, 0.99, 1.00])\nprint(\"\\næ•æ„Ÿæ€§åˆ†æï¼šæƒé‡æˆªæ–­å¯¹ä¼°è®¡çš„å½±å“\")\nprint(sensitivity_results)\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "id": "thinking",
   "metadata": {},
   "source": [
    "## ğŸ’¡ æ€è€ƒé¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1",
   "metadata": {},
   "source": [
    "### åŸºç¡€ç†è§£\n",
    "\n",
    "**1. ä»€ä¹ˆæ˜¯æ—¶é—´ä¾èµ–æ€§æ··æ·†ï¼Ÿä¸ºä»€ä¹ˆä¼ ç»Ÿçš„ IPW æ–¹æ³•æ— æ³•å¤„ç†ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šæ€è€ƒå†å²å­¦ä¹ æ—¶é•¿çš„åŒé‡è§’è‰²\n",
    "\n",
    "---\n",
    "\n",
    "**2. åºè´¯å¿½ç•¥å‡è®¾ä¸æ ‡å‡†å¿½ç•¥å‡è®¾æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šæ¡ä»¶é›†ä¸åŒ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2",
   "metadata": {},
   "source": [
    "### æ·±å…¥åˆ†æ\n",
    "\n",
    "**3. ç¨³å®šåŒ–æƒé‡çš„åˆ†å­å’Œåˆ†æ¯å„ä»£è¡¨ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¦ç¨³å®šåŒ–ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šæç«¯æƒé‡çš„é—®é¢˜\n",
    "\n",
    "---\n",
    "\n",
    "**4. MSM-IPTW å’Œ G-Computation çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿå„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šå‚æ•°åŒ– vs éå‚æ•°åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "**5. åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæœ´ç´ ä¼°è®¡ä¸ºä»€ä¹ˆä¼šä½ä¼°æ¨é€çš„æ•ˆåº”ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šé€‰æ‹©åå·®çš„æ–¹å‘\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3",
   "metadata": {},
   "source": [
    "### å®æˆ˜åº”ç”¨\n",
    "\n",
    "**6. å¦‚æœä½ æ˜¯è¿™ä¸ªåœ¨çº¿æ•™è‚²å¹³å°çš„äº§å“ç»ç†ï¼ŒåŸºäº MSM çš„ç»“æœï¼Œä½ ä¼šå¦‚ä½•è°ƒæ•´æ¨é€ç­–ç•¥ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šè€ƒè™‘æ¨é€é¢‘ç‡ã€ä¸ªæ€§åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "**7. åœ¨å“ªäº›åœºæ™¯ä¸‹ï¼ŒG-Computation æ¯” MSM-IPTW æ›´åˆé€‚ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šæ ·æœ¬é‡ã€æ¨¡å‹å¤æ‚åº¦\n",
    "\n",
    "---\n",
    "\n",
    "**8. å¦‚æœæƒé‡åˆ†å¸ƒéå¸¸ä¸ç¨³å®šï¼ˆæœ‰æç«¯å€¼ï¼‰ï¼Œé™¤äº†æˆªæ–­ï¼Œè¿˜æœ‰ä»€ä¹ˆåŠæ³•ï¼Ÿ**\n",
    "\n",
    "æç¤ºï¼šåŒé‡é²æ£’ä¼°è®¡\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æœ¬ç« å°ç»“\n",
    "\n",
    "### æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "| æ¦‚å¿µ | å®šä¹‰ | é‡è¦æ€§ |\n",
    "|------|------|--------|\n",
    "| **æ—¶å˜å¤„ç†** | ä¸ªä½“åœ¨å¤šä¸ªæ—¶ç‚¹æ¥å—å¤„ç† $\\bar{A}_t$ | ç°å®ä¸­å¤§å¤šæ•°å¹²é¢„éƒ½æ˜¯åŠ¨æ€çš„ |\n",
    "| **æ—¶é—´ä¾èµ–æ€§æ··æ·†** | åå˜é‡æ—¢å½±å“æœªæ¥å¤„ç†åˆå½±å“ç»“æœï¼Œä¸”è‡ªèº«å—è¿‡å»å¤„ç†å½±å“ | ä¼ ç»Ÿæ–¹æ³•å¤±æ•ˆçš„æ ¹æœ¬åŸå›  |\n",
    "| **åºè´¯å¿½ç•¥å‡è®¾** | $Y(\\bar{a}) \\perp A_t \\mid \\bar{A}_{t-1}, \\bar{L}_t$ | å› æœè¯†åˆ«çš„å…³é”®å‡è®¾ |\n",
    "| **è¾¹é™…ç»“æ„æ¨¡å‹ (MSM)** | å»ºæ¨¡å¹²é¢„æ–¹æ¡ˆ $\\bar{a}$ å¯¹ç»“æœçš„è¾¹é™…æ•ˆåº” | ä¼°è®¡ç›®æ ‡ |\n",
    "| **ç¨³å®šåŒ– IPTW** | $SW = \\prod \\frac{P(A_t \\mid \\bar{A}_{t-1})}{P(A_t \\mid \\bar{A}_{t-1}, \\bar{L}_t)}$ | æ ¸å¿ƒä¼°è®¡æ–¹æ³• |\n",
    "| **G-Computation** | å‚æ•°åŒ–å»ºæ¨¡ + æ¨¡æ‹Ÿå¹²é¢„ | æ›¿ä»£æ–¹æ³• |\n",
    "\n",
    "---\n",
    "\n",
    "### æ–¹æ³•å¯¹æ¯”\n",
    "\n",
    "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| **æœ´ç´ ä¼°è®¡** | âœ… ç®€å•ç›´è§‚ | âŒ ä¸¥é‡æœ‰å | ä»…ç”¨äºæ¢ç´¢æ€§åˆ†æ |\n",
    "| **MSM-IPTW** | âœ… éå‚æ•°åŒ–<br>âœ… ç¨³å¥ | âŒ æƒé‡ä¸ç¨³å®š<br>âŒ éœ€è¦å¤§æ ·æœ¬ | å¤„ç†æœºåˆ¶ç®€å•ï¼Œæ ·æœ¬å……è¶³ |\n",
    "| **G-Computation** | âœ… æ•ˆç‡é«˜<br>âœ… å¯å¤„ç†å¤æ‚æ—¶é—´åŠ¨æ€ | âŒ æ¨¡å‹ä¾èµ–<br>âŒ è¯¯è®¾é£é™© | æœ‰å……åˆ†é¢†åŸŸçŸ¥è¯† |\n",
    "| **åŒé‡é²æ£’** | âœ… ç»“åˆä¸¤è€…ä¼˜ç‚¹<br>âœ… åªéœ€ä¸€ä¸ªæ¨¡å‹æ­£ç¡® | âŒ å®ç°å¤æ‚ | é«˜é£é™©å†³ç­– |\n",
    "\n",
    "---\n",
    "\n",
    "### å®è·µè¦ç‚¹\n",
    "\n",
    "| æ­¥éª¤ | å…³é”®ç‚¹ | å¸¸è§é™·é˜± |\n",
    "|------|--------|----------|\n",
    "| **1. è¯†åˆ«æ··æ·†** | ç”»å› æœå›¾ï¼Œè¯†åˆ«æ—¶é—´ä¾èµ–æ€§æ··æ·† | é—æ¼é‡è¦åå˜é‡ |\n",
    "| **2. ä¼°è®¡å€¾å‘å¾—åˆ†** | æ¯ä¸ªæ—¶ç‚¹å»ºç«‹æ¨¡å‹ï¼ŒåŒ…å«å†å²ä¿¡æ¯ | æ¨¡å‹è¯¯è®¾ |\n",
    "| **3. è®¡ç®—æƒé‡** | ä½¿ç”¨ç¨³å®šåŒ–æƒé‡ï¼Œæ£€æŸ¥åˆ†å¸ƒ | å¿½ç•¥æç«¯æƒé‡ |\n",
    "| **4. æ‹Ÿåˆ MSM** | åŠ æƒå›å½’ï¼Œé€‰æ‹©åˆé€‚çš„å‡½æ•°å½¢å¼ | è¿‡åº¦å‚æ•°åŒ– |\n",
    "| **5. æ•æ„Ÿæ€§åˆ†æ** | æµ‹è¯•ä¸åŒæˆªæ–­é˜ˆå€¼ã€æ¨¡å‹è®¾å®š | åªæŠ¥å‘Šä¸€ä¸ªä¼°è®¡ |\n",
    "\n",
    "---\n",
    "\n",
    "### å…³é”®å…¬å¼é€ŸæŸ¥\n",
    "\n",
    "**ç¨³å®šåŒ– IPTW æƒé‡ï¼š**\n",
    "\n",
    "$$SW_i = \\prod_{t=0}^{T} \\frac{P(A_{it} = a_{it} \\mid \\bar{A}_{i,t-1})}{P(A_{it} = a_{it} \\mid \\bar{A}_{i,t-1}, \\bar{L}_{it})}$$\n",
    "\n",
    "**è¾¹é™…ç»“æ„æ¨¡å‹ï¼š**\n",
    "\n",
    "$$E[Y(\\bar{a})] = g(\\bar{a}; \\beta)$$\n",
    "\n",
    "**åºè´¯å¿½ç•¥å‡è®¾ï¼š**\n",
    "\n",
    "$$Y(\\bar{a}) \\perp A_t \\mid \\bar{A}_{t-1} = \\bar{a}_{t-1}, \\bar{L}_t \\quad \\forall t$$\n",
    "\n",
    "---\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "1. **åŒé‡é²æ£’ä¼°è®¡**ï¼šç»“åˆ IPW å’Œ outcome regression\n",
    "2. **åŠ¨æ€æ²»ç–—æ–¹æ¡ˆ (Dynamic Treatment Regimes)**ï¼šä¼˜åŒ–ä¸ªæ€§åŒ–å¹²é¢„ç­–ç•¥\n",
    "3. **ä¸­ä»‹åˆ†æ (Mediation Analysis)**ï¼šåˆ†è§£æ—¶é—´ä¸Šçš„å› æœè·¯å¾„\n",
    "4. **å·¥å…·å˜é‡ (IV) åœ¨é¢æ¿æ•°æ®ä¸­çš„åº”ç”¨**\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œä½ å®Œæˆäº†æ—¶å˜å¤„ç†æ•ˆåº”çš„å­¦ä¹ ï¼** ğŸ‰\n",
    "\n",
    "æ—¶å˜å› æœæ¨æ–­æ˜¯å› æœæ¨æ–­ä¸­æœ€å¤æ‚ä½†ä¹Ÿæœ€è´´è¿‘ç°å®çš„é¢†åŸŸã€‚ä½ ç°åœ¨æŒæ¡äº†å¤„ç†åŠ¨æ€å¹²é¢„é—®é¢˜çš„æ ¸å¿ƒå·¥å…·ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}