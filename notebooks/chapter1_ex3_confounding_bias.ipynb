{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ ç¬¬ä¸€ç«  ç»ƒä¹  3: æ··æ·†åå·® (Confounding Bias)\n",
    "\n",
    "---\n",
    "\n",
    "## è®©åå·®ã€Œç°å½¢ã€\n",
    "\n",
    "åœ¨å‰ä¸¤ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“äº†ï¼š\n",
    "- æ½œåœ¨ç»“æœæ¡†æ¶å‘Šè¯‰æˆ‘ä»¬ã€Œä»€ä¹ˆæ˜¯å› æœæ•ˆåº”ã€\n",
    "- å› æœå›¾å‘Šè¯‰æˆ‘ä»¬ã€Œä¸ºä»€ä¹ˆä¼šæœ‰åå·®ã€\n",
    "\n",
    "ä½†ä¸€ä¸ªå…³é”®é—®é¢˜è¿˜æ²¡å›ç­”ï¼š**åå·®åˆ°åº•æœ‰å¤šå¤§ï¼Ÿèƒ½é‡åŒ–å—ï¼Ÿ**\n",
    "\n",
    "### ä¸€ä¸ªè®©ä½ å´©æºƒçš„åœºæ™¯ ğŸ˜±\n",
    "\n",
    "ä½ æ˜¯ä¸€ååŒ»å­¦ç ”ç©¶è€…ï¼Œå‘ç°äº†ä¸€ä¸ªæƒŠäººçš„ç»“è®ºï¼š\n",
    "\n",
    "> \"å–çº¢é…’çš„äººæ¯”ä¸å–é…’çš„äººå¿ƒè„ç—…å‘ç—…ç‡ä½ 30%ï¼\"\n",
    "\n",
    "æ–°é—»æ ‡é¢˜å·²ç»æƒ³å¥½äº†ï¼š*ã€Šç§‘å­¦è¯æ˜ï¼šçº¢é…’æ˜¯å¿ƒè„çš„ä¿æŠ¤ç¥ã€‹*\n",
    "\n",
    "ä½†ç­‰ç­‰...ä½ çš„åŒäº‹é—®äº†å‡ ä¸ªé—®é¢˜ï¼š\n",
    "\n",
    "- å–çº¢é…’çš„äººæ˜¯ä¸æ˜¯æ”¶å…¥æ›´é«˜ï¼Ÿï¼ˆèƒ½ä¹°å¾—èµ·çº¢é…’ï¼‰\n",
    "- æ”¶å…¥é«˜çš„äººæ˜¯ä¸æ˜¯åŒ»ç–—æ¡ä»¶æ›´å¥½ï¼Ÿ\n",
    "- é‚£è¿™ 30% çš„å·®å¼‚æœ‰å¤šå°‘æ˜¯çº¢é…’çš„åŠŸåŠ³ï¼Œå¤šå°‘æ˜¯æ”¶å…¥çš„åŠŸåŠ³ï¼Ÿ\n",
    "\n",
    "è¿™å°±æ˜¯**æ··æ·†åå·®**çš„å¯æ€•ä¹‹å¤„â€”â€”å®ƒä¼šè®©æ— æ•ˆçš„æ²»ç–—çœ‹èµ·æ¥æœ‰æ•ˆï¼Œä¹Ÿä¼šè®©æœ‰æ•ˆçš„æ²»ç–—çœ‹èµ·æ¥æ— æ•ˆï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. ç†è§£æ··æ·†åå·®çš„æ•°å­¦å…¬å¼\n",
    "2. å­¦ä¼šé‡åŒ–åå·®çš„å¤§å°\n",
    "3. æ·±å…¥ç†è§£ Simpson's Paradox\n",
    "4. æŒæ¡æ•æ„Ÿæ€§åˆ†ææ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§® Part 1: æ··æ·†åå·®å…¬å¼ (Omitted Variable Bias)\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "å½“æˆ‘ä»¬é—æ¼äº†ä¸€ä¸ªæ··æ·†å˜é‡ Xï¼Œæœ´ç´ ä¼°è®¡å’ŒçœŸå®æ•ˆåº”ä¹‹é—´çš„åå·®ä¸ºï¼š\n",
    "\n",
    "$$\\text{Bias} = \\gamma \\times \\delta$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\gamma$ï¼šæ··æ·†å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n",
    "- $\\delta$ï¼šæ··æ·†å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n",
    "\n",
    "### ç›´è§‰ç†è§£\n",
    "\n",
    "åå·® = (X å¯¹ Y çš„å½±å“) Ã— (X ä¸ T çš„å…³è”)\n",
    "\n",
    "**ä¸¤ä¸ªä¹˜æ•°éƒ½ä¸ä¸ºé›¶æ—¶ï¼Œæ‰ä¼šæœ‰åå·®ï¼**\n",
    "\n",
    "| æƒ…å†µ | Î³ (Xâ†’Y) | Î´ (X-T) | åå·® | ä¾‹å­ |\n",
    "|-----|---------|---------|------|------|\n",
    "| æ— åå·® | 0 | ä»»æ„ | 0 | X ä¸å½±å“ Y |\n",
    "| æ— åå·® | ä»»æ„ | 0 | 0 | X ä¸ T æ— å…³ |\n",
    "| æ­£å‘åå·® | + | + | + | é«˜ä¼°æ•ˆåº” |\n",
    "| æ­£å‘åå·® | - | - | + | é«˜ä¼°æ•ˆåº” |\n",
    "| è´Ÿå‘åå·® | + | - | - | ä½ä¼°æ•ˆåº” |\n",
    "| è´Ÿå‘åå·® | - | + | - | ä½ä¼°æ•ˆåº” |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¬ åŠ¨æ‰‹éªŒè¯åå·®å…¬å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿæ•°æ®\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "# DAG: X â†’ T, X â†’ Y, T â†’ Y\n",
    "# X æ˜¯æ··æ·†å˜é‡\n",
    "\n",
    "# ç”Ÿæˆæ··æ·†å˜é‡ X\n",
    "X = np.random.randn(n)\n",
    "\n",
    "# ç”Ÿæˆå¤„ç† Tï¼ˆå— X å½±å“ï¼‰\n",
    "# delta = 1.5ï¼ˆX å¯¹ T çš„å½±å“ç³»æ•°ï¼‰\n",
    "T = (np.random.randn(n) + 1.5 * X > 0).astype(int)\n",
    "\n",
    "# ç”Ÿæˆç»“æœ Yï¼ˆå— T å’Œ X å½±å“ï¼‰\n",
    "# çœŸå® ATE = 2, gamma = 1.5\n",
    "true_ate = 2.0\n",
    "gamma_true = 1.5\n",
    "Y = 5 + true_ate * T + gamma_true * X + np.random.randn(n) * 0.5\n",
    "\n",
    "df = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "\n",
    "print(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "print(f\"   æ ·æœ¬é‡: {n}\")\n",
    "print(f\"   çœŸå® ATE: {true_ate}\")\n",
    "print(f\"   X å¯¹ Y çš„çœŸå®æ•ˆåº” (Î³): {gamma_true}\")\n",
    "print(f\"   å¤„ç†ç»„æ¯”ä¾‹: {T.mean():.2%}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confounding_bias(df: pd.DataFrame, confound_var: str = 'X') -> dict:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ··æ·†åå·®çš„å„ä¸ªç»„æˆéƒ¨åˆ†\n",
    "    \n",
    "    Omitted Variable Bias å…¬å¼: bias = Î³ Ã— Î´\n",
    "    \n",
    "    - Î³ (gamma): æ··æ·†å˜é‡ X å¯¹ç»“æœ Y çš„æ•ˆåº”ï¼ˆæ§åˆ¶ T åï¼‰\n",
    "    - Î´ (delta): æ··æ·†å˜é‡ X ä¸å¤„ç† T çš„å…³è”\n",
    "    \"\"\"\n",
    "    # æœ´ç´ ä¼°è®¡ï¼ˆä¸æ§åˆ¶ Xï¼‰\n",
    "    model_naive = LinearRegression()\n",
    "    model_naive.fit(df[['T']], df['Y'])\n",
    "    naive_estimate = model_naive.coef_[0]\n",
    "    \n",
    "    # è°ƒæ•´ä¼°è®¡ï¼ˆæ§åˆ¶ Xï¼‰\n",
    "    model_adjusted = LinearRegression()\n",
    "    model_adjusted.fit(df[['T', confound_var]], df['Y'])\n",
    "    adjusted_estimate = model_adjusted.coef_[0]\n",
    "    \n",
    "    # TODO: è®¡ç®— gammaï¼ˆX å¯¹ Y çš„æ•ˆåº”ï¼Œæ§åˆ¶ Tï¼‰\n",
    "    # æç¤º: è¿™æ˜¯ model_adjusted ä¸­ X çš„ç³»æ•°\n",
    "    gamma = None  # ğŸ‘ˆ ä½ çš„ä»£ç : model_adjusted.coef_[1]\n",
    "    \n",
    "    # TODO: è®¡ç®— deltaï¼ˆX ä¸ T çš„å…³è”ï¼‰\n",
    "    # å›å½’ T ~ Xï¼Œå– X çš„ç³»æ•°\n",
    "    model_delta = LinearRegression()\n",
    "    model_delta.fit(df[[confound_var]], df['T'])\n",
    "    delta = None  # ğŸ‘ˆ ä½ çš„ä»£ç : model_delta.coef_[0]\n",
    "    \n",
    "    # TODO: è®¡ç®—ç†è®ºåå·®\n",
    "    theoretical_bias = None  # ğŸ‘ˆ ä½ çš„ä»£ç : gamma * delta\n",
    "    \n",
    "    # å®é™…åå·®ï¼ˆä»ä¼°è®¡å€¼ç›´æ¥ç®—ï¼‰\n",
    "    actual_bias = naive_estimate - adjusted_estimate\n",
    "    \n",
    "    return {\n",
    "        'gamma': gamma,\n",
    "        'delta': delta,\n",
    "        'theoretical_bias': theoretical_bias,\n",
    "        'actual_bias': actual_bias,\n",
    "        'naive_estimate': naive_estimate,\n",
    "        'adjusted_estimate': adjusted_estimate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä½ çš„ä»£ç \n",
    "result = calculate_confounding_bias(df)\n",
    "\n",
    "if result['gamma'] is not None:\n",
    "    print(\"ğŸ”¬ æ··æ·†åå·®åˆ†è§£:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nğŸ“ åå·®å…¬å¼ç»„æˆéƒ¨åˆ†:\")\n",
    "    print(f\"   Î³ (X å¯¹ Y çš„æ•ˆåº”): {result['gamma']:.4f}\")\n",
    "    print(f\"   Î´ (X ä¸ T çš„å…³è”): {result['delta']:.4f}\")\n",
    "    print(f\"\\nğŸ“Š åå·®è®¡ç®—:\")\n",
    "    print(f\"   ç†è®ºåå·® (Î³ Ã— Î´): {result['theoretical_bias']:.4f}\")\n",
    "    print(f\"   å®é™…åå·®: {result['actual_bias']:.4f}\")\n",
    "    print(f\"\\nğŸ“ˆ æ•ˆåº”ä¼°è®¡:\")\n",
    "    print(f\"   æœ´ç´ ä¼°è®¡: {result['naive_estimate']:.4f}\")\n",
    "    print(f\"   è°ƒæ•´ä¼°è®¡: {result['adjusted_estimate']:.4f}\")\n",
    "    print(f\"   çœŸå®æ•ˆåº”: {true_ate:.4f}\")\n",
    "    \n",
    "    # éªŒè¯å…¬å¼\n",
    "    if abs(result['theoretical_bias'] - result['actual_bias']) < 0.1:\n",
    "        print(f\"\\nâœ… å…¬å¼éªŒè¯æˆåŠŸï¼ç†è®ºåå·® â‰ˆ å®é™…åå·®\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ calculate_confounding_bias å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ å…³é”®æ´å¯Ÿ\n",
    "\n",
    "ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹åˆ°ï¼š\n",
    "\n",
    "1. **åå·®çš„æ–¹å‘**ï¼šÎ³ å’Œ Î´ åŒå·æ—¶ï¼Œåå·®ä¸ºæ­£ï¼ˆé«˜ä¼°æ•ˆåº”ï¼‰\n",
    "2. **åå·®çš„å¤§å°**ï¼šå–å†³äº Î³ å’Œ Î´ çš„ä¹˜ç§¯\n",
    "3. **å…¬å¼çš„ç²¾ç¡®æ€§**ï¼šç†è®ºåå·®å’Œå®é™…åå·®å‡ ä¹å®Œå…¨ä¸€è‡´ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ Part 2: æ··æ·†å¼ºåº¦å®éªŒ\n",
    "\n",
    "è®©æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶ï¼šæ··æ·†å¼ºåº¦å¦‚ä½•å½±å“ä¼°è®¡åå·®ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_confounding_strength(\n",
    "    n: int = 2000,\n",
    "    true_ate: float = 2.0,\n",
    "    confounding_strengths: list = None,\n",
    "    seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å®éªŒä¸åŒæ··æ·†å¼ºåº¦å¯¹ä¼°è®¡çš„å½±å“\n",
    "    \n",
    "    æ··æ·†å¼ºåº¦åŒæ—¶å½±å“:\n",
    "    - P(T=1|X): å€¾å‘å¾—åˆ†\n",
    "    - Y ä¸­ X çš„ç³»æ•°\n",
    "    \"\"\"\n",
    "    if confounding_strengths is None:\n",
    "        confounding_strengths = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    results = []\n",
    "    \n",
    "    for strength in confounding_strengths:\n",
    "        # ç”Ÿæˆæ··æ·†æ•°æ®\n",
    "        X = np.random.randn(n)\n",
    "        \n",
    "        # TODO: ç”Ÿæˆå¤„ç† T\n",
    "        # P(T=1|X) = sigmoid(strength * X)\n",
    "        propensity = 1 / (1 + np.exp(-strength * X))\n",
    "        T = None  # ğŸ‘ˆ ä½ çš„ä»£ç : np.random.binomial(1, propensity)\n",
    "        \n",
    "        # TODO: ç”Ÿæˆç»“æœ Y\n",
    "        # Y = 5 + true_ate * T + strength * X + noise\n",
    "        Y = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "        \n",
    "        if T is not None and Y is not None:\n",
    "            df_temp = pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "            \n",
    "            # è®¡ç®—æœ´ç´ ä¼°è®¡\n",
    "            naive_est = df_temp[df_temp['T']==1]['Y'].mean() - df_temp[df_temp['T']==0]['Y'].mean()\n",
    "            \n",
    "            # è®¡ç®—è°ƒæ•´ä¼°è®¡\n",
    "            model = LinearRegression()\n",
    "            model.fit(df_temp[['T', 'X']], df_temp['Y'])\n",
    "            adjusted_est = model.coef_[0]\n",
    "            \n",
    "            results.append({\n",
    "                'confounding_strength': strength,\n",
    "                'naive_estimate': naive_est,\n",
    "                'adjusted_estimate': adjusted_est,\n",
    "                'naive_bias': naive_est - true_ate,\n",
    "                'adjusted_bias': adjusted_est - true_ate\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results) if results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œå®éªŒ\n",
    "exp_results = experiment_confounding_strength(n=3000)\n",
    "\n",
    "if exp_results is not None and not exp_results.empty:\n",
    "    print(\"ğŸ“Š æ··æ·†å¼ºåº¦å®éªŒç»“æœ:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(exp_results.round(3).to_string(index=False))\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å›¾1: ä¼°è®¡å€¼ vs æ··æ·†å¼ºåº¦\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(exp_results['confounding_strength'], exp_results['naive_estimate'], \n",
    "             'o-', label='æœ´ç´ ä¼°è®¡', color='red', markersize=8)\n",
    "    ax1.plot(exp_results['confounding_strength'], exp_results['adjusted_estimate'], \n",
    "             's-', label='è°ƒæ•´ä¼°è®¡', color='green', markersize=8)\n",
    "    ax1.axhline(2.0, color='blue', linestyle='--', label='çœŸå® ATE = 2', linewidth=2)\n",
    "    ax1.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax1.set_ylabel('ATE ä¼°è®¡', fontsize=12)\n",
    "    ax1.set_title('ä¼°è®¡å€¼éšæ··æ·†å¼ºåº¦çš„å˜åŒ–', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å›¾2: åå·® vs æ··æ·†å¼ºåº¦\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(exp_results['confounding_strength'] - 0.1, exp_results['naive_bias'], \n",
    "            width=0.2, label='æœ´ç´ ä¼°è®¡åå·®', color='red', alpha=0.7)\n",
    "    ax2.bar(exp_results['confounding_strength'] + 0.1, exp_results['adjusted_bias'], \n",
    "            width=0.2, label='è°ƒæ•´ä¼°è®¡åå·®', color='green', alpha=0.7)\n",
    "    ax2.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "    ax2.set_xlabel('æ··æ·†å¼ºåº¦', fontsize=12)\n",
    "    ax2.set_ylabel('åå·®', fontsize=12)\n",
    "    ax2.set_title('åå·®éšæ··æ·†å¼ºåº¦çš„å˜åŒ–', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "    print(\"   1. æ··æ·†å¼ºåº¦ä¸º 0 æ—¶ï¼Œæœ´ç´ ä¼°è®¡ä¹Ÿæ˜¯æ— åçš„\")\n",
    "    print(\"   2. æ··æ·†å¼ºåº¦è¶Šå¤§ï¼Œæœ´ç´ ä¼°è®¡åå·®è¶Šå¤§\")\n",
    "    print(\"   3. è°ƒæ•´ä¼°è®¡å§‹ç»ˆæ¥è¿‘çœŸå®å€¼ï¼\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ experiment_confounding_strength å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸª Part 3: Simpson's Paradoxï¼ˆè¾›æ™®æ£®æ‚–è®ºï¼‰\n",
    "\n",
    "### ä¸€ä¸ªçœŸå®çš„æ•…äº‹\n",
    "\n",
    "1973 å¹´ï¼Œä¼¯å…‹åˆ©å¤§å­¦è¢«æŒ‡æ§æ€§åˆ«æ­§è§†ï¼š\n",
    "\n",
    "| | ç”³è¯·äºº | å½•å–ç‡ |\n",
    "|--|--------|--------|\n",
    "| ç”·æ€§ | 8442 | 44% |\n",
    "| å¥³æ€§ | 4321 | 35% |\n",
    "\n",
    "çœ‹èµ·æ¥ç¡®å®æ­§è§†å¥³æ€§ï¼\n",
    "\n",
    "ä½†ä»”ç»†åˆ†ææ¯ä¸ªç³»çš„æ•°æ®åï¼Œå‘ç°**å¤§å¤šæ•°ç³»çš„å¥³æ€§å½•å–ç‡åè€Œæ›´é«˜**ï¼\n",
    "\n",
    "æ€ä¹ˆå›äº‹ï¼ŸğŸ¤¯\n",
    "\n",
    "åŸæ¥ï¼š\n",
    "- å¥³æ€§å€¾å‘äºç”³è¯·å½•å–ç‡ä½çš„ã€Œçƒ­é—¨ç³»ã€ï¼ˆå¦‚å¿ƒç†å­¦ï¼‰\n",
    "- ç”·æ€§å€¾å‘äºç”³è¯·å½•å–ç‡é«˜çš„ã€Œå†·é—¨ç³»ã€ï¼ˆå¦‚å·¥ç¨‹å­¦ï¼‰\n",
    "\n",
    "ã€Œé™¢ç³»ã€æ˜¯æ··æ·†å˜é‡ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simpson_paradox_data(n_per_group: int = 500, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºå±•ç¤º Simpson's Paradox çš„æ•°æ®\n",
    "    \n",
    "    åœºæ™¯: ç ”ç©¶æŸè¯ç‰©å¯¹åº·å¤ç‡çš„å½±å“\n",
    "    - æœ‰ä¸¤ä¸ªåŒ»é™¢ (A å’Œ B)\n",
    "    - åŒ»é™¢ A æ¥æ”¶é‡ç—‡æ‚£è€…å¤šï¼Œè¯ç‰©ä½¿ç”¨ç‡é«˜\n",
    "    - åŒ»é™¢ B æ¥æ”¶è½»ç—‡æ‚£è€…å¤šï¼Œè¯ç‰©ä½¿ç”¨ç‡ä½\n",
    "    - è¯ç‰©å®é™…ä¸Šæœ‰æ­£æ•ˆåº”ï¼\n",
    "    \n",
    "    è®¾è®¡æ•°æ®ä½¿å¾—:\n",
    "    - æ•´ä½“: ç”¨è¯ç»„åº·å¤ç‡ < æœªç”¨è¯ç»„åº·å¤ç‡ï¼ˆçœ‹èµ·æ¥è¯ç‰©æœ‰å®³!ï¼‰\n",
    "    - åˆ†åŒ»é™¢: ç”¨è¯ç»„åº·å¤ç‡ > æœªç”¨è¯ç»„åº·å¤ç‡ï¼ˆè¯ç‰©å®é™…æœ‰ç›Šï¼‰\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data = []\n",
    "    \n",
    "    # TODO: åŒ»é™¢ Aï¼ˆé‡ç—‡å¤šï¼Œç”¨è¯å¤šï¼‰\n",
    "    # é‡ç—‡åŸºç¡€åº·å¤ç‡ä½ï¼ˆ30%ï¼‰ï¼Œç”¨è¯æé«˜åˆ° 50%\n",
    "    # å¤§éƒ¨åˆ†é‡ç—‡æ‚£è€…åœ¨è¿™é‡Œï¼Œå¤§éƒ¨åˆ†æ¥å—æ²»ç–—\n",
    "    \n",
    "    # åŒ»é™¢ A - ç”¨è¯ç»„\n",
    "    n_A_treated = int(n_per_group * 1.5)  # ç”¨è¯äººæ•°å¤š\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : ç”¨è¯ååº·å¤ç‡ 50%\n",
    "    recovery_A_treated = None  # np.random.binomial(1, 0.50, n_A_treated)\n",
    "    \n",
    "    for i in range(n_A_treated):\n",
    "        if recovery_A_treated is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'A (é‡ç—‡)',\n",
    "                'ç”¨è¯': 1,\n",
    "                'åº·å¤': recovery_A_treated[i],\n",
    "                'ç—…æƒ…': 'é‡ç—‡'\n",
    "            })\n",
    "    \n",
    "    # åŒ»é™¢ A - æœªç”¨è¯ç»„\n",
    "    n_A_control = int(n_per_group * 0.3)  # æœªç”¨è¯äººæ•°å°‘\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : æœªç”¨è¯åº·å¤ç‡ 30%\n",
    "    recovery_A_control = None  # np.random.binomial(1, 0.30, n_A_control)\n",
    "    \n",
    "    for i in range(n_A_control):\n",
    "        if recovery_A_control is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'A (é‡ç—‡)',\n",
    "                'ç”¨è¯': 0,\n",
    "                'åº·å¤': recovery_A_control[i],\n",
    "                'ç—…æƒ…': 'é‡ç—‡'\n",
    "            })\n",
    "    \n",
    "    # TODO: åŒ»é™¢ Bï¼ˆè½»ç—‡å¤šï¼Œç”¨è¯å°‘ï¼‰\n",
    "    # è½»ç—‡åŸºç¡€åº·å¤ç‡é«˜ï¼ˆ70%ï¼‰ï¼Œç”¨è¯æé«˜åˆ° 90%\n",
    "    \n",
    "    # åŒ»é™¢ B - ç”¨è¯ç»„\n",
    "    n_B_treated = int(n_per_group * 0.3)  # ç”¨è¯äººæ•°å°‘\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : ç”¨è¯ååº·å¤ç‡ 90%\n",
    "    recovery_B_treated = None  # np.random.binomial(1, 0.90, n_B_treated)\n",
    "    \n",
    "    for i in range(n_B_treated):\n",
    "        if recovery_B_treated is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'B (è½»ç—‡)',\n",
    "                'ç”¨è¯': 1,\n",
    "                'åº·å¤': recovery_B_treated[i],\n",
    "                'ç—…æƒ…': 'è½»ç—‡'\n",
    "            })\n",
    "    \n",
    "    # åŒ»é™¢ B - æœªç”¨è¯ç»„\n",
    "    n_B_control = int(n_per_group * 1.5)  # æœªç”¨è¯äººæ•°å¤š\n",
    "    # ğŸ‘ˆ ä½ çš„ä»£ç : æœªç”¨è¯åº·å¤ç‡ 70%\n",
    "    recovery_B_control = None  # np.random.binomial(1, 0.70, n_B_control)\n",
    "    \n",
    "    for i in range(n_B_control):\n",
    "        if recovery_B_control is not None:\n",
    "            data.append({\n",
    "                'åŒ»é™¢': 'B (è½»ç—‡)',\n",
    "                'ç”¨è¯': 0,\n",
    "                'åº·å¤': recovery_B_control[i],\n",
    "                'ç—…æƒ…': 'è½»ç—‡'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_simpson_paradox(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    åˆ†æ Simpson's Paradox\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: æ•´ä½“æ•ˆåº”\n",
    "    overall_treated = df[df['ç”¨è¯'] == 1]['åº·å¤'].mean()\n",
    "    overall_control = df[df['ç”¨è¯'] == 0]['åº·å¤'].mean()\n",
    "    results['æ•´ä½“-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['æ•´ä½“-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['æ•´ä½“-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç : overall_treated - overall_control\n",
    "    \n",
    "    # TODO: åŒ»é™¢ A æ•ˆåº”\n",
    "    df_A = df[df['åŒ»é™¢'] == 'A (é‡ç—‡)']\n",
    "    results['åŒ»é™¢A-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢A-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢A-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    # TODO: åŒ»é™¢ B æ•ˆåº”\n",
    "    df_B = df[df['åŒ»é™¢'] == 'B (è½»ç—‡)']\n",
    "    results['åŒ»é™¢B-ç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢B-æœªç”¨è¯ç»„åº·å¤ç‡'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    results['åŒ»é™¢B-æ•ˆåº”'] = None  # ğŸ‘ˆ ä½ çš„ä»£ç \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¹¶åˆ†æ Simpson's Paradox æ•°æ®\n",
    "simpson_df = create_simpson_paradox_data(n_per_group=500)\n",
    "\n",
    "if simpson_df is not None and not simpson_df.empty:\n",
    "    print(\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"   æ€»æ ·æœ¬é‡: {len(simpson_df)}\")\n",
    "    print(f\"\\nå„ç»„äººæ•°:\")\n",
    "    print(simpson_df.groupby(['åŒ»é™¢', 'ç”¨è¯']).size().unstack())\n",
    "    \n",
    "    analysis = analyze_simpson_paradox(simpson_df)\n",
    "    \n",
    "    if analysis.get('æ•´ä½“-æ•ˆåº”') is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸª Simpson's Paradox åˆ†æç»“æœ:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ æ•´ä½“åˆ†æï¼ˆä¸åˆ†åŒ»é™¢ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['æ•´ä½“-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['æ•´ä½“-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['æ•´ä½“-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['æ•´ä½“-æ•ˆåº”'] < 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯ä¼¼ä¹æœ‰å®³ï¼âŒ\")\n",
    "        \n",
    "        print(f\"\\nğŸ¥ åŒ»é™¢ Aï¼ˆé‡ç—‡æ‚£è€…ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢A-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢A-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['åŒ»é™¢A-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['åŒ»é™¢A-æ•ˆåº”'] > 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯æœ‰æ•ˆï¼âœ…\")\n",
    "        \n",
    "        print(f\"\\nğŸ¥ åŒ»é™¢ Bï¼ˆè½»ç—‡æ‚£è€…ï¼‰:\")\n",
    "        print(f\"   ç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢B-ç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æœªç”¨è¯ç»„åº·å¤ç‡: {analysis['åŒ»é™¢B-æœªç”¨è¯ç»„åº·å¤ç‡']:.1%}\")\n",
    "        print(f\"   æ•ˆåº”: {analysis['åŒ»é™¢B-æ•ˆåº”']:+.1%}\")\n",
    "        if analysis['åŒ»é™¢B-æ•ˆåº”'] > 0:\n",
    "            print(f\"   ç»“è®º: ç”¨è¯æœ‰æ•ˆï¼âœ…\")\n",
    "        \n",
    "        print(f\"\\nğŸ­ æ‚–è®ºè§£é‡Š:\")\n",
    "        print(f\"   æ•´ä½“çœ‹èµ·æ¥ç”¨è¯æœ‰å®³ï¼Œä½†åˆ†å±‚åæ¯ä¸ªåŒ»é™¢ç”¨è¯éƒ½æœ‰æ•ˆï¼\")\n",
    "        print(f\"   åŸå› : é‡ç—‡æ‚£è€…ç”¨è¯å¤šï¼Œè½»ç—‡æ‚£è€…ä¸ç”¨è¯å¤š\")\n",
    "        print(f\"   'ç—…æƒ…ä¸¥é‡ç¨‹åº¦'æ˜¯æ··æ·†å˜é‡ï¼\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å®Œæˆ create_simpson_paradox_data å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– Simpson's Paradox\n",
    "if simpson_df is not None and not simpson_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å›¾1: æ•´ä½“å¯¹æ¯”\n",
    "    ax1 = axes[0]\n",
    "    overall_data = simpson_df.groupby('ç”¨è¯')['åº·å¤'].mean()\n",
    "    colors = ['steelblue', 'coral']\n",
    "    bars = ax1.bar(['æœªç”¨è¯', 'ç”¨è¯'], [overall_data[0], overall_data[1]], color=colors)\n",
    "    ax1.set_ylabel('åº·å¤ç‡', fontsize=12)\n",
    "    ax1.set_title('æ•´ä½“åº·å¤ç‡\\n(çœ‹èµ·æ¥ç”¨è¯æœ‰å®³!)', fontsize=14)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    for bar, val in zip(bars, [overall_data[0], overall_data[1]]):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{val:.1%}', ha='center', fontsize=12)\n",
    "    \n",
    "    # å›¾2: åˆ†å±‚å¯¹æ¯”\n",
    "    ax2 = axes[1]\n",
    "    stratified_data = simpson_df.groupby(['åŒ»é™¢', 'ç”¨è¯'])['åº·å¤'].mean().unstack()\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    bars1 = ax2.bar(x - width/2, stratified_data[0], width, label='æœªç”¨è¯', color='steelblue')\n",
    "    bars2 = ax2.bar(x + width/2, stratified_data[1], width, label='ç”¨è¯', color='coral')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(['åŒ»é™¢A (é‡ç—‡)', 'åŒ»é™¢B (è½»ç—‡)'])\n",
    "    ax2.set_ylabel('åº·å¤ç‡', fontsize=12)\n",
    "    ax2.set_title('åˆ†å±‚åº·å¤ç‡\\n(ä¸¤ä¸ªåŒ»é™¢ç”¨è¯éƒ½æœ‰æ•ˆ!)', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    for bars, col in [(bars1, 0), (bars2, 1)]:\n",
    "        for bar, hospital in zip(bars, stratified_data.index):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{stratified_data.loc[hospital, col]:.1%}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Simpson's Paradox çš„æœ¬è´¨\n",
    "\n",
    "Simpson's Paradox ä¸æ˜¯çœŸæ­£çš„ã€Œæ‚–è®ºã€ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ï¼š\n",
    "\n",
    "1. **æ•´ä½“è¶‹åŠ¿ â‰  åˆ†å±‚è¶‹åŠ¿**: å½“å­˜åœ¨æ··æ·†å˜é‡æ—¶ï¼Œæ•´ä½“æ•°æ®å¯èƒ½ç»™å‡ºè¯¯å¯¼æ€§çš„ç»“è®º\n",
    "2. **å› æœæ–¹å‘å¾ˆé‡è¦**: åº”è¯¥æ§åˆ¶æ··æ·†å˜é‡ï¼ˆç—…æƒ…ï¼‰ï¼Œè€Œä¸æ˜¯è¢«å®ƒè¯¯å¯¼\n",
    "3. **æ•°æ®ä¼šè¯´è°**: æ²¡æœ‰æ­£ç¡®çš„å› æœåˆ†æï¼Œæ•°æ®å¯èƒ½å‘Šè¯‰ä½ å®Œå…¨ç›¸åçš„ç»“è®ºï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”® Part 4: æ•æ„Ÿæ€§åˆ†æ\n",
    "\n",
    "### å½“æ··æ·†å˜é‡ä¸å¯è§‚æµ‹æ—¶æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "åœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸æ— æ³•è§‚æµ‹åˆ°æ‰€æœ‰æ··æ·†å˜é‡ã€‚æ¯”å¦‚ï¼š\n",
    "\n",
    "- ç ”ç©¶å¸çƒŸå¯¹è‚ºç™Œçš„å½±å“ï¼Œä½†æ— æ³•è§‚æµ‹ã€Œé—ä¼ å› ç´ ã€\n",
    "- ç ”ç©¶æ•™è‚²å¯¹æ”¶å…¥çš„å½±å“ï¼Œä½†æ— æ³•è§‚æµ‹ã€Œå¤©èµ‹ã€\n",
    "\n",
    "è¿™æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦é—®ï¼š**å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·†ï¼Œæˆ‘ä»¬çš„ç»“è®ºè¿˜å¯é å—ï¼Ÿ**\n",
    "\n",
    "æ•æ„Ÿæ€§åˆ†æå°±æ˜¯ç”¨æ¥å›ç­”è¿™ä¸ªé—®é¢˜çš„ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_to_unmeasured_confounding(\n",
    "    df: pd.DataFrame,\n",
    "    gamma_range: np.ndarray = None,\n",
    "    delta_range: np.ndarray = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æ•æ„Ÿæ€§åˆ†æ: å¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·†ï¼Œä¼°è®¡ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ\n",
    "    \n",
    "    å‡è®¾å­˜åœ¨æœªè§‚æµ‹æ··æ·†å˜é‡ U:\n",
    "    - U å¯¹ Y çš„æ•ˆåº”ä¸º gamma_u\n",
    "    - U ä¸ T çš„å…³è”ä¸º delta_u\n",
    "    - åå·® = gamma_u * delta_u\n",
    "    \n",
    "    å¯¹ä¸åŒçš„ (gamma_u, delta_u) ç»„åˆï¼Œè®¡ç®—å¯èƒ½çš„çœŸå®æ•ˆåº”\n",
    "    \"\"\"\n",
    "    if gamma_range is None:\n",
    "        gamma_range = np.linspace(-2, 2, 9)\n",
    "    if delta_range is None:\n",
    "        delta_range = np.linspace(-1, 1, 9)\n",
    "    \n",
    "    # å½“å‰ä¼°è®¡ï¼ˆå‡è®¾å·²æ§åˆ¶è§‚æµ‹åˆ°çš„æ··æ·†ï¼‰\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[['T', 'X']], df['Y'])\n",
    "    current_estimate = model.coef_[0]\n",
    "    \n",
    "    results = []\n",
    "    for gamma_u in gamma_range:\n",
    "        for delta_u in delta_range:\n",
    "            # å¯èƒ½çš„åå·®ï¼ˆå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† Uï¼‰\n",
    "            possible_bias = gamma_u * delta_u\n",
    "            # å¯èƒ½çš„çœŸå®æ•ˆåº”\n",
    "            possible_true_effect = current_estimate - possible_bias\n",
    "            \n",
    "            results.append({\n",
    "                'gamma_u': gamma_u,\n",
    "                'delta_u': delta_u,\n",
    "                'possible_bias': possible_bias,\n",
    "                'possible_true_effect': possible_true_effect\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ•æ„Ÿæ€§åˆ†æ\n",
    "sensitivity = sensitivity_to_unmeasured_confounding(df)\n",
    "\n",
    "print(\"ğŸ”® æ•æ„Ÿæ€§åˆ†æç»“æœ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å½“å‰ä¼°è®¡\n",
    "model = LinearRegression()\n",
    "model.fit(df[['T', 'X']], df['Y'])\n",
    "current_est = model.coef_[0]\n",
    "\n",
    "print(f\"å½“å‰ä¼°è®¡ï¼ˆæ§åˆ¶ X åï¼‰: {current_est:.4f}\")\n",
    "print(f\"\\nå¦‚æœå­˜åœ¨æœªè§‚æµ‹æ··æ·† U:\")\n",
    "print(f\"   å¯èƒ½çš„çœŸå®æ•ˆåº”èŒƒå›´: [{sensitivity['possible_true_effect'].min():.2f}, {sensitivity['possible_true_effect'].max():.2f}]\")\n",
    "\n",
    "# æ‰¾å‡ºä½¿æ•ˆåº”å˜ä¸º 0 æˆ–è´Ÿæ•°çš„æ¡ä»¶\n",
    "zero_effect = sensitivity[sensitivity['possible_true_effect'] < 0]\n",
    "if len(zero_effect) > 0:\n",
    "    print(f\"\\nâš ï¸ ä»¥ä¸‹æƒ…å†µä¼šä½¿æ•ˆåº”å˜ä¸ºè´Ÿæ•°:\")\n",
    "    print(f\"   éœ€è¦ Î³_u Ã— Î´_u > {current_est:.2f}\")\n",
    "    print(f\"   ä¾‹å¦‚: Î³_u = 2, Î´_u = 1.5 (å¼ºæ··æ·†)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ•æ„Ÿæ€§åˆ†æ\n",
    "pivot = sensitivity.pivot_table(\n",
    "    index='gamma_u', \n",
    "    columns='delta_u', \n",
    "    values='possible_true_effect'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    pivot, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap='RdYlGn',\n",
    "    center=0,\n",
    "    vmin=-2,\n",
    "    vmax=4\n",
    ")\n",
    "plt.xlabel('Î´_u (U ä¸ T çš„å…³è”)', fontsize=12)\n",
    "plt.ylabel('Î³_u (U å¯¹ Y çš„æ•ˆåº”)', fontsize=12)\n",
    "plt.title('æ•æ„Ÿæ€§åˆ†æ: å¯èƒ½çš„çœŸå®æ•ˆåº”\\n(çº¢è‰² = è´Ÿæ•ˆåº”, ç»¿è‰² = æ­£æ•ˆåº”)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š çƒ­åŠ›å›¾è§£è¯»:\")\n",
    "print(\"   - ä¸­å¿ƒ (0, 0): æ— æœªè§‚æµ‹æ··æ·†ï¼Œæ•ˆåº” = å½“å‰ä¼°è®¡\")\n",
    "print(\"   - ç»¿è‰²åŒºåŸŸ: å³ä½¿æœ‰æœªè§‚æµ‹æ··æ·†ï¼Œæ•ˆåº”ä»ä¸ºæ­£\")\n",
    "print(\"   - çº¢è‰²åŒºåŸŸ: æœªè§‚æµ‹æ··æ·†å¯èƒ½ä½¿æ•ˆåº”ä¸ºè´Ÿ\")\n",
    "print(\"   - æ•ˆåº”ä¸º 0 çš„è¾¹ç•Œ: Î³_u Ã— Î´_u = å½“å‰ä¼°è®¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Part 5: æ€è€ƒé¢˜\n",
    "\n",
    "### é—®é¢˜ 1: ä¸ºä»€ä¹ˆ Simpson's Paradox ä¸æ˜¯çœŸæ­£çš„æ‚–è®ºï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 2: åå·®å…¬å¼ Bias = Î³ Ã— Î´ ä¸­ï¼Œä»€ä¹ˆæƒ…å†µä¸‹åå·®ä¸º 0ï¼Ÿ\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 3: åœ¨å®è·µä¸­ï¼Œå¦‚ä½•åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ··æ·†ï¼Ÿ\n",
    "\n",
    "**æç¤º:** æƒ³æƒ³é¢†åŸŸçŸ¥è¯†ã€æ•æ„Ÿæ€§åˆ†æã€é˜´æ€§å¯¹ç…§...\n",
    "\n",
    "**ä½ çš„ç­”æ¡ˆ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ€è€ƒ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é—®é¢˜ 4: å®é™…æ¡ˆä¾‹åˆ†æ\n",
    "\n",
    "**åœºæ™¯**: è¯„ä¼°ä¿ƒé”€é‚®ä»¶å¯¹è´­ä¹°çš„å½±å“\n",
    "\n",
    "- T: æ˜¯å¦æ”¶åˆ°ä¿ƒé”€é‚®ä»¶\n",
    "- Y: æ˜¯å¦è´­ä¹°\n",
    "\n",
    "è¯·åˆ†æ:\n",
    "1. åˆ—å‡ºå¯èƒ½çš„æ··æ·†å˜é‡\n",
    "2. æ¯ä¸ªæ··æ·†å˜é‡å¦‚ä½•åŒæ—¶å½±å“ T å’Œ Yï¼Ÿ\n",
    "3. æœ´ç´ ä¼°è®¡ä¼šé«˜ä¼°è¿˜æ˜¯ä½ä¼°çœŸå®æ•ˆåº”ï¼Ÿ\n",
    "4. å¦‚ä½•æ§åˆ¶è¿™äº›æ··æ·†ï¼Ÿ\n",
    "\n",
    "**ä½ çš„åˆ†æ:**\n",
    "\n",
    "*ï¼ˆåœ¨è¿™é‡Œå†™ä¸‹ä½ çš„åˆ†æ...ï¼‰*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "$$\\text{åå·®} = \\gamma \\times \\delta = \\text{(Xå¯¹Yçš„æ•ˆåº”)} \\times \\text{(Xä¸Tçš„å…³è”)}$$\n",
    "\n",
    "### å…³é”®çŸ¥è¯†ç‚¹\n",
    "\n",
    "| æ¦‚å¿µ | å®šä¹‰ | å¯ç¤º |\n",
    "|-----|------|------|\n",
    "| æ··æ·†åå·® | é—æ¼æ··æ·†å˜é‡å¯¼è‡´çš„ä¼°è®¡åå·® | å¯ä»¥ç²¾ç¡®è®¡ç®—ï¼ |\n",
    "| Simpson's Paradox | æ•´ä½“è¶‹åŠ¿ä¸åˆ†å±‚è¶‹åŠ¿ç›¸å | ä¸æ˜¯æ‚–è®ºï¼Œæ˜¯æ··æ·† |\n",
    "| æ•æ„Ÿæ€§åˆ†æ | è¯„ä¼°æœªè§‚æµ‹æ··æ·†çš„å½±å“ | å³ä½¿æœ‰é—æ¼ï¼Œä¹Ÿèƒ½è¯„ä¼° |\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "1. **ç”»å› æœå›¾**: åœ¨åˆ†æå‰å…ˆç”»å‡ºå‡è®¾çš„å› æœç»“æ„\n",
    "2. **è¯†åˆ«æ··æ·†**: æ‰¾å‡ºåŒæ—¶å½±å“å¤„ç†å’Œç»“æœçš„å˜é‡\n",
    "3. **åšæ•æ„Ÿæ€§åˆ†æ**: è¯„ä¼°ç»“è®ºå¯¹æœªè§‚æµ‹æ··æ·†çš„ç¨³å¥æ€§\n",
    "4. **è°¨æ…ä¸‹ç»“è®º**: é™¤éæœ‰å¼ºæœ‰åŠ›çš„è¯æ®ï¼Œå¦åˆ™ä¿æŒè°¨æ…\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬æ·±åˆ»ç†è§£äº†æ··æ·†åå·®ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ å…·ä½“çš„**å¤„ç†æ–¹æ³•**â€”â€”å€¾å‘å¾—åˆ†åŒ¹é… (PSM)ï¼\n",
    "\n",
    "---\n",
    "\n",
    "**ã€ŒçŸ¥é“åå·®æœ‰å¤šå¤§ï¼Œæ¯”ä¸çŸ¥é“åå·®å­˜åœ¨è¦å¥½å¾—å¤šã€‚ã€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
